CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007194094541441001
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.731723105083079e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.383110 	Lval: 0.394484
Epoch: 5 	Ltrain: 0.020165 	Lval: 0.021456
Epoch: 10 	Ltrain: 0.016945 	Lval: 0.015724
Epoch: 15 	Ltrain: 0.011771 	Lval: 0.011748
Epoch: 20 	Ltrain: 0.011085 	Lval: 0.016107
Epoch 00022: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 25 	Ltrain: 0.009832 	Lval: 0.009576
Epoch: 30 	Ltrain: 0.009487 	Lval: 0.008819
Epoch: 35 	Ltrain: 0.008550 	Lval: 0.008599
Epoch: 40 	Ltrain: 0.008475 	Lval: 0.008489
Epoch 00042: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 45 	Ltrain: 0.008826 	Lval: 0.008360
Epoch: 50 	Ltrain: 0.008636 	Lval: 0.008292
Epoch: 55 	Ltrain: 0.008430 	Lval: 0.008254
Epoch 00059: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 60 	Ltrain: 0.008048 	Lval: 0.008294
Epoch: 65 	Ltrain: 0.008130 	Lval: 0.008296
Epoch: 70 	Ltrain: 0.010306 	Lval: 0.008287
Epoch 00071: reducing learning rate of group 0 to 7.1941e-07.
Epoch: 75 	Ltrain: 0.010561 	Lval: 0.008286
EarlyStopper: stopping at epoch 76 with best_val_loss = 0.008254


	Fold 2/5
Epoch: 1 	Ltrain: 0.128163 	Lval: 0.037878
Epoch: 5 	Ltrain: 0.017865 	Lval: 0.017233
Epoch: 10 	Ltrain: 0.009287 	Lval: 0.007921
Epoch: 15 	Ltrain: 0.007386 	Lval: 0.007024
Epoch: 20 	Ltrain: 0.007512 	Lval: 0.008142
Epoch 00022: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 25 	Ltrain: 0.006458 	Lval: 0.006584
Epoch: 30 	Ltrain: 0.006077 	Lval: 0.006530
Epoch: 35 	Ltrain: 0.006367 	Lval: 0.006491
Epoch 00039: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 40 	Ltrain: 0.006232 	Lval: 0.006510
Epoch: 45 	Ltrain: 0.006162 	Lval: 0.006512
Epoch: 50 	Ltrain: 0.006687 	Lval: 0.006538
Epoch 00051: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 55 	Ltrain: 0.006861 	Lval: 0.006517
EarlyStopper: stopping at epoch 56 with best_val_loss = 0.006491


	Fold 3/5
Epoch: 1 	Ltrain: 0.162191 	Lval: 0.044450
Epoch: 5 	Ltrain: 0.010089 	Lval: 0.010048
Epoch: 10 	Ltrain: 0.007310 	Lval: 0.007239
Epoch: 15 	Ltrain: 0.006469 	Lval: 0.006980
Epoch: 20 	Ltrain: 0.006316 	Lval: 0.007145
Epoch: 25 	Ltrain: 0.006253 	Lval: 0.006634
Epoch: 30 	Ltrain: 0.005962 	Lval: 0.006491
Epoch 00031: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 35 	Ltrain: 0.005485 	Lval: 0.006089
Epoch: 40 	Ltrain: 0.005696 	Lval: 0.006076
Epoch: 45 	Ltrain: 0.005339 	Lval: 0.006027
Epoch: 50 	Ltrain: 0.005367 	Lval: 0.005956
Epoch: 55 	Ltrain: 0.005268 	Lval: 0.005913
Epoch: 60 	Ltrain: 0.005383 	Lval: 0.005919
Epoch 00063: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 65 	Ltrain: 0.005227 	Lval: 0.005859
Epoch: 70 	Ltrain: 0.005178 	Lval: 0.005845
Epoch: 75 	Ltrain: 0.005294 	Lval: 0.005831
Epoch 00079: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 80 	Ltrain: 0.005210 	Lval: 0.005833
Epoch: 85 	Ltrain: 0.005269 	Lval: 0.005831
Epoch: 90 	Ltrain: 0.005258 	Lval: 0.005830
Epoch 00091: reducing learning rate of group 0 to 7.1941e-07.
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.005838


	Fold 4/5
Epoch: 1 	Ltrain: 0.063288 	Lval: 0.021483
Epoch: 5 	Ltrain: 0.006718 	Lval: 0.007627
Epoch: 10 	Ltrain: 0.005860 	Lval: 0.009379
Epoch 00012: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 15 	Ltrain: 0.005460 	Lval: 0.006617
Epoch: 20 	Ltrain: 0.005313 	Lval: 0.006597
Epoch: 25 	Ltrain: 0.005366 	Lval: 0.006528
Epoch: 30 	Ltrain: 0.005261 	Lval: 0.006497
Epoch 00034: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 35 	Ltrain: 0.005255 	Lval: 0.006471
Epoch: 40 	Ltrain: 0.005174 	Lval: 0.006484
Epoch: 45 	Ltrain: 0.005252 	Lval: 0.006485
Epoch 00046: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 50 	Ltrain: 0.005109 	Lval: 0.006452
Epoch: 55 	Ltrain: 0.005169 	Lval: 0.006454
Epoch 00058: reducing learning rate of group 0 to 7.1941e-07.
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.006453


	Fold 5/5
Epoch: 1 	Ltrain: 0.089451 	Lval: 0.030393
Epoch: 5 	Ltrain: 0.007358 	Lval: 0.009316
Epoch: 10 	Ltrain: 0.006203 	Lval: 0.009685
Epoch: 15 	Ltrain: 0.005563 	Lval: 0.007990
Epoch 00018: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 20 	Ltrain: 0.005220 	Lval: 0.006724
Epoch: 25 	Ltrain: 0.005169 	Lval: 0.006586
Epoch 00030: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 30 	Ltrain: 0.005175 	Lval: 0.006676
Epoch: 35 	Ltrain: 0.005073 	Lval: 0.006569
Epoch: 40 	Ltrain: 0.005046 	Lval: 0.006553
Epoch 00042: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 45 	Ltrain: 0.005056 	Lval: 0.006542
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.006510

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00012585356404335105
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.7667185122397169e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.057561 	Lval: 0.046444
Epoch: 5 	Ltrain: 0.025203 	Lval: 0.019215
Epoch: 10 	Ltrain: 0.015921 	Lval: 0.015876
Epoch: 15 	Ltrain: 0.017552 	Lval: 0.014944
Epoch: 20 	Ltrain: 0.015549 	Lval: 0.014575
Epoch: 25 	Ltrain: 0.014124 	Lval: 0.013831
Epoch: 30 	Ltrain: 0.014570 	Lval: 0.013253
Epoch: 35 	Ltrain: 0.013230 	Lval: 0.012709
Epoch: 40 	Ltrain: 0.012489 	Lval: 0.012113
Epoch: 45 	Ltrain: 0.012101 	Lval: 0.011599
Epoch: 50 	Ltrain: 0.013759 	Lval: 0.011239
Epoch: 55 	Ltrain: 0.013438 	Lval: 0.010911
Epoch: 60 	Ltrain: 0.010363 	Lval: 0.010513
Epoch 00064: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 65 	Ltrain: 0.012087 	Lval: 0.010621
Epoch: 70 	Ltrain: 0.010868 	Lval: 0.010481
Epoch: 75 	Ltrain: 0.010577 	Lval: 0.010396
Epoch: 80 	Ltrain: 0.010488 	Lval: 0.010373
Epoch: 85 	Ltrain: 0.010567 	Lval: 0.010281
Epoch: 90 	Ltrain: 0.010876 	Lval: 0.010202
Epoch 00094: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 95 	Ltrain: 0.009900 	Lval: 0.010224
Epoch: 100 	Ltrain: 0.010878 	Lval: 0.010219
Epoch: 105 	Ltrain: 0.010196 	Lval: 0.010220
Epoch 00106: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 110 	Ltrain: 0.009915 	Lval: 0.010221
Epoch: 115 	Ltrain: 0.010698 	Lval: 0.010221
EarlyStopper: stopping at epoch 115 with best_val_loss = 0.010202


	Fold 2/5
Epoch: 1 	Ltrain: 0.061960 	Lval: 0.047419
Epoch: 5 	Ltrain: 0.019079 	Lval: 0.016894
Epoch: 10 	Ltrain: 0.016614 	Lval: 0.015437
Epoch: 15 	Ltrain: 0.014510 	Lval: 0.014175
Epoch: 20 	Ltrain: 0.013330 	Lval: 0.012982
Epoch: 25 	Ltrain: 0.012227 	Lval: 0.012258
Epoch: 30 	Ltrain: 0.011383 	Lval: 0.010833
Epoch: 35 	Ltrain: 0.010203 	Lval: 0.010132
Epoch: 40 	Ltrain: 0.010027 	Lval: 0.009748
Epoch: 45 	Ltrain: 0.009593 	Lval: 0.009416
Epoch: 50 	Ltrain: 0.009135 	Lval: 0.009091
Epoch: 55 	Ltrain: 0.008953 	Lval: 0.008800
Epoch: 60 	Ltrain: 0.008947 	Lval: 0.008669
Epoch: 65 	Ltrain: 0.008581 	Lval: 0.008593
Epoch: 70 	Ltrain: 0.008366 	Lval: 0.008318
Epoch: 75 	Ltrain: 0.008091 	Lval: 0.008179
Epoch: 80 	Ltrain: 0.007932 	Lval: 0.008093
Epoch: 85 	Ltrain: 0.007881 	Lval: 0.007958
Epoch: 90 	Ltrain: 0.008155 	Lval: 0.007898
Epoch: 95 	Ltrain: 0.007907 	Lval: 0.007758
Epoch: 100 	Ltrain: 0.007429 	Lval: 0.007647
Epoch: 105 	Ltrain: 0.007428 	Lval: 0.007562
Epoch: 110 	Ltrain: 0.007625 	Lval: 0.007475
Epoch: 115 	Ltrain: 0.007505 	Lval: 0.007408
Epoch: 120 	Ltrain: 0.007350 	Lval: 0.007327
Epoch: 125 	Ltrain: 0.007166 	Lval: 0.007290
Epoch 00127: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 130 	Ltrain: 0.007153 	Lval: 0.007268
Epoch: 135 	Ltrain: 0.007238 	Lval: 0.007247
Epoch 00139: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 140 	Ltrain: 0.007039 	Lval: 0.007240
Epoch: 145 	Ltrain: 0.007140 	Lval: 0.007244
Epoch: 150 	Ltrain: 0.007229 	Lval: 0.007247
Epoch 00151: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 155 	Ltrain: 0.006928 	Lval: 0.007249
EarlyStopper: stopping at epoch 158 with best_val_loss = 0.007237


	Fold 3/5
Epoch: 1 	Ltrain: 0.048857 	Lval: 0.035453
Epoch: 5 	Ltrain: 0.016294 	Lval: 0.014036
Epoch: 10 	Ltrain: 0.013349 	Lval: 0.012159
Epoch: 15 	Ltrain: 0.010869 	Lval: 0.011096
Epoch: 20 	Ltrain: 0.009762 	Lval: 0.010418
Epoch: 25 	Ltrain: 0.009128 	Lval: 0.009833
Epoch: 30 	Ltrain: 0.008908 	Lval: 0.009382
Epoch: 35 	Ltrain: 0.008603 	Lval: 0.009192
Epoch: 40 	Ltrain: 0.008226 	Lval: 0.008797
Epoch: 45 	Ltrain: 0.008040 	Lval: 0.008520
Epoch: 50 	Ltrain: 0.007921 	Lval: 0.008294
Epoch: 55 	Ltrain: 0.007571 	Lval: 0.008076
Epoch: 60 	Ltrain: 0.007381 	Lval: 0.007912
Epoch: 65 	Ltrain: 0.007117 	Lval: 0.007760
Epoch: 70 	Ltrain: 0.006887 	Lval: 0.007643
Epoch: 75 	Ltrain: 0.006975 	Lval: 0.007516
Epoch: 80 	Ltrain: 0.006900 	Lval: 0.007401
Epoch: 85 	Ltrain: 0.006695 	Lval: 0.007333
Epoch: 90 	Ltrain: 0.006522 	Lval: 0.007245
Epoch: 95 	Ltrain: 0.006655 	Lval: 0.007200
Epoch: 100 	Ltrain: 0.006565 	Lval: 0.007135
Epoch: 105 	Ltrain: 0.006433 	Lval: 0.007093
Epoch: 110 	Ltrain: 0.006263 	Lval: 0.007082
Epoch: 115 	Ltrain: 0.006294 	Lval: 0.007018
Epoch: 120 	Ltrain: 0.006264 	Lval: 0.006990
Epoch: 125 	Ltrain: 0.006260 	Lval: 0.006901
Epoch: 130 	Ltrain: 0.006231 	Lval: 0.006899
Epoch 00132: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 135 	Ltrain: 0.006372 	Lval: 0.006868
Epoch: 140 	Ltrain: 0.006238 	Lval: 0.006860
Epoch: 145 	Ltrain: 0.006273 	Lval: 0.006855
Epoch 00150: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 150 	Ltrain: 0.006236 	Lval: 0.006852
Epoch: 155 	Ltrain: 0.006113 	Lval: 0.006852
Epoch: 160 	Ltrain: 0.006363 	Lval: 0.006850
Epoch 00165: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 165 	Ltrain: 0.006305 	Lval: 0.006851
EarlyStopper: stopping at epoch 167 with best_val_loss = 0.006857


	Fold 4/5
Epoch: 1 	Ltrain: 0.083062 	Lval: 0.063014
Epoch: 5 	Ltrain: 0.015540 	Lval: 0.015195
Epoch: 10 	Ltrain: 0.012359 	Lval: 0.012907
Epoch: 15 	Ltrain: 0.009777 	Lval: 0.011117
Epoch: 20 	Ltrain: 0.009027 	Lval: 0.010538
Epoch: 25 	Ltrain: 0.008481 	Lval: 0.010053
Epoch: 30 	Ltrain: 0.008180 	Lval: 0.009598
Epoch: 35 	Ltrain: 0.007762 	Lval: 0.009173
Epoch: 40 	Ltrain: 0.007546 	Lval: 0.008792
Epoch: 45 	Ltrain: 0.007148 	Lval: 0.008639
Epoch: 50 	Ltrain: 0.006804 	Lval: 0.008220
Epoch: 55 	Ltrain: 0.006695 	Lval: 0.007945
Epoch: 60 	Ltrain: 0.006491 	Lval: 0.007682
Epoch: 65 	Ltrain: 0.006221 	Lval: 0.007624
Epoch: 70 	Ltrain: 0.006201 	Lval: 0.007631
Epoch: 75 	Ltrain: 0.006034 	Lval: 0.007415
Epoch 00080: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 80 	Ltrain: 0.005964 	Lval: 0.007290
Epoch: 85 	Ltrain: 0.005876 	Lval: 0.007202
Epoch: 90 	Ltrain: 0.005913 	Lval: 0.007211
Epoch 00093: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 95 	Ltrain: 0.005852 	Lval: 0.007203
Epoch: 100 	Ltrain: 0.005827 	Lval: 0.007202
Epoch 00105: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 105 	Ltrain: 0.005855 	Lval: 0.007201
Epoch: 110 	Ltrain: 0.005812 	Lval: 0.007201
Epoch: 115 	Ltrain: 0.005876 	Lval: 0.007201
EarlyStopper: stopping at epoch 114 with best_val_loss = 0.007189


	Fold 5/5
Epoch: 1 	Ltrain: 0.025427 	Lval: 0.022624
Epoch: 5 	Ltrain: 0.014672 	Lval: 0.017328
Epoch: 10 	Ltrain: 0.011263 	Lval: 0.014451
Epoch: 15 	Ltrain: 0.009159 	Lval: 0.011809
Epoch: 20 	Ltrain: 0.008205 	Lval: 0.010352
Epoch: 25 	Ltrain: 0.007765 	Lval: 0.009660
Epoch: 30 	Ltrain: 0.007414 	Lval: 0.009009
Epoch: 35 	Ltrain: 0.007013 	Lval: 0.008796
Epoch: 40 	Ltrain: 0.006755 	Lval: 0.008369
Epoch: 45 	Ltrain: 0.006459 	Lval: 0.008164
Epoch: 50 	Ltrain: 0.006322 	Lval: 0.007862
Epoch: 55 	Ltrain: 0.006148 	Lval: 0.007832
Epoch: 60 	Ltrain: 0.005930 	Lval: 0.007699
Epoch: 65 	Ltrain: 0.005874 	Lval: 0.007395
Epoch: 70 	Ltrain: 0.005735 	Lval: 0.007486
Epoch 00072: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 75 	Ltrain: 0.005773 	Lval: 0.007349
Epoch: 80 	Ltrain: 0.005692 	Lval: 0.007324
Epoch: 85 	Ltrain: 0.005635 	Lval: 0.007330
Epoch: 90 	Ltrain: 0.005705 	Lval: 0.007290
Epoch 00091: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 95 	Ltrain: 0.005726 	Lval: 0.007299
Epoch: 100 	Ltrain: 0.005646 	Lval: 0.007300
Epoch 00103: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 105 	Ltrain: 0.005684 	Lval: 0.007298
EarlyStopper: stopping at epoch 108 with best_val_loss = 0.007294

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.002622655022886898
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.390321341492203e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.046548 	Lval: 0.019012
Epoch: 5 	Ltrain: 0.019093 	Lval: 0.015922
Epoch: 10 	Ltrain: 0.012006 	Lval: 0.013220
Epoch: 15 	Ltrain: 0.010110 	Lval: 0.009273
Epoch: 20 	Ltrain: 0.009138 	Lval: 0.008539
Epoch: 25 	Ltrain: 0.007137 	Lval: 0.007327
Epoch 00029: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 30 	Ltrain: 0.007909 	Lval: 0.007517
Epoch: 35 	Ltrain: 0.007709 	Lval: 0.007042
Epoch: 40 	Ltrain: 0.006798 	Lval: 0.006989
Epoch: 45 	Ltrain: 0.008944 	Lval: 0.007067
Epoch 00047: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 50 	Ltrain: 0.007255 	Lval: 0.007042
Epoch: 55 	Ltrain: 0.007417 	Lval: 0.006973
Epoch 00059: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 60 	Ltrain: 0.007236 	Lval: 0.006961
Epoch: 65 	Ltrain: 0.007087 	Lval: 0.006955
Epoch: 70 	Ltrain: 0.007163 	Lval: 0.006957
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.006951


	Fold 2/5
Epoch: 1 	Ltrain: 0.031289 	Lval: 0.016658
Epoch: 5 	Ltrain: 0.013346 	Lval: 0.011802
Epoch: 10 	Ltrain: 0.008561 	Lval: 0.007657
Epoch: 15 	Ltrain: 0.007056 	Lval: 0.007712
Epoch: 20 	Ltrain: 0.006944 	Lval: 0.006771
Epoch 00024: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 25 	Ltrain: 0.007088 	Lval: 0.006704
Epoch: 30 	Ltrain: 0.006338 	Lval: 0.006638
Epoch: 35 	Ltrain: 0.006166 	Lval: 0.006542
Epoch: 40 	Ltrain: 0.006616 	Lval: 0.006498
Epoch 00042: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 45 	Ltrain: 0.006291 	Lval: 0.006483
Epoch: 50 	Ltrain: 0.005984 	Lval: 0.006475
Epoch 00055: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 55 	Ltrain: 0.006122 	Lval: 0.006477
Epoch: 60 	Ltrain: 0.006268 	Lval: 0.006471
Epoch: 65 	Ltrain: 0.006312 	Lval: 0.006472
Epoch 00067: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 70 	Ltrain: 0.006571 	Lval: 0.006474
Epoch: 75 	Ltrain: 0.006283 	Lval: 0.006474
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.006477


	Fold 3/5
Epoch: 1 	Ltrain: 0.029183 	Lval: 0.016725
Epoch: 5 	Ltrain: 0.010409 	Lval: 0.010489
Epoch: 10 	Ltrain: 0.007206 	Lval: 0.007709
Epoch: 15 	Ltrain: 0.006146 	Lval: 0.006812
Epoch 00020: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 20 	Ltrain: 0.006302 	Lval: 0.006696
Epoch: 25 	Ltrain: 0.005741 	Lval: 0.006411
Epoch: 30 	Ltrain: 0.005765 	Lval: 0.006354
Epoch: 35 	Ltrain: 0.005674 	Lval: 0.006341
Epoch: 40 	Ltrain: 0.005581 	Lval: 0.006307
Epoch: 45 	Ltrain: 0.005593 	Lval: 0.006259
Epoch: 50 	Ltrain: 0.005546 	Lval: 0.006266
Epoch: 55 	Ltrain: 0.005440 	Lval: 0.006203
Epoch 00057: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 60 	Ltrain: 0.005766 	Lval: 0.006155
Epoch: 65 	Ltrain: 0.005489 	Lval: 0.006149
Epoch: 70 	Ltrain: 0.005390 	Lval: 0.006142
Epoch: 75 	Ltrain: 0.005490 	Lval: 0.006138
Epoch 00077: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 80 	Ltrain: 0.005681 	Lval: 0.006138
Epoch: 85 	Ltrain: 0.005602 	Lval: 0.006138
Epoch 00089: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 90 	Ltrain: 0.005428 	Lval: 0.006137
Epoch: 95 	Ltrain: 0.005512 	Lval: 0.006137
Epoch: 100 	Ltrain: 0.005473 	Lval: 0.006137
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.006136


	Fold 4/5
Epoch: 1 	Ltrain: 0.025279 	Lval: 0.017577
Epoch: 5 	Ltrain: 0.007557 	Lval: 0.007833
Epoch: 10 	Ltrain: 0.005817 	Lval: 0.007321
Epoch: 15 	Ltrain: 0.005388 	Lval: 0.006620
Epoch: 20 	Ltrain: 0.005479 	Lval: 0.006445
Epoch: 25 	Ltrain: 0.005439 	Lval: 0.006314
Epoch: 30 	Ltrain: 0.005139 	Lval: 0.006284
Epoch: 35 	Ltrain: 0.004973 	Lval: 0.006584
Epoch: 40 	Ltrain: 0.004842 	Lval: 0.005638
Epoch: 45 	Ltrain: 0.004759 	Lval: 0.005800
Epoch 00046: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 50 	Ltrain: 0.004355 	Lval: 0.005319
Epoch: 55 	Ltrain: 0.004301 	Lval: 0.005282
Epoch: 60 	Ltrain: 0.004245 	Lval: 0.005203
Epoch: 65 	Ltrain: 0.004195 	Lval: 0.005234
Epoch: 70 	Ltrain: 0.004224 	Lval: 0.005167
Epoch: 75 	Ltrain: 0.004315 	Lval: 0.005084
Epoch: 80 	Ltrain: 0.004118 	Lval: 0.004965
Epoch: 85 	Ltrain: 0.004077 	Lval: 0.004926
Epoch: 90 	Ltrain: 0.004031 	Lval: 0.004871
Epoch: 95 	Ltrain: 0.003991 	Lval: 0.004704
Epoch: 100 	Ltrain: 0.003957 	Lval: 0.004636
Epoch: 105 	Ltrain: 0.003901 	Lval: 0.004623
Epoch: 110 	Ltrain: 0.003817 	Lval: 0.004460
Epoch: 115 	Ltrain: 0.003777 	Lval: 0.004408
Epoch: 120 	Ltrain: 0.003704 	Lval: 0.004428
Epoch: 125 	Ltrain: 0.003754 	Lval: 0.004270
Epoch: 130 	Ltrain: 0.003646 	Lval: 0.004153
Epoch: 135 	Ltrain: 0.003621 	Lval: 0.004238
Epoch: 140 	Ltrain: 0.003563 	Lval: 0.004004
Epoch: 145 	Ltrain: 0.003563 	Lval: 0.003960
Epoch: 150 	Ltrain: 0.003421 	Lval: 0.003936
Epoch: 155 	Ltrain: 0.003367 	Lval: 0.003802
Epoch 00157: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 160 	Ltrain: 0.003265 	Lval: 0.003727
Epoch: 165 	Ltrain: 0.003275 	Lval: 0.003717
Epoch: 170 	Ltrain: 0.003237 	Lval: 0.003706
Epoch: 175 	Ltrain: 0.003320 	Lval: 0.003696
Epoch: 180 	Ltrain: 0.003230 	Lval: 0.003680
Epoch 00184: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 185 	Ltrain: 0.003239 	Lval: 0.003679
Epoch: 190 	Ltrain: 0.003224 	Lval: 0.003682
Epoch: 195 	Ltrain: 0.003300 	Lval: 0.003681
Epoch 00196: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 200 	Ltrain: 0.003215 	Lval: 0.003681
Epoch: 205 	Ltrain: 0.003227 	Lval: 0.003680
EarlyStopper: stopping at epoch 204 with best_val_loss = 0.003687


	Fold 5/5
Epoch: 1 	Ltrain: 0.020825 	Lval: 0.021278
Epoch: 5 	Ltrain: 0.007485 	Lval: 0.008010
Epoch: 10 	Ltrain: 0.005779 	Lval: 0.008793
Epoch 00013: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 15 	Ltrain: 0.005239 	Lval: 0.006842
Epoch: 20 	Ltrain: 0.005305 	Lval: 0.006729
Epoch 00025: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 25 	Ltrain: 0.005255 	Lval: 0.006733
Epoch: 30 	Ltrain: 0.005164 	Lval: 0.006693
Epoch: 35 	Ltrain: 0.005230 	Lval: 0.006701
Epoch 00037: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 40 	Ltrain: 0.005143 	Lval: 0.006693
EarlyStopper: stopping at epoch 40 with best_val_loss = 0.006669

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0036708403685720837
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.036796529397564e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.022092 	Lval: 0.018557
Epoch: 5 	Ltrain: 0.015229 	Lval: 0.012680
Epoch: 10 	Ltrain: 0.015343 	Lval: 0.009785
Epoch: 15 	Ltrain: 0.008359 	Lval: 0.008798
Epoch: 20 	Ltrain: 0.008466 	Lval: 0.007799
Epoch: 25 	Ltrain: 0.010543 	Lval: 0.008041
Epoch 00030: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 30 	Ltrain: 0.008452 	Lval: 0.007430
Epoch: 35 	Ltrain: 0.008274 	Lval: 0.007711
Epoch: 40 	Ltrain: 0.007340 	Lval: 0.007117
Epoch: 45 	Ltrain: 0.006249 	Lval: 0.007051
Epoch 00048: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 50 	Ltrain: 0.007450 	Lval: 0.007202
Epoch: 55 	Ltrain: 0.006541 	Lval: 0.007090
Epoch 00060: reducing learning rate of group 0 to 3.6708e-06.
Epoch: 60 	Ltrain: 0.006715 	Lval: 0.007044
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.006998


	Fold 2/5
Epoch: 1 	Ltrain: 0.072092 	Lval: 0.034265
Epoch: 5 	Ltrain: 0.013434 	Lval: 0.012490
Epoch: 10 	Ltrain: 0.009469 	Lval: 0.008320
Epoch: 15 	Ltrain: 0.009560 	Lval: 0.008359
Epoch: 20 	Ltrain: 0.007746 	Lval: 0.007498
Epoch: 25 	Ltrain: 0.007384 	Lval: 0.007476
Epoch: 30 	Ltrain: 0.006894 	Lval: 0.007362
Epoch: 35 	Ltrain: 0.008353 	Lval: 0.007742
Epoch: 40 	Ltrain: 0.007046 	Lval: 0.007860
Epoch: 45 	Ltrain: 0.008307 	Lval: 0.007000
Epoch 00050: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 50 	Ltrain: 0.007187 	Lval: 0.007013
Epoch: 55 	Ltrain: 0.006391 	Lval: 0.006896
Epoch: 60 	Ltrain: 0.006322 	Lval: 0.006750
Epoch 00062: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 65 	Ltrain: 0.006761 	Lval: 0.006779
Epoch: 70 	Ltrain: 0.006358 	Lval: 0.006742
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.006694


	Fold 3/5
Epoch: 1 	Ltrain: 0.029870 	Lval: 0.015483
Epoch: 5 	Ltrain: 0.009928 	Lval: 0.009771
Epoch: 10 	Ltrain: 0.007591 	Lval: 0.008012
Epoch: 15 	Ltrain: 0.006676 	Lval: 0.007129
Epoch 00018: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 20 	Ltrain: 0.006623 	Lval: 0.006920
Epoch: 25 	Ltrain: 0.006359 	Lval: 0.006696
Epoch: 30 	Ltrain: 0.006243 	Lval: 0.006690
Epoch: 35 	Ltrain: 0.006028 	Lval: 0.006646
Epoch: 40 	Ltrain: 0.006350 	Lval: 0.006729
Epoch: 45 	Ltrain: 0.006205 	Lval: 0.006615
Epoch 00046: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 50 	Ltrain: 0.006196 	Lval: 0.006614
Epoch: 55 	Ltrain: 0.006216 	Lval: 0.006615
Epoch 00058: reducing learning rate of group 0 to 3.6708e-06.
Epoch: 60 	Ltrain: 0.006323 	Lval: 0.006617
Epoch: 65 	Ltrain: 0.006323 	Lval: 0.006617
EarlyStopper: stopping at epoch 67 with best_val_loss = 0.006605


	Fold 4/5
Epoch: 1 	Ltrain: 0.029456 	Lval: 0.013381
Epoch: 5 	Ltrain: 0.008305 	Lval: 0.008027
Epoch: 10 	Ltrain: 0.006976 	Lval: 0.007380
Epoch: 15 	Ltrain: 0.006329 	Lval: 0.007258
Epoch: 20 	Ltrain: 0.005688 	Lval: 0.006635
Epoch: 25 	Ltrain: 0.005556 	Lval: 0.006912
Epoch: 30 	Ltrain: 0.005941 	Lval: 0.006558
Epoch 00033: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 35 	Ltrain: 0.005335 	Lval: 0.006451
Epoch: 40 	Ltrain: 0.005305 	Lval: 0.006410
Epoch: 45 	Ltrain: 0.005367 	Lval: 0.006408
Epoch 00046: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 50 	Ltrain: 0.005459 	Lval: 0.006393
Epoch: 55 	Ltrain: 0.005400 	Lval: 0.006397
Epoch 00058: reducing learning rate of group 0 to 3.6708e-06.
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.006393


	Fold 5/5
Epoch: 1 	Ltrain: 0.064162 	Lval: 0.020956
Epoch: 5 	Ltrain: 0.010287 	Lval: 0.011802
Epoch: 10 	Ltrain: 0.007593 	Lval: 0.009311
Epoch: 15 	Ltrain: 0.007277 	Lval: 0.008037
Epoch: 20 	Ltrain: 0.006312 	Lval: 0.007602
Epoch: 25 	Ltrain: 0.005973 	Lval: 0.007419
Epoch: 30 	Ltrain: 0.005806 	Lval: 0.007118
Epoch 00032: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 35 	Ltrain: 0.005481 	Lval: 0.006970
Epoch: 40 	Ltrain: 0.005646 	Lval: 0.007002
Epoch: 45 	Ltrain: 0.005421 	Lval: 0.006942
Epoch: 50 	Ltrain: 0.005471 	Lval: 0.006943
Epoch 00051: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 55 	Ltrain: 0.005481 	Lval: 0.006945
Epoch: 60 	Ltrain: 0.005254 	Lval: 0.006935
Epoch 00063: reducing learning rate of group 0 to 3.6708e-06.
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.006858

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0028563448143027245
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.8400362376385822e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 10
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.033050 	Lval: 0.017829
Epoch: 5 	Ltrain: 0.015300 	Lval: 0.013470
Epoch: 10 	Ltrain: 0.010088 	Lval: 0.009795
Epoch: 15 	Ltrain: 0.009518 	Lval: 0.009987
Epoch: 20 	Ltrain: 0.008292 	Lval: 0.007749
Epoch: 25 	Ltrain: 0.007331 	Lval: 0.007339
Epoch: 30 	Ltrain: 0.007415 	Lval: 0.008759
Epoch: 35 	Ltrain: 0.007555 	Lval: 0.007198
Epoch 00037: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 40 	Ltrain: 0.007034 	Lval: 0.007091
Epoch: 45 	Ltrain: 0.007055 	Lval: 0.006996
Epoch: 50 	Ltrain: 0.007237 	Lval: 0.007029
Epoch 00051: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 55 	Ltrain: 0.007016 	Lval: 0.006987
EarlyStopper: stopping at epoch 56 with best_val_loss = 0.006985


	Fold 2/5
Epoch: 1 	Ltrain: 0.028040 	Lval: 0.017913
Epoch: 5 	Ltrain: 0.010740 	Lval: 0.009569
Epoch: 10 	Ltrain: 0.007722 	Lval: 0.008207
Epoch: 15 	Ltrain: 0.007293 	Lval: 0.007326
Epoch: 20 	Ltrain: 0.006933 	Lval: 0.007255
Epoch: 25 	Ltrain: 0.007295 	Lval: 0.007467
Epoch 00028: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 30 	Ltrain: 0.006526 	Lval: 0.006923
Epoch: 35 	Ltrain: 0.006508 	Lval: 0.006899
Epoch 00040: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 40 	Ltrain: 0.006514 	Lval: 0.006898
Epoch: 45 	Ltrain: 0.006488 	Lval: 0.006881
Epoch: 50 	Ltrain: 0.006522 	Lval: 0.006882
Epoch 00052: reducing learning rate of group 0 to 2.8563e-06.
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.006880


	Fold 3/5
Epoch: 1 	Ltrain: 0.029304 	Lval: 0.016828
Epoch: 5 	Ltrain: 0.008550 	Lval: 0.009109
Epoch: 10 	Ltrain: 0.006669 	Lval: 0.009068
Epoch: 15 	Ltrain: 0.006680 	Lval: 0.007491
Epoch 00020: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 20 	Ltrain: 0.006504 	Lval: 0.007091
Epoch: 25 	Ltrain: 0.006088 	Lval: 0.006871
Epoch: 30 	Ltrain: 0.006159 	Lval: 0.006865
Epoch 00033: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 35 	Ltrain: 0.005987 	Lval: 0.006854
EarlyStopper: stopping at epoch 38 with best_val_loss = 0.006847


	Fold 4/5
Epoch: 1 	Ltrain: 0.018356 	Lval: 0.016137
Epoch: 5 	Ltrain: 0.006674 	Lval: 0.007938
Epoch: 10 	Ltrain: 0.006027 	Lval: 0.007381
Epoch 00015: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 15 	Ltrain: 0.005831 	Lval: 0.007235
Epoch: 20 	Ltrain: 0.005465 	Lval: 0.006921
Epoch: 25 	Ltrain: 0.005438 	Lval: 0.006936
Epoch 00027: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 30 	Ltrain: 0.005416 	Lval: 0.006928
EarlyStopper: stopping at epoch 31 with best_val_loss = 0.006874


	Fold 5/5
Epoch: 1 	Ltrain: 0.016901 	Lval: 0.014344
Epoch: 5 	Ltrain: 0.006385 	Lval: 0.007728
Epoch: 10 	Ltrain: 0.005885 	Lval: 0.007195
Epoch 00014: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 15 	Ltrain: 0.005551 	Lval: 0.007158
Epoch: 20 	Ltrain: 0.005460 	Lval: 0.007281
Epoch: 25 	Ltrain: 0.005450 	Lval: 0.007084
Epoch: 30 	Ltrain: 0.005437 	Lval: 0.007272
Epoch 00032: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 35 	Ltrain: 0.005410 	Lval: 0.007091
EarlyStopper: stopping at epoch 35 with best_val_loss = 0.007051

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00044590431323154054
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.414035159965753e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.075734 	Lval: 0.052902
Epoch: 5 	Ltrain: 0.023113 	Lval: 0.016602
Epoch 00010: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.017920 	Lval: 0.018790
Epoch: 15 	Ltrain: 0.018769 	Lval: 0.018190
Epoch: 20 	Ltrain: 0.016731 	Lval: 0.017339
Epoch 00022: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 25 	Ltrain: 0.017020 	Lval: 0.016930
EarlyStopper: stopping at epoch 27 with best_val_loss = 0.016013


	Fold 2/5
Epoch: 1 	Ltrain: 0.063676 	Lval: 0.034608
Epoch: 5 	Ltrain: 0.018578 	Lval: 0.017234
Epoch 00007: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.018136 	Lval: 0.015967
Epoch: 15 	Ltrain: 0.018404 	Lval: 0.015423
Epoch: 20 	Ltrain: 0.018261 	Lval: 0.015118
Epoch: 25 	Ltrain: 0.018102 	Lval: 0.014843
Epoch: 30 	Ltrain: 0.015919 	Lval: 0.014639
Epoch: 35 	Ltrain: 0.016202 	Lval: 0.014501
Epoch: 40 	Ltrain: 0.016522 	Lval: 0.014527
Epoch 00041: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 45 	Ltrain: 0.017380 	Lval: 0.014529
Epoch: 50 	Ltrain: 0.016025 	Lval: 0.014500
Epoch 00053: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 55 	Ltrain: 0.016296 	Lval: 0.014489
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.014478


	Fold 3/5
Epoch: 1 	Ltrain: 0.174132 	Lval: 0.134463
Epoch: 5 	Ltrain: 0.021888 	Lval: 0.019058
Epoch 00008: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.018338 	Lval: 0.014929
Epoch: 15 	Ltrain: 0.017481 	Lval: 0.014422
Epoch: 20 	Ltrain: 0.018044 	Lval: 0.014298
Epoch: 25 	Ltrain: 0.018436 	Lval: 0.014207
Epoch: 30 	Ltrain: 0.017378 	Lval: 0.014135
Epoch: 35 	Ltrain: 0.018201 	Lval: 0.014052
Epoch: 40 	Ltrain: 0.016644 	Lval: 0.013969
Epoch: 45 	Ltrain: 0.016857 	Lval: 0.013860
Epoch: 50 	Ltrain: 0.016254 	Lval: 0.013790
Epoch: 55 	Ltrain: 0.015697 	Lval: 0.013691
Epoch: 60 	Ltrain: 0.016372 	Lval: 0.013582
Epoch: 65 	Ltrain: 0.015733 	Lval: 0.013509
Epoch: 70 	Ltrain: 0.015157 	Lval: 0.013383
Epoch: 75 	Ltrain: 0.015355 	Lval: 0.013213
Epoch: 80 	Ltrain: 0.014543 	Lval: 0.013133
Epoch: 85 	Ltrain: 0.014256 	Lval: 0.013043
Epoch: 90 	Ltrain: 0.013635 	Lval: 0.012972
Epoch: 95 	Ltrain: 0.013772 	Lval: 0.012754
Epoch: 100 	Ltrain: 0.013399 	Lval: 0.012728
Epoch: 105 	Ltrain: 0.013655 	Lval: 0.012560
Epoch 00109: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 110 	Ltrain: 0.012852 	Lval: 0.012630
Epoch: 115 	Ltrain: 0.012663 	Lval: 0.012576
Epoch: 120 	Ltrain: 0.012490 	Lval: 0.012556
Epoch: 125 	Ltrain: 0.012763 	Lval: 0.012528
Epoch 00130: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 130 	Ltrain: 0.012439 	Lval: 0.012531
Epoch: 135 	Ltrain: 0.012664 	Lval: 0.012528
Epoch: 140 	Ltrain: 0.012727 	Lval: 0.012524
Epoch: 145 	Ltrain: 0.012558 	Lval: 0.012522
Epoch 00148: reducing learning rate of group 0 to 4.4590e-08.
EarlyStopper: stopping at epoch 147 with best_val_loss = 0.012525


	Fold 4/5
Epoch: 1 	Ltrain: 0.032449 	Lval: 0.019553
Epoch: 5 	Ltrain: 0.016859 	Lval: 0.014523
Epoch: 10 	Ltrain: 0.012914 	Lval: 0.011767
Epoch: 15 	Ltrain: 0.010258 	Lval: 0.010504
Epoch: 20 	Ltrain: 0.009471 	Lval: 0.009896
Epoch: 25 	Ltrain: 0.008463 	Lval: 0.009531
Epoch: 30 	Ltrain: 0.008057 	Lval: 0.009228
Epoch: 35 	Ltrain: 0.008105 	Lval: 0.008668
Epoch: 40 	Ltrain: 0.007658 	Lval: 0.008477
Epoch: 45 	Ltrain: 0.007551 	Lval: 0.008120
Epoch: 50 	Ltrain: 0.006993 	Lval: 0.008000
Epoch: 55 	Ltrain: 0.006920 	Lval: 0.007607
Epoch: 60 	Ltrain: 0.006904 	Lval: 0.007533
Epoch: 65 	Ltrain: 0.006464 	Lval: 0.007321
Epoch: 70 	Ltrain: 0.006317 	Lval: 0.007202
Epoch: 75 	Ltrain: 0.006159 	Lval: 0.007061
Epoch: 80 	Ltrain: 0.006468 	Lval: 0.007128
Epoch 00082: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 85 	Ltrain: 0.006070 	Lval: 0.007084
Epoch: 90 	Ltrain: 0.006151 	Lval: 0.007085
Epoch: 95 	Ltrain: 0.006065 	Lval: 0.007063
EarlyStopper: stopping at epoch 96 with best_val_loss = 0.007061


	Fold 5/5
Epoch: 1 	Ltrain: 0.133079 	Lval: 0.113317
Epoch: 5 	Ltrain: 0.018602 	Lval: 0.017455
Epoch: 10 	Ltrain: 0.013543 	Lval: 0.016241
Epoch: 15 	Ltrain: 0.010931 	Lval: 0.014533
Epoch: 20 	Ltrain: 0.009841 	Lval: 0.012563
Epoch: 25 	Ltrain: 0.008977 	Lval: 0.011421
Epoch: 30 	Ltrain: 0.008768 	Lval: 0.010968
Epoch: 35 	Ltrain: 0.008588 	Lval: 0.010386
Epoch: 40 	Ltrain: 0.008261 	Lval: 0.009916
Epoch: 45 	Ltrain: 0.007904 	Lval: 0.009536
Epoch: 50 	Ltrain: 0.007776 	Lval: 0.009356
Epoch: 55 	Ltrain: 0.007606 	Lval: 0.009024
Epoch: 60 	Ltrain: 0.007069 	Lval: 0.008695
Epoch: 65 	Ltrain: 0.007013 	Lval: 0.008609
Epoch: 70 	Ltrain: 0.006855 	Lval: 0.008409
Epoch: 75 	Ltrain: 0.006579 	Lval: 0.008360
Epoch: 80 	Ltrain: 0.006698 	Lval: 0.008050
Epoch: 85 	Ltrain: 0.006439 	Lval: 0.008047
Epoch: 90 	Ltrain: 0.006254 	Lval: 0.007863
Epoch: 95 	Ltrain: 0.006467 	Lval: 0.008001
Epoch: 100 	Ltrain: 0.006191 	Lval: 0.007739
Epoch: 105 	Ltrain: 0.006168 	Lval: 0.007742
Epoch: 110 	Ltrain: 0.005956 	Lval: 0.007763
Epoch: 115 	Ltrain: 0.006215 	Lval: 0.007876
Epoch 00117: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 120 	Ltrain: 0.006004 	Lval: 0.007647
Epoch: 125 	Ltrain: 0.006058 	Lval: 0.007568
Epoch 00130: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 130 	Ltrain: 0.006201 	Lval: 0.007580
Epoch: 135 	Ltrain: 0.005921 	Lval: 0.007586
Epoch: 140 	Ltrain: 0.006155 	Lval: 0.007590
Epoch 00142: reducing learning rate of group 0 to 4.4590e-07.
EarlyStopper: stopping at epoch 143 with best_val_loss = 0.007567

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0002488305190859649
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.13621701509778e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.057459 	Lval: 0.054328
Epoch: 5 	Ltrain: 0.024533 	Lval: 0.022544
Epoch: 10 	Ltrain: 0.015975 	Lval: 0.015908
Epoch: 15 	Ltrain: 0.014514 	Lval: 0.015096
Epoch: 20 	Ltrain: 0.015440 	Lval: 0.014721
Epoch: 25 	Ltrain: 0.016239 	Lval: 0.013895
Epoch: 30 	Ltrain: 0.012627 	Lval: 0.013363
Epoch: 35 	Ltrain: 0.012831 	Lval: 0.012815
Epoch: 40 	Ltrain: 0.011903 	Lval: 0.011994
Epoch: 45 	Ltrain: 0.011678 	Lval: 0.011569
Epoch: 50 	Ltrain: 0.012006 	Lval: 0.011229
Epoch: 55 	Ltrain: 0.010827 	Lval: 0.010545
Epoch: 60 	Ltrain: 0.010608 	Lval: 0.010205
Epoch: 65 	Ltrain: 0.010067 	Lval: 0.009970
Epoch: 70 	Ltrain: 0.009664 	Lval: 0.009839
Epoch: 75 	Ltrain: 0.009990 	Lval: 0.009570
Epoch: 80 	Ltrain: 0.009671 	Lval: 0.009368
Epoch: 85 	Ltrain: 0.009703 	Lval: 0.009177
Epoch: 90 	Ltrain: 0.011735 	Lval: 0.009306
Epoch: 95 	Ltrain: 0.009915 	Lval: 0.008896
Epoch: 100 	Ltrain: 0.009377 	Lval: 0.008793
Epoch: 105 	Ltrain: 0.009375 	Lval: 0.008726
Epoch 00110: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 110 	Ltrain: 0.009630 	Lval: 0.008686
Epoch: 115 	Ltrain: 0.008155 	Lval: 0.008517
Epoch: 120 	Ltrain: 0.008806 	Lval: 0.008523
Epoch 00122: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 125 	Ltrain: 0.011110 	Lval: 0.008542
Epoch: 130 	Ltrain: 0.008551 	Lval: 0.008542
Epoch 00134: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 135 	Ltrain: 0.008807 	Lval: 0.008541
EarlyStopper: stopping at epoch 138 with best_val_loss = 0.008507


	Fold 2/5
Epoch: 1 	Ltrain: 0.081372 	Lval: 0.069329
Epoch: 5 	Ltrain: 0.021277 	Lval: 0.017374
Epoch: 10 	Ltrain: 0.014199 	Lval: 0.013199
Epoch: 15 	Ltrain: 0.013045 	Lval: 0.011767
Epoch: 20 	Ltrain: 0.011321 	Lval: 0.010644
Epoch: 25 	Ltrain: 0.010361 	Lval: 0.010050
Epoch: 30 	Ltrain: 0.010285 	Lval: 0.009717
Epoch: 35 	Ltrain: 0.009110 	Lval: 0.009374
Epoch: 40 	Ltrain: 0.009564 	Lval: 0.009186
Epoch: 45 	Ltrain: 0.008830 	Lval: 0.008882
Epoch: 50 	Ltrain: 0.008634 	Lval: 0.008606
Epoch: 55 	Ltrain: 0.008288 	Lval: 0.008426
Epoch: 60 	Ltrain: 0.008303 	Lval: 0.008289
Epoch: 65 	Ltrain: 0.008861 	Lval: 0.008254
Epoch: 70 	Ltrain: 0.008439 	Lval: 0.007991
Epoch: 75 	Ltrain: 0.008021 	Lval: 0.007852
Epoch: 80 	Ltrain: 0.007553 	Lval: 0.007823
Epoch: 85 	Ltrain: 0.007695 	Lval: 0.007689
Epoch: 90 	Ltrain: 0.007706 	Lval: 0.007622
Epoch: 95 	Ltrain: 0.007502 	Lval: 0.007630
Epoch: 100 	Ltrain: 0.008151 	Lval: 0.007443
Epoch: 105 	Ltrain: 0.007519 	Lval: 0.007409
Epoch: 110 	Ltrain: 0.007418 	Lval: 0.007354
Epoch: 115 	Ltrain: 0.007764 	Lval: 0.007281
Epoch: 120 	Ltrain: 0.007527 	Lval: 0.007288
Epoch: 125 	Ltrain: 0.007190 	Lval: 0.007263
Epoch: 130 	Ltrain: 0.007005 	Lval: 0.007152
Epoch: 135 	Ltrain: 0.006785 	Lval: 0.007114
Epoch: 140 	Ltrain: 0.007112 	Lval: 0.007111
Epoch: 145 	Ltrain: 0.007070 	Lval: 0.007051
Epoch: 150 	Ltrain: 0.007051 	Lval: 0.007033
Epoch: 155 	Ltrain: 0.006878 	Lval: 0.007014
Epoch 00158: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 160 	Ltrain: 0.006619 	Lval: 0.007030
Epoch: 165 	Ltrain: 0.006825 	Lval: 0.006986
Epoch 00170: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 170 	Ltrain: 0.006983 	Lval: 0.007014
Epoch: 175 	Ltrain: 0.006746 	Lval: 0.007008
EarlyStopper: stopping at epoch 176 with best_val_loss = 0.006993


	Fold 3/5
Epoch: 1 	Ltrain: 0.050260 	Lval: 0.035095
Epoch: 5 	Ltrain: 0.018387 	Lval: 0.015001
Epoch: 10 	Ltrain: 0.014943 	Lval: 0.013267
Epoch: 15 	Ltrain: 0.012273 	Lval: 0.011523
Epoch: 20 	Ltrain: 0.010580 	Lval: 0.010339
Epoch: 25 	Ltrain: 0.009786 	Lval: 0.009756
Epoch: 30 	Ltrain: 0.009019 	Lval: 0.009258
Epoch: 35 	Ltrain: 0.008724 	Lval: 0.008943
Epoch: 40 	Ltrain: 0.008252 	Lval: 0.008638
Epoch: 45 	Ltrain: 0.007945 	Lval: 0.008340
Epoch: 50 	Ltrain: 0.007614 	Lval: 0.008024
Epoch: 55 	Ltrain: 0.007531 	Lval: 0.007781
Epoch: 60 	Ltrain: 0.007261 	Lval: 0.007562
Epoch: 65 	Ltrain: 0.006993 	Lval: 0.007476
Epoch: 70 	Ltrain: 0.006845 	Lval: 0.007450
Epoch: 75 	Ltrain: 0.006599 	Lval: 0.007267
Epoch: 80 	Ltrain: 0.006882 	Lval: 0.007235
Epoch: 85 	Ltrain: 0.006473 	Lval: 0.007130
Epoch: 90 	Ltrain: 0.006575 	Lval: 0.007076
Epoch: 95 	Ltrain: 0.006450 	Lval: 0.006991
Epoch 00099: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 100 	Ltrain: 0.006430 	Lval: 0.006975
Epoch: 105 	Ltrain: 0.006365 	Lval: 0.006966
Epoch: 110 	Ltrain: 0.006527 	Lval: 0.006966
Epoch 00115: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 115 	Ltrain: 0.006521 	Lval: 0.006960
Epoch: 120 	Ltrain: 0.006492 	Lval: 0.006960
Epoch: 125 	Ltrain: 0.006311 	Lval: 0.006960
Epoch 00127: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 130 	Ltrain: 0.006247 	Lval: 0.006960
EarlyStopper: stopping at epoch 131 with best_val_loss = 0.006962


	Fold 4/5
Epoch: 1 	Ltrain: 0.054740 	Lval: 0.041212
Epoch: 5 	Ltrain: 0.016658 	Lval: 0.016075
Epoch: 10 	Ltrain: 0.013325 	Lval: 0.013209
Epoch: 15 	Ltrain: 0.009877 	Lval: 0.010769
Epoch: 20 	Ltrain: 0.008598 	Lval: 0.009799
Epoch: 25 	Ltrain: 0.007868 	Lval: 0.008974
Epoch: 30 	Ltrain: 0.007479 	Lval: 0.008650
Epoch: 35 	Ltrain: 0.007091 	Lval: 0.008336
Epoch: 40 	Ltrain: 0.006811 	Lval: 0.008047
Epoch: 45 	Ltrain: 0.006645 	Lval: 0.007995
Epoch: 50 	Ltrain: 0.006269 	Lval: 0.007710
Epoch: 55 	Ltrain: 0.006185 	Lval: 0.007534
Epoch: 60 	Ltrain: 0.006001 	Lval: 0.007317
Epoch: 65 	Ltrain: 0.005908 	Lval: 0.007182
Epoch: 70 	Ltrain: 0.005968 	Lval: 0.007091
Epoch: 75 	Ltrain: 0.005695 	Lval: 0.007060
Epoch: 80 	Ltrain: 0.005736 	Lval: 0.007073
Epoch: 85 	Ltrain: 0.005681 	Lval: 0.006925
Epoch: 90 	Ltrain: 0.005688 	Lval: 0.007005
Epoch: 95 	Ltrain: 0.005597 	Lval: 0.006883
Epoch: 100 	Ltrain: 0.005539 	Lval: 0.006904
Epoch: 105 	Ltrain: 0.005577 	Lval: 0.006808
Epoch 00108: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 110 	Ltrain: 0.005539 	Lval: 0.006818
Epoch: 115 	Ltrain: 0.005465 	Lval: 0.006793
Epoch 00120: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 120 	Ltrain: 0.005428 	Lval: 0.006809
Epoch: 125 	Ltrain: 0.005412 	Lval: 0.006803
EarlyStopper: stopping at epoch 126 with best_val_loss = 0.006796


	Fold 5/5
Epoch: 1 	Ltrain: 0.083681 	Lval: 0.071859
Epoch: 5 	Ltrain: 0.017319 	Lval: 0.020482
Epoch: 10 	Ltrain: 0.014493 	Lval: 0.017373
Epoch: 15 	Ltrain: 0.010644 	Lval: 0.013915
Epoch: 20 	Ltrain: 0.008893 	Lval: 0.011920
Epoch: 25 	Ltrain: 0.008427 	Lval: 0.010882
Epoch: 30 	Ltrain: 0.007798 	Lval: 0.009914
Epoch: 35 	Ltrain: 0.007368 	Lval: 0.009542
Epoch: 40 	Ltrain: 0.007054 	Lval: 0.008863
Epoch: 45 	Ltrain: 0.006658 	Lval: 0.008370
Epoch: 50 	Ltrain: 0.006360 	Lval: 0.008168
Epoch: 55 	Ltrain: 0.006374 	Lval: 0.007724
Epoch 00059: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 60 	Ltrain: 0.006036 	Lval: 0.007736
Epoch: 65 	Ltrain: 0.006069 	Lval: 0.007693
Epoch: 70 	Ltrain: 0.006208 	Lval: 0.007685
Epoch 00075: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 75 	Ltrain: 0.006126 	Lval: 0.007676
Epoch: 80 	Ltrain: 0.006073 	Lval: 0.007674
Epoch: 85 	Ltrain: 0.006093 	Lval: 0.007670
Epoch 00087: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 90 	Ltrain: 0.006064 	Lval: 0.007668
EarlyStopper: stopping at epoch 93 with best_val_loss = 0.007667

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0012757488347627612
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.30569702558947e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.024745 	Lval: 0.016208
Epoch: 5 	Ltrain: 0.012620 	Lval: 0.011771
Epoch: 10 	Ltrain: 0.009072 	Lval: 0.008924
Epoch: 15 	Ltrain: 0.008551 	Lval: 0.007620
Epoch: 20 	Ltrain: 0.008469 	Lval: 0.008520
Epoch: 25 	Ltrain: 0.007299 	Lval: 0.007621
Epoch 00028: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 30 	Ltrain: 0.006873 	Lval: 0.006966
Epoch: 35 	Ltrain: 0.007157 	Lval: 0.006936
Epoch 00040: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 40 	Ltrain: 0.007341 	Lval: 0.006905
Epoch: 45 	Ltrain: 0.007047 	Lval: 0.006885
Epoch: 50 	Ltrain: 0.006804 	Lval: 0.006885
Epoch 00052: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 55 	Ltrain: 0.006849 	Lval: 0.006871
Epoch: 60 	Ltrain: 0.007200 	Lval: 0.006870
Epoch 00064: reducing learning rate of group 0 to 1.2757e-07.
Epoch: 65 	Ltrain: 0.007414 	Lval: 0.006869
Epoch: 70 	Ltrain: 0.006842 	Lval: 0.006869
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.006871


	Fold 2/5
Epoch: 1 	Ltrain: 0.025625 	Lval: 0.016559
Epoch: 5 	Ltrain: 0.009487 	Lval: 0.008991
Epoch: 10 	Ltrain: 0.007499 	Lval: 0.007506
Epoch: 15 	Ltrain: 0.006992 	Lval: 0.007003
Epoch 00020: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 20 	Ltrain: 0.007023 	Lval: 0.007216
Epoch: 25 	Ltrain: 0.006455 	Lval: 0.006831
Epoch: 30 	Ltrain: 0.006653 	Lval: 0.006801
Epoch 00032: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 35 	Ltrain: 0.006607 	Lval: 0.006780
Epoch: 40 	Ltrain: 0.006401 	Lval: 0.006784
Epoch 00044: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 45 	Ltrain: 0.006410 	Lval: 0.006786
Epoch: 50 	Ltrain: 0.006387 	Lval: 0.006783
Epoch: 55 	Ltrain: 0.006533 	Lval: 0.006781
Epoch 00056: reducing learning rate of group 0 to 1.2757e-07.
Epoch: 60 	Ltrain: 0.006632 	Lval: 0.006781
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.006780


	Fold 3/5
Epoch: 1 	Ltrain: 0.022080 	Lval: 0.015188
Epoch: 5 	Ltrain: 0.007815 	Lval: 0.008679
Epoch: 10 	Ltrain: 0.006692 	Lval: 0.007041
Epoch: 15 	Ltrain: 0.006183 	Lval: 0.006924
Epoch 00018: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 20 	Ltrain: 0.005929 	Lval: 0.006769
Epoch: 25 	Ltrain: 0.005938 	Lval: 0.006772
Epoch: 30 	Ltrain: 0.006013 	Lval: 0.006827
Epoch 00035: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 35 	Ltrain: 0.005913 	Lval: 0.006711
Epoch: 40 	Ltrain: 0.005831 	Lval: 0.006696
Epoch: 45 	Ltrain: 0.005908 	Lval: 0.006695
Epoch 00047: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 50 	Ltrain: 0.005864 	Lval: 0.006695
Epoch: 55 	Ltrain: 0.005861 	Lval: 0.006695
EarlyStopper: stopping at epoch 56 with best_val_loss = 0.006687


	Fold 4/5
Epoch: 1 	Ltrain: 0.021963 	Lval: 0.015396
Epoch: 5 	Ltrain: 0.006754 	Lval: 0.009356
Epoch: 10 	Ltrain: 0.005722 	Lval: 0.006976
Epoch: 15 	Ltrain: 0.005545 	Lval: 0.006955
Epoch: 20 	Ltrain: 0.005545 	Lval: 0.006991
Epoch 00022: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 25 	Ltrain: 0.005227 	Lval: 0.006781
Epoch: 30 	Ltrain: 0.005279 	Lval: 0.006672
Epoch 00034: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 35 	Ltrain: 0.005187 	Lval: 0.006721
Epoch: 40 	Ltrain: 0.005208 	Lval: 0.006714
Epoch: 45 	Ltrain: 0.005199 	Lval: 0.006722
Epoch 00046: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 50 	Ltrain: 0.005195 	Lval: 0.006715
Epoch: 55 	Ltrain: 0.005196 	Lval: 0.006716
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.006672


	Fold 5/5
Epoch: 1 	Ltrain: 0.022879 	Lval: 0.015578
Epoch: 5 	Ltrain: 0.006973 	Lval: 0.008503
Epoch: 10 	Ltrain: 0.005846 	Lval: 0.007529
Epoch 00012: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 15 	Ltrain: 0.005435 	Lval: 0.007052
Epoch: 20 	Ltrain: 0.005433 	Lval: 0.007001
Epoch 00024: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 25 	Ltrain: 0.005383 	Lval: 0.007021
Epoch: 30 	Ltrain: 0.005384 	Lval: 0.007013
Epoch: 35 	Ltrain: 0.005375 	Lval: 0.007046
Epoch 00036: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 40 	Ltrain: 0.005367 	Lval: 0.007021
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.006942

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00030969089415100936
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.4813380065359772e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.123864 	Lval: 0.076091
Epoch: 5 	Ltrain: 0.018511 	Lval: 0.017465
Epoch: 10 	Ltrain: 0.016693 	Lval: 0.016605
Epoch: 15 	Ltrain: 0.016401 	Lval: 0.016260
Epoch: 20 	Ltrain: 0.016323 	Lval: 0.015708
Epoch: 25 	Ltrain: 0.014528 	Lval: 0.014150
Epoch: 30 	Ltrain: 0.012391 	Lval: 0.012459
Epoch: 35 	Ltrain: 0.011028 	Lval: 0.011221
Epoch: 40 	Ltrain: 0.011653 	Lval: 0.010604
Epoch: 45 	Ltrain: 0.010132 	Lval: 0.009947
Epoch: 50 	Ltrain: 0.009836 	Lval: 0.009439
Epoch: 55 	Ltrain: 0.009054 	Lval: 0.008984
Epoch: 60 	Ltrain: 0.009045 	Lval: 0.009021
Epoch: 65 	Ltrain: 0.009049 	Lval: 0.008588
Epoch 00066: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 70 	Ltrain: 0.008494 	Lval: 0.008338
Epoch: 75 	Ltrain: 0.008773 	Lval: 0.008351
Epoch: 80 	Ltrain: 0.008463 	Lval: 0.008281
Epoch 00084: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 85 	Ltrain: 0.008680 	Lval: 0.008287
Epoch: 90 	Ltrain: 0.008819 	Lval: 0.008269
Epoch: 95 	Ltrain: 0.008336 	Lval: 0.008265
Epoch 00096: reducing learning rate of group 0 to 3.0969e-07.
Epoch: 100 	Ltrain: 0.008200 	Lval: 0.008264
Epoch: 105 	Ltrain: 0.007975 	Lval: 0.008265
EarlyStopper: stopping at epoch 105 with best_val_loss = 0.008265


	Fold 2/5
Epoch: 1 	Ltrain: 0.150903 	Lval: 0.084709
Epoch: 5 	Ltrain: 0.017989 	Lval: 0.016582
Epoch: 10 	Ltrain: 0.015944 	Lval: 0.014891
Epoch: 15 	Ltrain: 0.012041 	Lval: 0.011868
Epoch: 20 	Ltrain: 0.010413 	Lval: 0.010323
Epoch: 25 	Ltrain: 0.009416 	Lval: 0.009968
Epoch: 30 	Ltrain: 0.008873 	Lval: 0.008975
Epoch: 35 	Ltrain: 0.008515 	Lval: 0.008634
Epoch: 40 	Ltrain: 0.008082 	Lval: 0.008382
Epoch: 45 	Ltrain: 0.008094 	Lval: 0.008112
Epoch: 50 	Ltrain: 0.007470 	Lval: 0.007867
Epoch: 55 	Ltrain: 0.007647 	Lval: 0.007649
Epoch: 60 	Ltrain: 0.007248 	Lval: 0.007474
Epoch: 65 	Ltrain: 0.007230 	Lval: 0.007375
Epoch: 70 	Ltrain: 0.007063 	Lval: 0.007248
Epoch: 75 	Ltrain: 0.007055 	Lval: 0.007404
Epoch 00078: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 80 	Ltrain: 0.006966 	Lval: 0.007126
Epoch: 85 	Ltrain: 0.006863 	Lval: 0.007131
Epoch 00090: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 90 	Ltrain: 0.006840 	Lval: 0.007109
Epoch: 95 	Ltrain: 0.006760 	Lval: 0.007112
Epoch: 100 	Ltrain: 0.006751 	Lval: 0.007113
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.007108


	Fold 3/5
Epoch: 1 	Ltrain: 0.078444 	Lval: 0.019174
Epoch: 5 	Ltrain: 0.018202 	Lval: 0.016098
Epoch: 10 	Ltrain: 0.011793 	Lval: 0.012157
Epoch: 15 	Ltrain: 0.009405 	Lval: 0.010004
Epoch: 20 	Ltrain: 0.008465 	Lval: 0.008950
Epoch: 25 	Ltrain: 0.007737 	Lval: 0.008280
Epoch: 30 	Ltrain: 0.007148 	Lval: 0.007825
Epoch: 35 	Ltrain: 0.006842 	Lval: 0.007480
Epoch: 40 	Ltrain: 0.006643 	Lval: 0.007435
Epoch: 45 	Ltrain: 0.006390 	Lval: 0.007119
Epoch: 50 	Ltrain: 0.006481 	Lval: 0.007091
Epoch 00053: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 55 	Ltrain: 0.006131 	Lval: 0.007021
Epoch: 60 	Ltrain: 0.006210 	Lval: 0.006991
Epoch: 65 	Ltrain: 0.006161 	Lval: 0.007008
Epoch 00066: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 70 	Ltrain: 0.006122 	Lval: 0.006978
Epoch: 75 	Ltrain: 0.006121 	Lval: 0.006983
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.006967


	Fold 4/5
Epoch: 1 	Ltrain: 0.035277 	Lval: 0.017876
Epoch: 5 	Ltrain: 0.013988 	Lval: 0.015663
Epoch: 10 	Ltrain: 0.009423 	Lval: 0.011813
Epoch: 15 	Ltrain: 0.007849 	Lval: 0.009831
Epoch: 20 	Ltrain: 0.006898 	Lval: 0.008323
Epoch: 25 	Ltrain: 0.006420 	Lval: 0.007866
Epoch: 30 	Ltrain: 0.005974 	Lval: 0.007418
Epoch: 35 	Ltrain: 0.005832 	Lval: 0.007803
Epoch 00038: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 40 	Ltrain: 0.005688 	Lval: 0.007221
Epoch: 45 	Ltrain: 0.005679 	Lval: 0.007231
Epoch: 50 	Ltrain: 0.005649 	Lval: 0.007139
Epoch: 55 	Ltrain: 0.005635 	Lval: 0.007201
Epoch 00057: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 60 	Ltrain: 0.005610 	Lval: 0.007170
Epoch: 65 	Ltrain: 0.005607 	Lval: 0.007170
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.007122


	Fold 5/5
Epoch: 1 	Ltrain: 0.078000 	Lval: 0.021152
Epoch: 5 	Ltrain: 0.012100 	Lval: 0.014150
Epoch: 10 	Ltrain: 0.009154 	Lval: 0.010903
Epoch: 15 	Ltrain: 0.007773 	Lval: 0.009491
Epoch: 20 	Ltrain: 0.006797 	Lval: 0.008360
Epoch: 25 	Ltrain: 0.006288 	Lval: 0.008276
Epoch: 30 	Ltrain: 0.005948 	Lval: 0.007706
Epoch 00031: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 35 	Ltrain: 0.005745 	Lval: 0.007500
Epoch: 40 	Ltrain: 0.005734 	Lval: 0.007491
Epoch 00043: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 45 	Ltrain: 0.005721 	Lval: 0.007508
Epoch: 50 	Ltrain: 0.005699 	Lval: 0.007481
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.007448

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005745115971827479
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.22616303415697e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.052028 	Lval: 0.058112
Epoch: 5 	Ltrain: 0.012867 	Lval: 0.013232
Epoch: 10 	Ltrain: 0.007722 	Lval: 0.008163
Epoch 00015: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 15 	Ltrain: 0.008109 	Lval: 0.008360
Epoch: 20 	Ltrain: 0.007066 	Lval: 0.007116
Epoch: 25 	Ltrain: 0.006990 	Lval: 0.006895
Epoch: 30 	Ltrain: 0.006935 	Lval: 0.006782
Epoch: 35 	Ltrain: 0.006574 	Lval: 0.006708
Epoch: 40 	Ltrain: 0.006632 	Lval: 0.006654
Epoch: 45 	Ltrain: 0.006761 	Lval: 0.006609
Epoch: 50 	Ltrain: 0.007201 	Lval: 0.006532
Epoch: 55 	Ltrain: 0.006526 	Lval: 0.006499
Epoch: 60 	Ltrain: 0.006544 	Lval: 0.006393
Epoch: 65 	Ltrain: 0.006655 	Lval: 0.006387
Epoch: 70 	Ltrain: 0.006503 	Lval: 0.006320
Epoch: 75 	Ltrain: 0.006411 	Lval: 0.006308
Epoch 00076: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 80 	Ltrain: 0.006082 	Lval: 0.006036
Epoch: 85 	Ltrain: 0.005973 	Lval: 0.006026
Epoch: 90 	Ltrain: 0.006273 	Lval: 0.006017
Epoch 00091: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 95 	Ltrain: 0.006207 	Lval: 0.006000
Epoch: 100 	Ltrain: 0.005777 	Lval: 0.005995
Epoch 00103: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 105 	Ltrain: 0.005959 	Lval: 0.005996
Epoch: 110 	Ltrain: 0.006036 	Lval: 0.005996
EarlyStopper: stopping at epoch 109 with best_val_loss = 0.005995


	Fold 2/5
Epoch: 1 	Ltrain: 0.041117 	Lval: 0.013687
Epoch: 5 	Ltrain: 0.008313 	Lval: 0.008256
Epoch: 10 	Ltrain: 0.007274 	Lval: 0.007244
Epoch: 15 	Ltrain: 0.007157 	Lval: 0.006826
Epoch: 20 	Ltrain: 0.006667 	Lval: 0.007421
Epoch 00022: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 25 	Ltrain: 0.005892 	Lval: 0.006101
Epoch: 30 	Ltrain: 0.005781 	Lval: 0.005998
Epoch: 35 	Ltrain: 0.005727 	Lval: 0.005956
Epoch: 40 	Ltrain: 0.005675 	Lval: 0.005868
Epoch: 45 	Ltrain: 0.005612 	Lval: 0.005978
Epoch 00047: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 50 	Ltrain: 0.005651 	Lval: 0.005777
Epoch: 55 	Ltrain: 0.005477 	Lval: 0.005774
Epoch: 60 	Ltrain: 0.005439 	Lval: 0.005775
Epoch: 65 	Ltrain: 0.005557 	Lval: 0.005765
Epoch: 70 	Ltrain: 0.005516 	Lval: 0.005751
Epoch: 75 	Ltrain: 0.005490 	Lval: 0.005726
Epoch: 80 	Ltrain: 0.005644 	Lval: 0.005728
Epoch: 85 	Ltrain: 0.005401 	Lval: 0.005721
Epoch: 90 	Ltrain: 0.005540 	Lval: 0.005709
Epoch 00091: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 95 	Ltrain: 0.005425 	Lval: 0.005709
Epoch: 100 	Ltrain: 0.005436 	Lval: 0.005710
Epoch 00103: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 105 	Ltrain: 0.005510 	Lval: 0.005711
Epoch: 110 	Ltrain: 0.005563 	Lval: 0.005710
EarlyStopper: stopping at epoch 109 with best_val_loss = 0.005709


	Fold 3/5
Epoch: 1 	Ltrain: 0.031457 	Lval: 0.014206
Epoch: 5 	Ltrain: 0.006598 	Lval: 0.007038
Epoch 00009: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 10 	Ltrain: 0.006175 	Lval: 0.006653
Epoch: 15 	Ltrain: 0.005863 	Lval: 0.006812
Epoch: 20 	Ltrain: 0.005808 	Lval: 0.006509
Epoch 00024: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 25 	Ltrain: 0.005769 	Lval: 0.006465
Epoch: 30 	Ltrain: 0.005686 	Lval: 0.006452
Epoch: 35 	Ltrain: 0.005714 	Lval: 0.006450
Epoch: 40 	Ltrain: 0.005729 	Lval: 0.006431
Epoch: 45 	Ltrain: 0.005719 	Lval: 0.006418
Epoch 00046: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 50 	Ltrain: 0.005669 	Lval: 0.006420
Epoch: 55 	Ltrain: 0.005670 	Lval: 0.006418
Epoch 00058: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 60 	Ltrain: 0.005629 	Lval: 0.006415
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.006423


	Fold 4/5
Epoch: 1 	Ltrain: 0.026826 	Lval: 0.011025
Epoch: 5 	Ltrain: 0.005978 	Lval: 0.006909
Epoch 00009: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 10 	Ltrain: 0.005655 	Lval: 0.006801
Epoch: 15 	Ltrain: 0.005198 	Lval: 0.006510
Epoch: 20 	Ltrain: 0.005152 	Lval: 0.006456
Epoch 00022: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 25 	Ltrain: 0.005055 	Lval: 0.006414
Epoch: 30 	Ltrain: 0.005022 	Lval: 0.006385
Epoch: 35 	Ltrain: 0.005013 	Lval: 0.006388
Epoch 00036: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 40 	Ltrain: 0.005022 	Lval: 0.006377
Epoch: 45 	Ltrain: 0.005009 	Lval: 0.006380
Epoch 00048: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 50 	Ltrain: 0.005003 	Lval: 0.006376
Epoch: 55 	Ltrain: 0.005007 	Lval: 0.006376
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.006361


	Fold 5/5
Epoch: 1 	Ltrain: 0.033124 	Lval: 0.015208
Epoch: 5 	Ltrain: 0.006383 	Lval: 0.007437
Epoch: 10 	Ltrain: 0.005969 	Lval: 0.006760
Epoch: 15 	Ltrain: 0.005414 	Lval: 0.006476
Epoch 00019: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 20 	Ltrain: 0.005092 	Lval: 0.006346
Epoch: 25 	Ltrain: 0.004985 	Lval: 0.006416
Epoch: 30 	Ltrain: 0.004961 	Lval: 0.006312
Epoch 00032: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 35 	Ltrain: 0.004824 	Lval: 0.006206
Epoch: 40 	Ltrain: 0.004807 	Lval: 0.006176
Epoch: 45 	Ltrain: 0.004804 	Lval: 0.006160
Epoch 00047: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 50 	Ltrain: 0.004817 	Lval: 0.006159
Epoch: 55 	Ltrain: 0.004799 	Lval: 0.006158
Epoch 00059: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 60 	Ltrain: 0.004790 	Lval: 0.006151
Epoch: 65 	Ltrain: 0.004797 	Lval: 0.006151
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.006145

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0013198186725013358
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.4957300408551598e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 29
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.035893 	Lval: 0.027944
Epoch: 5 	Ltrain: 0.017564 	Lval: 0.016171
Epoch: 10 	Ltrain: 0.016154 	Lval: 0.013658
Epoch: 15 	Ltrain: 0.011805 	Lval: 0.010896
Epoch: 20 	Ltrain: 0.010649 	Lval: 0.008635
Epoch: 25 	Ltrain: 0.008734 	Lval: 0.008766
Epoch: 30 	Ltrain: 0.008877 	Lval: 0.007667
Epoch: 35 	Ltrain: 0.009078 	Lval: 0.008268
Epoch 00038: reducing learning rate of group 0 to 1.3198e-04.
Epoch: 40 	Ltrain: 0.007540 	Lval: 0.007093
Epoch: 45 	Ltrain: 0.007180 	Lval: 0.006996
Epoch 00050: reducing learning rate of group 0 to 1.3198e-05.
Epoch: 50 	Ltrain: 0.006648 	Lval: 0.007019
Epoch: 55 	Ltrain: 0.007676 	Lval: 0.006997
Epoch: 60 	Ltrain: 0.007137 	Lval: 0.006987
Epoch 00062: reducing learning rate of group 0 to 1.3198e-06.
Epoch: 65 	Ltrain: 0.006880 	Lval: 0.006982
Epoch: 70 	Ltrain: 0.006988 	Lval: 0.006983
Epoch 00074: reducing learning rate of group 0 to 1.3198e-07.
Epoch: 75 	Ltrain: 0.006715 	Lval: 0.006988
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.006958


	Fold 2/5
Epoch: 1 	Ltrain: 0.025243 	Lval: 0.018586
Epoch: 5 	Ltrain: 0.015103 	Lval: 0.013734
Epoch: 10 	Ltrain: 0.009325 	Lval: 0.008715
Epoch: 15 	Ltrain: 0.007614 	Lval: 0.007537
Epoch: 20 	Ltrain: 0.007151 	Lval: 0.007269
Epoch 00024: reducing learning rate of group 0 to 1.3198e-04.
Epoch: 25 	Ltrain: 0.007179 	Lval: 0.006959
Epoch: 30 	Ltrain: 0.006557 	Lval: 0.006840
Epoch: 35 	Ltrain: 0.006673 	Lval: 0.006830
Epoch 00040: reducing learning rate of group 0 to 1.3198e-05.
Epoch: 40 	Ltrain: 0.006454 	Lval: 0.006849
Epoch: 45 	Ltrain: 0.006583 	Lval: 0.006786
Epoch: 50 	Ltrain: 0.006492 	Lval: 0.006789
Epoch 00052: reducing learning rate of group 0 to 1.3198e-06.
Epoch: 55 	Ltrain: 0.006614 	Lval: 0.006800
Epoch: 60 	Ltrain: 0.006519 	Lval: 0.006799
Epoch 00064: reducing learning rate of group 0 to 1.3198e-07.
Epoch: 65 	Ltrain: 0.006563 	Lval: 0.006795
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.006776


	Fold 3/5
Epoch: 1 	Ltrain: 0.028845 	Lval: 0.017518
Epoch: 5 	Ltrain: 0.013147 	Lval: 0.012128
Epoch: 10 	Ltrain: 0.007694 	Lval: 0.008325
Epoch: 15 	Ltrain: 0.006856 	Lval: 0.007716
Epoch: 20 	Ltrain: 0.006422 	Lval: 0.007719
Epoch: 25 	Ltrain: 0.006219 	Lval: 0.006759
Epoch: 30 	Ltrain: 0.006292 	Lval: 0.007084
Epoch 00032: reducing learning rate of group 0 to 1.3198e-04.
Epoch: 35 	Ltrain: 0.005894 	Lval: 0.006505
Epoch: 40 	Ltrain: 0.005832 	Lval: 0.006450
Epoch: 45 	Ltrain: 0.006046 	Lval: 0.006436
Epoch: 50 	Ltrain: 0.005899 	Lval: 0.006410
Epoch: 55 	Ltrain: 0.005636 	Lval: 0.006404
Epoch: 60 	Ltrain: 0.005807 	Lval: 0.006393
Epoch 00062: reducing learning rate of group 0 to 1.3198e-05.
Epoch: 65 	Ltrain: 0.005633 	Lval: 0.006360
Epoch: 70 	Ltrain: 0.005717 	Lval: 0.006359
Epoch: 75 	Ltrain: 0.005894 	Lval: 0.006357
Epoch 00077: reducing learning rate of group 0 to 1.3198e-06.
Epoch: 80 	Ltrain: 0.005674 	Lval: 0.006358
Epoch: 85 	Ltrain: 0.005627 	Lval: 0.006358
Epoch 00089: reducing learning rate of group 0 to 1.3198e-07.
Epoch: 90 	Ltrain: 0.005882 	Lval: 0.006357
Epoch: 95 	Ltrain: 0.005724 	Lval: 0.006357
Epoch: 100 	Ltrain: 0.005633 	Lval: 0.006357
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.006358


	Fold 4/5
Epoch: 1 	Ltrain: 0.027684 	Lval: 0.018560
Epoch: 5 	Ltrain: 0.009486 	Lval: 0.011299
Epoch: 10 	Ltrain: 0.006697 	Lval: 0.007900
Epoch: 15 	Ltrain: 0.005729 	Lval: 0.006994
Epoch: 20 	Ltrain: 0.005632 	Lval: 0.007020
Epoch: 25 	Ltrain: 0.005544 	Lval: 0.006647
Epoch: 30 	Ltrain: 0.005322 	Lval: 0.006361
Epoch: 35 	Ltrain: 0.005045 	Lval: 0.006185
Epoch: 40 	Ltrain: 0.005122 	Lval: 0.006103
Epoch: 45 	Ltrain: 0.005003 	Lval: 0.005933
Epoch: 50 	Ltrain: 0.004858 	Lval: 0.005989
Epoch 00053: reducing learning rate of group 0 to 1.3198e-04.
Epoch: 55 	Ltrain: 0.004613 	Lval: 0.005702
Epoch: 60 	Ltrain: 0.004622 	Lval: 0.005677
Epoch: 65 	Ltrain: 0.004607 	Lval: 0.005666
Epoch: 70 	Ltrain: 0.004526 	Lval: 0.005642
Epoch: 75 	Ltrain: 0.004569 	Lval: 0.005637
Epoch: 80 	Ltrain: 0.004643 	Lval: 0.005602
Epoch: 85 	Ltrain: 0.004560 	Lval: 0.005615
Epoch: 90 	Ltrain: 0.004562 	Lval: 0.005563
Epoch: 95 	Ltrain: 0.004521 	Lval: 0.005550
Epoch: 100 	Ltrain: 0.004556 	Lval: 0.005524
Epoch: 105 	Ltrain: 0.004423 	Lval: 0.005518
Epoch 00108: reducing learning rate of group 0 to 1.3198e-05.
Epoch: 110 	Ltrain: 0.004467 	Lval: 0.005486
Epoch: 115 	Ltrain: 0.004464 	Lval: 0.005483
Epoch: 120 	Ltrain: 0.004414 	Lval: 0.005479
Epoch 00122: reducing learning rate of group 0 to 1.3198e-06.
Epoch: 125 	Ltrain: 0.004376 	Lval: 0.005476
Epoch: 130 	Ltrain: 0.004412 	Lval: 0.005476
EarlyStopper: stopping at epoch 132 with best_val_loss = 0.005483


	Fold 5/5
Epoch: 1 	Ltrain: 0.029956 	Lval: 0.019371
Epoch: 5 	Ltrain: 0.009744 	Lval: 0.010664
Epoch: 10 	Ltrain: 0.006097 	Lval: 0.007886
Epoch: 15 	Ltrain: 0.006073 	Lval: 0.007134
Epoch: 20 	Ltrain: 0.005583 	Lval: 0.007223
Epoch 00022: reducing learning rate of group 0 to 1.3198e-04.
Epoch: 25 	Ltrain: 0.005282 	Lval: 0.006885
Epoch: 30 	Ltrain: 0.005266 	Lval: 0.006952
Epoch 00034: reducing learning rate of group 0 to 1.3198e-05.
Epoch: 35 	Ltrain: 0.005298 	Lval: 0.006839
Epoch: 40 	Ltrain: 0.005270 	Lval: 0.006835
Epoch: 45 	Ltrain: 0.005262 	Lval: 0.006826
Epoch 00046: reducing learning rate of group 0 to 1.3198e-06.
Epoch: 50 	Ltrain: 0.005241 	Lval: 0.006838
Epoch: 55 	Ltrain: 0.005146 	Lval: 0.006841
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.006805

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009970611084534795
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.2977727788740128e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 28
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.180960 	Lval: 0.020465
Epoch: 5 	Ltrain: 0.009557 	Lval: 0.008702
Epoch: 10 	Ltrain: 0.008259 	Lval: 0.008555
Epoch 00011: reducing learning rate of group 0 to 9.9706e-04.
Epoch: 15 	Ltrain: 0.007073 	Lval: 0.006928
Epoch: 20 	Ltrain: 0.007181 	Lval: 0.006694
Epoch: 25 	Ltrain: 0.006704 	Lval: 0.006633
Epoch: 30 	Ltrain: 0.006676 	Lval: 0.006418
Epoch: 35 	Ltrain: 0.006423 	Lval: 0.006320
Epoch: 40 	Ltrain: 0.006239 	Lval: 0.006214
Epoch: 45 	Ltrain: 0.006872 	Lval: 0.006107
Epoch: 50 	Ltrain: 0.005983 	Lval: 0.005985
Epoch: 55 	Ltrain: 0.006213 	Lval: 0.006148
Epoch: 60 	Ltrain: 0.006513 	Lval: 0.005914
Epoch: 65 	Ltrain: 0.005821 	Lval: 0.005592
Epoch: 70 	Ltrain: 0.005481 	Lval: 0.005387
Epoch: 75 	Ltrain: 0.005401 	Lval: 0.005872
Epoch: 80 	Ltrain: 0.005030 	Lval: 0.004911
Epoch: 85 	Ltrain: 0.004688 	Lval: 0.004682
Epoch: 90 	Ltrain: 0.004896 	Lval: 0.004871
Epoch 00092: reducing learning rate of group 0 to 9.9706e-05.
Epoch: 95 	Ltrain: 0.004300 	Lval: 0.004409
Epoch: 100 	Ltrain: 0.004268 	Lval: 0.004352
Epoch: 105 	Ltrain: 0.004293 	Lval: 0.004312
Epoch: 110 	Ltrain: 0.004375 	Lval: 0.004278
Epoch: 115 	Ltrain: 0.004240 	Lval: 0.004258
Epoch: 120 	Ltrain: 0.004247 	Lval: 0.004235
Epoch: 125 	Ltrain: 0.004093 	Lval: 0.004221
Epoch: 130 	Ltrain: 0.004088 	Lval: 0.004184
Epoch: 135 	Ltrain: 0.004165 	Lval: 0.004170
Epoch: 140 	Ltrain: 0.003969 	Lval: 0.004118
Epoch: 145 	Ltrain: 0.003902 	Lval: 0.004114
Epoch: 150 	Ltrain: 0.003941 	Lval: 0.004038
Epoch: 155 	Ltrain: 0.004193 	Lval: 0.004008
Epoch: 160 	Ltrain: 0.003899 	Lval: 0.003988
Epoch: 165 	Ltrain: 0.003961 	Lval: 0.003963
Epoch: 170 	Ltrain: 0.003885 	Lval: 0.003924
Epoch: 175 	Ltrain: 0.003842 	Lval: 0.003919
Epoch: 180 	Ltrain: 0.003810 	Lval: 0.003870
Epoch: 185 	Ltrain: 0.003922 	Lval: 0.003841
Epoch: 190 	Ltrain: 0.003718 	Lval: 0.003807
Epoch: 195 	Ltrain: 0.003952 	Lval: 0.003765
Epoch: 200 	Ltrain: 0.003574 	Lval: 0.003756
Epoch: 205 	Ltrain: 0.003512 	Lval: 0.003709
Epoch 00210: reducing learning rate of group 0 to 9.9706e-06.
Epoch: 210 	Ltrain: 0.003675 	Lval: 0.003698
Epoch: 215 	Ltrain: 0.003518 	Lval: 0.003673
Epoch: 220 	Ltrain: 0.003535 	Lval: 0.003665
Epoch: 225 	Ltrain: 0.003458 	Lval: 0.003660
Epoch 00229: reducing learning rate of group 0 to 9.9706e-07.
Epoch: 230 	Ltrain: 0.003477 	Lval: 0.003660
Epoch: 235 	Ltrain: 0.003781 	Lval: 0.003659
Epoch: 240 	Ltrain: 0.003682 	Lval: 0.003659
Epoch 00241: reducing learning rate of group 0 to 9.9706e-08.
Epoch: 245 	Ltrain: 0.003579 	Lval: 0.003659
Epoch: 250 	Ltrain: 0.003511 	Lval: 0.003659
Epoch 00253: reducing learning rate of group 0 to 9.9706e-09.
Epoch: 255 	Ltrain: 0.003569 	Lval: 0.003659
Epoch: 260 	Ltrain: 0.003792 	Lval: 0.003659
EarlyStopper: stopping at epoch 259 with best_val_loss = 0.003659


	Fold 2/5
Epoch: 1 	Ltrain: 0.091802 	Lval: 0.017138
Epoch: 5 	Ltrain: 0.008578 	Lval: 0.011880
Epoch 00007: reducing learning rate of group 0 to 9.9706e-04.
Epoch: 10 	Ltrain: 0.006441 	Lval: 0.006882
Epoch: 15 	Ltrain: 0.006243 	Lval: 0.006604
Epoch: 20 	Ltrain: 0.006152 	Lval: 0.006577
Epoch: 25 	Ltrain: 0.006123 	Lval: 0.006376
Epoch: 30 	Ltrain: 0.006010 	Lval: 0.006240
Epoch: 35 	Ltrain: 0.005885 	Lval: 0.006134
Epoch: 40 	Ltrain: 0.005867 	Lval: 0.006052
Epoch: 45 	Ltrain: 0.005672 	Lval: 0.005985
Epoch: 50 	Ltrain: 0.005616 	Lval: 0.005790
Epoch: 55 	Ltrain: 0.005316 	Lval: 0.005601
Epoch: 60 	Ltrain: 0.005226 	Lval: 0.005394
Epoch: 65 	Ltrain: 0.005088 	Lval: 0.005149
Epoch: 70 	Ltrain: 0.004867 	Lval: 0.005085
Epoch: 75 	Ltrain: 0.004729 	Lval: 0.004793
Epoch: 80 	Ltrain: 0.004530 	Lval: 0.004543
Epoch 00085: reducing learning rate of group 0 to 9.9706e-05.
Epoch: 85 	Ltrain: 0.004381 	Lval: 0.004582
Epoch: 90 	Ltrain: 0.003980 	Lval: 0.004140
Epoch: 95 	Ltrain: 0.003988 	Lval: 0.004079
Epoch: 100 	Ltrain: 0.003911 	Lval: 0.004040
Epoch: 105 	Ltrain: 0.003940 	Lval: 0.004009
Epoch: 110 	Ltrain: 0.003892 	Lval: 0.003984
Epoch: 115 	Ltrain: 0.003890 	Lval: 0.003954
Epoch: 120 	Ltrain: 0.003841 	Lval: 0.003917
Epoch: 125 	Ltrain: 0.003787 	Lval: 0.003874
Epoch: 130 	Ltrain: 0.003821 	Lval: 0.003852
Epoch: 135 	Ltrain: 0.003787 	Lval: 0.003803
Epoch: 140 	Ltrain: 0.003692 	Lval: 0.003794
Epoch: 145 	Ltrain: 0.003735 	Lval: 0.003762
Epoch: 150 	Ltrain: 0.003690 	Lval: 0.003703
Epoch: 155 	Ltrain: 0.003616 	Lval: 0.003674
Epoch: 160 	Ltrain: 0.003601 	Lval: 0.003624
Epoch: 165 	Ltrain: 0.003513 	Lval: 0.003583
Epoch: 170 	Ltrain: 0.003509 	Lval: 0.003568
Epoch: 175 	Ltrain: 0.003505 	Lval: 0.003508
Epoch: 180 	Ltrain: 0.003526 	Lval: 0.003491
Epoch: 185 	Ltrain: 0.003414 	Lval: 0.003431
Epoch: 190 	Ltrain: 0.003353 	Lval: 0.003388
Epoch: 195 	Ltrain: 0.003327 	Lval: 0.003341
Epoch: 200 	Ltrain: 0.003332 	Lval: 0.003277
Epoch: 205 	Ltrain: 0.003273 	Lval: 0.003273
Epoch: 210 	Ltrain: 0.003239 	Lval: 0.003217
Epoch: 215 	Ltrain: 0.003185 	Lval: 0.003164
Epoch: 220 	Ltrain: 0.003149 	Lval: 0.003133
Epoch: 225 	Ltrain: 0.003069 	Lval: 0.003072
Epoch: 230 	Ltrain: 0.003068 	Lval: 0.003049
Epoch: 235 	Ltrain: 0.003036 	Lval: 0.003012
Epoch 00236: reducing learning rate of group 0 to 9.9706e-06.
Epoch: 240 	Ltrain: 0.002931 	Lval: 0.002944
Epoch: 245 	Ltrain: 0.002913 	Lval: 0.002934
Epoch: 250 	Ltrain: 0.002970 	Lval: 0.002930
Epoch: 255 	Ltrain: 0.002982 	Lval: 0.002924
Epoch: 260 	Ltrain: 0.002925 	Lval: 0.002918
Epoch: 265 	Ltrain: 0.002887 	Lval: 0.002912
Epoch: 270 	Ltrain: 0.002924 	Lval: 0.002905
Epoch: 275 	Ltrain: 0.002923 	Lval: 0.002902
Epoch: 280 	Ltrain: 0.002962 	Lval: 0.002897
Epoch: 285 	Ltrain: 0.002909 	Lval: 0.002892
Epoch: 290 	Ltrain: 0.002905 	Lval: 0.002884
Epoch: 295 	Ltrain: 0.002917 	Lval: 0.002878
Epoch: 300 	Ltrain: 0.002871 	Lval: 0.002874
Epoch: 305 	Ltrain: 0.002878 	Lval: 0.002868
Epoch: 310 	Ltrain: 0.002865 	Lval: 0.002861
Epoch: 315 	Ltrain: 0.002872 	Lval: 0.002856
Epoch: 320 	Ltrain: 0.002845 	Lval: 0.002851
Epoch: 325 	Ltrain: 0.002871 	Lval: 0.002846
Epoch: 330 	Ltrain: 0.002888 	Lval: 0.002839
Epoch: 335 	Ltrain: 0.002893 	Lval: 0.002834
Epoch: 340 	Ltrain: 0.002851 	Lval: 0.002826
Epoch: 345 	Ltrain: 0.002839 	Lval: 0.002821
Epoch: 350 	Ltrain: 0.002823 	Lval: 0.002814
Epoch: 355 	Ltrain: 0.002863 	Lval: 0.002808
Epoch: 360 	Ltrain: 0.002823 	Lval: 0.002799
Epoch: 365 	Ltrain: 0.002818 	Lval: 0.002794
Epoch: 370 	Ltrain: 0.002806 	Lval: 0.002788
Epoch: 375 	Ltrain: 0.002806 	Lval: 0.002783
Epoch: 380 	Ltrain: 0.002777 	Lval: 0.002776
Epoch: 385 	Ltrain: 0.002767 	Lval: 0.002771
Epoch: 390 	Ltrain: 0.002779 	Lval: 0.002764
Epoch: 395 	Ltrain: 0.002852 	Lval: 0.002759
Epoch: 400 	Ltrain: 0.002776 	Lval: 0.002748
Epoch: 405 	Ltrain: 0.002733 	Lval: 0.002744
Epoch: 410 	Ltrain: 0.002736 	Lval: 0.002740
Epoch: 415 	Ltrain: 0.002726 	Lval: 0.002730
Epoch: 420 	Ltrain: 0.002755 	Lval: 0.002726
Epoch: 425 	Ltrain: 0.002800 	Lval: 0.002715
Epoch: 430 	Ltrain: 0.002726 	Lval: 0.002712
Epoch: 435 	Ltrain: 0.002771 	Lval: 0.002704
Epoch: 440 	Ltrain: 0.002720 	Lval: 0.002697
Epoch: 445 	Ltrain: 0.002738 	Lval: 0.002688
Epoch: 450 	Ltrain: 0.002731 	Lval: 0.002682
Epoch: 455 	Ltrain: 0.002677 	Lval: 0.002678
Epoch: 460 	Ltrain: 0.002694 	Lval: 0.002670
Epoch: 465 	Ltrain: 0.002702 	Lval: 0.002663
Epoch: 470 	Ltrain: 0.002664 	Lval: 0.002659
Epoch: 475 	Ltrain: 0.002710 	Lval: 0.002650
Epoch: 480 	Ltrain: 0.002656 	Lval: 0.002645
Epoch: 485 	Ltrain: 0.002638 	Lval: 0.002636
Epoch: 490 	Ltrain: 0.002672 	Lval: 0.002628
Epoch: 495 	Ltrain: 0.002623 	Lval: 0.002619
Epoch: 500 	Ltrain: 0.002633 	Lval: 0.002614
Epoch: 505 	Ltrain: 0.002631 	Lval: 0.002612
Epoch: 510 	Ltrain: 0.002662 	Lval: 0.002603
Epoch: 515 	Ltrain: 0.002631 	Lval: 0.002595
Epoch: 520 	Ltrain: 0.002611 	Lval: 0.002586
Epoch: 525 	Ltrain: 0.002668 	Lval: 0.002580
Epoch: 530 	Ltrain: 0.002707 	Lval: 0.002574
Epoch: 535 	Ltrain: 0.002615 	Lval: 0.002569
Epoch: 540 	Ltrain: 0.002604 	Lval: 0.002560
Epoch: 545 	Ltrain: 0.002674 	Lval: 0.002551
Epoch: 550 	Ltrain: 0.002618 	Lval: 0.002545
Epoch: 555 	Ltrain: 0.002618 	Lval: 0.002540
Epoch: 560 	Ltrain: 0.002591 	Lval: 0.002535
Epoch: 565 	Ltrain: 0.002577 	Lval: 0.002524
Epoch: 570 	Ltrain: 0.002568 	Lval: 0.002519
Epoch: 575 	Ltrain: 0.002568 	Lval: 0.002515
Epoch: 580 	Ltrain: 0.002569 	Lval: 0.002506
Epoch: 585 	Ltrain: 0.002541 	Lval: 0.002496
Epoch: 590 	Ltrain: 0.002536 	Lval: 0.002493
Epoch: 595 	Ltrain: 0.002515 	Lval: 0.002484
Epoch: 600 	Ltrain: 0.002550 	Lval: 0.002477
Epoch: 605 	Ltrain: 0.002486 	Lval: 0.002473
Epoch: 610 	Ltrain: 0.002500 	Lval: 0.002465
Epoch: 615 	Ltrain: 0.002509 	Lval: 0.002458
Epoch: 620 	Ltrain: 0.002533 	Lval: 0.002451
Epoch: 625 	Ltrain: 0.002490 	Lval: 0.002447
Epoch: 630 	Ltrain: 0.002504 	Lval: 0.002437
Epoch: 635 	Ltrain: 0.002504 	Lval: 0.002429
Epoch: 640 	Ltrain: 0.002506 	Lval: 0.002424
Epoch: 645 	Ltrain: 0.002494 	Lval: 0.002418
Epoch: 650 	Ltrain: 0.002463 	Lval: 0.002412
Epoch: 655 	Ltrain: 0.002468 	Lval: 0.002402
Epoch: 660 	Ltrain: 0.002456 	Lval: 0.002398
Epoch: 665 	Ltrain: 0.002479 	Lval: 0.002394
Epoch: 670 	Ltrain: 0.002461 	Lval: 0.002383
Epoch: 675 	Ltrain: 0.002438 	Lval: 0.002377
Epoch: 680 	Ltrain: 0.002427 	Lval: 0.002370
Epoch: 685 	Ltrain: 0.002436 	Lval: 0.002363
Epoch: 690 	Ltrain: 0.002404 	Lval: 0.002358
Epoch: 695 	Ltrain: 0.002408 	Lval: 0.002351
Epoch: 700 	Ltrain: 0.002428 	Lval: 0.002347
Epoch: 705 	Ltrain: 0.002386 	Lval: 0.002337
Epoch: 710 	Ltrain: 0.002368 	Lval: 0.002331
Epoch: 715 	Ltrain: 0.002386 	Lval: 0.002326
Epoch: 720 	Ltrain: 0.002393 	Lval: 0.002318
Epoch: 725 	Ltrain: 0.002372 	Lval: 0.002310
Epoch: 730 	Ltrain: 0.002370 	Lval: 0.002306
Epoch: 735 	Ltrain: 0.002342 	Lval: 0.002299
Epoch: 740 	Ltrain: 0.002350 	Lval: 0.002291
Epoch: 745 	Ltrain: 0.002362 	Lval: 0.002288
Epoch: 750 	Ltrain: 0.002333 	Lval: 0.002281
Epoch: 755 	Ltrain: 0.002418 	Lval: 0.002274
Epoch: 760 	Ltrain: 0.002321 	Lval: 0.002266
Epoch: 765 	Ltrain: 0.002357 	Lval: 0.002259
Epoch: 770 	Ltrain: 0.002302 	Lval: 0.002255
Epoch: 775 	Ltrain: 0.002319 	Lval: 0.002254
Epoch: 780 	Ltrain: 0.002301 	Lval: 0.002243
Epoch: 785 	Ltrain: 0.002314 	Lval: 0.002242
Epoch: 790 	Ltrain: 0.002333 	Lval: 0.002228
Epoch: 795 	Ltrain: 0.002311 	Lval: 0.002225
Epoch 00800: reducing learning rate of group 0 to 9.9706e-07.
Epoch: 800 	Ltrain: 0.002261 	Lval: 0.002220
Epoch: 805 	Ltrain: 0.002258 	Lval: 0.002214
Epoch: 810 	Ltrain: 0.002281 	Lval: 0.002213
Epoch 00812: reducing learning rate of group 0 to 9.9706e-08.
Epoch: 815 	Ltrain: 0.002258 	Lval: 0.002213
Epoch: 820 	Ltrain: 0.002273 	Lval: 0.002213
Epoch 00824: reducing learning rate of group 0 to 9.9706e-09.
EarlyStopper: stopping at epoch 823 with best_val_loss = 0.002219


	Fold 3/5
Epoch: 1 	Ltrain: 0.107814 	Lval: 0.013992
Epoch: 5 	Ltrain: 0.006941 	Lval: 0.007323
Epoch: 10 	Ltrain: 0.006318 	Lval: 0.006634
Epoch: 15 	Ltrain: 0.005723 	Lval: 0.006276
Epoch 00017: reducing learning rate of group 0 to 9.9706e-04.
Epoch: 20 	Ltrain: 0.005178 	Lval: 0.005796
Epoch: 25 	Ltrain: 0.004988 	Lval: 0.005638
Epoch: 30 	Ltrain: 0.004902 	Lval: 0.005538
Epoch: 35 	Ltrain: 0.004844 	Lval: 0.005452
Epoch: 40 	Ltrain: 0.004759 	Lval: 0.005335
Epoch: 45 	Ltrain: 0.004697 	Lval: 0.005513
Epoch: 50 	Ltrain: 0.004585 	Lval: 0.005142
Epoch: 55 	Ltrain: 0.004495 	Lval: 0.004929
Epoch: 60 	Ltrain: 0.004184 	Lval: 0.004740
Epoch: 65 	Ltrain: 0.003983 	Lval: 0.004309
Epoch: 70 	Ltrain: 0.003857 	Lval: 0.004048
Epoch: 75 	Ltrain: 0.003774 	Lval: 0.004168
Epoch: 80 	Ltrain: 0.003390 	Lval: 0.003653
Epoch: 85 	Ltrain: 0.003111 	Lval: 0.003401
Epoch: 90 	Ltrain: 0.003064 	Lval: 0.003179
Epoch: 95 	Ltrain: 0.002798 	Lval: 0.002933
Epoch: 100 	Ltrain: 0.002600 	Lval: 0.002671
Epoch: 105 	Ltrain: 0.002368 	Lval: 0.002674
Epoch: 110 	Ltrain: 0.002367 	Lval: 0.002401
Epoch: 115 	Ltrain: 0.002067 	Lval: 0.002127
Epoch 00120: reducing learning rate of group 0 to 9.9706e-05.
Epoch: 120 	Ltrain: 0.002005 	Lval: 0.002159
Epoch: 125 	Ltrain: 0.001557 	Lval: 0.001671
Epoch: 130 	Ltrain: 0.001515 	Lval: 0.001615
Epoch: 135 	Ltrain: 0.001484 	Lval: 0.001577
Epoch: 140 	Ltrain: 0.001452 	Lval: 0.001550
Epoch: 145 	Ltrain: 0.001437 	Lval: 0.001528
Epoch: 150 	Ltrain: 0.001415 	Lval: 0.001498
Epoch: 155 	Ltrain: 0.001394 	Lval: 0.001471
Epoch: 160 	Ltrain: 0.001362 	Lval: 0.001442
Epoch: 165 	Ltrain: 0.001340 	Lval: 0.001419
Epoch: 170 	Ltrain: 0.001314 	Lval: 0.001390
Epoch: 175 	Ltrain: 0.001288 	Lval: 0.001363
Epoch: 180 	Ltrain: 0.001265 	Lval: 0.001336
Epoch: 185 	Ltrain: 0.001239 	Lval: 0.001314
Epoch: 190 	Ltrain: 0.001212 	Lval: 0.001280
Epoch: 195 	Ltrain: 0.001174 	Lval: 0.001254
Epoch: 200 	Ltrain: 0.001161 	Lval: 0.001224
Epoch: 205 	Ltrain: 0.001130 	Lval: 0.001195
Epoch: 210 	Ltrain: 0.001108 	Lval: 0.001172
Epoch: 215 	Ltrain: 0.001078 	Lval: 0.001137
Epoch: 220 	Ltrain: 0.001054 	Lval: 0.001117
Epoch: 225 	Ltrain: 0.001023 	Lval: 0.001083
Epoch: 230 	Ltrain: 0.001003 	Lval: 0.001050
Epoch: 235 	Ltrain: 0.000970 	Lval: 0.001023
Epoch: 240 	Ltrain: 0.000948 	Lval: 0.001005
Epoch: 245 	Ltrain: 0.000923 	Lval: 0.000973
Epoch: 250 	Ltrain: 0.000898 	Lval: 0.000950
Epoch: 255 	Ltrain: 0.000888 	Lval: 0.000910
Epoch: 260 	Ltrain: 0.000852 	Lval: 0.000898
Epoch: 265 	Ltrain: 0.000845 	Lval: 0.000870
Epoch: 270 	Ltrain: 0.000800 	Lval: 0.000844
Epoch: 275 	Ltrain: 0.000792 	Lval: 0.000824
Epoch: 280 	Ltrain: 0.000776 	Lval: 0.000818
Epoch: 285 	Ltrain: 0.000753 	Lval: 0.000785
Epoch: 290 	Ltrain: 0.000730 	Lval: 0.000763
Epoch: 295 	Ltrain: 0.000720 	Lval: 0.000746
Epoch: 300 	Ltrain: 0.000703 	Lval: 0.000719
Epoch: 305 	Ltrain: 0.000679 	Lval: 0.000701
Epoch: 310 	Ltrain: 0.000676 	Lval: 0.000708
Epoch 00313: reducing learning rate of group 0 to 9.9706e-06.
Epoch: 315 	Ltrain: 0.000630 	Lval: 0.000654
Epoch: 320 	Ltrain: 0.000621 	Lval: 0.000642
Epoch: 325 	Ltrain: 0.000616 	Lval: 0.000639
Epoch: 330 	Ltrain: 0.000615 	Lval: 0.000636
Epoch: 335 	Ltrain: 0.000610 	Lval: 0.000633
Epoch: 340 	Ltrain: 0.000607 	Lval: 0.000631
Epoch: 345 	Ltrain: 0.000607 	Lval: 0.000628
Epoch: 350 	Ltrain: 0.000605 	Lval: 0.000626
Epoch: 355 	Ltrain: 0.000600 	Lval: 0.000623
Epoch: 360 	Ltrain: 0.000598 	Lval: 0.000621
Epoch: 365 	Ltrain: 0.000595 	Lval: 0.000618
Epoch: 370 	Ltrain: 0.000594 	Lval: 0.000616
Epoch: 375 	Ltrain: 0.000596 	Lval: 0.000613
Epoch: 380 	Ltrain: 0.000593 	Lval: 0.000610
Epoch: 385 	Ltrain: 0.000586 	Lval: 0.000607
Epoch: 390 	Ltrain: 0.000583 	Lval: 0.000604
Epoch: 395 	Ltrain: 0.000582 	Lval: 0.000601
Epoch: 400 	Ltrain: 0.000579 	Lval: 0.000599
Epoch: 405 	Ltrain: 0.000577 	Lval: 0.000596
Epoch: 410 	Ltrain: 0.000574 	Lval: 0.000592
Epoch: 415 	Ltrain: 0.000571 	Lval: 0.000590
Epoch: 420 	Ltrain: 0.000570 	Lval: 0.000586
Epoch: 425 	Ltrain: 0.000564 	Lval: 0.000583
Epoch: 430 	Ltrain: 0.000564 	Lval: 0.000580
Epoch: 435 	Ltrain: 0.000560 	Lval: 0.000578
Epoch: 440 	Ltrain: 0.000557 	Lval: 0.000574
Epoch: 445 	Ltrain: 0.000557 	Lval: 0.000570
Epoch: 450 	Ltrain: 0.000551 	Lval: 0.000568
Epoch: 455 	Ltrain: 0.000546 	Lval: 0.000564
Epoch: 460 	Ltrain: 0.000547 	Lval: 0.000561
Epoch: 465 	Ltrain: 0.000539 	Lval: 0.000558
Epoch: 470 	Ltrain: 0.000539 	Lval: 0.000555
Epoch: 475 	Ltrain: 0.000535 	Lval: 0.000552
Epoch: 480 	Ltrain: 0.000533 	Lval: 0.000548
Epoch: 485 	Ltrain: 0.000528 	Lval: 0.000545
Epoch: 490 	Ltrain: 0.000527 	Lval: 0.000542
Epoch: 495 	Ltrain: 0.000524 	Lval: 0.000539
Epoch: 500 	Ltrain: 0.000519 	Lval: 0.000535
Epoch: 505 	Ltrain: 0.000517 	Lval: 0.000532
Epoch: 510 	Ltrain: 0.000512 	Lval: 0.000529
Epoch: 515 	Ltrain: 0.000510 	Lval: 0.000525
Epoch: 520 	Ltrain: 0.000509 	Lval: 0.000522
Epoch: 525 	Ltrain: 0.000506 	Lval: 0.000520
Epoch: 530 	Ltrain: 0.000516 	Lval: 0.000516
Epoch: 535 	Ltrain: 0.000498 	Lval: 0.000513
Epoch: 540 	Ltrain: 0.000502 	Lval: 0.000510
Epoch: 545 	Ltrain: 0.000493 	Lval: 0.000507
Epoch: 550 	Ltrain: 0.000489 	Lval: 0.000503
Epoch: 555 	Ltrain: 0.000501 	Lval: 0.000501
Epoch: 560 	Ltrain: 0.000504 	Lval: 0.000497
Epoch: 565 	Ltrain: 0.000482 	Lval: 0.000494
Epoch: 570 	Ltrain: 0.000477 	Lval: 0.000491
Epoch: 575 	Ltrain: 0.000475 	Lval: 0.000488
Epoch: 580 	Ltrain: 0.000473 	Lval: 0.000485
Epoch: 585 	Ltrain: 0.000470 	Lval: 0.000482
Epoch: 590 	Ltrain: 0.000469 	Lval: 0.000479
Epoch: 595 	Ltrain: 0.000464 	Lval: 0.000476
Epoch: 600 	Ltrain: 0.000460 	Lval: 0.000473
Epoch: 605 	Ltrain: 0.000458 	Lval: 0.000470
Epoch: 610 	Ltrain: 0.000456 	Lval: 0.000467
Epoch: 615 	Ltrain: 0.000454 	Lval: 0.000464
Epoch: 620 	Ltrain: 0.000451 	Lval: 0.000462
Epoch: 625 	Ltrain: 0.000448 	Lval: 0.000458
Epoch: 630 	Ltrain: 0.000445 	Lval: 0.000455
Epoch: 635 	Ltrain: 0.000440 	Lval: 0.000452
Epoch: 640 	Ltrain: 0.000442 	Lval: 0.000449
Epoch: 645 	Ltrain: 0.000435 	Lval: 0.000447
Epoch: 650 	Ltrain: 0.000433 	Lval: 0.000444
Epoch: 655 	Ltrain: 0.000432 	Lval: 0.000441
Epoch: 660 	Ltrain: 0.000429 	Lval: 0.000439
Epoch: 665 	Ltrain: 0.000438 	Lval: 0.000435
Epoch: 670 	Ltrain: 0.000425 	Lval: 0.000433
Epoch: 675 	Ltrain: 0.000418 	Lval: 0.000430
Epoch: 680 	Ltrain: 0.000417 	Lval: 0.000427
Epoch: 685 	Ltrain: 0.000418 	Lval: 0.000424
Epoch: 690 	Ltrain: 0.000412 	Lval: 0.000422
Epoch: 695 	Ltrain: 0.000409 	Lval: 0.000419
Epoch: 700 	Ltrain: 0.000409 	Lval: 0.000416
Epoch: 705 	Ltrain: 0.000406 	Lval: 0.000413
Epoch: 710 	Ltrain: 0.000401 	Lval: 0.000411
Epoch: 715 	Ltrain: 0.000400 	Lval: 0.000408
Epoch: 720 	Ltrain: 0.000399 	Lval: 0.000405
Epoch: 725 	Ltrain: 0.000397 	Lval: 0.000402
Epoch: 730 	Ltrain: 0.000395 	Lval: 0.000400
Epoch: 735 	Ltrain: 0.000404 	Lval: 0.000397
Epoch: 740 	Ltrain: 0.000387 	Lval: 0.000395
Epoch: 745 	Ltrain: 0.000384 	Lval: 0.000391
Epoch: 750 	Ltrain: 0.000383 	Lval: 0.000389
Epoch: 755 	Ltrain: 0.000380 	Lval: 0.000387
Epoch: 760 	Ltrain: 0.000377 	Lval: 0.000385
Epoch: 765 	Ltrain: 0.000375 	Lval: 0.000381
Epoch: 770 	Ltrain: 0.000374 	Lval: 0.000379
Epoch: 775 	Ltrain: 0.000371 	Lval: 0.000376
Epoch: 780 	Ltrain: 0.000371 	Lval: 0.000374
Epoch: 785 	Ltrain: 0.000368 	Lval: 0.000372
Epoch: 790 	Ltrain: 0.000366 	Lval: 0.000369
Epoch: 795 	Ltrain: 0.000362 	Lval: 0.000366
Epoch: 800 	Ltrain: 0.000361 	Lval: 0.000364
Epoch: 805 	Ltrain: 0.000359 	Lval: 0.000362
Epoch: 810 	Ltrain: 0.000356 	Lval: 0.000359
Epoch: 815 	Ltrain: 0.000354 	Lval: 0.000356
Epoch: 820 	Ltrain: 0.000351 	Lval: 0.000354
Epoch: 825 	Ltrain: 0.000360 	Lval: 0.000352
Epoch: 830 	Ltrain: 0.000345 	Lval: 0.000349
Epoch: 835 	Ltrain: 0.000346 	Lval: 0.000347
Epoch: 840 	Ltrain: 0.000342 	Lval: 0.000344
Epoch: 845 	Ltrain: 0.000339 	Lval: 0.000342
Epoch: 850 	Ltrain: 0.000337 	Lval: 0.000340
Epoch: 855 	Ltrain: 0.000335 	Lval: 0.000337
Epoch: 860 	Ltrain: 0.000333 	Lval: 0.000335
Epoch: 865 	Ltrain: 0.000330 	Lval: 0.000333
Epoch: 870 	Ltrain: 0.000340 	Lval: 0.000331
Epoch: 875 	Ltrain: 0.000327 	Lval: 0.000329
Epoch: 880 	Ltrain: 0.000324 	Lval: 0.000326
Epoch: 885 	Ltrain: 0.000323 	Lval: 0.000324
Epoch: 890 	Ltrain: 0.000321 	Lval: 0.000322
Epoch: 895 	Ltrain: 0.000320 	Lval: 0.000319
Epoch: 900 	Ltrain: 0.000320 	Lval: 0.000318
Epoch: 905 	Ltrain: 0.000315 	Lval: 0.000315
Epoch: 910 	Ltrain: 0.000314 	Lval: 0.000313
Epoch: 915 	Ltrain: 0.000310 	Lval: 0.000311
Epoch: 920 	Ltrain: 0.000309 	Lval: 0.000309
Epoch: 925 	Ltrain: 0.000307 	Lval: 0.000306
Epoch: 930 	Ltrain: 0.000304 	Lval: 0.000305
Epoch: 935 	Ltrain: 0.000304 	Lval: 0.000303
Epoch: 940 	Ltrain: 0.000300 	Lval: 0.000300
Epoch: 945 	Ltrain: 0.000300 	Lval: 0.000298
Epoch: 950 	Ltrain: 0.000295 	Lval: 0.000296
Epoch: 955 	Ltrain: 0.000296 	Lval: 0.000294
Epoch: 960 	Ltrain: 0.000292 	Lval: 0.000293
Epoch: 965 	Ltrain: 0.000291 	Lval: 0.000290
Epoch: 970 	Ltrain: 0.000289 	Lval: 0.000288
Epoch: 975 	Ltrain: 0.000288 	Lval: 0.000287
Epoch: 980 	Ltrain: 0.000285 	Lval: 0.000284
Epoch: 985 	Ltrain: 0.000284 	Lval: 0.000282
Epoch: 990 	Ltrain: 0.000280 	Lval: 0.000280
Epoch: 995 	Ltrain: 0.000282 	Lval: 0.000278
Epoch: 1000 	Ltrain: 0.000278 	Lval: 0.000276
Epoch: 1005 	Ltrain: 0.000276 	Lval: 0.000275
Epoch: 1010 	Ltrain: 0.000274 	Lval: 0.000272
Epoch: 1015 	Ltrain: 0.000273 	Lval: 0.000270
Epoch: 1020 	Ltrain: 0.000271 	Lval: 0.000269
Epoch: 1025 	Ltrain: 0.000269 	Lval: 0.000267
Epoch: 1030 	Ltrain: 0.000267 	Lval: 0.000265
Epoch: 1035 	Ltrain: 0.000269 	Lval: 0.000263
Epoch: 1040 	Ltrain: 0.000264 	Lval: 0.000261
Epoch: 1045 	Ltrain: 0.000263 	Lval: 0.000260
Epoch: 1050 	Ltrain: 0.000261 	Lval: 0.000258
Epoch: 1055 	Ltrain: 0.000260 	Lval: 0.000256
Epoch: 1060 	Ltrain: 0.000257 	Lval: 0.000254
Epoch: 1065 	Ltrain: 0.000256 	Lval: 0.000252
Epoch: 1070 	Ltrain: 0.000253 	Lval: 0.000251
EarlyStopper: stopping at epoch 1069 with best_val_loss = 0.000260


	Fold 4/5
Epoch: 1 	Ltrain: 0.121644 	Lval: 0.016029
Epoch: 5 	Ltrain: 0.006058 	Lval: 0.007845
Epoch: 10 	Ltrain: 0.005517 	Lval: 0.007163
Epoch: 15 	Ltrain: 0.005545 	Lval: 0.008395
Epoch 00017: reducing learning rate of group 0 to 9.9706e-04.
Epoch: 20 	Ltrain: 0.004905 	Lval: 0.006337
Epoch: 25 	Ltrain: 0.004910 	Lval: 0.006198
Epoch 00030: reducing learning rate of group 0 to 9.9706e-05.
Epoch: 30 	Ltrain: 0.004862 	Lval: 0.006225
Epoch: 35 	Ltrain: 0.004807 	Lval: 0.006145
Epoch: 40 	Ltrain: 0.004787 	Lval: 0.006145
Epoch 00042: reducing learning rate of group 0 to 9.9706e-06.
Epoch: 45 	Ltrain: 0.004781 	Lval: 0.006141
Epoch: 50 	Ltrain: 0.004782 	Lval: 0.006140
EarlyStopper: stopping at epoch 53 with best_val_loss = 0.006149


	Fold 5/5
Epoch: 1 	Ltrain: 0.065605 	Lval: 0.011439
Epoch: 5 	Ltrain: 0.006229 	Lval: 0.009376
Epoch 00007: reducing learning rate of group 0 to 9.9706e-04.
Epoch: 10 	Ltrain: 0.005044 	Lval: 0.006362
Epoch: 15 	Ltrain: 0.005018 	Lval: 0.006399
Epoch 00020: reducing learning rate of group 0 to 9.9706e-05.
Epoch: 20 	Ltrain: 0.004916 	Lval: 0.006220
Epoch: 25 	Ltrain: 0.004783 	Lval: 0.006134
Epoch: 30 	Ltrain: 0.004791 	Lval: 0.006225
Epoch: 35 	Ltrain: 0.004764 	Lval: 0.006098
Epoch: 40 	Ltrain: 0.004754 	Lval: 0.006061
Epoch 00045: reducing learning rate of group 0 to 9.9706e-06.
Epoch: 45 	Ltrain: 0.004760 	Lval: 0.006055
Epoch: 50 	Ltrain: 0.004738 	Lval: 0.006082
Epoch: 55 	Ltrain: 0.004715 	Lval: 0.006071
Epoch 00057: reducing learning rate of group 0 to 9.9706e-07.
Epoch: 60 	Ltrain: 0.004720 	Lval: 0.006074
Epoch: 65 	Ltrain: 0.004717 	Lval: 0.006075
Epoch 00069: reducing learning rate of group 0 to 9.9706e-08.
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.006046

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00948413613338205
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.1901225173803297e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 29
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.184350 	Lval: 0.055859
Epoch: 5 	Ltrain: 0.013948 	Lval: 0.011646
Epoch: 10 	Ltrain: 0.008589 	Lval: 0.008905
Epoch: 15 	Ltrain: 0.006985 	Lval: 0.006860
Epoch: 20 	Ltrain: 0.007152 	Lval: 0.006764
Epoch 00022: reducing learning rate of group 0 to 9.4841e-04.
Epoch: 25 	Ltrain: 0.006567 	Lval: 0.006241
Epoch: 30 	Ltrain: 0.006010 	Lval: 0.006041
Epoch: 35 	Ltrain: 0.006390 	Lval: 0.005994
Epoch: 40 	Ltrain: 0.005820 	Lval: 0.005907
Epoch: 45 	Ltrain: 0.006272 	Lval: 0.005826
Epoch: 50 	Ltrain: 0.005792 	Lval: 0.005847
Epoch: 55 	Ltrain: 0.005675 	Lval: 0.005768
Epoch: 60 	Ltrain: 0.005528 	Lval: 0.005740
Epoch: 65 	Ltrain: 0.005491 	Lval: 0.005551
Epoch: 70 	Ltrain: 0.005517 	Lval: 0.005503
Epoch 00073: reducing learning rate of group 0 to 9.4841e-05.
Epoch: 75 	Ltrain: 0.005310 	Lval: 0.005400
Epoch: 80 	Ltrain: 0.005179 	Lval: 0.005386
Epoch: 85 	Ltrain: 0.005408 	Lval: 0.005384
Epoch: 90 	Ltrain: 0.005696 	Lval: 0.005379
Epoch 00092: reducing learning rate of group 0 to 9.4841e-06.
Epoch: 95 	Ltrain: 0.005574 	Lval: 0.005377
Epoch: 100 	Ltrain: 0.005352 	Lval: 0.005376
Epoch 00104: reducing learning rate of group 0 to 9.4841e-07.
Epoch: 105 	Ltrain: 0.005468 	Lval: 0.005375
Epoch: 110 	Ltrain: 0.005264 	Lval: 0.005375
Epoch: 115 	Ltrain: 0.005156 	Lval: 0.005375
Epoch 00116: reducing learning rate of group 0 to 9.4841e-08.
EarlyStopper: stopping at epoch 116 with best_val_loss = 0.005373


	Fold 2/5
Epoch: 1 	Ltrain: 0.080349 	Lval: 0.019316
Epoch: 5 	Ltrain: 0.007812 	Lval: 0.007893
Epoch: 10 	Ltrain: 0.006904 	Lval: 0.007007
Epoch: 15 	Ltrain: 0.006420 	Lval: 0.006454
Epoch: 20 	Ltrain: 0.006807 	Lval: 0.007007
Epoch 00022: reducing learning rate of group 0 to 9.4841e-04.
Epoch: 25 	Ltrain: 0.005388 	Lval: 0.005595
Epoch: 30 	Ltrain: 0.005247 	Lval: 0.005520
Epoch: 35 	Ltrain: 0.005157 	Lval: 0.005396
Epoch: 40 	Ltrain: 0.005088 	Lval: 0.005372
Epoch: 45 	Ltrain: 0.005039 	Lval: 0.005235
Epoch: 50 	Ltrain: 0.005028 	Lval: 0.005160
Epoch: 55 	Ltrain: 0.004667 	Lval: 0.004934
Epoch: 60 	Ltrain: 0.004609 	Lval: 0.004827
Epoch: 65 	Ltrain: 0.004580 	Lval: 0.004559
Epoch: 70 	Ltrain: 0.004217 	Lval: 0.004341
Epoch: 75 	Ltrain: 0.004032 	Lval: 0.004110
Epoch: 80 	Ltrain: 0.003906 	Lval: 0.003783
Epoch: 85 	Ltrain: 0.003696 	Lval: 0.003683
Epoch: 90 	Ltrain: 0.003454 	Lval: 0.003375
Epoch: 95 	Ltrain: 0.003219 	Lval: 0.003286
Epoch: 100 	Ltrain: 0.003071 	Lval: 0.003047
Epoch: 105 	Ltrain: 0.002870 	Lval: 0.002778
Epoch: 110 	Ltrain: 0.002711 	Lval: 0.002665
Epoch 00112: reducing learning rate of group 0 to 9.4841e-05.
Epoch: 115 	Ltrain: 0.002304 	Lval: 0.002266
Epoch: 120 	Ltrain: 0.002199 	Lval: 0.002206
Epoch: 125 	Ltrain: 0.002171 	Lval: 0.002171
Epoch: 130 	Ltrain: 0.002134 	Lval: 0.002136
Epoch: 135 	Ltrain: 0.002079 	Lval: 0.002100
Epoch: 140 	Ltrain: 0.002066 	Lval: 0.002071
Epoch: 145 	Ltrain: 0.002048 	Lval: 0.002040
Epoch: 150 	Ltrain: 0.002025 	Lval: 0.002011
Epoch: 155 	Ltrain: 0.001991 	Lval: 0.001980
Epoch: 160 	Ltrain: 0.001966 	Lval: 0.001942
Epoch: 165 	Ltrain: 0.001911 	Lval: 0.001906
Epoch: 170 	Ltrain: 0.001913 	Lval: 0.001879
Epoch: 175 	Ltrain: 0.001865 	Lval: 0.001842
Epoch: 180 	Ltrain: 0.001853 	Lval: 0.001810
Epoch: 185 	Ltrain: 0.001775 	Lval: 0.001786
Epoch: 190 	Ltrain: 0.001767 	Lval: 0.001744
Epoch: 195 	Ltrain: 0.001732 	Lval: 0.001712
Epoch: 200 	Ltrain: 0.001701 	Lval: 0.001680
Epoch: 205 	Ltrain: 0.001657 	Lval: 0.001647
Epoch: 210 	Ltrain: 0.001628 	Lval: 0.001620
Epoch: 215 	Ltrain: 0.001587 	Lval: 0.001576
Epoch: 220 	Ltrain: 0.001598 	Lval: 0.001550
Epoch: 225 	Ltrain: 0.001525 	Lval: 0.001511
Epoch: 230 	Ltrain: 0.001507 	Lval: 0.001490
Epoch: 235 	Ltrain: 0.001466 	Lval: 0.001450
Epoch: 240 	Ltrain: 0.001423 	Lval: 0.001419
Epoch: 245 	Ltrain: 0.001419 	Lval: 0.001389
Epoch: 250 	Ltrain: 0.001361 	Lval: 0.001362
Epoch: 255 	Ltrain: 0.001342 	Lval: 0.001338
Epoch: 260 	Ltrain: 0.001311 	Lval: 0.001301
Epoch: 265 	Ltrain: 0.001283 	Lval: 0.001274
Epoch: 270 	Ltrain: 0.001251 	Lval: 0.001238
Epoch: 275 	Ltrain: 0.001240 	Lval: 0.001213
Epoch: 280 	Ltrain: 0.001213 	Lval: 0.001198
Epoch: 285 	Ltrain: 0.001183 	Lval: 0.001169
Epoch: 290 	Ltrain: 0.001143 	Lval: 0.001143
Epoch: 295 	Ltrain: 0.001118 	Lval: 0.001108
Epoch: 300 	Ltrain: 0.001092 	Lval: 0.001085
Epoch: 305 	Ltrain: 0.001062 	Lval: 0.001056
Epoch 00310: reducing learning rate of group 0 to 9.4841e-06.
Epoch: 310 	Ltrain: 0.001068 	Lval: 0.001055
Epoch: 315 	Ltrain: 0.001021 	Lval: 0.001011
Epoch: 320 	Ltrain: 0.001001 	Lval: 0.001006
Epoch: 325 	Ltrain: 0.001000 	Lval: 0.001003
Epoch: 330 	Ltrain: 0.000993 	Lval: 0.001000
Epoch: 335 	Ltrain: 0.000997 	Lval: 0.000997
Epoch: 340 	Ltrain: 0.000997 	Lval: 0.000994
Epoch: 345 	Ltrain: 0.000981 	Lval: 0.000991
Epoch: 350 	Ltrain: 0.000981 	Lval: 0.000988
Epoch: 355 	Ltrain: 0.000973 	Lval: 0.000984
Epoch: 360 	Ltrain: 0.000977 	Lval: 0.000981
Epoch: 365 	Ltrain: 0.000975 	Lval: 0.000978
Epoch: 370 	Ltrain: 0.000962 	Lval: 0.000975
Epoch: 375 	Ltrain: 0.000978 	Lval: 0.000972
Epoch: 380 	Ltrain: 0.000972 	Lval: 0.000969
Epoch: 385 	Ltrain: 0.000956 	Lval: 0.000965
Epoch: 390 	Ltrain: 0.000963 	Lval: 0.000962
Epoch: 395 	Ltrain: 0.000954 	Lval: 0.000959
Epoch: 400 	Ltrain: 0.000960 	Lval: 0.000956
Epoch: 405 	Ltrain: 0.000944 	Lval: 0.000952
Epoch: 410 	Ltrain: 0.000948 	Lval: 0.000948
Epoch: 415 	Ltrain: 0.000937 	Lval: 0.000945
Epoch: 420 	Ltrain: 0.000925 	Lval: 0.000941
Epoch: 425 	Ltrain: 0.000923 	Lval: 0.000938
Epoch: 430 	Ltrain: 0.000930 	Lval: 0.000935
Epoch: 435 	Ltrain: 0.000918 	Lval: 0.000931
Epoch: 440 	Ltrain: 0.000917 	Lval: 0.000928
Epoch: 445 	Ltrain: 0.000927 	Lval: 0.000924
Epoch: 450 	Ltrain: 0.000927 	Lval: 0.000920
Epoch: 455 	Ltrain: 0.000925 	Lval: 0.000917
Epoch: 460 	Ltrain: 0.000904 	Lval: 0.000913
Epoch: 465 	Ltrain: 0.000914 	Lval: 0.000909
Epoch: 470 	Ltrain: 0.000909 	Lval: 0.000905
Epoch: 475 	Ltrain: 0.000928 	Lval: 0.000902
Epoch: 480 	Ltrain: 0.000902 	Lval: 0.000898
Epoch: 485 	Ltrain: 0.000895 	Lval: 0.000894
Epoch: 490 	Ltrain: 0.000899 	Lval: 0.000891
Epoch: 495 	Ltrain: 0.000884 	Lval: 0.000886
Epoch: 500 	Ltrain: 0.000873 	Lval: 0.000883
Epoch: 505 	Ltrain: 0.000888 	Lval: 0.000879
Epoch: 510 	Ltrain: 0.000879 	Lval: 0.000875
Epoch: 515 	Ltrain: 0.000874 	Lval: 0.000872
Epoch: 520 	Ltrain: 0.000866 	Lval: 0.000869
Epoch: 525 	Ltrain: 0.000863 	Lval: 0.000865
Epoch: 530 	Ltrain: 0.000859 	Lval: 0.000861
Epoch: 535 	Ltrain: 0.000855 	Lval: 0.000857
Epoch: 540 	Ltrain: 0.000851 	Lval: 0.000854
Epoch: 545 	Ltrain: 0.000842 	Lval: 0.000850
Epoch: 550 	Ltrain: 0.000843 	Lval: 0.000845
Epoch: 555 	Ltrain: 0.000833 	Lval: 0.000841
Epoch: 560 	Ltrain: 0.000827 	Lval: 0.000838
Epoch: 565 	Ltrain: 0.000844 	Lval: 0.000834
Epoch: 570 	Ltrain: 0.000820 	Lval: 0.000831
Epoch: 575 	Ltrain: 0.000820 	Lval: 0.000827
Epoch: 580 	Ltrain: 0.000815 	Lval: 0.000823
Epoch: 585 	Ltrain: 0.000818 	Lval: 0.000819
Epoch: 590 	Ltrain: 0.000824 	Lval: 0.000815
Epoch: 595 	Ltrain: 0.000807 	Lval: 0.000812
Epoch: 600 	Ltrain: 0.000813 	Lval: 0.000809
Epoch: 605 	Ltrain: 0.000806 	Lval: 0.000804
Epoch: 610 	Ltrain: 0.000798 	Lval: 0.000801
Epoch: 615 	Ltrain: 0.000797 	Lval: 0.000797
Epoch: 620 	Ltrain: 0.000793 	Lval: 0.000793
Epoch: 625 	Ltrain: 0.000799 	Lval: 0.000790
Epoch: 630 	Ltrain: 0.000790 	Lval: 0.000785
Epoch: 635 	Ltrain: 0.000778 	Lval: 0.000782
Epoch: 640 	Ltrain: 0.000793 	Lval: 0.000778
Epoch: 645 	Ltrain: 0.000769 	Lval: 0.000775
Epoch: 650 	Ltrain: 0.000778 	Lval: 0.000772
Epoch: 655 	Ltrain: 0.000770 	Lval: 0.000768
Epoch: 660 	Ltrain: 0.000769 	Lval: 0.000765
Epoch: 665 	Ltrain: 0.000750 	Lval: 0.000760
Epoch: 670 	Ltrain: 0.000752 	Lval: 0.000757
Epoch: 675 	Ltrain: 0.000753 	Lval: 0.000754
Epoch: 680 	Ltrain: 0.000749 	Lval: 0.000750
Epoch: 685 	Ltrain: 0.000757 	Lval: 0.000747
Epoch: 690 	Ltrain: 0.000740 	Lval: 0.000744
Epoch: 695 	Ltrain: 0.000734 	Lval: 0.000740
Epoch: 700 	Ltrain: 0.000737 	Lval: 0.000736
Epoch: 705 	Ltrain: 0.000726 	Lval: 0.000733
Epoch: 710 	Ltrain: 0.000730 	Lval: 0.000730
Epoch: 715 	Ltrain: 0.000723 	Lval: 0.000727
Epoch: 720 	Ltrain: 0.000718 	Lval: 0.000723
Epoch: 725 	Ltrain: 0.000727 	Lval: 0.000720
Epoch: 730 	Ltrain: 0.000724 	Lval: 0.000716
Epoch: 735 	Ltrain: 0.000709 	Lval: 0.000713
Epoch: 740 	Ltrain: 0.000712 	Lval: 0.000710
Epoch: 745 	Ltrain: 0.000706 	Lval: 0.000707
Epoch: 750 	Ltrain: 0.000696 	Lval: 0.000703
Epoch: 755 	Ltrain: 0.000700 	Lval: 0.000700
Epoch: 760 	Ltrain: 0.000692 	Lval: 0.000696
Epoch: 765 	Ltrain: 0.000702 	Lval: 0.000693
Epoch: 770 	Ltrain: 0.000693 	Lval: 0.000690
Epoch: 775 	Ltrain: 0.000692 	Lval: 0.000686
Epoch: 780 	Ltrain: 0.000687 	Lval: 0.000683
Epoch: 785 	Ltrain: 0.000683 	Lval: 0.000680
Epoch: 790 	Ltrain: 0.000677 	Lval: 0.000677
Epoch: 795 	Ltrain: 0.000688 	Lval: 0.000674
Epoch: 800 	Ltrain: 0.000664 	Lval: 0.000671
Epoch: 805 	Ltrain: 0.000670 	Lval: 0.000668
Epoch: 810 	Ltrain: 0.000675 	Lval: 0.000664
Epoch: 815 	Ltrain: 0.000662 	Lval: 0.000661
Epoch: 820 	Ltrain: 0.000658 	Lval: 0.000659
Epoch: 825 	Ltrain: 0.000649 	Lval: 0.000656
Epoch: 830 	Ltrain: 0.000647 	Lval: 0.000652
Epoch: 835 	Ltrain: 0.000653 	Lval: 0.000649
Epoch: 840 	Ltrain: 0.000655 	Lval: 0.000646
Epoch: 845 	Ltrain: 0.000637 	Lval: 0.000643
Epoch: 850 	Ltrain: 0.000632 	Lval: 0.000640
Epoch: 855 	Ltrain: 0.000632 	Lval: 0.000638
Epoch: 860 	Ltrain: 0.000633 	Lval: 0.000634
Epoch: 865 	Ltrain: 0.000629 	Lval: 0.000631
Epoch: 870 	Ltrain: 0.000625 	Lval: 0.000628
Epoch: 875 	Ltrain: 0.000633 	Lval: 0.000625
Epoch: 880 	Ltrain: 0.000613 	Lval: 0.000622
Epoch: 885 	Ltrain: 0.000620 	Lval: 0.000619
Epoch: 890 	Ltrain: 0.000622 	Lval: 0.000616
Epoch: 895 	Ltrain: 0.000618 	Lval: 0.000613
Epoch: 900 	Ltrain: 0.000606 	Lval: 0.000611
Epoch: 905 	Ltrain: 0.000600 	Lval: 0.000607
Epoch: 910 	Ltrain: 0.000600 	Lval: 0.000605
Epoch: 915 	Ltrain: 0.000597 	Lval: 0.000602
Epoch: 920 	Ltrain: 0.000597 	Lval: 0.000599
Epoch: 925 	Ltrain: 0.000598 	Lval: 0.000596
Epoch: 930 	Ltrain: 0.000600 	Lval: 0.000593
Epoch: 935 	Ltrain: 0.000594 	Lval: 0.000590
Epoch: 940 	Ltrain: 0.000587 	Lval: 0.000588
Epoch: 945 	Ltrain: 0.000582 	Lval: 0.000585
Epoch: 950 	Ltrain: 0.000580 	Lval: 0.000583
Epoch: 955 	Ltrain: 0.000579 	Lval: 0.000580
Epoch: 960 	Ltrain: 0.000569 	Lval: 0.000577
Epoch: 965 	Ltrain: 0.000570 	Lval: 0.000574
Epoch: 970 	Ltrain: 0.000571 	Lval: 0.000571
Epoch: 975 	Ltrain: 0.000581 	Lval: 0.000568
Epoch: 980 	Ltrain: 0.000560 	Lval: 0.000566
Epoch: 985 	Ltrain: 0.000562 	Lval: 0.000563
Epoch: 990 	Ltrain: 0.000568 	Lval: 0.000561
Epoch: 995 	Ltrain: 0.000547 	Lval: 0.000557
Epoch: 1000 	Ltrain: 0.000556 	Lval: 0.000555
Epoch: 1005 	Ltrain: 0.000550 	Lval: 0.000553
Epoch: 1010 	Ltrain: 0.000548 	Lval: 0.000550
Epoch: 1015 	Ltrain: 0.000543 	Lval: 0.000548
Epoch: 1020 	Ltrain: 0.000551 	Lval: 0.000545
Epoch: 1025 	Ltrain: 0.000537 	Lval: 0.000542
Epoch: 1030 	Ltrain: 0.000533 	Lval: 0.000539
Epoch: 1035 	Ltrain: 0.000532 	Lval: 0.000537
Epoch: 1040 	Ltrain: 0.000526 	Lval: 0.000534
Epoch: 1045 	Ltrain: 0.000529 	Lval: 0.000532
Epoch: 1050 	Ltrain: 0.000521 	Lval: 0.000529
Epoch: 1055 	Ltrain: 0.000526 	Lval: 0.000527
Epoch: 1060 	Ltrain: 0.000523 	Lval: 0.000525
Epoch: 1065 	Ltrain: 0.000521 	Lval: 0.000523
Epoch: 1070 	Ltrain: 0.000509 	Lval: 0.000520
Epoch: 1075 	Ltrain: 0.000511 	Lval: 0.000517
Epoch: 1080 	Ltrain: 0.000516 	Lval: 0.000515
Epoch: 1085 	Ltrain: 0.000507 	Lval: 0.000513
Epoch: 1090 	Ltrain: 0.000515 	Lval: 0.000510
Epoch: 1095 	Ltrain: 0.000512 	Lval: 0.000508
Epoch: 1100 	Ltrain: 0.000511 	Lval: 0.000506
Epoch: 1105 	Ltrain: 0.000494 	Lval: 0.000503
Epoch: 1110 	Ltrain: 0.000493 	Lval: 0.000500
Epoch: 1115 	Ltrain: 0.000492 	Lval: 0.000498
Epoch: 1120 	Ltrain: 0.000489 	Lval: 0.000496
Epoch: 1125 	Ltrain: 0.000484 	Lval: 0.000493
Epoch: 1130 	Ltrain: 0.000489 	Lval: 0.000491
Epoch: 1135 	Ltrain: 0.000488 	Lval: 0.000489
Epoch: 1140 	Ltrain: 0.000489 	Lval: 0.000486
Epoch: 1145 	Ltrain: 0.000477 	Lval: 0.000484
Epoch: 1150 	Ltrain: 0.000479 	Lval: 0.000482
Epoch: 1155 	Ltrain: 0.000475 	Lval: 0.000479
Epoch: 1160 	Ltrain: 0.000476 	Lval: 0.000476
Epoch: 1165 	Ltrain: 0.000471 	Lval: 0.000474
Epoch: 1170 	Ltrain: 0.000467 	Lval: 0.000472
Epoch: 1175 	Ltrain: 0.000464 	Lval: 0.000470
Epoch: 1180 	Ltrain: 0.000466 	Lval: 0.000468
Epoch: 1185 	Ltrain: 0.000459 	Lval: 0.000465
Epoch: 1190 	Ltrain: 0.000463 	Lval: 0.000463
Epoch: 1195 	Ltrain: 0.000454 	Lval: 0.000461
Epoch: 1200 	Ltrain: 0.000458 	Lval: 0.000460
Epoch: 1205 	Ltrain: 0.000452 	Lval: 0.000457
Epoch: 1210 	Ltrain: 0.000456 	Lval: 0.000455
Epoch: 1215 	Ltrain: 0.000452 	Lval: 0.000452
Epoch: 1220 	Ltrain: 0.000445 	Lval: 0.000449
Epoch: 1225 	Ltrain: 0.000441 	Lval: 0.000449
Epoch: 1230 	Ltrain: 0.000444 	Lval: 0.000446
Epoch: 1235 	Ltrain: 0.000447 	Lval: 0.000444
Epoch: 1240 	Ltrain: 0.000437 	Lval: 0.000442
Epoch: 1245 	Ltrain: 0.000437 	Lval: 0.000440
Epoch: 1250 	Ltrain: 0.000435 	Lval: 0.000438
Epoch: 1255 	Ltrain: 0.000432 	Lval: 0.000435
Epoch: 1260 	Ltrain: 0.000433 	Lval: 0.000434
Epoch: 1265 	Ltrain: 0.000430 	Lval: 0.000431
Epoch: 1270 	Ltrain: 0.000422 	Lval: 0.000429
Epoch: 1275 	Ltrain: 0.000423 	Lval: 0.000427
Epoch: 1280 	Ltrain: 0.000421 	Lval: 0.000426
Epoch: 1285 	Ltrain: 0.000428 	Lval: 0.000423
Epoch: 1290 	Ltrain: 0.000436 	Lval: 0.000421
Epoch: 1295 	Ltrain: 0.000415 	Lval: 0.000419
Epoch: 1300 	Ltrain: 0.000409 	Lval: 0.000417
Epoch: 1305 	Ltrain: 0.000408 	Lval: 0.000416
Epoch: 1310 	Ltrain: 0.000413 	Lval: 0.000413
Epoch: 1315 	Ltrain: 0.000414 	Lval: 0.000412
Epoch: 1320 	Ltrain: 0.000406 	Lval: 0.000409
Epoch: 1325 	Ltrain: 0.000403 	Lval: 0.000407
Epoch: 1330 	Ltrain: 0.000414 	Lval: 0.000405
Epoch: 1335 	Ltrain: 0.000397 	Lval: 0.000403
Epoch: 1340 	Ltrain: 0.000396 	Lval: 0.000401
Epoch: 1345 	Ltrain: 0.000400 	Lval: 0.000400
Epoch: 1350 	Ltrain: 0.000402 	Lval: 0.000397
Epoch: 1355 	Ltrain: 0.000391 	Lval: 0.000395
Epoch: 1360 	Ltrain: 0.000388 	Lval: 0.000394
Epoch: 1365 	Ltrain: 0.000388 	Lval: 0.000392
Epoch: 1370 	Ltrain: 0.000390 	Lval: 0.000391
Epoch: 1375 	Ltrain: 0.000383 	Lval: 0.000389
Epoch: 1380 	Ltrain: 0.000382 	Lval: 0.000387
Epoch: 1385 	Ltrain: 0.000382 	Lval: 0.000385
Epoch: 1390 	Ltrain: 0.000377 	Lval: 0.000383
Epoch: 1395 	Ltrain: 0.000382 	Lval: 0.000382
Epoch: 1400 	Ltrain: 0.000379 	Lval: 0.000379
Epoch: 1405 	Ltrain: 0.000374 	Lval: 0.000378
Epoch: 1410 	Ltrain: 0.000370 	Lval: 0.000376
Epoch: 1415 	Ltrain: 0.000375 	Lval: 0.000374
Epoch: 1420 	Ltrain: 0.000367 	Lval: 0.000373
Epoch: 1425 	Ltrain: 0.000370 	Lval: 0.000370
Epoch: 1430 	Ltrain: 0.000370 	Lval: 0.000369
EarlyStopper: stopping at epoch 1430 with best_val_loss = 0.000378


	Fold 3/5
Epoch: 1 	Ltrain: 0.081465 	Lval: 0.015644
Epoch: 5 	Ltrain: 0.006710 	Lval: 0.007258
Epoch 00010: reducing learning rate of group 0 to 9.4841e-04.
Epoch: 10 	Ltrain: 0.006374 	Lval: 0.007097
Epoch: 15 	Ltrain: 0.005506 	Lval: 0.006244
Epoch: 20 	Ltrain: 0.005481 	Lval: 0.006119
Epoch: 25 	Ltrain: 0.005438 	Lval: 0.006047
Epoch: 30 	Ltrain: 0.005361 	Lval: 0.005986
Epoch 00033: reducing learning rate of group 0 to 9.4841e-05.
Epoch: 35 	Ltrain: 0.005186 	Lval: 0.005923
Epoch: 40 	Ltrain: 0.005208 	Lval: 0.005917
Epoch 00045: reducing learning rate of group 0 to 9.4841e-06.
Epoch: 45 	Ltrain: 0.005216 	Lval: 0.005899
Epoch: 50 	Ltrain: 0.005128 	Lval: 0.005895
Epoch: 55 	Ltrain: 0.005154 	Lval: 0.005892
Epoch: 60 	Ltrain: 0.005154 	Lval: 0.005888
Epoch 00064: reducing learning rate of group 0 to 9.4841e-07.
Epoch: 65 	Ltrain: 0.005164 	Lval: 0.005890
Epoch: 70 	Ltrain: 0.005141 	Lval: 0.005889
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.005895


	Fold 4/5
Epoch: 1 	Ltrain: 0.069856 	Lval: 0.018013
Epoch: 5 	Ltrain: 0.006494 	Lval: 0.007207
Epoch: 10 	Ltrain: 0.005686 	Lval: 0.007152
Epoch: 15 	Ltrain: 0.005472 	Lval: 0.006568
Epoch: 20 	Ltrain: 0.005033 	Lval: 0.005923
Epoch: 25 	Ltrain: 0.004703 	Lval: 0.005624
Epoch 00026: reducing learning rate of group 0 to 9.4841e-04.
Epoch: 30 	Ltrain: 0.003911 	Lval: 0.004844
Epoch: 35 	Ltrain: 0.003777 	Lval: 0.004598
Epoch: 40 	Ltrain: 0.003670 	Lval: 0.004383
Epoch: 45 	Ltrain: 0.003537 	Lval: 0.004185
Epoch: 50 	Ltrain: 0.003444 	Lval: 0.003938
Epoch: 55 	Ltrain: 0.003242 	Lval: 0.003756
Epoch: 60 	Ltrain: 0.003084 	Lval: 0.003483
Epoch: 65 	Ltrain: 0.002904 	Lval: 0.003103
Epoch: 70 	Ltrain: 0.002704 	Lval: 0.003072
Epoch: 75 	Ltrain: 0.002518 	Lval: 0.002684
Epoch: 80 	Ltrain: 0.002376 	Lval: 0.002558
Epoch: 85 	Ltrain: 0.002266 	Lval: 0.002371
Epoch: 90 	Ltrain: 0.002026 	Lval: 0.002127
Epoch: 95 	Ltrain: 0.001838 	Lval: 0.002071
Epoch: 100 	Ltrain: 0.001657 	Lval: 0.001730
Epoch: 105 	Ltrain: 0.001585 	Lval: 0.001628
Epoch: 110 	Ltrain: 0.001670 	Lval: 0.001963
Epoch 00111: reducing learning rate of group 0 to 9.4841e-05.
Epoch: 115 	Ltrain: 0.001194 	Lval: 0.001272
Epoch: 120 	Ltrain: 0.001141 	Lval: 0.001221
Epoch: 125 	Ltrain: 0.001117 	Lval: 0.001192
Epoch: 130 	Ltrain: 0.001088 	Lval: 0.001165
Epoch: 135 	Ltrain: 0.001071 	Lval: 0.001145
Epoch: 140 	Ltrain: 0.001048 	Lval: 0.001120
Epoch: 145 	Ltrain: 0.001025 	Lval: 0.001098
Epoch: 150 	Ltrain: 0.001005 	Lval: 0.001071
Epoch: 155 	Ltrain: 0.000986 	Lval: 0.001049
Epoch: 160 	Ltrain: 0.000962 	Lval: 0.001024
Epoch: 165 	Ltrain: 0.000940 	Lval: 0.001000
Epoch: 170 	Ltrain: 0.000917 	Lval: 0.000971
Epoch: 175 	Ltrain: 0.000894 	Lval: 0.000949
Epoch: 180 	Ltrain: 0.000870 	Lval: 0.000923
Epoch: 185 	Ltrain: 0.000851 	Lval: 0.000896
Epoch: 190 	Ltrain: 0.000825 	Lval: 0.000868
Epoch: 195 	Ltrain: 0.000799 	Lval: 0.000847
Epoch: 200 	Ltrain: 0.000775 	Lval: 0.000818
Epoch: 205 	Ltrain: 0.000749 	Lval: 0.000790
Epoch: 210 	Ltrain: 0.000730 	Lval: 0.000767
Epoch: 215 	Ltrain: 0.000706 	Lval: 0.000739
Epoch: 220 	Ltrain: 0.000689 	Lval: 0.000723
Epoch: 225 	Ltrain: 0.000662 	Lval: 0.000686
Epoch: 230 	Ltrain: 0.000641 	Lval: 0.000670
Epoch: 235 	Ltrain: 0.000620 	Lval: 0.000652
Epoch: 240 	Ltrain: 0.000604 	Lval: 0.000630
Epoch: 245 	Ltrain: 0.000589 	Lval: 0.000624
Epoch 00247: reducing learning rate of group 0 to 9.4841e-06.
Epoch: 250 	Ltrain: 0.000555 	Lval: 0.000580
Epoch: 255 	Ltrain: 0.000549 	Lval: 0.000575
Epoch: 260 	Ltrain: 0.000547 	Lval: 0.000572
Epoch: 265 	Ltrain: 0.000545 	Lval: 0.000569
Epoch: 270 	Ltrain: 0.000542 	Lval: 0.000566
Epoch: 275 	Ltrain: 0.000541 	Lval: 0.000563
Epoch: 280 	Ltrain: 0.000537 	Lval: 0.000561
Epoch: 285 	Ltrain: 0.000535 	Lval: 0.000558
Epoch: 290 	Ltrain: 0.000534 	Lval: 0.000555
Epoch: 295 	Ltrain: 0.000531 	Lval: 0.000552
Epoch: 300 	Ltrain: 0.000528 	Lval: 0.000549
Epoch: 305 	Ltrain: 0.000525 	Lval: 0.000546
Epoch: 310 	Ltrain: 0.000523 	Lval: 0.000542
Epoch: 315 	Ltrain: 0.000519 	Lval: 0.000539
Epoch: 320 	Ltrain: 0.000517 	Lval: 0.000536
Epoch: 325 	Ltrain: 0.000514 	Lval: 0.000533
Epoch: 330 	Ltrain: 0.000511 	Lval: 0.000529
Epoch: 335 	Ltrain: 0.000507 	Lval: 0.000526
Epoch: 340 	Ltrain: 0.000505 	Lval: 0.000522
Epoch: 345 	Ltrain: 0.000502 	Lval: 0.000519
Epoch: 350 	Ltrain: 0.000500 	Lval: 0.000516
Epoch: 355 	Ltrain: 0.000496 	Lval: 0.000512
Epoch: 360 	Ltrain: 0.000493 	Lval: 0.000509
Epoch: 365 	Ltrain: 0.000489 	Lval: 0.000506
Epoch: 370 	Ltrain: 0.000487 	Lval: 0.000502
Epoch: 375 	Ltrain: 0.000484 	Lval: 0.000498
Epoch: 380 	Ltrain: 0.000480 	Lval: 0.000495
Epoch: 385 	Ltrain: 0.000478 	Lval: 0.000491
Epoch: 390 	Ltrain: 0.000474 	Lval: 0.000488
Epoch: 395 	Ltrain: 0.000472 	Lval: 0.000485
Epoch: 400 	Ltrain: 0.000468 	Lval: 0.000481
Epoch: 405 	Ltrain: 0.000466 	Lval: 0.000478
Epoch: 410 	Ltrain: 0.000462 	Lval: 0.000475
Epoch: 415 	Ltrain: 0.000460 	Lval: 0.000471
Epoch: 420 	Ltrain: 0.000457 	Lval: 0.000468
Epoch: 425 	Ltrain: 0.000454 	Lval: 0.000465
Epoch: 430 	Ltrain: 0.000451 	Lval: 0.000461
Epoch: 435 	Ltrain: 0.000449 	Lval: 0.000458
Epoch: 440 	Ltrain: 0.000444 	Lval: 0.000455
Epoch: 445 	Ltrain: 0.000442 	Lval: 0.000451
Epoch: 450 	Ltrain: 0.000439 	Lval: 0.000448
Epoch: 455 	Ltrain: 0.000437 	Lval: 0.000445
Epoch: 460 	Ltrain: 0.000434 	Lval: 0.000441
Epoch: 465 	Ltrain: 0.000431 	Lval: 0.000438
Epoch: 470 	Ltrain: 0.000428 	Lval: 0.000435
Epoch: 475 	Ltrain: 0.000425 	Lval: 0.000432
Epoch: 480 	Ltrain: 0.000423 	Lval: 0.000429
Epoch: 485 	Ltrain: 0.000420 	Lval: 0.000426
Epoch: 490 	Ltrain: 0.000417 	Lval: 0.000422
Epoch: 495 	Ltrain: 0.000416 	Lval: 0.000419
Epoch: 500 	Ltrain: 0.000411 	Lval: 0.000416
Epoch: 505 	Ltrain: 0.000409 	Lval: 0.000413
Epoch: 510 	Ltrain: 0.000407 	Lval: 0.000410
Epoch: 515 	Ltrain: 0.000403 	Lval: 0.000407
Epoch: 520 	Ltrain: 0.000400 	Lval: 0.000404
Epoch: 525 	Ltrain: 0.000398 	Lval: 0.000401
Epoch: 530 	Ltrain: 0.000395 	Lval: 0.000398
Epoch: 535 	Ltrain: 0.000393 	Lval: 0.000395
Epoch: 540 	Ltrain: 0.000390 	Lval: 0.000392
Epoch: 545 	Ltrain: 0.000388 	Lval: 0.000388
Epoch: 550 	Ltrain: 0.000385 	Lval: 0.000386
Epoch: 555 	Ltrain: 0.000382 	Lval: 0.000383
Epoch: 560 	Ltrain: 0.000379 	Lval: 0.000379
Epoch: 565 	Ltrain: 0.000377 	Lval: 0.000377
Epoch: 570 	Ltrain: 0.000373 	Lval: 0.000374
Epoch: 575 	Ltrain: 0.000371 	Lval: 0.000371
Epoch: 580 	Ltrain: 0.000369 	Lval: 0.000368
Epoch: 585 	Ltrain: 0.000367 	Lval: 0.000365
Epoch: 590 	Ltrain: 0.000365 	Lval: 0.000362
Epoch: 595 	Ltrain: 0.000366 	Lval: 0.000359
Epoch: 600 	Ltrain: 0.000360 	Lval: 0.000357
Epoch: 605 	Ltrain: 0.000357 	Lval: 0.000354
Epoch: 610 	Ltrain: 0.000355 	Lval: 0.000351
Epoch: 615 	Ltrain: 0.000352 	Lval: 0.000349
Epoch: 620 	Ltrain: 0.000349 	Lval: 0.000346
Epoch: 625 	Ltrain: 0.000347 	Lval: 0.000343
Epoch: 630 	Ltrain: 0.000345 	Lval: 0.000340
Epoch: 635 	Ltrain: 0.000343 	Lval: 0.000337
Epoch: 640 	Ltrain: 0.000340 	Lval: 0.000334
Epoch: 645 	Ltrain: 0.000337 	Lval: 0.000333
Epoch: 650 	Ltrain: 0.000336 	Lval: 0.000329
Epoch: 655 	Ltrain: 0.000333 	Lval: 0.000327
Epoch: 660 	Ltrain: 0.000331 	Lval: 0.000324
Epoch: 665 	Ltrain: 0.000330 	Lval: 0.000322
Epoch: 670 	Ltrain: 0.000326 	Lval: 0.000319
Epoch: 675 	Ltrain: 0.000324 	Lval: 0.000317
Epoch: 680 	Ltrain: 0.000322 	Lval: 0.000314
Epoch: 685 	Ltrain: 0.000320 	Lval: 0.000312
Epoch: 690 	Ltrain: 0.000317 	Lval: 0.000309
Epoch: 695 	Ltrain: 0.000315 	Lval: 0.000307
Epoch: 700 	Ltrain: 0.000313 	Lval: 0.000304
Epoch: 705 	Ltrain: 0.000311 	Lval: 0.000302
Epoch: 710 	Ltrain: 0.000308 	Lval: 0.000300
Epoch: 715 	Ltrain: 0.000305 	Lval: 0.000297
Epoch: 720 	Ltrain: 0.000304 	Lval: 0.000295
Epoch: 725 	Ltrain: 0.000302 	Lval: 0.000292
Epoch: 730 	Ltrain: 0.000299 	Lval: 0.000290
Epoch: 735 	Ltrain: 0.000298 	Lval: 0.000288
Epoch: 740 	Ltrain: 0.000296 	Lval: 0.000286
Epoch: 745 	Ltrain: 0.000294 	Lval: 0.000283
Epoch: 750 	Ltrain: 0.000292 	Lval: 0.000281
Epoch: 755 	Ltrain: 0.000290 	Lval: 0.000279
Epoch: 760 	Ltrain: 0.000287 	Lval: 0.000276
Epoch: 765 	Ltrain: 0.000285 	Lval: 0.000274
Epoch: 770 	Ltrain: 0.000283 	Lval: 0.000272
Epoch: 775 	Ltrain: 0.000282 	Lval: 0.000270
Epoch: 780 	Ltrain: 0.000279 	Lval: 0.000268
Epoch: 785 	Ltrain: 0.000278 	Lval: 0.000266
Epoch: 790 	Ltrain: 0.000279 	Lval: 0.000264
Epoch: 795 	Ltrain: 0.000274 	Lval: 0.000262
Epoch: 800 	Ltrain: 0.000274 	Lval: 0.000259
Epoch: 805 	Ltrain: 0.000270 	Lval: 0.000258
Epoch: 810 	Ltrain: 0.000267 	Lval: 0.000255
Epoch: 815 	Ltrain: 0.000266 	Lval: 0.000254
Epoch: 820 	Ltrain: 0.000263 	Lval: 0.000251
Epoch: 825 	Ltrain: 0.000262 	Lval: 0.000249
Epoch: 830 	Ltrain: 0.000260 	Lval: 0.000247
Epoch: 835 	Ltrain: 0.000258 	Lval: 0.000246
Epoch: 840 	Ltrain: 0.000256 	Lval: 0.000244
Epoch: 845 	Ltrain: 0.000254 	Lval: 0.000242
Epoch: 850 	Ltrain: 0.000253 	Lval: 0.000240
Epoch: 855 	Ltrain: 0.000251 	Lval: 0.000238
Epoch: 860 	Ltrain: 0.000249 	Lval: 0.000236
Epoch: 865 	Ltrain: 0.000247 	Lval: 0.000234
Epoch: 870 	Ltrain: 0.000248 	Lval: 0.000232
Epoch: 875 	Ltrain: 0.000243 	Lval: 0.000231
Epoch: 880 	Ltrain: 0.000242 	Lval: 0.000229
Epoch: 885 	Ltrain: 0.000240 	Lval: 0.000227
Epoch: 890 	Ltrain: 0.000239 	Lval: 0.000225
Epoch: 895 	Ltrain: 0.000236 	Lval: 0.000223
Epoch: 900 	Ltrain: 0.000235 	Lval: 0.000222
Epoch: 905 	Ltrain: 0.000234 	Lval: 0.000220
Epoch: 910 	Ltrain: 0.000232 	Lval: 0.000218
Epoch: 915 	Ltrain: 0.000231 	Lval: 0.000216
Epoch: 920 	Ltrain: 0.000228 	Lval: 0.000215
Epoch: 925 	Ltrain: 0.000227 	Lval: 0.000213
Epoch: 930 	Ltrain: 0.000225 	Lval: 0.000212
Epoch: 935 	Ltrain: 0.000224 	Lval: 0.000210
Epoch: 940 	Ltrain: 0.000222 	Lval: 0.000208
Epoch: 945 	Ltrain: 0.000220 	Lval: 0.000207
EarlyStopper: stopping at epoch 946 with best_val_loss = 0.000216


	Fold 5/5
Epoch: 1 	Ltrain: 0.054381 	Lval: 0.023786
Epoch: 5 	Ltrain: 0.006437 	Lval: 0.007533
Epoch: 10 	Ltrain: 0.006158 	Lval: 0.010306
Epoch 00011: reducing learning rate of group 0 to 9.4841e-04.
Epoch: 15 	Ltrain: 0.005139 	Lval: 0.006562
Epoch: 20 	Ltrain: 0.005053 	Lval: 0.006658
Epoch 00023: reducing learning rate of group 0 to 9.4841e-05.
Epoch: 25 	Ltrain: 0.004963 	Lval: 0.006471
Epoch: 30 	Ltrain: 0.004960 	Lval: 0.006446
Epoch 00035: reducing learning rate of group 0 to 9.4841e-06.
Epoch: 35 	Ltrain: 0.004952 	Lval: 0.006465
Epoch: 40 	Ltrain: 0.004938 	Lval: 0.006452
Epoch: 45 	Ltrain: 0.004964 	Lval: 0.006452
Epoch 00047: reducing learning rate of group 0 to 9.4841e-07.
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.006439

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009685916534202978
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.5349677455963487e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 30
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.306899 	Lval: 0.023552
Epoch: 5 	Ltrain: 0.013601 	Lval: 0.012874
Epoch: 10 	Ltrain: 0.008064 	Lval: 0.007756
Epoch 00015: reducing learning rate of group 0 to 9.6859e-04.
Epoch: 15 	Ltrain: 0.007029 	Lval: 0.007360
Epoch: 20 	Ltrain: 0.006643 	Lval: 0.006541
Epoch: 25 	Ltrain: 0.006897 	Lval: 0.006448
Epoch: 30 	Ltrain: 0.006357 	Lval: 0.006415
Epoch: 35 	Ltrain: 0.007075 	Lval: 0.006329
Epoch: 40 	Ltrain: 0.006280 	Lval: 0.006312
Epoch: 45 	Ltrain: 0.006363 	Lval: 0.006222
Epoch: 50 	Ltrain: 0.006177 	Lval: 0.006123
Epoch 00054: reducing learning rate of group 0 to 9.6859e-05.
Epoch: 55 	Ltrain: 0.006477 	Lval: 0.006137
Epoch: 60 	Ltrain: 0.005952 	Lval: 0.006082
Epoch: 65 	Ltrain: 0.006399 	Lval: 0.006057
Epoch 00069: reducing learning rate of group 0 to 9.6859e-06.
Epoch: 70 	Ltrain: 0.006087 	Lval: 0.006062
Epoch: 75 	Ltrain: 0.006379 	Lval: 0.006059
Epoch: 80 	Ltrain: 0.005911 	Lval: 0.006051
Epoch 00084: reducing learning rate of group 0 to 9.6859e-07.
Epoch: 85 	Ltrain: 0.005742 	Lval: 0.006052
Epoch: 90 	Ltrain: 0.006375 	Lval: 0.006052
Epoch: 95 	Ltrain: 0.005803 	Lval: 0.006052
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.006057


	Fold 2/5
Epoch: 1 	Ltrain: 0.136092 	Lval: 0.016933
Epoch: 5 	Ltrain: 0.008336 	Lval: 0.008133
Epoch: 10 	Ltrain: 0.006923 	Lval: 0.006826
Epoch 00014: reducing learning rate of group 0 to 9.6859e-04.
Epoch: 15 	Ltrain: 0.006216 	Lval: 0.006405
Epoch: 20 	Ltrain: 0.005810 	Lval: 0.006068
Epoch: 25 	Ltrain: 0.005761 	Lval: 0.006001
Epoch: 30 	Ltrain: 0.005711 	Lval: 0.005894
Epoch: 35 	Ltrain: 0.005594 	Lval: 0.005786
Epoch: 40 	Ltrain: 0.005468 	Lval: 0.005733
Epoch: 45 	Ltrain: 0.005342 	Lval: 0.005605
Epoch: 50 	Ltrain: 0.005276 	Lval: 0.005548
Epoch: 55 	Ltrain: 0.005206 	Lval: 0.005393
Epoch: 60 	Ltrain: 0.005109 	Lval: 0.005234
Epoch: 65 	Ltrain: 0.004907 	Lval: 0.005091
Epoch: 70 	Ltrain: 0.004761 	Lval: 0.004883
Epoch: 75 	Ltrain: 0.004726 	Lval: 0.005127
Epoch: 80 	Ltrain: 0.004428 	Lval: 0.004517
Epoch: 85 	Ltrain: 0.004226 	Lval: 0.004178
Epoch: 90 	Ltrain: 0.004083 	Lval: 0.004057
Epoch: 95 	Ltrain: 0.003996 	Lval: 0.003892
Epoch: 100 	Ltrain: 0.003497 	Lval: 0.003608
Epoch: 105 	Ltrain: 0.003364 	Lval: 0.003197
Epoch 00110: reducing learning rate of group 0 to 9.6859e-05.
Epoch: 110 	Ltrain: 0.003392 	Lval: 0.003197
Epoch: 115 	Ltrain: 0.002831 	Lval: 0.002831
Epoch: 120 	Ltrain: 0.002760 	Lval: 0.002774
Epoch: 125 	Ltrain: 0.002726 	Lval: 0.002730
Epoch: 130 	Ltrain: 0.002666 	Lval: 0.002689
Epoch: 135 	Ltrain: 0.002620 	Lval: 0.002651
Epoch: 140 	Ltrain: 0.002597 	Lval: 0.002614
Epoch: 145 	Ltrain: 0.002550 	Lval: 0.002574
Epoch: 150 	Ltrain: 0.002534 	Lval: 0.002538
Epoch: 155 	Ltrain: 0.002539 	Lval: 0.002508
Epoch: 160 	Ltrain: 0.002462 	Lval: 0.002470
Epoch: 165 	Ltrain: 0.002467 	Lval: 0.002446
Epoch: 170 	Ltrain: 0.002394 	Lval: 0.002404
Epoch: 175 	Ltrain: 0.002378 	Lval: 0.002376
Epoch: 180 	Ltrain: 0.002362 	Lval: 0.002329
Epoch: 185 	Ltrain: 0.002289 	Lval: 0.002295
Epoch: 190 	Ltrain: 0.002271 	Lval: 0.002261
Epoch: 195 	Ltrain: 0.002243 	Lval: 0.002224
Epoch: 200 	Ltrain: 0.002201 	Lval: 0.002199
Epoch: 205 	Ltrain: 0.002167 	Lval: 0.002166
Epoch: 210 	Ltrain: 0.002114 	Lval: 0.002126
Epoch: 215 	Ltrain: 0.002100 	Lval: 0.002092
Epoch: 220 	Ltrain: 0.002068 	Lval: 0.002054
Epoch: 225 	Ltrain: 0.002019 	Lval: 0.002021
Epoch: 230 	Ltrain: 0.001968 	Lval: 0.001996
Epoch: 235 	Ltrain: 0.001966 	Lval: 0.001950
Epoch: 240 	Ltrain: 0.001933 	Lval: 0.001915
Epoch: 245 	Ltrain: 0.001888 	Lval: 0.001883
Epoch: 250 	Ltrain: 0.001862 	Lval: 0.001849
Epoch: 255 	Ltrain: 0.001821 	Lval: 0.001822
Epoch: 260 	Ltrain: 0.001759 	Lval: 0.001791
Epoch: 265 	Ltrain: 0.001749 	Lval: 0.001768
Epoch: 270 	Ltrain: 0.001714 	Lval: 0.001723
Epoch: 275 	Ltrain: 0.001658 	Lval: 0.001677
Epoch: 280 	Ltrain: 0.001635 	Lval: 0.001650
Epoch: 285 	Ltrain: 0.001612 	Lval: 0.001617
Epoch: 290 	Ltrain: 0.001555 	Lval: 0.001598
Epoch: 295 	Ltrain: 0.001521 	Lval: 0.001558
Epoch: 300 	Ltrain: 0.001495 	Lval: 0.001559
Epoch: 305 	Ltrain: 0.001500 	Lval: 0.001493
Epoch: 310 	Ltrain: 0.001457 	Lval: 0.001458
Epoch: 315 	Ltrain: 0.001445 	Lval: 0.001455
Epoch 00316: reducing learning rate of group 0 to 9.6859e-06.
Epoch: 320 	Ltrain: 0.001369 	Lval: 0.001404
Epoch: 325 	Ltrain: 0.001366 	Lval: 0.001400
Epoch: 330 	Ltrain: 0.001337 	Lval: 0.001397
Epoch: 335 	Ltrain: 0.001350 	Lval: 0.001393
Epoch: 340 	Ltrain: 0.001342 	Lval: 0.001390
Epoch: 345 	Ltrain: 0.001329 	Lval: 0.001386
Epoch: 350 	Ltrain: 0.001353 	Lval: 0.001383
Epoch: 355 	Ltrain: 0.001349 	Lval: 0.001379
Epoch: 360 	Ltrain: 0.001327 	Lval: 0.001376
Epoch: 365 	Ltrain: 0.001318 	Lval: 0.001372
Epoch: 370 	Ltrain: 0.001326 	Lval: 0.001368
Epoch: 375 	Ltrain: 0.001325 	Lval: 0.001364
Epoch: 380 	Ltrain: 0.001315 	Lval: 0.001361
Epoch: 385 	Ltrain: 0.001328 	Lval: 0.001357
Epoch: 390 	Ltrain: 0.001306 	Lval: 0.001353
Epoch: 395 	Ltrain: 0.001308 	Lval: 0.001349
Epoch: 400 	Ltrain: 0.001300 	Lval: 0.001344
Epoch: 405 	Ltrain: 0.001300 	Lval: 0.001341
Epoch: 410 	Ltrain: 0.001305 	Lval: 0.001337
Epoch: 415 	Ltrain: 0.001291 	Lval: 0.001332
Epoch: 420 	Ltrain: 0.001296 	Lval: 0.001329
Epoch: 425 	Ltrain: 0.001275 	Lval: 0.001324
Epoch: 430 	Ltrain: 0.001273 	Lval: 0.001321
Epoch: 435 	Ltrain: 0.001271 	Lval: 0.001315
Epoch: 440 	Ltrain: 0.001269 	Lval: 0.001311
Epoch: 445 	Ltrain: 0.001261 	Lval: 0.001307
Epoch: 450 	Ltrain: 0.001254 	Lval: 0.001303
Epoch: 455 	Ltrain: 0.001274 	Lval: 0.001299
Epoch: 460 	Ltrain: 0.001257 	Lval: 0.001294
Epoch: 465 	Ltrain: 0.001257 	Lval: 0.001289
Epoch: 470 	Ltrain: 0.001257 	Lval: 0.001285
Epoch: 475 	Ltrain: 0.001219 	Lval: 0.001280
Epoch: 480 	Ltrain: 0.001229 	Lval: 0.001276
Epoch: 485 	Ltrain: 0.001229 	Lval: 0.001271
Epoch: 490 	Ltrain: 0.001233 	Lval: 0.001267
Epoch: 495 	Ltrain: 0.001227 	Lval: 0.001263
Epoch: 500 	Ltrain: 0.001204 	Lval: 0.001257
Epoch: 505 	Ltrain: 0.001210 	Lval: 0.001253
Epoch: 510 	Ltrain: 0.001217 	Lval: 0.001248
Epoch: 515 	Ltrain: 0.001216 	Lval: 0.001244
Epoch: 520 	Ltrain: 0.001190 	Lval: 0.001240
Epoch: 525 	Ltrain: 0.001200 	Lval: 0.001236
Epoch: 530 	Ltrain: 0.001187 	Lval: 0.001229
Epoch: 535 	Ltrain: 0.001181 	Lval: 0.001224
Epoch: 540 	Ltrain: 0.001176 	Lval: 0.001220
Epoch: 545 	Ltrain: 0.001166 	Lval: 0.001216
Epoch: 550 	Ltrain: 0.001164 	Lval: 0.001210
Epoch: 555 	Ltrain: 0.001164 	Lval: 0.001206
Epoch: 560 	Ltrain: 0.001163 	Lval: 0.001202
Epoch: 565 	Ltrain: 0.001144 	Lval: 0.001197
Epoch: 570 	Ltrain: 0.001146 	Lval: 0.001192
Epoch: 575 	Ltrain: 0.001151 	Lval: 0.001186
Epoch: 580 	Ltrain: 0.001159 	Lval: 0.001183
Epoch: 585 	Ltrain: 0.001137 	Lval: 0.001178
Epoch: 590 	Ltrain: 0.001141 	Lval: 0.001174
Epoch: 595 	Ltrain: 0.001137 	Lval: 0.001169
Epoch: 600 	Ltrain: 0.001119 	Lval: 0.001163
Epoch: 605 	Ltrain: 0.001117 	Lval: 0.001158
Epoch: 610 	Ltrain: 0.001111 	Lval: 0.001154
Epoch: 615 	Ltrain: 0.001113 	Lval: 0.001149
Epoch: 620 	Ltrain: 0.001114 	Lval: 0.001145
Epoch: 625 	Ltrain: 0.001112 	Lval: 0.001140
Epoch: 630 	Ltrain: 0.001082 	Lval: 0.001136
Epoch: 635 	Ltrain: 0.001089 	Lval: 0.001132
Epoch: 640 	Ltrain: 0.001101 	Lval: 0.001128
Epoch: 645 	Ltrain: 0.001092 	Lval: 0.001121
Epoch: 650 	Ltrain: 0.001080 	Lval: 0.001117
Epoch: 655 	Ltrain: 0.001088 	Lval: 0.001113
Epoch: 660 	Ltrain: 0.001075 	Lval: 0.001107
Epoch: 665 	Ltrain: 0.001082 	Lval: 0.001103
Epoch: 670 	Ltrain: 0.001069 	Lval: 0.001097
Epoch: 675 	Ltrain: 0.001055 	Lval: 0.001093
Epoch: 680 	Ltrain: 0.001057 	Lval: 0.001089
Epoch: 685 	Ltrain: 0.001053 	Lval: 0.001085
Epoch: 690 	Ltrain: 0.001053 	Lval: 0.001079
Epoch: 695 	Ltrain: 0.001048 	Lval: 0.001075
Epoch: 700 	Ltrain: 0.001039 	Lval: 0.001071
Epoch: 705 	Ltrain: 0.001032 	Lval: 0.001066
Epoch: 710 	Ltrain: 0.001029 	Lval: 0.001061
Epoch: 715 	Ltrain: 0.001025 	Lval: 0.001057
Epoch: 720 	Ltrain: 0.001010 	Lval: 0.001053
Epoch: 725 	Ltrain: 0.001018 	Lval: 0.001047
Epoch: 730 	Ltrain: 0.001029 	Lval: 0.001043
Epoch: 735 	Ltrain: 0.001002 	Lval: 0.001039
Epoch: 740 	Ltrain: 0.001014 	Lval: 0.001033
Epoch: 745 	Ltrain: 0.000993 	Lval: 0.001029
Epoch: 750 	Ltrain: 0.000997 	Lval: 0.001025
Epoch: 755 	Ltrain: 0.000988 	Lval: 0.001020
Epoch: 760 	Ltrain: 0.000992 	Lval: 0.001016
Epoch: 765 	Ltrain: 0.000979 	Lval: 0.001012
Epoch: 770 	Ltrain: 0.000969 	Lval: 0.001007
Epoch: 775 	Ltrain: 0.000978 	Lval: 0.001003
Epoch: 780 	Ltrain: 0.000972 	Lval: 0.000998
Epoch: 785 	Ltrain: 0.000968 	Lval: 0.000993
Epoch: 790 	Ltrain: 0.000966 	Lval: 0.000989
Epoch: 795 	Ltrain: 0.000962 	Lval: 0.000985
Epoch: 800 	Ltrain: 0.000945 	Lval: 0.000979
Epoch: 805 	Ltrain: 0.000935 	Lval: 0.000975
Epoch: 810 	Ltrain: 0.000945 	Lval: 0.000971
Epoch: 815 	Ltrain: 0.000940 	Lval: 0.000967
Epoch: 820 	Ltrain: 0.000934 	Lval: 0.000962
Epoch: 825 	Ltrain: 0.000938 	Lval: 0.000958
Epoch: 830 	Ltrain: 0.000928 	Lval: 0.000955
Epoch: 835 	Ltrain: 0.000927 	Lval: 0.000950
Epoch: 840 	Ltrain: 0.000905 	Lval: 0.000946
Epoch: 845 	Ltrain: 0.000918 	Lval: 0.000941
Epoch: 850 	Ltrain: 0.000895 	Lval: 0.000938
Epoch: 855 	Ltrain: 0.000907 	Lval: 0.000933
Epoch: 860 	Ltrain: 0.000899 	Lval: 0.000928
Epoch: 865 	Ltrain: 0.000898 	Lval: 0.000925
Epoch: 870 	Ltrain: 0.000890 	Lval: 0.000922
Epoch: 875 	Ltrain: 0.000894 	Lval: 0.000916
Epoch: 880 	Ltrain: 0.000879 	Lval: 0.000911
Epoch: 885 	Ltrain: 0.000887 	Lval: 0.000907
Epoch: 890 	Ltrain: 0.000879 	Lval: 0.000903
Epoch: 895 	Ltrain: 0.000872 	Lval: 0.000898
Epoch: 900 	Ltrain: 0.000868 	Lval: 0.000895
Epoch: 905 	Ltrain: 0.000861 	Lval: 0.000891
Epoch: 910 	Ltrain: 0.000854 	Lval: 0.000886
Epoch: 915 	Ltrain: 0.000863 	Lval: 0.000882
Epoch: 920 	Ltrain: 0.000848 	Lval: 0.000878
Epoch: 925 	Ltrain: 0.000853 	Lval: 0.000874
Epoch: 930 	Ltrain: 0.000844 	Lval: 0.000869
Epoch: 935 	Ltrain: 0.000851 	Lval: 0.000866
Epoch: 940 	Ltrain: 0.000833 	Lval: 0.000862
Epoch: 945 	Ltrain: 0.000857 	Lval: 0.000857
Epoch: 950 	Ltrain: 0.000838 	Lval: 0.000854
Epoch: 955 	Ltrain: 0.000826 	Lval: 0.000850
Epoch: 960 	Ltrain: 0.000828 	Lval: 0.000846
Epoch: 965 	Ltrain: 0.000808 	Lval: 0.000841
Epoch: 970 	Ltrain: 0.000808 	Lval: 0.000837
Epoch: 975 	Ltrain: 0.000817 	Lval: 0.000833
Epoch: 980 	Ltrain: 0.000805 	Lval: 0.000830
Epoch: 985 	Ltrain: 0.000796 	Lval: 0.000825
Epoch: 990 	Ltrain: 0.000795 	Lval: 0.000821
Epoch: 995 	Ltrain: 0.000802 	Lval: 0.000817
Epoch: 1000 	Ltrain: 0.000788 	Lval: 0.000813
Epoch: 1005 	Ltrain: 0.000784 	Lval: 0.000809
Epoch: 1010 	Ltrain: 0.000789 	Lval: 0.000805
Epoch: 1015 	Ltrain: 0.000777 	Lval: 0.000802
Epoch: 1020 	Ltrain: 0.000781 	Lval: 0.000798
Epoch: 1025 	Ltrain: 0.000773 	Lval: 0.000793
Epoch: 1030 	Ltrain: 0.000778 	Lval: 0.000790
Epoch: 1035 	Ltrain: 0.000763 	Lval: 0.000786
Epoch: 1040 	Ltrain: 0.000766 	Lval: 0.000782
Epoch: 1045 	Ltrain: 0.000761 	Lval: 0.000779
Epoch: 1050 	Ltrain: 0.000743 	Lval: 0.000775
Epoch: 1055 	Ltrain: 0.000750 	Lval: 0.000771
Epoch: 1060 	Ltrain: 0.000747 	Lval: 0.000766
Epoch: 1065 	Ltrain: 0.000750 	Lval: 0.000763
Epoch: 1070 	Ltrain: 0.000746 	Lval: 0.000760
Epoch: 1075 	Ltrain: 0.000737 	Lval: 0.000756
Epoch: 1080 	Ltrain: 0.000731 	Lval: 0.000753
Epoch: 1085 	Ltrain: 0.000732 	Lval: 0.000748
Epoch: 1090 	Ltrain: 0.000721 	Lval: 0.000744
Epoch: 1095 	Ltrain: 0.000732 	Lval: 0.000742
Epoch: 1100 	Ltrain: 0.000723 	Lval: 0.000737
Epoch: 1105 	Ltrain: 0.000721 	Lval: 0.000734
Epoch: 1110 	Ltrain: 0.000710 	Lval: 0.000731
Epoch: 1115 	Ltrain: 0.000721 	Lval: 0.000726
Epoch: 1120 	Ltrain: 0.000708 	Lval: 0.000723
Epoch: 1125 	Ltrain: 0.000699 	Lval: 0.000720
Epoch: 1130 	Ltrain: 0.000707 	Lval: 0.000716
Epoch: 1135 	Ltrain: 0.000698 	Lval: 0.000714
Epoch: 1140 	Ltrain: 0.000692 	Lval: 0.000709
Epoch: 1145 	Ltrain: 0.000695 	Lval: 0.000705
Epoch: 1150 	Ltrain: 0.000680 	Lval: 0.000702
Epoch: 1155 	Ltrain: 0.000680 	Lval: 0.000698
Epoch: 1160 	Ltrain: 0.000677 	Lval: 0.000694
Epoch: 1165 	Ltrain: 0.000681 	Lval: 0.000692
Epoch: 1170 	Ltrain: 0.000669 	Lval: 0.000688
Epoch: 1175 	Ltrain: 0.000672 	Lval: 0.000684
Epoch: 1180 	Ltrain: 0.000674 	Lval: 0.000682
Epoch: 1185 	Ltrain: 0.000667 	Lval: 0.000678
Epoch: 1190 	Ltrain: 0.000679 	Lval: 0.000675
Epoch: 1195 	Ltrain: 0.000664 	Lval: 0.000671
Epoch: 1200 	Ltrain: 0.000654 	Lval: 0.000669
Epoch: 1205 	Ltrain: 0.000659 	Lval: 0.000667
Epoch: 1210 	Ltrain: 0.000648 	Lval: 0.000662
Epoch: 1215 	Ltrain: 0.000652 	Lval: 0.000659
Epoch: 1220 	Ltrain: 0.000644 	Lval: 0.000655
Epoch: 1225 	Ltrain: 0.000641 	Lval: 0.000652
Epoch: 1230 	Ltrain: 0.000645 	Lval: 0.000648
Epoch: 1235 	Ltrain: 0.000639 	Lval: 0.000645
Epoch: 1240 	Ltrain: 0.000625 	Lval: 0.000642
Epoch: 1245 	Ltrain: 0.000629 	Lval: 0.000639
Epoch: 1250 	Ltrain: 0.000626 	Lval: 0.000636
Epoch: 1255 	Ltrain: 0.000613 	Lval: 0.000632
Epoch: 1260 	Ltrain: 0.000619 	Lval: 0.000629
Epoch: 1265 	Ltrain: 0.000620 	Lval: 0.000626
Epoch: 1270 	Ltrain: 0.000610 	Lval: 0.000623
Epoch: 1275 	Ltrain: 0.000609 	Lval: 0.000620
Epoch: 1280 	Ltrain: 0.000603 	Lval: 0.000617
Epoch: 1285 	Ltrain: 0.000599 	Lval: 0.000614
Epoch: 1290 	Ltrain: 0.000595 	Lval: 0.000611
Epoch: 1295 	Ltrain: 0.000604 	Lval: 0.000608
Epoch: 1300 	Ltrain: 0.000591 	Lval: 0.000604
Epoch: 1305 	Ltrain: 0.000597 	Lval: 0.000601
Epoch: 1310 	Ltrain: 0.000589 	Lval: 0.000599
Epoch: 1315 	Ltrain: 0.000583 	Lval: 0.000595
Epoch: 1320 	Ltrain: 0.000593 	Lval: 0.000594
Epoch: 1325 	Ltrain: 0.000585 	Lval: 0.000590
Epoch: 1330 	Ltrain: 0.000582 	Lval: 0.000587
Epoch: 1335 	Ltrain: 0.000577 	Lval: 0.000583
Epoch: 1340 	Ltrain: 0.000571 	Lval: 0.000581
Epoch: 1345 	Ltrain: 0.000574 	Lval: 0.000579
Epoch: 1350 	Ltrain: 0.000581 	Lval: 0.000575
Epoch: 1355 	Ltrain: 0.000561 	Lval: 0.000572
Epoch: 1360 	Ltrain: 0.000555 	Lval: 0.000571
Epoch: 1365 	Ltrain: 0.000566 	Lval: 0.000567
Epoch: 1370 	Ltrain: 0.000559 	Lval: 0.000565
Epoch: 1375 	Ltrain: 0.000558 	Lval: 0.000562
Epoch: 1380 	Ltrain: 0.000546 	Lval: 0.000559
Epoch: 1385 	Ltrain: 0.000549 	Lval: 0.000556
Epoch: 1390 	Ltrain: 0.000547 	Lval: 0.000554
Epoch: 1395 	Ltrain: 0.000546 	Lval: 0.000551
Epoch: 1400 	Ltrain: 0.000546 	Lval: 0.000549
Epoch: 1405 	Ltrain: 0.000540 	Lval: 0.000546
Epoch: 1410 	Ltrain: 0.000534 	Lval: 0.000543
Epoch: 1415 	Ltrain: 0.000533 	Lval: 0.000540
Epoch: 1420 	Ltrain: 0.000540 	Lval: 0.000538
Epoch: 1425 	Ltrain: 0.000523 	Lval: 0.000534
Epoch: 1430 	Ltrain: 0.000521 	Lval: 0.000533
Epoch: 1435 	Ltrain: 0.000524 	Lval: 0.000529
Epoch: 1440 	Ltrain: 0.000521 	Lval: 0.000527
Epoch: 1445 	Ltrain: 0.000520 	Lval: 0.000525
Epoch: 1450 	Ltrain: 0.000514 	Lval: 0.000523
Epoch: 1455 	Ltrain: 0.000512 	Lval: 0.000524
Epoch 01456: reducing learning rate of group 0 to 9.6859e-07.
Epoch: 1460 	Ltrain: 0.000508 	Lval: 0.000517
Epoch: 1465 	Ltrain: 0.000509 	Lval: 0.000517
Epoch: 1470 	Ltrain: 0.000510 	Lval: 0.000517
EarlyStopper: stopping at epoch 1472 with best_val_loss = 0.000525


	Fold 3/5
Epoch: 1 	Ltrain: 0.118314 	Lval: 0.020605
Epoch: 5 	Ltrain: 0.008009 	Lval: 0.009298
Epoch: 10 	Ltrain: 0.006701 	Lval: 0.007498
Epoch 00013: reducing learning rate of group 0 to 9.6859e-04.
Epoch: 15 	Ltrain: 0.005827 	Lval: 0.006596
Epoch: 20 	Ltrain: 0.005726 	Lval: 0.006422
Epoch 00025: reducing learning rate of group 0 to 9.6859e-05.
Epoch: 25 	Ltrain: 0.005648 	Lval: 0.006432
Epoch: 30 	Ltrain: 0.005626 	Lval: 0.006383
Epoch: 35 	Ltrain: 0.005571 	Lval: 0.006385
Epoch 00038: reducing learning rate of group 0 to 9.6859e-06.
Epoch: 40 	Ltrain: 0.005619 	Lval: 0.006370
Epoch: 45 	Ltrain: 0.005622 	Lval: 0.006365
Epoch 00050: reducing learning rate of group 0 to 9.6859e-07.
Epoch: 50 	Ltrain: 0.005611 	Lval: 0.006370
Epoch: 55 	Ltrain: 0.005570 	Lval: 0.006369
Epoch: 60 	Ltrain: 0.005581 	Lval: 0.006369
Epoch 00062: reducing learning rate of group 0 to 9.6859e-08.
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.006360


	Fold 4/5
Epoch: 1 	Ltrain: 0.079224 	Lval: 0.020994
Epoch: 5 	Ltrain: 0.006416 	Lval: 0.007392
Epoch: 10 	Ltrain: 0.005386 	Lval: 0.006601
Epoch: 15 	Ltrain: 0.005068 	Lval: 0.005888
Epoch: 20 	Ltrain: 0.004676 	Lval: 0.005647
Epoch 00023: reducing learning rate of group 0 to 9.6859e-04.
Epoch: 25 	Ltrain: 0.004055 	Lval: 0.005079
Epoch: 30 	Ltrain: 0.003923 	Lval: 0.004857
Epoch: 35 	Ltrain: 0.003783 	Lval: 0.004655
Epoch: 40 	Ltrain: 0.003637 	Lval: 0.004373
Epoch: 45 	Ltrain: 0.003508 	Lval: 0.004164
Epoch: 50 	Ltrain: 0.003331 	Lval: 0.003874
Epoch: 55 	Ltrain: 0.003212 	Lval: 0.003531
Epoch: 60 	Ltrain: 0.003044 	Lval: 0.003405
Epoch: 65 	Ltrain: 0.002762 	Lval: 0.002922
Epoch: 70 	Ltrain: 0.002581 	Lval: 0.002950
Epoch: 75 	Ltrain: 0.002376 	Lval: 0.002523
Epoch: 80 	Ltrain: 0.002225 	Lval: 0.002478
Epoch: 85 	Ltrain: 0.002003 	Lval: 0.002065
Epoch: 90 	Ltrain: 0.001890 	Lval: 0.001951
Epoch: 95 	Ltrain: 0.001722 	Lval: 0.001784
Epoch: 100 	Ltrain: 0.001641 	Lval: 0.001614
Epoch: 105 	Ltrain: 0.001557 	Lval: 0.001722
Epoch: 110 	Ltrain: 0.001441 	Lval: 0.001448
Epoch: 115 	Ltrain: 0.001360 	Lval: 0.001437
Epoch: 120 	Ltrain: 0.001243 	Lval: 0.001206
Epoch: 125 	Ltrain: 0.001140 	Lval: 0.001380
Epoch 00128: reducing learning rate of group 0 to 9.6859e-05.
Epoch: 130 	Ltrain: 0.000842 	Lval: 0.000848
Epoch: 135 	Ltrain: 0.000785 	Lval: 0.000800
Epoch: 140 	Ltrain: 0.000762 	Lval: 0.000778
Epoch: 145 	Ltrain: 0.000745 	Lval: 0.000760
Epoch: 150 	Ltrain: 0.000732 	Lval: 0.000745
Epoch: 155 	Ltrain: 0.000718 	Lval: 0.000731
Epoch: 160 	Ltrain: 0.000704 	Lval: 0.000716
Epoch: 165 	Ltrain: 0.000691 	Lval: 0.000702
Epoch: 170 	Ltrain: 0.000675 	Lval: 0.000687
Epoch: 175 	Ltrain: 0.000661 	Lval: 0.000673
Epoch: 180 	Ltrain: 0.000646 	Lval: 0.000656
Epoch: 185 	Ltrain: 0.000631 	Lval: 0.000641
Epoch: 190 	Ltrain: 0.000616 	Lval: 0.000625
Epoch: 195 	Ltrain: 0.000600 	Lval: 0.000609
Epoch: 200 	Ltrain: 0.000584 	Lval: 0.000596
Epoch: 205 	Ltrain: 0.000568 	Lval: 0.000581
Epoch: 210 	Ltrain: 0.000551 	Lval: 0.000565
Epoch: 215 	Ltrain: 0.000535 	Lval: 0.000546
Epoch: 220 	Ltrain: 0.000520 	Lval: 0.000530
Epoch: 225 	Ltrain: 0.000502 	Lval: 0.000512
Epoch: 230 	Ltrain: 0.000488 	Lval: 0.000496
Epoch: 235 	Ltrain: 0.000472 	Lval: 0.000481
Epoch: 240 	Ltrain: 0.000458 	Lval: 0.000471
Epoch: 245 	Ltrain: 0.000445 	Lval: 0.000453
Epoch: 250 	Ltrain: 0.000428 	Lval: 0.000436
Epoch: 255 	Ltrain: 0.000412 	Lval: 0.000424
Epoch: 260 	Ltrain: 0.000399 	Lval: 0.000408
Epoch: 265 	Ltrain: 0.000385 	Lval: 0.000393
Epoch: 270 	Ltrain: 0.000371 	Lval: 0.000386
Epoch 00275: reducing learning rate of group 0 to 9.6859e-06.
Epoch: 275 	Ltrain: 0.000378 	Lval: 0.000380
Epoch: 280 	Ltrain: 0.000342 	Lval: 0.000354
Epoch: 285 	Ltrain: 0.000339 	Lval: 0.000351
Epoch: 290 	Ltrain: 0.000338 	Lval: 0.000349
Epoch: 295 	Ltrain: 0.000337 	Lval: 0.000348
Epoch: 300 	Ltrain: 0.000335 	Lval: 0.000346
Epoch: 305 	Ltrain: 0.000333 	Lval: 0.000344
Epoch: 310 	Ltrain: 0.000332 	Lval: 0.000343
Epoch: 315 	Ltrain: 0.000330 	Lval: 0.000341
Epoch: 320 	Ltrain: 0.000328 	Lval: 0.000339
Epoch: 325 	Ltrain: 0.000327 	Lval: 0.000337
Epoch: 330 	Ltrain: 0.000325 	Lval: 0.000335
Epoch: 335 	Ltrain: 0.000323 	Lval: 0.000334
Epoch: 340 	Ltrain: 0.000321 	Lval: 0.000331
Epoch: 345 	Ltrain: 0.000319 	Lval: 0.000329
Epoch: 350 	Ltrain: 0.000317 	Lval: 0.000327
Epoch: 355 	Ltrain: 0.000316 	Lval: 0.000326
Epoch: 360 	Ltrain: 0.000313 	Lval: 0.000323
Epoch: 365 	Ltrain: 0.000311 	Lval: 0.000321
Epoch: 370 	Ltrain: 0.000309 	Lval: 0.000319
Epoch: 375 	Ltrain: 0.000307 	Lval: 0.000317
Epoch: 380 	Ltrain: 0.000305 	Lval: 0.000315
Epoch: 385 	Ltrain: 0.000303 	Lval: 0.000313
Epoch: 390 	Ltrain: 0.000301 	Lval: 0.000311
Epoch: 395 	Ltrain: 0.000299 	Lval: 0.000309
Epoch: 400 	Ltrain: 0.000297 	Lval: 0.000307
Epoch: 405 	Ltrain: 0.000295 	Lval: 0.000304
Epoch: 410 	Ltrain: 0.000293 	Lval: 0.000302
Epoch: 415 	Ltrain: 0.000292 	Lval: 0.000300
Epoch: 420 	Ltrain: 0.000289 	Lval: 0.000298
Epoch: 425 	Ltrain: 0.000288 	Lval: 0.000296
Epoch: 430 	Ltrain: 0.000285 	Lval: 0.000294
Epoch: 435 	Ltrain: 0.000283 	Lval: 0.000292
Epoch: 440 	Ltrain: 0.000281 	Lval: 0.000290
Epoch: 445 	Ltrain: 0.000279 	Lval: 0.000288
Epoch: 450 	Ltrain: 0.000277 	Lval: 0.000285
Epoch: 455 	Ltrain: 0.000275 	Lval: 0.000283
Epoch: 460 	Ltrain: 0.000273 	Lval: 0.000281
Epoch: 465 	Ltrain: 0.000271 	Lval: 0.000280
Epoch: 470 	Ltrain: 0.000270 	Lval: 0.000277
Epoch: 475 	Ltrain: 0.000267 	Lval: 0.000276
Epoch: 480 	Ltrain: 0.000266 	Lval: 0.000273
Epoch: 485 	Ltrain: 0.000264 	Lval: 0.000272
Epoch: 490 	Ltrain: 0.000262 	Lval: 0.000269
Epoch: 495 	Ltrain: 0.000260 	Lval: 0.000268
Epoch: 500 	Ltrain: 0.000258 	Lval: 0.000266
Epoch: 505 	Ltrain: 0.000257 	Lval: 0.000264
Epoch: 510 	Ltrain: 0.000255 	Lval: 0.000261
Epoch: 515 	Ltrain: 0.000253 	Lval: 0.000259
Epoch: 520 	Ltrain: 0.000251 	Lval: 0.000258
Epoch: 525 	Ltrain: 0.000249 	Lval: 0.000256
Epoch: 530 	Ltrain: 0.000247 	Lval: 0.000254
Epoch: 535 	Ltrain: 0.000246 	Lval: 0.000252
Epoch: 540 	Ltrain: 0.000244 	Lval: 0.000250
Epoch: 545 	Ltrain: 0.000242 	Lval: 0.000248
Epoch: 550 	Ltrain: 0.000240 	Lval: 0.000246
Epoch: 555 	Ltrain: 0.000239 	Lval: 0.000245
Epoch: 560 	Ltrain: 0.000237 	Lval: 0.000243
Epoch: 565 	Ltrain: 0.000235 	Lval: 0.000241
Epoch: 570 	Ltrain: 0.000233 	Lval: 0.000239
Epoch: 575 	Ltrain: 0.000231 	Lval: 0.000237
Epoch: 580 	Ltrain: 0.000230 	Lval: 0.000236
Epoch: 585 	Ltrain: 0.000229 	Lval: 0.000234
Epoch: 590 	Ltrain: 0.000227 	Lval: 0.000232
Epoch: 595 	Ltrain: 0.000225 	Lval: 0.000230
Epoch: 600 	Ltrain: 0.000224 	Lval: 0.000228
Epoch: 605 	Ltrain: 0.000222 	Lval: 0.000227
Epoch: 610 	Ltrain: 0.000221 	Lval: 0.000225
Epoch: 615 	Ltrain: 0.000219 	Lval: 0.000223
Epoch: 620 	Ltrain: 0.000217 	Lval: 0.000221
Epoch: 625 	Ltrain: 0.000215 	Lval: 0.000220
Epoch: 630 	Ltrain: 0.000214 	Lval: 0.000218
Epoch: 635 	Ltrain: 0.000212 	Lval: 0.000217
EarlyStopper: stopping at epoch 634 with best_val_loss = 0.000227


	Fold 5/5
Epoch: 1 	Ltrain: 0.076982 	Lval: 0.016740
Epoch: 5 	Ltrain: 0.006203 	Lval: 0.006951
Epoch: 10 	Ltrain: 0.005517 	Lval: 0.007139
Epoch: 15 	Ltrain: 0.005353 	Lval: 0.006627
Epoch: 20 	Ltrain: 0.005473 	Lval: 0.006747
Epoch: 25 	Ltrain: 0.004807 	Lval: 0.005612
Epoch: 30 	Ltrain: 0.004414 	Lval: 0.005317
Epoch 00035: reducing learning rate of group 0 to 9.6859e-04.
Epoch: 35 	Ltrain: 0.004340 	Lval: 0.005279
Epoch: 40 	Ltrain: 0.003505 	Lval: 0.004179
Epoch: 45 	Ltrain: 0.003382 	Lval: 0.003990
Epoch: 50 	Ltrain: 0.003239 	Lval: 0.003803
Epoch: 55 	Ltrain: 0.003103 	Lval: 0.003552
Epoch: 60 	Ltrain: 0.002961 	Lval: 0.003407
Epoch: 65 	Ltrain: 0.002815 	Lval: 0.003087
Epoch: 70 	Ltrain: 0.002654 	Lval: 0.002930
Epoch: 75 	Ltrain: 0.002500 	Lval: 0.002827
Epoch: 80 	Ltrain: 0.002296 	Lval: 0.002545
Epoch: 85 	Ltrain: 0.002148 	Lval: 0.002401
Epoch: 90 	Ltrain: 0.002060 	Lval: 0.002220
Epoch: 95 	Ltrain: 0.001852 	Lval: 0.002018
Epoch: 100 	Ltrain: 0.001732 	Lval: 0.001904
Epoch 00105: reducing learning rate of group 0 to 9.6859e-05.
Epoch: 105 	Ltrain: 0.001688 	Lval: 0.002009
Epoch: 110 	Ltrain: 0.001327 	Lval: 0.001486
Epoch: 115 	Ltrain: 0.001292 	Lval: 0.001432
Epoch: 120 	Ltrain: 0.001261 	Lval: 0.001398
Epoch: 125 	Ltrain: 0.001239 	Lval: 0.001373
Epoch: 130 	Ltrain: 0.001217 	Lval: 0.001342
Epoch: 135 	Ltrain: 0.001197 	Lval: 0.001314
Epoch: 140 	Ltrain: 0.001176 	Lval: 0.001289
Epoch: 145 	Ltrain: 0.001153 	Lval: 0.001268
Epoch: 150 	Ltrain: 0.001132 	Lval: 0.001240
Epoch: 155 	Ltrain: 0.001106 	Lval: 0.001208
Epoch: 160 	Ltrain: 0.001086 	Lval: 0.001182
Epoch: 165 	Ltrain: 0.001059 	Lval: 0.001156
Epoch: 170 	Ltrain: 0.001036 	Lval: 0.001124
Epoch: 175 	Ltrain: 0.001012 	Lval: 0.001101
Epoch: 180 	Ltrain: 0.000988 	Lval: 0.001074
Epoch: 185 	Ltrain: 0.000963 	Lval: 0.001044
Epoch: 190 	Ltrain: 0.000941 	Lval: 0.001019
Epoch: 195 	Ltrain: 0.000918 	Lval: 0.000996
Epoch: 200 	Ltrain: 0.000890 	Lval: 0.000962
Epoch: 205 	Ltrain: 0.000868 	Lval: 0.000936
Epoch: 210 	Ltrain: 0.000842 	Lval: 0.000904
Epoch: 215 	Ltrain: 0.000820 	Lval: 0.000878
Epoch: 220 	Ltrain: 0.000799 	Lval: 0.000857
Epoch: 225 	Ltrain: 0.000779 	Lval: 0.000831
Epoch: 230 	Ltrain: 0.000757 	Lval: 0.000809
Epoch: 235 	Ltrain: 0.000733 	Lval: 0.000781
Epoch: 240 	Ltrain: 0.000714 	Lval: 0.000752
Epoch: 245 	Ltrain: 0.000691 	Lval: 0.000735
Epoch: 250 	Ltrain: 0.000683 	Lval: 0.000726
Epoch: 255 	Ltrain: 0.000657 	Lval: 0.000695
Epoch: 260 	Ltrain: 0.000640 	Lval: 0.000673
Epoch 00264: reducing learning rate of group 0 to 9.6859e-06.
Epoch: 265 	Ltrain: 0.000629 	Lval: 0.000656
Epoch: 270 	Ltrain: 0.000598 	Lval: 0.000635
Epoch: 275 	Ltrain: 0.000594 	Lval: 0.000631
Epoch: 280 	Ltrain: 0.000591 	Lval: 0.000629
Epoch: 285 	Ltrain: 0.000590 	Lval: 0.000626
Epoch: 290 	Ltrain: 0.000587 	Lval: 0.000623
Epoch: 295 	Ltrain: 0.000585 	Lval: 0.000620
Epoch: 300 	Ltrain: 0.000583 	Lval: 0.000618
Epoch: 305 	Ltrain: 0.000580 	Lval: 0.000615
Epoch: 310 	Ltrain: 0.000578 	Lval: 0.000612
Epoch: 315 	Ltrain: 0.000575 	Lval: 0.000609
Epoch: 320 	Ltrain: 0.000572 	Lval: 0.000606
Epoch: 325 	Ltrain: 0.000571 	Lval: 0.000603
Epoch: 330 	Ltrain: 0.000567 	Lval: 0.000601
Epoch: 335 	Ltrain: 0.000566 	Lval: 0.000597
Epoch: 340 	Ltrain: 0.000562 	Lval: 0.000594
Epoch: 345 	Ltrain: 0.000560 	Lval: 0.000591
Epoch: 350 	Ltrain: 0.000556 	Lval: 0.000588
Epoch: 355 	Ltrain: 0.000553 	Lval: 0.000585
Epoch: 360 	Ltrain: 0.000550 	Lval: 0.000582
Epoch: 365 	Ltrain: 0.000547 	Lval: 0.000578
Epoch: 370 	Ltrain: 0.000545 	Lval: 0.000575
Epoch: 375 	Ltrain: 0.000542 	Lval: 0.000572
Epoch: 380 	Ltrain: 0.000539 	Lval: 0.000568
Epoch: 385 	Ltrain: 0.000535 	Lval: 0.000565
Epoch: 390 	Ltrain: 0.000534 	Lval: 0.000562
Epoch: 395 	Ltrain: 0.000532 	Lval: 0.000559
Epoch: 400 	Ltrain: 0.000528 	Lval: 0.000556
Epoch: 405 	Ltrain: 0.000529 	Lval: 0.000553
Epoch: 410 	Ltrain: 0.000521 	Lval: 0.000549
Epoch: 415 	Ltrain: 0.000519 	Lval: 0.000546
Epoch: 420 	Ltrain: 0.000517 	Lval: 0.000543
Epoch: 425 	Ltrain: 0.000513 	Lval: 0.000539
Epoch: 430 	Ltrain: 0.000511 	Lval: 0.000536
Epoch: 435 	Ltrain: 0.000507 	Lval: 0.000533
Epoch: 440 	Ltrain: 0.000504 	Lval: 0.000530
Epoch: 445 	Ltrain: 0.000502 	Lval: 0.000527
Epoch: 450 	Ltrain: 0.000499 	Lval: 0.000524
Epoch: 455 	Ltrain: 0.000496 	Lval: 0.000521
Epoch: 460 	Ltrain: 0.000493 	Lval: 0.000518
Epoch: 465 	Ltrain: 0.000491 	Lval: 0.000515
Epoch: 470 	Ltrain: 0.000488 	Lval: 0.000512
Epoch: 475 	Ltrain: 0.000489 	Lval: 0.000509
Epoch: 480 	Ltrain: 0.000482 	Lval: 0.000506
Epoch: 485 	Ltrain: 0.000480 	Lval: 0.000502
Epoch: 490 	Ltrain: 0.000478 	Lval: 0.000500
Epoch: 495 	Ltrain: 0.000474 	Lval: 0.000498
Epoch: 500 	Ltrain: 0.000471 	Lval: 0.000494
Epoch: 505 	Ltrain: 0.000469 	Lval: 0.000491
Epoch: 510 	Ltrain: 0.000466 	Lval: 0.000488
Epoch: 515 	Ltrain: 0.000463 	Lval: 0.000485
Epoch: 520 	Ltrain: 0.000461 	Lval: 0.000482
Epoch: 525 	Ltrain: 0.000458 	Lval: 0.000480
Epoch: 530 	Ltrain: 0.000456 	Lval: 0.000477
Epoch: 535 	Ltrain: 0.000453 	Lval: 0.000474
Epoch: 540 	Ltrain: 0.000454 	Lval: 0.000472
Epoch: 545 	Ltrain: 0.000449 	Lval: 0.000469
Epoch: 550 	Ltrain: 0.000445 	Lval: 0.000466
Epoch: 555 	Ltrain: 0.000444 	Lval: 0.000463
Epoch: 560 	Ltrain: 0.000440 	Lval: 0.000460
Epoch: 565 	Ltrain: 0.000438 	Lval: 0.000458
Epoch: 570 	Ltrain: 0.000434 	Lval: 0.000455
Epoch: 575 	Ltrain: 0.000437 	Lval: 0.000452
Epoch: 580 	Ltrain: 0.000430 	Lval: 0.000450
Epoch: 585 	Ltrain: 0.000428 	Lval: 0.000447
Epoch: 590 	Ltrain: 0.000425 	Lval: 0.000444
Epoch: 595 	Ltrain: 0.000428 	Lval: 0.000442
Epoch: 600 	Ltrain: 0.000420 	Lval: 0.000439
Epoch: 605 	Ltrain: 0.000418 	Lval: 0.000437
Epoch: 610 	Ltrain: 0.000416 	Lval: 0.000434
Epoch: 615 	Ltrain: 0.000414 	Lval: 0.000431
Epoch: 620 	Ltrain: 0.000410 	Lval: 0.000429
Epoch: 625 	Ltrain: 0.000409 	Lval: 0.000426
Epoch: 630 	Ltrain: 0.000406 	Lval: 0.000424
Epoch: 635 	Ltrain: 0.000403 	Lval: 0.000421
Epoch: 640 	Ltrain: 0.000401 	Lval: 0.000419
Epoch: 645 	Ltrain: 0.000399 	Lval: 0.000416
Epoch: 650 	Ltrain: 0.000401 	Lval: 0.000414
Epoch: 655 	Ltrain: 0.000395 	Lval: 0.000411
Epoch: 660 	Ltrain: 0.000392 	Lval: 0.000409
Epoch: 665 	Ltrain: 0.000390 	Lval: 0.000406
Epoch: 670 	Ltrain: 0.000388 	Lval: 0.000404
Epoch: 675 	Ltrain: 0.000385 	Lval: 0.000401
Epoch: 680 	Ltrain: 0.000383 	Lval: 0.000399
Epoch: 685 	Ltrain: 0.000381 	Lval: 0.000397
Epoch: 690 	Ltrain: 0.000378 	Lval: 0.000394
Epoch: 695 	Ltrain: 0.000377 	Lval: 0.000391
Epoch: 700 	Ltrain: 0.000374 	Lval: 0.000389
Epoch: 705 	Ltrain: 0.000372 	Lval: 0.000387
Epoch: 710 	Ltrain: 0.000371 	Lval: 0.000384
Epoch: 715 	Ltrain: 0.000368 	Lval: 0.000382
Epoch: 720 	Ltrain: 0.000366 	Lval: 0.000380
Epoch: 725 	Ltrain: 0.000363 	Lval: 0.000377
Epoch: 730 	Ltrain: 0.000361 	Lval: 0.000374
Epoch: 735 	Ltrain: 0.000358 	Lval: 0.000372
Epoch: 740 	Ltrain: 0.000357 	Lval: 0.000370
Epoch: 745 	Ltrain: 0.000356 	Lval: 0.000368
Epoch: 750 	Ltrain: 0.000353 	Lval: 0.000366
Epoch: 755 	Ltrain: 0.000351 	Lval: 0.000364
Epoch: 760 	Ltrain: 0.000349 	Lval: 0.000361
Epoch: 765 	Ltrain: 0.000347 	Lval: 0.000358
Epoch: 770 	Ltrain: 0.000345 	Lval: 0.000356
Epoch: 775 	Ltrain: 0.000343 	Lval: 0.000354
Epoch: 780 	Ltrain: 0.000341 	Lval: 0.000352
Epoch: 785 	Ltrain: 0.000338 	Lval: 0.000349
Epoch: 790 	Ltrain: 0.000336 	Lval: 0.000347
Epoch: 795 	Ltrain: 0.000335 	Lval: 0.000345
Epoch: 800 	Ltrain: 0.000333 	Lval: 0.000342
Epoch: 805 	Ltrain: 0.000331 	Lval: 0.000340
Epoch: 810 	Ltrain: 0.000329 	Lval: 0.000338
Epoch: 815 	Ltrain: 0.000326 	Lval: 0.000336
Epoch: 820 	Ltrain: 0.000325 	Lval: 0.000334
Epoch: 825 	Ltrain: 0.000323 	Lval: 0.000331
Epoch: 830 	Ltrain: 0.000321 	Lval: 0.000329
Epoch: 835 	Ltrain: 0.000319 	Lval: 0.000328
Epoch: 840 	Ltrain: 0.000317 	Lval: 0.000325
Epoch: 845 	Ltrain: 0.000315 	Lval: 0.000323
Epoch: 850 	Ltrain: 0.000313 	Lval: 0.000321
Epoch: 855 	Ltrain: 0.000311 	Lval: 0.000318
Epoch: 860 	Ltrain: 0.000310 	Lval: 0.000316
Epoch: 865 	Ltrain: 0.000308 	Lval: 0.000314
Epoch: 870 	Ltrain: 0.000306 	Lval: 0.000312
Epoch: 875 	Ltrain: 0.000304 	Lval: 0.000310
Epoch: 880 	Ltrain: 0.000302 	Lval: 0.000308
Epoch: 885 	Ltrain: 0.000300 	Lval: 0.000306
Epoch: 890 	Ltrain: 0.000299 	Lval: 0.000304
Epoch: 895 	Ltrain: 0.000300 	Lval: 0.000302
Epoch: 900 	Ltrain: 0.000296 	Lval: 0.000300
Epoch: 905 	Ltrain: 0.000293 	Lval: 0.000298
Epoch: 910 	Ltrain: 0.000292 	Lval: 0.000296
Epoch: 915 	Ltrain: 0.000290 	Lval: 0.000294
Epoch: 920 	Ltrain: 0.000288 	Lval: 0.000292
Epoch: 925 	Ltrain: 0.000286 	Lval: 0.000290
Epoch: 930 	Ltrain: 0.000284 	Lval: 0.000288
Epoch: 935 	Ltrain: 0.000283 	Lval: 0.000286
Epoch: 940 	Ltrain: 0.000281 	Lval: 0.000284
Epoch: 945 	Ltrain: 0.000280 	Lval: 0.000282
Epoch: 950 	Ltrain: 0.000278 	Lval: 0.000280
Epoch: 955 	Ltrain: 0.000276 	Lval: 0.000278
Epoch: 960 	Ltrain: 0.000275 	Lval: 0.000276
Epoch: 965 	Ltrain: 0.000272 	Lval: 0.000274
Epoch: 970 	Ltrain: 0.000271 	Lval: 0.000273
Epoch: 975 	Ltrain: 0.000270 	Lval: 0.000271
Epoch: 980 	Ltrain: 0.000268 	Lval: 0.000269
Epoch: 985 	Ltrain: 0.000266 	Lval: 0.000267
Epoch: 990 	Ltrain: 0.000264 	Lval: 0.000265
Epoch: 995 	Ltrain: 0.000263 	Lval: 0.000264
Epoch: 1000 	Ltrain: 0.000262 	Lval: 0.000262
Epoch: 1005 	Ltrain: 0.000260 	Lval: 0.000261
Epoch: 1010 	Ltrain: 0.000258 	Lval: 0.000258
Epoch: 1015 	Ltrain: 0.000258 	Lval: 0.000257
Epoch: 1020 	Ltrain: 0.000255 	Lval: 0.000255
Epoch: 1025 	Ltrain: 0.000254 	Lval: 0.000254
EarlyStopper: stopping at epoch 1027 with best_val_loss = 0.000263

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004809399715283114
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.915314468484723e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 29
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.046950 	Lval: 0.017239
Epoch: 5 	Ltrain: 0.011138 	Lval: 0.009448
Epoch: 10 	Ltrain: 0.007593 	Lval: 0.008300
Epoch 00012: reducing learning rate of group 0 to 4.8094e-04.
Epoch: 15 	Ltrain: 0.006990 	Lval: 0.007113
Epoch: 20 	Ltrain: 0.007775 	Lval: 0.006895
Epoch: 25 	Ltrain: 0.007094 	Lval: 0.006742
Epoch: 30 	Ltrain: 0.006943 	Lval: 0.006667
Epoch: 35 	Ltrain: 0.006996 	Lval: 0.006577
Epoch: 40 	Ltrain: 0.006382 	Lval: 0.006684
Epoch: 45 	Ltrain: 0.006596 	Lval: 0.006471
Epoch: 50 	Ltrain: 0.006436 	Lval: 0.006263
Epoch: 55 	Ltrain: 0.006098 	Lval: 0.006185
Epoch: 60 	Ltrain: 0.006564 	Lval: 0.006219
Epoch: 65 	Ltrain: 0.006230 	Lval: 0.005979
Epoch 00069: reducing learning rate of group 0 to 4.8094e-05.
Epoch: 70 	Ltrain: 0.006242 	Lval: 0.005963
Epoch: 75 	Ltrain: 0.006571 	Lval: 0.005958
Epoch: 80 	Ltrain: 0.005747 	Lval: 0.005893
Epoch 00085: reducing learning rate of group 0 to 4.8094e-06.
Epoch: 85 	Ltrain: 0.006235 	Lval: 0.005889
Epoch: 90 	Ltrain: 0.006049 	Lval: 0.005890
Epoch: 95 	Ltrain: 0.005899 	Lval: 0.005884
Epoch: 100 	Ltrain: 0.005656 	Lval: 0.005883
Epoch: 105 	Ltrain: 0.006146 	Lval: 0.005879
Epoch 00108: reducing learning rate of group 0 to 4.8094e-07.
Epoch: 110 	Ltrain: 0.005831 	Lval: 0.005880
EarlyStopper: stopping at epoch 109 with best_val_loss = 0.005889


	Fold 2/5
Epoch: 1 	Ltrain: 0.030873 	Lval: 0.011352
Epoch: 5 	Ltrain: 0.008252 	Lval: 0.008874
Epoch: 10 	Ltrain: 0.006892 	Lval: 0.006968
Epoch: 15 	Ltrain: 0.006638 	Lval: 0.006753
Epoch: 20 	Ltrain: 0.006838 	Lval: 0.006487
Epoch: 25 	Ltrain: 0.005947 	Lval: 0.006325
Epoch: 30 	Ltrain: 0.005795 	Lval: 0.006020
Epoch: 35 	Ltrain: 0.005862 	Lval: 0.005868
Epoch: 40 	Ltrain: 0.005655 	Lval: 0.005467
Epoch: 45 	Ltrain: 0.005022 	Lval: 0.004703
Epoch: 50 	Ltrain: 0.004716 	Lval: 0.004984
Epoch: 55 	Ltrain: 0.005206 	Lval: 0.004888
Epoch: 60 	Ltrain: 0.003879 	Lval: 0.003469
Epoch: 65 	Ltrain: 0.003818 	Lval: 0.003793
Epoch 00066: reducing learning rate of group 0 to 4.8094e-04.
Epoch: 70 	Ltrain: 0.002588 	Lval: 0.002522
Epoch: 75 	Ltrain: 0.002349 	Lval: 0.002298
Epoch: 80 	Ltrain: 0.002184 	Lval: 0.002113
Epoch: 85 	Ltrain: 0.002044 	Lval: 0.001955
Epoch: 90 	Ltrain: 0.001899 	Lval: 0.001818
Epoch: 95 	Ltrain: 0.001751 	Lval: 0.001693
Epoch: 100 	Ltrain: 0.001639 	Lval: 0.001558
Epoch: 105 	Ltrain: 0.001498 	Lval: 0.001431
Epoch: 110 	Ltrain: 0.001411 	Lval: 0.001332
Epoch: 115 	Ltrain: 0.001271 	Lval: 0.001204
Epoch: 120 	Ltrain: 0.001186 	Lval: 0.001112
Epoch: 125 	Ltrain: 0.001084 	Lval: 0.001027
Epoch: 130 	Ltrain: 0.000996 	Lval: 0.000938
Epoch: 135 	Ltrain: 0.000932 	Lval: 0.000873
Epoch: 140 	Ltrain: 0.000917 	Lval: 0.000848
Epoch: 145 	Ltrain: 0.000814 	Lval: 0.000751
Epoch: 150 	Ltrain: 0.000740 	Lval: 0.000717
Epoch 00151: reducing learning rate of group 0 to 4.8094e-05.
Epoch: 155 	Ltrain: 0.000660 	Lval: 0.000626
Epoch: 160 	Ltrain: 0.000637 	Lval: 0.000614
Epoch: 165 	Ltrain: 0.000636 	Lval: 0.000606
Epoch: 170 	Ltrain: 0.000631 	Lval: 0.000600
Epoch: 175 	Ltrain: 0.000624 	Lval: 0.000593
Epoch: 180 	Ltrain: 0.000606 	Lval: 0.000587
Epoch: 185 	Ltrain: 0.000609 	Lval: 0.000579
Epoch: 190 	Ltrain: 0.000616 	Lval: 0.000572
Epoch: 195 	Ltrain: 0.000598 	Lval: 0.000566
Epoch: 200 	Ltrain: 0.000582 	Lval: 0.000558
Epoch: 205 	Ltrain: 0.000577 	Lval: 0.000552
Epoch: 210 	Ltrain: 0.000573 	Lval: 0.000545
Epoch: 215 	Ltrain: 0.000560 	Lval: 0.000537
Epoch: 220 	Ltrain: 0.000554 	Lval: 0.000530
Epoch: 225 	Ltrain: 0.000555 	Lval: 0.000523
Epoch: 230 	Ltrain: 0.000550 	Lval: 0.000516
Epoch: 235 	Ltrain: 0.000543 	Lval: 0.000509
Epoch: 240 	Ltrain: 0.000526 	Lval: 0.000502
Epoch: 245 	Ltrain: 0.000525 	Lval: 0.000495
Epoch: 250 	Ltrain: 0.000524 	Lval: 0.000488
Epoch: 255 	Ltrain: 0.000507 	Lval: 0.000481
Epoch: 260 	Ltrain: 0.000497 	Lval: 0.000472
Epoch: 265 	Ltrain: 0.000498 	Lval: 0.000465
Epoch: 270 	Ltrain: 0.000482 	Lval: 0.000458
Epoch: 275 	Ltrain: 0.000473 	Lval: 0.000450
Epoch: 280 	Ltrain: 0.000462 	Lval: 0.000443
Epoch: 285 	Ltrain: 0.000459 	Lval: 0.000438
Epoch: 290 	Ltrain: 0.000452 	Lval: 0.000429
Epoch: 295 	Ltrain: 0.000444 	Lval: 0.000422
Epoch: 300 	Ltrain: 0.000445 	Lval: 0.000415
Epoch: 305 	Ltrain: 0.000430 	Lval: 0.000408
Epoch: 310 	Ltrain: 0.000426 	Lval: 0.000402
Epoch: 315 	Ltrain: 0.000419 	Lval: 0.000394
Epoch: 320 	Ltrain: 0.000414 	Lval: 0.000390
Epoch: 325 	Ltrain: 0.000400 	Lval: 0.000380
Epoch: 330 	Ltrain: 0.000398 	Lval: 0.000375
Epoch: 335 	Ltrain: 0.000388 	Lval: 0.000368
Epoch: 340 	Ltrain: 0.000386 	Lval: 0.000362
Epoch: 345 	Ltrain: 0.000379 	Lval: 0.000359
Epoch: 350 	Ltrain: 0.000368 	Lval: 0.000350
Epoch: 355 	Ltrain: 0.000360 	Lval: 0.000343
Epoch: 360 	Ltrain: 0.000354 	Lval: 0.000337
Epoch: 365 	Ltrain: 0.000348 	Lval: 0.000330
Epoch: 370 	Ltrain: 0.000337 	Lval: 0.000324
Epoch: 375 	Ltrain: 0.000330 	Lval: 0.000319
Epoch: 380 	Ltrain: 0.000332 	Lval: 0.000316
Epoch: 385 	Ltrain: 0.000321 	Lval: 0.000307
Epoch: 390 	Ltrain: 0.000320 	Lval: 0.000303
Epoch: 395 	Ltrain: 0.000314 	Lval: 0.000299
Epoch: 400 	Ltrain: 0.000313 	Lval: 0.000292
Epoch: 405 	Ltrain: 0.000302 	Lval: 0.000287
Epoch: 410 	Ltrain: 0.000294 	Lval: 0.000281
Epoch: 415 	Ltrain: 0.000290 	Lval: 0.000276
Epoch: 420 	Ltrain: 0.000286 	Lval: 0.000271
Epoch: 425 	Ltrain: 0.000281 	Lval: 0.000271
Epoch: 430 	Ltrain: 0.000276 	Lval: 0.000262
Epoch: 435 	Ltrain: 0.000268 	Lval: 0.000260
Epoch: 440 	Ltrain: 0.000262 	Lval: 0.000252
Epoch: 445 	Ltrain: 0.000260 	Lval: 0.000248
Epoch: 450 	Ltrain: 0.000252 	Lval: 0.000243
Epoch: 455 	Ltrain: 0.000250 	Lval: 0.000238
Epoch: 460 	Ltrain: 0.000244 	Lval: 0.000235
Epoch: 465 	Ltrain: 0.000238 	Lval: 0.000230
Epoch: 470 	Ltrain: 0.000235 	Lval: 0.000229
Epoch: 475 	Ltrain: 0.000239 	Lval: 0.000223
Epoch: 480 	Ltrain: 0.000225 	Lval: 0.000218
Epoch: 485 	Ltrain: 0.000224 	Lval: 0.000216
Epoch: 490 	Ltrain: 0.000215 	Lval: 0.000210
Epoch: 495 	Ltrain: 0.000216 	Lval: 0.000206
Epoch: 500 	Ltrain: 0.000215 	Lval: 0.000208
Epoch 00505: reducing learning rate of group 0 to 4.8094e-06.
Epoch: 505 	Ltrain: 0.000219 	Lval: 0.000205
Epoch: 510 	Ltrain: 0.000202 	Lval: 0.000197
Epoch: 515 	Ltrain: 0.000202 	Lval: 0.000196
Epoch: 520 	Ltrain: 0.000201 	Lval: 0.000196
Epoch: 525 	Ltrain: 0.000200 	Lval: 0.000195
Epoch: 530 	Ltrain: 0.000200 	Lval: 0.000195
Epoch: 535 	Ltrain: 0.000199 	Lval: 0.000194
EarlyStopper: stopping at epoch 538 with best_val_loss = 0.000197


	Fold 3/5
Epoch: 1 	Ltrain: 0.029281 	Lval: 0.011794
Epoch: 5 	Ltrain: 0.006721 	Lval: 0.006997
Epoch: 10 	Ltrain: 0.007058 	Lval: 0.009952
Epoch: 15 	Ltrain: 0.006359 	Lval: 0.006795
Epoch 00016: reducing learning rate of group 0 to 4.8094e-04.
Epoch: 20 	Ltrain: 0.005562 	Lval: 0.006283
Epoch: 25 	Ltrain: 0.005479 	Lval: 0.006246
Epoch: 30 	Ltrain: 0.005482 	Lval: 0.006117
Epoch 00031: reducing learning rate of group 0 to 4.8094e-05.
Epoch: 35 	Ltrain: 0.005400 	Lval: 0.006045
Epoch: 40 	Ltrain: 0.005389 	Lval: 0.006044
Epoch: 45 	Ltrain: 0.005365 	Lval: 0.006025
Epoch: 50 	Ltrain: 0.005375 	Lval: 0.006019
Epoch: 55 	Ltrain: 0.005337 	Lval: 0.006003
Epoch: 60 	Ltrain: 0.005368 	Lval: 0.006006
Epoch: 65 	Ltrain: 0.005353 	Lval: 0.005976
Epoch: 70 	Ltrain: 0.005337 	Lval: 0.005967
Epoch: 75 	Ltrain: 0.005335 	Lval: 0.005952
Epoch: 80 	Ltrain: 0.005288 	Lval: 0.005946
Epoch: 85 	Ltrain: 0.005346 	Lval: 0.005935
Epoch: 90 	Ltrain: 0.005275 	Lval: 0.005937
Epoch 00092: reducing learning rate of group 0 to 4.8094e-06.
Epoch: 95 	Ltrain: 0.005289 	Lval: 0.005925
Epoch: 100 	Ltrain: 0.005271 	Lval: 0.005920
Epoch: 105 	Ltrain: 0.005271 	Lval: 0.005918
Epoch: 110 	Ltrain: 0.005274 	Lval: 0.005917
Epoch 00111: reducing learning rate of group 0 to 4.8094e-07.
Epoch: 115 	Ltrain: 0.005255 	Lval: 0.005916
Epoch: 120 	Ltrain: 0.005224 	Lval: 0.005916
Epoch 00123: reducing learning rate of group 0 to 4.8094e-08.
Epoch: 125 	Ltrain: 0.005270 	Lval: 0.005916
Epoch: 130 	Ltrain: 0.005288 	Lval: 0.005916
EarlyStopper: stopping at epoch 132 with best_val_loss = 0.005916


	Fold 4/5
Epoch: 1 	Ltrain: 0.021389 	Lval: 0.011566
Epoch: 5 	Ltrain: 0.005892 	Lval: 0.007259
Epoch: 10 	Ltrain: 0.005450 	Lval: 0.006730
Epoch 00015: reducing learning rate of group 0 to 4.8094e-04.
Epoch: 15 	Ltrain: 0.005573 	Lval: 0.006530
Epoch: 20 	Ltrain: 0.004782 	Lval: 0.006038
Epoch: 25 	Ltrain: 0.004712 	Lval: 0.005885
Epoch: 30 	Ltrain: 0.004687 	Lval: 0.005892
Epoch: 35 	Ltrain: 0.004579 	Lval: 0.005917
Epoch 00040: reducing learning rate of group 0 to 4.8094e-05.
Epoch: 40 	Ltrain: 0.004508 	Lval: 0.005657
Epoch: 45 	Ltrain: 0.004405 	Lval: 0.005552
Epoch: 50 	Ltrain: 0.004386 	Lval: 0.005528
Epoch: 55 	Ltrain: 0.004399 	Lval: 0.005537
Epoch 00057: reducing learning rate of group 0 to 4.8094e-06.
Epoch: 60 	Ltrain: 0.004354 	Lval: 0.005513
Epoch: 65 	Ltrain: 0.004353 	Lval: 0.005518
Epoch 00069: reducing learning rate of group 0 to 4.8094e-07.
Epoch: 70 	Ltrain: 0.004350 	Lval: 0.005517
Epoch: 75 	Ltrain: 0.004345 	Lval: 0.005518
Epoch: 80 	Ltrain: 0.004344 	Lval: 0.005517
Epoch 00081: reducing learning rate of group 0 to 4.8094e-08.
EarlyStopper: stopping at epoch 81 with best_val_loss = 0.005508


	Fold 5/5
Epoch: 1 	Ltrain: 0.023480 	Lval: 0.010207
Epoch: 5 	Ltrain: 0.006316 	Lval: 0.007645
Epoch 00007: reducing learning rate of group 0 to 4.8094e-04.
Epoch: 10 	Ltrain: 0.005225 	Lval: 0.006767
Epoch: 15 	Ltrain: 0.005193 	Lval: 0.006631
Epoch: 20 	Ltrain: 0.005151 	Lval: 0.006565
Epoch: 25 	Ltrain: 0.005082 	Lval: 0.006425
Epoch 00029: reducing learning rate of group 0 to 4.8094e-05.
Epoch: 30 	Ltrain: 0.004982 	Lval: 0.006400
Epoch: 35 	Ltrain: 0.004935 	Lval: 0.006412
Epoch: 40 	Ltrain: 0.004930 	Lval: 0.006428
Epoch 00043: reducing learning rate of group 0 to 4.8094e-06.
Epoch: 45 	Ltrain: 0.004909 	Lval: 0.006357
Epoch: 50 	Ltrain: 0.004910 	Lval: 0.006352
Epoch 00055: reducing learning rate of group 0 to 4.8094e-07.
Epoch: 55 	Ltrain: 0.004912 	Lval: 0.006351
Epoch: 60 	Ltrain: 0.004910 	Lval: 0.006351
Epoch: 65 	Ltrain: 0.004910 	Lval: 0.006352
Epoch 00067: reducing learning rate of group 0 to 4.8094e-08.
EarlyStopper: stopping at epoch 67 with best_val_loss = 0.006324

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0018730011922935604
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.137604875685652e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 30
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.035985 	Lval: 0.016608
Epoch: 5 	Ltrain: 0.013023 	Lval: 0.012398
Epoch: 10 	Ltrain: 0.009996 	Lval: 0.008386
Epoch: 15 	Ltrain: 0.007648 	Lval: 0.007756
Epoch: 20 	Ltrain: 0.008876 	Lval: 0.010289
Epoch 00021: reducing learning rate of group 0 to 1.8730e-04.
Epoch: 25 	Ltrain: 0.006975 	Lval: 0.006895
Epoch: 30 	Ltrain: 0.006925 	Lval: 0.006828
Epoch: 35 	Ltrain: 0.006748 	Lval: 0.006759
Epoch: 40 	Ltrain: 0.006549 	Lval: 0.006739
Epoch: 45 	Ltrain: 0.007500 	Lval: 0.006676
Epoch: 50 	Ltrain: 0.006625 	Lval: 0.006621
Epoch: 55 	Ltrain: 0.006572 	Lval: 0.006547
Epoch: 60 	Ltrain: 0.007319 	Lval: 0.006622
Epoch 00062: reducing learning rate of group 0 to 1.8730e-05.
Epoch: 65 	Ltrain: 0.006605 	Lval: 0.006477
Epoch: 70 	Ltrain: 0.006447 	Lval: 0.006451
Epoch: 75 	Ltrain: 0.006724 	Lval: 0.006442
Epoch: 80 	Ltrain: 0.006745 	Lval: 0.006434
Epoch: 85 	Ltrain: 0.006800 	Lval: 0.006425
Epoch: 90 	Ltrain: 0.006246 	Lval: 0.006416
Epoch 00094: reducing learning rate of group 0 to 1.8730e-06.
Epoch: 95 	Ltrain: 0.006390 	Lval: 0.006418
Epoch: 100 	Ltrain: 0.006520 	Lval: 0.006419
Epoch: 105 	Ltrain: 0.006835 	Lval: 0.006416
Epoch 00108: reducing learning rate of group 0 to 1.8730e-07.
Epoch: 110 	Ltrain: 0.007260 	Lval: 0.006416
Epoch: 115 	Ltrain: 0.006487 	Lval: 0.006416
Epoch 00120: reducing learning rate of group 0 to 1.8730e-08.
Epoch: 120 	Ltrain: 0.007166 	Lval: 0.006416
EarlyStopper: stopping at epoch 119 with best_val_loss = 0.006416


	Fold 2/5
Epoch: 1 	Ltrain: 0.026245 	Lval: 0.016192
Epoch: 5 	Ltrain: 0.008011 	Lval: 0.009618
Epoch: 10 	Ltrain: 0.007214 	Lval: 0.007262
Epoch: 15 	Ltrain: 0.006609 	Lval: 0.006834
Epoch: 20 	Ltrain: 0.006699 	Lval: 0.007029
Epoch: 25 	Ltrain: 0.006416 	Lval: 0.006345
Epoch: 30 	Ltrain: 0.006228 	Lval: 0.006024
Epoch 00034: reducing learning rate of group 0 to 1.8730e-04.
Epoch: 35 	Ltrain: 0.005880 	Lval: 0.005811
Epoch: 40 	Ltrain: 0.005389 	Lval: 0.005700
Epoch: 45 	Ltrain: 0.005465 	Lval: 0.005630
Epoch: 50 	Ltrain: 0.005352 	Lval: 0.005587
Epoch 00053: reducing learning rate of group 0 to 1.8730e-05.
Epoch: 55 	Ltrain: 0.005276 	Lval: 0.005557
Epoch: 60 	Ltrain: 0.005243 	Lval: 0.005532
Epoch: 65 	Ltrain: 0.005391 	Lval: 0.005534
Epoch 00070: reducing learning rate of group 0 to 1.8730e-06.
Epoch: 70 	Ltrain: 0.005174 	Lval: 0.005527
Epoch: 75 	Ltrain: 0.005455 	Lval: 0.005525
Epoch: 80 	Ltrain: 0.005252 	Lval: 0.005527
Epoch 00082: reducing learning rate of group 0 to 1.8730e-07.
Epoch: 85 	Ltrain: 0.005234 	Lval: 0.005526
Epoch: 90 	Ltrain: 0.005460 	Lval: 0.005526
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.005532


	Fold 3/5
Epoch: 1 	Ltrain: 0.024754 	Lval: 0.015530
Epoch: 5 	Ltrain: 0.006868 	Lval: 0.008155
Epoch: 10 	Ltrain: 0.006412 	Lval: 0.008560
Epoch 00011: reducing learning rate of group 0 to 1.8730e-04.
Epoch: 15 	Ltrain: 0.005904 	Lval: 0.006712
Epoch: 20 	Ltrain: 0.005896 	Lval: 0.006647
Epoch 00024: reducing learning rate of group 0 to 1.8730e-05.
Epoch: 25 	Ltrain: 0.005814 	Lval: 0.006633
Epoch: 30 	Ltrain: 0.005811 	Lval: 0.006617
Epoch: 35 	Ltrain: 0.005781 	Lval: 0.006623
Epoch: 40 	Ltrain: 0.005861 	Lval: 0.006603
Epoch 00045: reducing learning rate of group 0 to 1.8730e-06.
Epoch: 45 	Ltrain: 0.005805 	Lval: 0.006604
Epoch: 50 	Ltrain: 0.005785 	Lval: 0.006602
Epoch: 55 	Ltrain: 0.005866 	Lval: 0.006603
Epoch 00057: reducing learning rate of group 0 to 1.8730e-07.
Epoch: 60 	Ltrain: 0.005782 	Lval: 0.006600
Epoch: 65 	Ltrain: 0.005753 	Lval: 0.006600
Epoch 00069: reducing learning rate of group 0 to 1.8730e-08.
Epoch: 70 	Ltrain: 0.005795 	Lval: 0.006600
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.006603


	Fold 4/5
Epoch: 1 	Ltrain: 0.016777 	Lval: 0.012272
Epoch: 5 	Ltrain: 0.006057 	Lval: 0.007636
Epoch: 10 	Ltrain: 0.005610 	Lval: 0.006741
Epoch: 15 	Ltrain: 0.005359 	Lval: 0.007015
Epoch 00018: reducing learning rate of group 0 to 1.8730e-04.
Epoch: 20 	Ltrain: 0.004875 	Lval: 0.006113
Epoch: 25 	Ltrain: 0.004815 	Lval: 0.005959
Epoch: 30 	Ltrain: 0.004750 	Lval: 0.005861
Epoch: 35 	Ltrain: 0.004702 	Lval: 0.006014
Epoch 00040: reducing learning rate of group 0 to 1.8730e-05.
Epoch: 40 	Ltrain: 0.004682 	Lval: 0.005894
Epoch: 45 	Ltrain: 0.004616 	Lval: 0.005757
Epoch: 50 	Ltrain: 0.004627 	Lval: 0.005744
Epoch: 55 	Ltrain: 0.004620 	Lval: 0.005752
Epoch 00058: reducing learning rate of group 0 to 1.8730e-06.
Epoch: 60 	Ltrain: 0.004591 	Lval: 0.005740
Epoch: 65 	Ltrain: 0.004584 	Lval: 0.005742
Epoch: 70 	Ltrain: 0.004587 	Lval: 0.005739
Epoch 00075: reducing learning rate of group 0 to 1.8730e-07.
Epoch: 75 	Ltrain: 0.004594 	Lval: 0.005738
Epoch: 80 	Ltrain: 0.004579 	Lval: 0.005738
Epoch: 85 	Ltrain: 0.004594 	Lval: 0.005738
Epoch 00087: reducing learning rate of group 0 to 1.8730e-08.
Epoch: 90 	Ltrain: 0.004586 	Lval: 0.005738
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.005740


	Fold 5/5
Epoch: 1 	Ltrain: 0.018772 	Lval: 0.014029
Epoch: 5 	Ltrain: 0.006107 	Lval: 0.007390
Epoch: 10 	Ltrain: 0.005538 	Lval: 0.008763
Epoch 00013: reducing learning rate of group 0 to 1.8730e-04.
Epoch: 15 	Ltrain: 0.005032 	Lval: 0.006618
Epoch: 20 	Ltrain: 0.005058 	Lval: 0.006542
Epoch 00025: reducing learning rate of group 0 to 1.8730e-05.
Epoch: 25 	Ltrain: 0.004976 	Lval: 0.006497
Epoch: 30 	Ltrain: 0.004923 	Lval: 0.006396
Epoch: 35 	Ltrain: 0.004922 	Lval: 0.006413
Epoch 00038: reducing learning rate of group 0 to 1.8730e-06.
Epoch: 40 	Ltrain: 0.004919 	Lval: 0.006386
Epoch: 45 	Ltrain: 0.004893 	Lval: 0.006386
Epoch 00050: reducing learning rate of group 0 to 1.8730e-07.
Epoch: 50 	Ltrain: 0.004932 	Lval: 0.006386
Epoch: 55 	Ltrain: 0.004912 	Lval: 0.006385
Epoch: 60 	Ltrain: 0.004890 	Lval: 0.006384
Epoch 00062: reducing learning rate of group 0 to 1.8730e-08.
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.006371

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0007163939848130372
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.178510946468472e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.036598 	Lval: 0.031429
Epoch: 5 	Ltrain: 0.021281 	Lval: 0.014923
Epoch 00009: reducing learning rate of group 0 to 7.1639e-05.
Epoch: 10 	Ltrain: 0.015562 	Lval: 0.015235
Epoch: 15 	Ltrain: 0.013423 	Lval: 0.013824
Epoch: 20 	Ltrain: 0.014811 	Lval: 0.014096
Epoch 00021: reducing learning rate of group 0 to 7.1639e-06.
Epoch: 25 	Ltrain: 0.012487 	Lval: 0.014017
Epoch: 30 	Ltrain: 0.012922 	Lval: 0.014022
Epoch 00033: reducing learning rate of group 0 to 7.1639e-07.
EarlyStopper: stopping at epoch 33 with best_val_loss = 0.013774


	Fold 2/5
Epoch: 1 	Ltrain: 0.029805 	Lval: 0.023160
Epoch: 5 	Ltrain: 0.017913 	Lval: 0.013943
Epoch: 10 	Ltrain: 0.011378 	Lval: 0.010926
Epoch: 15 	Ltrain: 0.010889 	Lval: 0.009385
Epoch: 20 	Ltrain: 0.008968 	Lval: 0.008601
Epoch: 25 	Ltrain: 0.008631 	Lval: 0.007420
Epoch 00030: reducing learning rate of group 0 to 7.1639e-05.
Epoch: 30 	Ltrain: 0.007581 	Lval: 0.007927
Epoch: 35 	Ltrain: 0.007630 	Lval: 0.007012
Epoch: 40 	Ltrain: 0.006981 	Lval: 0.006869
Epoch 00042: reducing learning rate of group 0 to 7.1639e-06.
Epoch: 45 	Ltrain: 0.007050 	Lval: 0.006990
Epoch: 50 	Ltrain: 0.007245 	Lval: 0.006949
EarlyStopper: stopping at epoch 51 with best_val_loss = 0.006864


	Fold 3/5
Epoch: 1 	Ltrain: 0.042343 	Lval: 0.017412
Epoch: 5 	Ltrain: 0.014080 	Lval: 0.016846
Epoch: 10 	Ltrain: 0.010170 	Lval: 0.010475
Epoch: 15 	Ltrain: 0.008397 	Lval: 0.008068
Epoch: 20 	Ltrain: 0.007013 	Lval: 0.007142
Epoch: 25 	Ltrain: 0.006744 	Lval: 0.006854
Epoch: 30 	Ltrain: 0.006574 	Lval: 0.007451
Epoch 00033: reducing learning rate of group 0 to 7.1639e-05.
Epoch: 35 	Ltrain: 0.005945 	Lval: 0.006663
Epoch: 40 	Ltrain: 0.006460 	Lval: 0.006546
Epoch: 45 	Ltrain: 0.006046 	Lval: 0.006526
Epoch 00047: reducing learning rate of group 0 to 7.1639e-06.
Epoch: 50 	Ltrain: 0.005993 	Lval: 0.006504
Epoch: 55 	Ltrain: 0.005973 	Lval: 0.006501
Epoch: 60 	Ltrain: 0.006237 	Lval: 0.006525
Epoch 00061: reducing learning rate of group 0 to 7.1639e-07.
Epoch: 65 	Ltrain: 0.006291 	Lval: 0.006515
Epoch: 70 	Ltrain: 0.006311 	Lval: 0.006513
Epoch 00073: reducing learning rate of group 0 to 7.1639e-08.
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.006493


	Fold 4/5
Epoch: 1 	Ltrain: 0.023684 	Lval: 0.015708
Epoch: 5 	Ltrain: 0.010014 	Lval: 0.010030
Epoch: 10 	Ltrain: 0.007175 	Lval: 0.008025
Epoch: 15 	Ltrain: 0.006084 	Lval: 0.007409
Epoch: 20 	Ltrain: 0.005513 	Lval: 0.007298
Epoch: 25 	Ltrain: 0.005658 	Lval: 0.007383
Epoch 00026: reducing learning rate of group 0 to 7.1639e-05.
Epoch: 30 	Ltrain: 0.005438 	Lval: 0.006472
Epoch: 35 	Ltrain: 0.005424 	Lval: 0.006456
Epoch: 40 	Ltrain: 0.005350 	Lval: 0.006439
Epoch: 45 	Ltrain: 0.005298 	Lval: 0.006411
Epoch: 50 	Ltrain: 0.005501 	Lval: 0.006412
Epoch 00053: reducing learning rate of group 0 to 7.1639e-06.
Epoch: 55 	Ltrain: 0.005607 	Lval: 0.006394
Epoch: 60 	Ltrain: 0.005458 	Lval: 0.006396
Epoch 00065: reducing learning rate of group 0 to 7.1639e-07.
Epoch: 65 	Ltrain: 0.005493 	Lval: 0.006395
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.006388


	Fold 5/5
Epoch: 1 	Ltrain: 0.028516 	Lval: 0.021893
Epoch: 5 	Ltrain: 0.010940 	Lval: 0.012470
Epoch: 10 	Ltrain: 0.008004 	Lval: 0.010223
Epoch: 15 	Ltrain: 0.006500 	Lval: 0.007842
Epoch: 20 	Ltrain: 0.005976 	Lval: 0.007242
Epoch 00023: reducing learning rate of group 0 to 7.1639e-05.
Epoch: 25 	Ltrain: 0.005784 	Lval: 0.007068
Epoch: 30 	Ltrain: 0.005472 	Lval: 0.006938
Epoch 00035: reducing learning rate of group 0 to 7.1639e-06.
Epoch: 35 	Ltrain: 0.005617 	Lval: 0.006945
Epoch: 40 	Ltrain: 0.005410 	Lval: 0.006943
Epoch: 45 	Ltrain: 0.005347 	Lval: 0.006923
Epoch 00047: reducing learning rate of group 0 to 7.1639e-07.
EarlyStopper: stopping at epoch 48 with best_val_loss = 0.006921

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.008997684387927822
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.623662713887274e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.214359 	Lval: 0.045195
Epoch: 5 	Ltrain: 0.014531 	Lval: 0.011909
Epoch: 10 	Ltrain: 0.009063 	Lval: 0.011114
Epoch: 15 	Ltrain: 0.008389 	Lval: 0.007778
Epoch: 20 	Ltrain: 0.007466 	Lval: 0.006875
Epoch: 25 	Ltrain: 0.007145 	Lval: 0.007196
Epoch 00026: reducing learning rate of group 0 to 8.9977e-04.
Epoch: 30 	Ltrain: 0.006607 	Lval: 0.006252
Epoch: 35 	Ltrain: 0.006848 	Lval: 0.006229
Epoch: 40 	Ltrain: 0.005918 	Lval: 0.006118
Epoch: 45 	Ltrain: 0.006031 	Lval: 0.006120
Epoch: 50 	Ltrain: 0.006022 	Lval: 0.005983
Epoch: 55 	Ltrain: 0.005992 	Lval: 0.005959
Epoch: 60 	Ltrain: 0.006129 	Lval: 0.005899
Epoch: 65 	Ltrain: 0.006267 	Lval: 0.005843
Epoch: 70 	Ltrain: 0.005827 	Lval: 0.005744
Epoch: 75 	Ltrain: 0.005639 	Lval: 0.005709
Epoch: 80 	Ltrain: 0.005897 	Lval: 0.005609
Epoch: 85 	Ltrain: 0.005983 	Lval: 0.005530
Epoch: 90 	Ltrain: 0.005461 	Lval: 0.005455
Epoch: 95 	Ltrain: 0.005402 	Lval: 0.005403
Epoch: 100 	Ltrain: 0.005329 	Lval: 0.005261
Epoch: 105 	Ltrain: 0.005122 	Lval: 0.005095
Epoch: 110 	Ltrain: 0.004954 	Lval: 0.004993
Epoch: 115 	Ltrain: 0.005318 	Lval: 0.004979
Epoch: 120 	Ltrain: 0.005115 	Lval: 0.004745
Epoch: 125 	Ltrain: 0.004538 	Lval: 0.004622
Epoch: 130 	Ltrain: 0.004702 	Lval: 0.004459
Epoch 00134: reducing learning rate of group 0 to 8.9977e-05.
Epoch: 135 	Ltrain: 0.004534 	Lval: 0.004418
Epoch: 140 	Ltrain: 0.004493 	Lval: 0.004332
Epoch: 145 	Ltrain: 0.004252 	Lval: 0.004319
Epoch: 150 	Ltrain: 0.004373 	Lval: 0.004306
Epoch: 155 	Ltrain: 0.004210 	Lval: 0.004258
Epoch: 160 	Ltrain: 0.004448 	Lval: 0.004236
Epoch: 165 	Ltrain: 0.004232 	Lval: 0.004225
Epoch: 170 	Ltrain: 0.004180 	Lval: 0.004202
Epoch: 175 	Ltrain: 0.004443 	Lval: 0.004186
Epoch: 180 	Ltrain: 0.004242 	Lval: 0.004171
Epoch 00183: reducing learning rate of group 0 to 8.9977e-06.
Epoch: 185 	Ltrain: 0.004318 	Lval: 0.004166
Epoch: 190 	Ltrain: 0.004416 	Lval: 0.004158
Epoch: 195 	Ltrain: 0.004151 	Lval: 0.004156
Epoch: 200 	Ltrain: 0.004350 	Lval: 0.004150
Epoch: 205 	Ltrain: 0.004403 	Lval: 0.004145
Epoch: 210 	Ltrain: 0.004260 	Lval: 0.004145
Epoch: 215 	Ltrain: 0.004157 	Lval: 0.004140
Epoch: 220 	Ltrain: 0.004088 	Lval: 0.004137
Epoch: 225 	Ltrain: 0.004084 	Lval: 0.004133
Epoch: 230 	Ltrain: 0.004237 	Lval: 0.004132
Epoch: 235 	Ltrain: 0.004191 	Lval: 0.004129
Epoch 00239: reducing learning rate of group 0 to 8.9977e-07.
Epoch: 240 	Ltrain: 0.003998 	Lval: 0.004130
Epoch: 245 	Ltrain: 0.003995 	Lval: 0.004129
Epoch: 250 	Ltrain: 0.004105 	Lval: 0.004128
Epoch 00251: reducing learning rate of group 0 to 8.9977e-08.
Epoch: 255 	Ltrain: 0.004130 	Lval: 0.004128
EarlyStopper: stopping at epoch 258 with best_val_loss = 0.004130


	Fold 2/5
Epoch: 1 	Ltrain: 0.105257 	Lval: 0.017656
Epoch: 5 	Ltrain: 0.007835 	Lval: 0.007572
Epoch: 10 	Ltrain: 0.006524 	Lval: 0.006384
Epoch 00014: reducing learning rate of group 0 to 8.9977e-04.
Epoch: 15 	Ltrain: 0.005977 	Lval: 0.006167
Epoch: 20 	Ltrain: 0.005690 	Lval: 0.005922
Epoch: 25 	Ltrain: 0.005545 	Lval: 0.005860
Epoch: 30 	Ltrain: 0.005502 	Lval: 0.005867
Epoch: 35 	Ltrain: 0.005503 	Lval: 0.005673
Epoch: 40 	Ltrain: 0.005398 	Lval: 0.005652
Epoch: 45 	Ltrain: 0.005203 	Lval: 0.005471
Epoch: 50 	Ltrain: 0.005359 	Lval: 0.005287
Epoch: 55 	Ltrain: 0.005036 	Lval: 0.005186
Epoch: 60 	Ltrain: 0.004965 	Lval: 0.005119
Epoch: 65 	Ltrain: 0.004617 	Lval: 0.004949
Epoch: 70 	Ltrain: 0.004662 	Lval: 0.004893
Epoch: 75 	Ltrain: 0.004411 	Lval: 0.004405
Epoch: 80 	Ltrain: 0.004202 	Lval: 0.004317
Epoch: 85 	Ltrain: 0.004327 	Lval: 0.004686
Epoch: 90 	Ltrain: 0.004007 	Lval: 0.004134
Epoch: 95 	Ltrain: 0.004021 	Lval: 0.004119
Epoch 00096: reducing learning rate of group 0 to 8.9977e-05.
Epoch: 100 	Ltrain: 0.003585 	Lval: 0.003638
Epoch: 105 	Ltrain: 0.003411 	Lval: 0.003580
Epoch: 110 	Ltrain: 0.003499 	Lval: 0.003551
Epoch: 115 	Ltrain: 0.003418 	Lval: 0.003524
Epoch: 120 	Ltrain: 0.003379 	Lval: 0.003492
Epoch: 125 	Ltrain: 0.003344 	Lval: 0.003466
Epoch: 130 	Ltrain: 0.003355 	Lval: 0.003437
Epoch: 135 	Ltrain: 0.003316 	Lval: 0.003402
Epoch: 140 	Ltrain: 0.003255 	Lval: 0.003374
Epoch: 145 	Ltrain: 0.003230 	Lval: 0.003339
Epoch: 150 	Ltrain: 0.003210 	Lval: 0.003308
Epoch: 155 	Ltrain: 0.003167 	Lval: 0.003274
Epoch: 160 	Ltrain: 0.003154 	Lval: 0.003246
Epoch: 165 	Ltrain: 0.003138 	Lval: 0.003201
Epoch: 170 	Ltrain: 0.003109 	Lval: 0.003170
Epoch: 175 	Ltrain: 0.003071 	Lval: 0.003131
Epoch: 180 	Ltrain: 0.003000 	Lval: 0.003106
Epoch: 185 	Ltrain: 0.003009 	Lval: 0.003057
Epoch: 190 	Ltrain: 0.002942 	Lval: 0.003020
Epoch: 195 	Ltrain: 0.002932 	Lval: 0.002995
Epoch: 200 	Ltrain: 0.002846 	Lval: 0.002950
Epoch: 205 	Ltrain: 0.002821 	Lval: 0.002928
Epoch: 210 	Ltrain: 0.002796 	Lval: 0.002878
Epoch: 215 	Ltrain: 0.002806 	Lval: 0.002830
Epoch: 220 	Ltrain: 0.002706 	Lval: 0.002811
Epoch: 225 	Ltrain: 0.002710 	Lval: 0.002764
Epoch: 230 	Ltrain: 0.002609 	Lval: 0.002703
Epoch: 235 	Ltrain: 0.002617 	Lval: 0.002652
Epoch: 240 	Ltrain: 0.002555 	Lval: 0.002625
Epoch: 245 	Ltrain: 0.002516 	Lval: 0.002572
Epoch: 250 	Ltrain: 0.002476 	Lval: 0.002534
Epoch: 255 	Ltrain: 0.002424 	Lval: 0.002491
Epoch: 260 	Ltrain: 0.002423 	Lval: 0.002473
Epoch: 265 	Ltrain: 0.002357 	Lval: 0.002416
Epoch: 270 	Ltrain: 0.002367 	Lval: 0.002390
Epoch: 275 	Ltrain: 0.002243 	Lval: 0.002365
Epoch: 280 	Ltrain: 0.002247 	Lval: 0.002322
Epoch: 285 	Ltrain: 0.002233 	Lval: 0.002253
Epoch: 290 	Ltrain: 0.002166 	Lval: 0.002214
Epoch: 295 	Ltrain: 0.002124 	Lval: 0.002215
Epoch: 300 	Ltrain: 0.002111 	Lval: 0.002175
Epoch: 305 	Ltrain: 0.002078 	Lval: 0.002182
Epoch: 310 	Ltrain: 0.002028 	Lval: 0.002090
Epoch: 315 	Ltrain: 0.002005 	Lval: 0.002031
Epoch: 320 	Ltrain: 0.001951 	Lval: 0.001983
Epoch: 325 	Ltrain: 0.001876 	Lval: 0.001961
Epoch: 330 	Ltrain: 0.001870 	Lval: 0.001908
Epoch: 335 	Ltrain: 0.001862 	Lval: 0.001879
Epoch: 340 	Ltrain: 0.001808 	Lval: 0.001922
Epoch 00343: reducing learning rate of group 0 to 8.9977e-06.
Epoch: 345 	Ltrain: 0.001740 	Lval: 0.001802
Epoch: 350 	Ltrain: 0.001738 	Lval: 0.001793
Epoch: 355 	Ltrain: 0.001722 	Lval: 0.001788
Epoch: 360 	Ltrain: 0.001708 	Lval: 0.001783
Epoch: 365 	Ltrain: 0.001700 	Lval: 0.001778
Epoch: 370 	Ltrain: 0.001744 	Lval: 0.001774
Epoch: 375 	Ltrain: 0.001739 	Lval: 0.001770
Epoch: 380 	Ltrain: 0.001726 	Lval: 0.001767
Epoch: 385 	Ltrain: 0.001700 	Lval: 0.001763
Epoch: 390 	Ltrain: 0.001704 	Lval: 0.001758
Epoch: 395 	Ltrain: 0.001693 	Lval: 0.001753
Epoch: 400 	Ltrain: 0.001674 	Lval: 0.001751
Epoch: 405 	Ltrain: 0.001705 	Lval: 0.001745
Epoch: 410 	Ltrain: 0.001685 	Lval: 0.001741
Epoch: 415 	Ltrain: 0.001676 	Lval: 0.001735
Epoch: 420 	Ltrain: 0.001672 	Lval: 0.001731
Epoch: 425 	Ltrain: 0.001685 	Lval: 0.001728
Epoch: 430 	Ltrain: 0.001669 	Lval: 0.001722
Epoch: 435 	Ltrain: 0.001668 	Lval: 0.001718
Epoch: 440 	Ltrain: 0.001652 	Lval: 0.001714
Epoch: 445 	Ltrain: 0.001669 	Lval: 0.001709
Epoch: 450 	Ltrain: 0.001657 	Lval: 0.001705
Epoch: 455 	Ltrain: 0.001674 	Lval: 0.001699
Epoch: 460 	Ltrain: 0.001633 	Lval: 0.001695
Epoch: 465 	Ltrain: 0.001631 	Lval: 0.001691
Epoch: 470 	Ltrain: 0.001642 	Lval: 0.001685
Epoch: 475 	Ltrain: 0.001605 	Lval: 0.001679
Epoch: 480 	Ltrain: 0.001622 	Lval: 0.001675
Epoch: 485 	Ltrain: 0.001603 	Lval: 0.001669
Epoch: 490 	Ltrain: 0.001611 	Lval: 0.001665
Epoch: 495 	Ltrain: 0.001606 	Lval: 0.001660
Epoch: 500 	Ltrain: 0.001615 	Lval: 0.001654
Epoch: 505 	Ltrain: 0.001594 	Lval: 0.001649
Epoch: 510 	Ltrain: 0.001591 	Lval: 0.001644
Epoch: 515 	Ltrain: 0.001581 	Lval: 0.001640
Epoch: 520 	Ltrain: 0.001598 	Lval: 0.001635
Epoch: 525 	Ltrain: 0.001570 	Lval: 0.001629
Epoch: 530 	Ltrain: 0.001612 	Lval: 0.001626
Epoch: 535 	Ltrain: 0.001560 	Lval: 0.001621
Epoch: 540 	Ltrain: 0.001564 	Lval: 0.001614
Epoch: 545 	Ltrain: 0.001557 	Lval: 0.001611
Epoch: 550 	Ltrain: 0.001562 	Lval: 0.001605
Epoch: 555 	Ltrain: 0.001543 	Lval: 0.001598
Epoch: 560 	Ltrain: 0.001545 	Lval: 0.001593
Epoch: 565 	Ltrain: 0.001549 	Lval: 0.001588
Epoch: 570 	Ltrain: 0.001522 	Lval: 0.001584
Epoch: 575 	Ltrain: 0.001535 	Lval: 0.001579
Epoch: 580 	Ltrain: 0.001526 	Lval: 0.001575
Epoch: 585 	Ltrain: 0.001537 	Lval: 0.001569
Epoch: 590 	Ltrain: 0.001537 	Lval: 0.001565
Epoch: 595 	Ltrain: 0.001511 	Lval: 0.001560
Epoch: 600 	Ltrain: 0.001506 	Lval: 0.001554
Epoch: 605 	Ltrain: 0.001486 	Lval: 0.001549
Epoch: 610 	Ltrain: 0.001482 	Lval: 0.001545
Epoch: 615 	Ltrain: 0.001485 	Lval: 0.001540
Epoch: 620 	Ltrain: 0.001487 	Lval: 0.001535
Epoch: 625 	Ltrain: 0.001485 	Lval: 0.001531
Epoch: 630 	Ltrain: 0.001474 	Lval: 0.001525
Epoch: 635 	Ltrain: 0.001476 	Lval: 0.001519
Epoch: 640 	Ltrain: 0.001504 	Lval: 0.001518
Epoch: 645 	Ltrain: 0.001450 	Lval: 0.001510
Epoch: 650 	Ltrain: 0.001443 	Lval: 0.001506
Epoch: 655 	Ltrain: 0.001463 	Lval: 0.001500
Epoch: 660 	Ltrain: 0.001451 	Lval: 0.001497
Epoch: 665 	Ltrain: 0.001459 	Lval: 0.001492
Epoch: 670 	Ltrain: 0.001433 	Lval: 0.001488
Epoch: 675 	Ltrain: 0.001422 	Lval: 0.001483
Epoch: 680 	Ltrain: 0.001446 	Lval: 0.001477
Epoch: 685 	Ltrain: 0.001430 	Lval: 0.001472
Epoch: 690 	Ltrain: 0.001414 	Lval: 0.001467
Epoch: 695 	Ltrain: 0.001406 	Lval: 0.001463
Epoch: 700 	Ltrain: 0.001397 	Lval: 0.001458
Epoch: 705 	Ltrain: 0.001399 	Lval: 0.001452
Epoch: 710 	Ltrain: 0.001403 	Lval: 0.001449
Epoch: 715 	Ltrain: 0.001403 	Lval: 0.001443
Epoch: 720 	Ltrain: 0.001392 	Lval: 0.001438
Epoch: 725 	Ltrain: 0.001385 	Lval: 0.001434
Epoch: 730 	Ltrain: 0.001380 	Lval: 0.001430
Epoch: 735 	Ltrain: 0.001371 	Lval: 0.001425
Epoch: 740 	Ltrain: 0.001369 	Lval: 0.001418
Epoch: 745 	Ltrain: 0.001379 	Lval: 0.001415
Epoch: 750 	Ltrain: 0.001372 	Lval: 0.001413
Epoch: 755 	Ltrain: 0.001354 	Lval: 0.001404
Epoch: 760 	Ltrain: 0.001361 	Lval: 0.001401
Epoch: 765 	Ltrain: 0.001358 	Lval: 0.001398
Epoch: 770 	Ltrain: 0.001340 	Lval: 0.001393
Epoch: 775 	Ltrain: 0.001348 	Lval: 0.001389
Epoch: 780 	Ltrain: 0.001338 	Lval: 0.001383
Epoch: 785 	Ltrain: 0.001329 	Lval: 0.001379
Epoch: 790 	Ltrain: 0.001328 	Lval: 0.001376
Epoch: 795 	Ltrain: 0.001343 	Lval: 0.001370
Epoch: 800 	Ltrain: 0.001348 	Lval: 0.001366
Epoch: 805 	Ltrain: 0.001306 	Lval: 0.001362
Epoch: 810 	Ltrain: 0.001311 	Lval: 0.001356
Epoch: 815 	Ltrain: 0.001316 	Lval: 0.001351
Epoch: 820 	Ltrain: 0.001307 	Lval: 0.001348
Epoch: 825 	Ltrain: 0.001289 	Lval: 0.001344
Epoch: 830 	Ltrain: 0.001295 	Lval: 0.001338
Epoch: 835 	Ltrain: 0.001288 	Lval: 0.001333
Epoch: 840 	Ltrain: 0.001282 	Lval: 0.001329
Epoch: 845 	Ltrain: 0.001278 	Lval: 0.001325
Epoch: 850 	Ltrain: 0.001287 	Lval: 0.001320
Epoch: 855 	Ltrain: 0.001269 	Lval: 0.001315
Epoch: 860 	Ltrain: 0.001282 	Lval: 0.001312
Epoch: 865 	Ltrain: 0.001275 	Lval: 0.001306
Epoch: 870 	Ltrain: 0.001253 	Lval: 0.001302
Epoch: 875 	Ltrain: 0.001258 	Lval: 0.001297
Epoch: 880 	Ltrain: 0.001261 	Lval: 0.001294
Epoch: 885 	Ltrain: 0.001247 	Lval: 0.001289
Epoch: 890 	Ltrain: 0.001257 	Lval: 0.001283
Epoch: 895 	Ltrain: 0.001244 	Lval: 0.001282
Epoch: 900 	Ltrain: 0.001257 	Lval: 0.001275
Epoch: 905 	Ltrain: 0.001216 	Lval: 0.001274
Epoch: 910 	Ltrain: 0.001229 	Lval: 0.001267
Epoch: 915 	Ltrain: 0.001218 	Lval: 0.001263
Epoch: 920 	Ltrain: 0.001226 	Lval: 0.001258
Epoch: 925 	Ltrain: 0.001224 	Lval: 0.001254
Epoch: 930 	Ltrain: 0.001212 	Lval: 0.001249
Epoch: 935 	Ltrain: 0.001219 	Lval: 0.001244
Epoch: 940 	Ltrain: 0.001217 	Lval: 0.001241
Epoch: 945 	Ltrain: 0.001201 	Lval: 0.001236
Epoch: 950 	Ltrain: 0.001188 	Lval: 0.001231
Epoch: 955 	Ltrain: 0.001198 	Lval: 0.001228
Epoch: 960 	Ltrain: 0.001182 	Lval: 0.001224
Epoch: 965 	Ltrain: 0.001201 	Lval: 0.001220
Epoch: 970 	Ltrain: 0.001171 	Lval: 0.001215
Epoch: 975 	Ltrain: 0.001181 	Lval: 0.001212
Epoch: 980 	Ltrain: 0.001160 	Lval: 0.001207
Epoch: 985 	Ltrain: 0.001162 	Lval: 0.001202
Epoch: 990 	Ltrain: 0.001166 	Lval: 0.001197
Epoch: 995 	Ltrain: 0.001151 	Lval: 0.001195
Epoch: 1000 	Ltrain: 0.001153 	Lval: 0.001190
Epoch: 1005 	Ltrain: 0.001138 	Lval: 0.001186
Epoch: 1010 	Ltrain: 0.001143 	Lval: 0.001182
Epoch: 1015 	Ltrain: 0.001138 	Lval: 0.001179
Epoch: 1020 	Ltrain: 0.001134 	Lval: 0.001172
Epoch: 1025 	Ltrain: 0.001130 	Lval: 0.001169
Epoch: 1030 	Ltrain: 0.001126 	Lval: 0.001165
Epoch: 1035 	Ltrain: 0.001118 	Lval: 0.001161
Epoch: 1040 	Ltrain: 0.001114 	Lval: 0.001160
Epoch: 1045 	Ltrain: 0.001131 	Lval: 0.001152
Epoch: 1050 	Ltrain: 0.001106 	Lval: 0.001149
Epoch: 1055 	Ltrain: 0.001103 	Lval: 0.001145
Epoch: 1060 	Ltrain: 0.001111 	Lval: 0.001140
Epoch: 1065 	Ltrain: 0.001089 	Lval: 0.001136
Epoch: 1070 	Ltrain: 0.001100 	Lval: 0.001133
Epoch: 1075 	Ltrain: 0.001095 	Lval: 0.001128
Epoch: 1080 	Ltrain: 0.001100 	Lval: 0.001125
Epoch: 1085 	Ltrain: 0.001081 	Lval: 0.001120
Epoch: 1090 	Ltrain: 0.001081 	Lval: 0.001115
Epoch: 1095 	Ltrain: 0.001070 	Lval: 0.001112
Epoch: 1100 	Ltrain: 0.001070 	Lval: 0.001108
Epoch: 1105 	Ltrain: 0.001081 	Lval: 0.001106
Epoch: 1110 	Ltrain: 0.001056 	Lval: 0.001100
Epoch: 1115 	Ltrain: 0.001062 	Lval: 0.001096
Epoch: 1120 	Ltrain: 0.001053 	Lval: 0.001093
Epoch: 1125 	Ltrain: 0.001055 	Lval: 0.001088
Epoch: 1130 	Ltrain: 0.001045 	Lval: 0.001084
Epoch: 1135 	Ltrain: 0.001040 	Lval: 0.001080
Epoch: 1140 	Ltrain: 0.001032 	Lval: 0.001076
Epoch: 1145 	Ltrain: 0.001044 	Lval: 0.001072
Epoch: 1150 	Ltrain: 0.001027 	Lval: 0.001067
Epoch: 1155 	Ltrain: 0.001038 	Lval: 0.001064
Epoch: 1160 	Ltrain: 0.001033 	Lval: 0.001062
Epoch: 1165 	Ltrain: 0.001023 	Lval: 0.001057
Epoch: 1170 	Ltrain: 0.001020 	Lval: 0.001052
Epoch: 1175 	Ltrain: 0.001019 	Lval: 0.001049
Epoch: 1180 	Ltrain: 0.001019 	Lval: 0.001044
Epoch: 1185 	Ltrain: 0.001004 	Lval: 0.001040
Epoch: 1190 	Ltrain: 0.001009 	Lval: 0.001035
Epoch: 1195 	Ltrain: 0.001002 	Lval: 0.001032
Epoch: 1200 	Ltrain: 0.000993 	Lval: 0.001029
Epoch: 1205 	Ltrain: 0.000992 	Lval: 0.001026
Epoch: 1210 	Ltrain: 0.000991 	Lval: 0.001022
Epoch: 1215 	Ltrain: 0.000994 	Lval: 0.001018
Epoch: 1220 	Ltrain: 0.000989 	Lval: 0.001014
Epoch: 1225 	Ltrain: 0.000976 	Lval: 0.001010
Epoch: 1230 	Ltrain: 0.000980 	Lval: 0.001005
Epoch: 1235 	Ltrain: 0.000964 	Lval: 0.001001
Epoch: 1240 	Ltrain: 0.000957 	Lval: 0.000999
Epoch: 1245 	Ltrain: 0.000965 	Lval: 0.000995
Epoch: 1250 	Ltrain: 0.000956 	Lval: 0.000992
Epoch: 1255 	Ltrain: 0.000952 	Lval: 0.000986
Epoch: 1260 	Ltrain: 0.000951 	Lval: 0.000982
Epoch: 1265 	Ltrain: 0.000954 	Lval: 0.000982
Epoch: 1270 	Ltrain: 0.000943 	Lval: 0.000974
Epoch: 1275 	Ltrain: 0.000931 	Lval: 0.000972
Epoch: 1280 	Ltrain: 0.000942 	Lval: 0.000967
Epoch: 1285 	Ltrain: 0.000931 	Lval: 0.000964
Epoch: 1290 	Ltrain: 0.000923 	Lval: 0.000961
Epoch: 1295 	Ltrain: 0.000920 	Lval: 0.000957
Epoch: 1300 	Ltrain: 0.000917 	Lval: 0.000954
Epoch: 1305 	Ltrain: 0.000921 	Lval: 0.000949
Epoch: 1310 	Ltrain: 0.000919 	Lval: 0.000946
Epoch: 1315 	Ltrain: 0.000912 	Lval: 0.000943
Epoch: 1320 	Ltrain: 0.000924 	Lval: 0.000939
Epoch: 1325 	Ltrain: 0.000902 	Lval: 0.000934
Epoch: 1330 	Ltrain: 0.000911 	Lval: 0.000932
Epoch: 1335 	Ltrain: 0.000890 	Lval: 0.000927
Epoch: 1340 	Ltrain: 0.000897 	Lval: 0.000924
Epoch: 1345 	Ltrain: 0.000900 	Lval: 0.000921
Epoch: 1350 	Ltrain: 0.000889 	Lval: 0.000917
Epoch: 1355 	Ltrain: 0.000891 	Lval: 0.000913
Epoch: 1360 	Ltrain: 0.000877 	Lval: 0.000910
Epoch: 1365 	Ltrain: 0.000873 	Lval: 0.000906
Epoch: 1370 	Ltrain: 0.000895 	Lval: 0.000903
Epoch: 1375 	Ltrain: 0.000874 	Lval: 0.000899
Epoch: 1380 	Ltrain: 0.000868 	Lval: 0.000896
Epoch: 1385 	Ltrain: 0.000880 	Lval: 0.000893
Epoch: 1390 	Ltrain: 0.000861 	Lval: 0.000891
Epoch: 1395 	Ltrain: 0.000866 	Lval: 0.000886
Epoch: 1400 	Ltrain: 0.000851 	Lval: 0.000883
Epoch: 1405 	Ltrain: 0.000859 	Lval: 0.000879
Epoch: 1410 	Ltrain: 0.000848 	Lval: 0.000876
Epoch: 1415 	Ltrain: 0.000853 	Lval: 0.000873
Epoch: 1420 	Ltrain: 0.000845 	Lval: 0.000870
Epoch: 1425 	Ltrain: 0.000844 	Lval: 0.000865
Epoch: 1430 	Ltrain: 0.000845 	Lval: 0.000864
Epoch: 1435 	Ltrain: 0.000833 	Lval: 0.000859
Epoch: 1440 	Ltrain: 0.000828 	Lval: 0.000856
Epoch: 1445 	Ltrain: 0.000828 	Lval: 0.000852
Epoch: 1450 	Ltrain: 0.000815 	Lval: 0.000849
Epoch: 1455 	Ltrain: 0.000812 	Lval: 0.000846
Epoch: 1460 	Ltrain: 0.000821 	Lval: 0.000842
Epoch: 1465 	Ltrain: 0.000806 	Lval: 0.000839
Epoch: 1470 	Ltrain: 0.000810 	Lval: 0.000836
Epoch: 1475 	Ltrain: 0.000808 	Lval: 0.000833
Epoch: 1480 	Ltrain: 0.000807 	Lval: 0.000829
Epoch: 1485 	Ltrain: 0.000814 	Lval: 0.000828
Epoch: 1490 	Ltrain: 0.000798 	Lval: 0.000824
Epoch: 1495 	Ltrain: 0.000797 	Lval: 0.000821
Epoch: 1500 	Ltrain: 0.000789 	Lval: 0.000816
Epoch: 1505 	Ltrain: 0.000786 	Lval: 0.000813
Epoch: 1510 	Ltrain: 0.000783 	Lval: 0.000811
Epoch: 1515 	Ltrain: 0.000785 	Lval: 0.000808
Epoch: 1520 	Ltrain: 0.000768 	Lval: 0.000804
Epoch: 1525 	Ltrain: 0.000775 	Lval: 0.000801
Epoch: 1530 	Ltrain: 0.000787 	Lval: 0.000797
Epoch: 1535 	Ltrain: 0.000779 	Lval: 0.000794
Epoch: 1540 	Ltrain: 0.000779 	Lval: 0.000792
Epoch: 1545 	Ltrain: 0.000769 	Lval: 0.000789
Epoch: 1550 	Ltrain: 0.000757 	Lval: 0.000786
Epoch: 1555 	Ltrain: 0.000763 	Lval: 0.000783
Epoch: 1560 	Ltrain: 0.000754 	Lval: 0.000780
Epoch: 1565 	Ltrain: 0.000758 	Lval: 0.000776
Epoch: 1570 	Ltrain: 0.000751 	Lval: 0.000774
Epoch: 1575 	Ltrain: 0.000752 	Lval: 0.000771
Epoch: 1580 	Ltrain: 0.000747 	Lval: 0.000768
Epoch: 1585 	Ltrain: 0.000744 	Lval: 0.000766
Epoch: 1590 	Ltrain: 0.000734 	Lval: 0.000761
Epoch: 1595 	Ltrain: 0.000734 	Lval: 0.000759
Epoch: 1600 	Ltrain: 0.000729 	Lval: 0.000756
Epoch 01603: reducing learning rate of group 0 to 8.9977e-07.
Epoch: 1605 	Ltrain: 0.000734 	Lval: 0.000753
Epoch: 1610 	Ltrain: 0.000730 	Lval: 0.000752
Epoch: 1615 	Ltrain: 0.000735 	Lval: 0.000752
Epoch: 1620 	Ltrain: 0.000731 	Lval: 0.000751
EarlyStopper: stopping at epoch 1619 with best_val_loss = 0.000759


	Fold 3/5
Epoch: 1 	Ltrain: 0.126272 	Lval: 0.014202
Epoch: 5 	Ltrain: 0.007265 	Lval: 0.008329
Epoch: 10 	Ltrain: 0.006104 	Lval: 0.007258
Epoch 00015: reducing learning rate of group 0 to 8.9977e-04.
Epoch: 15 	Ltrain: 0.006478 	Lval: 0.006651
Epoch: 20 	Ltrain: 0.005043 	Lval: 0.005742
Epoch: 25 	Ltrain: 0.004985 	Lval: 0.005674
Epoch: 30 	Ltrain: 0.004817 	Lval: 0.005430
Epoch: 35 	Ltrain: 0.004729 	Lval: 0.005307
Epoch: 40 	Ltrain: 0.004668 	Lval: 0.005157
Epoch: 45 	Ltrain: 0.004448 	Lval: 0.004989
Epoch: 50 	Ltrain: 0.004317 	Lval: 0.004600
Epoch: 55 	Ltrain: 0.004038 	Lval: 0.004392
Epoch: 60 	Ltrain: 0.003898 	Lval: 0.004355
Epoch: 65 	Ltrain: 0.003776 	Lval: 0.004084
Epoch: 70 	Ltrain: 0.003567 	Lval: 0.003799
Epoch: 75 	Ltrain: 0.003363 	Lval: 0.003777
Epoch: 80 	Ltrain: 0.003154 	Lval: 0.003293
Epoch: 85 	Ltrain: 0.002858 	Lval: 0.002944
Epoch: 90 	Ltrain: 0.002558 	Lval: 0.002747
Epoch: 95 	Ltrain: 0.002409 	Lval: 0.002582
Epoch: 100 	Ltrain: 0.002253 	Lval: 0.002372
Epoch: 105 	Ltrain: 0.002011 	Lval: 0.002255
Epoch: 110 	Ltrain: 0.001891 	Lval: 0.001887
Epoch: 115 	Ltrain: 0.001704 	Lval: 0.001960
Epoch: 120 	Ltrain: 0.001519 	Lval: 0.001574
Epoch: 125 	Ltrain: 0.001445 	Lval: 0.001418
Epoch: 130 	Ltrain: 0.001386 	Lval: 0.001500
Epoch: 135 	Ltrain: 0.001385 	Lval: 0.001289
Epoch: 140 	Ltrain: 0.001149 	Lval: 0.001130
Epoch: 145 	Ltrain: 0.001029 	Lval: 0.001072
Epoch: 150 	Ltrain: 0.001045 	Lval: 0.001071
Epoch: 155 	Ltrain: 0.000966 	Lval: 0.000986
Epoch 00158: reducing learning rate of group 0 to 8.9977e-05.
Epoch: 160 	Ltrain: 0.000738 	Lval: 0.000719
Epoch: 165 	Ltrain: 0.000665 	Lval: 0.000668
Epoch: 170 	Ltrain: 0.000641 	Lval: 0.000650
Epoch: 175 	Ltrain: 0.000629 	Lval: 0.000636
Epoch: 180 	Ltrain: 0.000620 	Lval: 0.000624
Epoch: 185 	Ltrain: 0.000605 	Lval: 0.000613
Epoch: 190 	Ltrain: 0.000598 	Lval: 0.000602
Epoch: 195 	Ltrain: 0.000597 	Lval: 0.000592
Epoch: 200 	Ltrain: 0.000578 	Lval: 0.000581
Epoch: 205 	Ltrain: 0.000568 	Lval: 0.000569
Epoch: 210 	Ltrain: 0.000553 	Lval: 0.000558
Epoch: 215 	Ltrain: 0.000547 	Lval: 0.000547
Epoch: 220 	Ltrain: 0.000534 	Lval: 0.000535
Epoch: 225 	Ltrain: 0.000526 	Lval: 0.000526
Epoch: 230 	Ltrain: 0.000513 	Lval: 0.000512
Epoch: 235 	Ltrain: 0.000501 	Lval: 0.000500
Epoch: 240 	Ltrain: 0.000489 	Lval: 0.000488
Epoch: 245 	Ltrain: 0.000478 	Lval: 0.000476
Epoch: 250 	Ltrain: 0.000466 	Lval: 0.000464
Epoch: 255 	Ltrain: 0.000453 	Lval: 0.000452
Epoch: 260 	Ltrain: 0.000448 	Lval: 0.000439
Epoch: 265 	Ltrain: 0.000430 	Lval: 0.000429
Epoch: 270 	Ltrain: 0.000417 	Lval: 0.000417
Epoch: 275 	Ltrain: 0.000406 	Lval: 0.000404
Epoch: 280 	Ltrain: 0.000396 	Lval: 0.000397
Epoch: 285 	Ltrain: 0.000382 	Lval: 0.000380
Epoch: 290 	Ltrain: 0.000370 	Lval: 0.000369
Epoch: 295 	Ltrain: 0.000358 	Lval: 0.000357
Epoch: 300 	Ltrain: 0.000348 	Lval: 0.000351
Epoch: 305 	Ltrain: 0.000337 	Lval: 0.000338
Epoch: 310 	Ltrain: 0.000325 	Lval: 0.000327
Epoch: 315 	Ltrain: 0.000316 	Lval: 0.000316
Epoch: 320 	Ltrain: 0.000309 	Lval: 0.000308
Epoch: 325 	Ltrain: 0.000298 	Lval: 0.000301
Epoch: 330 	Ltrain: 0.000288 	Lval: 0.000292
Epoch: 335 	Ltrain: 0.000278 	Lval: 0.000280
Epoch: 340 	Ltrain: 0.000272 	Lval: 0.000272
Epoch 00344: reducing learning rate of group 0 to 8.9977e-06.
Epoch: 345 	Ltrain: 0.000264 	Lval: 0.000261
Epoch: 350 	Ltrain: 0.000250 	Lval: 0.000254
Epoch: 355 	Ltrain: 0.000248 	Lval: 0.000253
Epoch: 360 	Ltrain: 0.000246 	Lval: 0.000252
Epoch: 365 	Ltrain: 0.000245 	Lval: 0.000251
Epoch: 370 	Ltrain: 0.000244 	Lval: 0.000249
Epoch: 375 	Ltrain: 0.000244 	Lval: 0.000248
Epoch: 380 	Ltrain: 0.000243 	Lval: 0.000247
Epoch: 385 	Ltrain: 0.000241 	Lval: 0.000246
Epoch: 390 	Ltrain: 0.000243 	Lval: 0.000245
EarlyStopper: stopping at epoch 390 with best_val_loss = 0.000251


	Fold 4/5
Epoch: 1 	Ltrain: 0.055461 	Lval: 0.009287
Epoch: 5 	Ltrain: 0.006002 	Lval: 0.006791
Epoch: 10 	Ltrain: 0.005372 	Lval: 0.008590
Epoch: 15 	Ltrain: 0.005161 	Lval: 0.006377
Epoch: 20 	Ltrain: 0.004736 	Lval: 0.005897
Epoch: 25 	Ltrain: 0.004661 	Lval: 0.005872
Epoch: 30 	Ltrain: 0.003963 	Lval: 0.004546
Epoch: 35 	Ltrain: 0.003685 	Lval: 0.004074
Epoch: 40 	Ltrain: 0.003376 	Lval: 0.003888
Epoch: 45 	Ltrain: 0.002996 	Lval: 0.003142
Epoch: 50 	Ltrain: 0.003059 	Lval: 0.002985
Epoch 00052: reducing learning rate of group 0 to 8.9977e-04.
Epoch: 55 	Ltrain: 0.001698 	Lval: 0.001770
Epoch: 60 	Ltrain: 0.001468 	Lval: 0.001510
Epoch: 65 	Ltrain: 0.001319 	Lval: 0.001359
Epoch: 70 	Ltrain: 0.001195 	Lval: 0.001220
Epoch: 75 	Ltrain: 0.001068 	Lval: 0.001082
Epoch: 80 	Ltrain: 0.000962 	Lval: 0.000970
Epoch: 85 	Ltrain: 0.000853 	Lval: 0.000855
Epoch: 90 	Ltrain: 0.000777 	Lval: 0.000783
Epoch: 95 	Ltrain: 0.000721 	Lval: 0.000724
Epoch: 100 	Ltrain: 0.000650 	Lval: 0.000651
Epoch: 105 	Ltrain: 0.000567 	Lval: 0.000587
Epoch: 110 	Ltrain: 0.000560 	Lval: 0.000549
Epoch: 115 	Ltrain: 0.000522 	Lval: 0.000502
Epoch: 120 	Ltrain: 0.000457 	Lval: 0.000462
Epoch: 125 	Ltrain: 0.000458 	Lval: 0.000474
Epoch 00126: reducing learning rate of group 0 to 8.9977e-05.
Epoch: 130 	Ltrain: 0.000328 	Lval: 0.000317
Epoch: 135 	Ltrain: 0.000311 	Lval: 0.000302
Epoch: 140 	Ltrain: 0.000304 	Lval: 0.000294
Epoch: 145 	Ltrain: 0.000298 	Lval: 0.000287
Epoch: 150 	Ltrain: 0.000291 	Lval: 0.000282
Epoch: 155 	Ltrain: 0.000286 	Lval: 0.000276
Epoch: 160 	Ltrain: 0.000280 	Lval: 0.000270
Epoch: 165 	Ltrain: 0.000274 	Lval: 0.000264
Epoch: 170 	Ltrain: 0.000269 	Lval: 0.000258
Epoch: 175 	Ltrain: 0.000262 	Lval: 0.000252
Epoch: 180 	Ltrain: 0.000256 	Lval: 0.000245
Epoch: 185 	Ltrain: 0.000250 	Lval: 0.000239
Epoch: 190 	Ltrain: 0.000243 	Lval: 0.000232
Epoch: 195 	Ltrain: 0.000237 	Lval: 0.000225
Epoch: 200 	Ltrain: 0.000230 	Lval: 0.000218
Epoch: 205 	Ltrain: 0.000223 	Lval: 0.000212
Epoch: 210 	Ltrain: 0.000216 	Lval: 0.000205
Epoch: 215 	Ltrain: 0.000210 	Lval: 0.000198
Epoch: 220 	Ltrain: 0.000203 	Lval: 0.000190
Epoch: 225 	Ltrain: 0.000196 	Lval: 0.000184
Epoch: 230 	Ltrain: 0.000189 	Lval: 0.000176
Epoch: 235 	Ltrain: 0.000182 	Lval: 0.000169
Epoch: 240 	Ltrain: 0.000176 	Lval: 0.000163
Epoch: 245 	Ltrain: 0.000169 	Lval: 0.000157
Epoch: 250 	Ltrain: 0.000164 	Lval: 0.000150
Epoch: 255 	Ltrain: 0.000161 	Lval: 0.000148
Epoch: 260 	Ltrain: 0.000152 	Lval: 0.000140
Epoch: 265 	Ltrain: 0.000148 	Lval: 0.000137
Epoch: 270 	Ltrain: 0.000145 	Lval: 0.000131
Epoch: 275 	Ltrain: 0.000138 	Lval: 0.000126
Epoch: 280 	Ltrain: 0.000134 	Lval: 0.000121
Epoch: 285 	Ltrain: 0.000131 	Lval: 0.000118
Epoch 00289: reducing learning rate of group 0 to 8.9977e-06.
Epoch: 290 	Ltrain: 0.000126 	Lval: 0.000111
Epoch: 295 	Ltrain: 0.000118 	Lval: 0.000106
Epoch: 300 	Ltrain: 0.000117 	Lval: 0.000105
Epoch: 305 	Ltrain: 0.000116 	Lval: 0.000105
Epoch: 310 	Ltrain: 0.000115 	Lval: 0.000104
Epoch: 315 	Ltrain: 0.000115 	Lval: 0.000103
EarlyStopper: stopping at epoch 316 with best_val_loss = 0.000109


	Fold 5/5
Epoch: 1 	Ltrain: 0.056342 	Lval: 0.015100
Epoch: 5 	Ltrain: 0.005798 	Lval: 0.007112
Epoch 00010: reducing learning rate of group 0 to 8.9977e-04.
Epoch: 10 	Ltrain: 0.005467 	Lval: 0.009047
Epoch: 15 	Ltrain: 0.004949 	Lval: 0.006334
Epoch: 20 	Ltrain: 0.004896 	Lval: 0.006513
Epoch: 25 	Ltrain: 0.004901 	Lval: 0.006142
Epoch: 30 	Ltrain: 0.004838 	Lval: 0.006141
Epoch: 35 	Ltrain: 0.004801 	Lval: 0.006032
Epoch 00039: reducing learning rate of group 0 to 8.9977e-05.
Epoch: 40 	Ltrain: 0.004607 	Lval: 0.005969
Epoch: 45 	Ltrain: 0.004565 	Lval: 0.005963
Epoch: 50 	Ltrain: 0.004556 	Lval: 0.005941
Epoch 00055: reducing learning rate of group 0 to 8.9977e-06.
Epoch: 55 	Ltrain: 0.004577 	Lval: 0.005999
Epoch: 60 	Ltrain: 0.004527 	Lval: 0.005900
Epoch: 65 	Ltrain: 0.004529 	Lval: 0.005901
Epoch 00067: reducing learning rate of group 0 to 8.9977e-07.
Epoch: 70 	Ltrain: 0.004532 	Lval: 0.005896
Epoch: 75 	Ltrain: 0.004537 	Lval: 0.005898
EarlyStopper: stopping at epoch 76 with best_val_loss = 0.005868

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005043960024360002
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.501367674243511e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 25
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.052558 	Lval: 0.014907
Epoch: 5 	Ltrain: 0.009919 	Lval: 0.008753
Epoch: 10 	Ltrain: 0.008008 	Lval: 0.007532
Epoch: 15 	Ltrain: 0.008065 	Lval: 0.008274
Epoch: 20 	Ltrain: 0.007204 	Lval: 0.007423
Epoch: 25 	Ltrain: 0.006951 	Lval: 0.006427
Epoch: 30 	Ltrain: 0.006516 	Lval: 0.006135
Epoch: 35 	Ltrain: 0.006808 	Lval: 0.007143
Epoch 00036: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 40 	Ltrain: 0.006089 	Lval: 0.005785
Epoch: 45 	Ltrain: 0.005411 	Lval: 0.005564
Epoch: 50 	Ltrain: 0.005434 	Lval: 0.005491
Epoch: 55 	Ltrain: 0.005701 	Lval: 0.005411
Epoch: 60 	Ltrain: 0.005313 	Lval: 0.005355
Epoch: 65 	Ltrain: 0.005359 	Lval: 0.005269
Epoch: 70 	Ltrain: 0.005655 	Lval: 0.005189
Epoch: 75 	Ltrain: 0.005076 	Lval: 0.005055
Epoch: 80 	Ltrain: 0.005084 	Lval: 0.004974
Epoch: 85 	Ltrain: 0.005123 	Lval: 0.004836
Epoch: 90 	Ltrain: 0.004748 	Lval: 0.004764
Epoch: 95 	Ltrain: 0.004769 	Lval: 0.004652
Epoch: 100 	Ltrain: 0.005006 	Lval: 0.004599
Epoch: 105 	Ltrain: 0.004736 	Lval: 0.004410
Epoch: 110 	Ltrain: 0.004839 	Lval: 0.004302
Epoch: 115 	Ltrain: 0.004216 	Lval: 0.004231
Epoch: 120 	Ltrain: 0.004155 	Lval: 0.004060
Epoch: 125 	Ltrain: 0.004479 	Lval: 0.003961
Epoch: 130 	Ltrain: 0.003943 	Lval: 0.003851
Epoch: 135 	Ltrain: 0.003942 	Lval: 0.003760
Epoch: 140 	Ltrain: 0.003774 	Lval: 0.003550
Epoch: 145 	Ltrain: 0.004082 	Lval: 0.003474
Epoch: 150 	Ltrain: 0.003256 	Lval: 0.003248
Epoch: 155 	Ltrain: 0.003480 	Lval: 0.003315
Epoch: 160 	Ltrain: 0.003509 	Lval: 0.003392
Epoch 00162: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 165 	Ltrain: 0.003104 	Lval: 0.003045
Epoch: 170 	Ltrain: 0.002886 	Lval: 0.002967
Epoch: 175 	Ltrain: 0.003070 	Lval: 0.002944
Epoch: 180 	Ltrain: 0.003012 	Lval: 0.002925
Epoch: 185 	Ltrain: 0.002837 	Lval: 0.002892
Epoch: 190 	Ltrain: 0.002938 	Lval: 0.002887
Epoch: 195 	Ltrain: 0.002823 	Lval: 0.002863
Epoch: 200 	Ltrain: 0.003012 	Lval: 0.002848
Epoch: 205 	Ltrain: 0.003116 	Lval: 0.002829
Epoch: 210 	Ltrain: 0.002829 	Lval: 0.002808
Epoch: 215 	Ltrain: 0.002826 	Lval: 0.002796
Epoch: 220 	Ltrain: 0.002757 	Lval: 0.002782
Epoch: 225 	Ltrain: 0.002752 	Lval: 0.002784
Epoch 00226: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 230 	Ltrain: 0.002851 	Lval: 0.002773
Epoch: 235 	Ltrain: 0.002798 	Lval: 0.002769
Epoch: 240 	Ltrain: 0.002705 	Lval: 0.002766
Epoch: 245 	Ltrain: 0.002817 	Lval: 0.002764
Epoch 00249: reducing learning rate of group 0 to 5.0440e-07.
Epoch: 250 	Ltrain: 0.002813 	Lval: 0.002764
Epoch: 255 	Ltrain: 0.002904 	Lval: 0.002763
EarlyStopper: stopping at epoch 255 with best_val_loss = 0.002771


	Fold 2/5
Epoch: 1 	Ltrain: 0.032323 	Lval: 0.014219
Epoch: 5 	Ltrain: 0.007847 	Lval: 0.007851
Epoch: 10 	Ltrain: 0.006749 	Lval: 0.006988
Epoch: 15 	Ltrain: 0.006432 	Lval: 0.006553
Epoch: 20 	Ltrain: 0.006350 	Lval: 0.007188
Epoch 00023: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 25 	Ltrain: 0.005541 	Lval: 0.005816
Epoch: 30 	Ltrain: 0.005586 	Lval: 0.005853
Epoch: 35 	Ltrain: 0.005507 	Lval: 0.005732
Epoch: 40 	Ltrain: 0.005519 	Lval: 0.005735
Epoch 00041: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 45 	Ltrain: 0.005342 	Lval: 0.005677
Epoch: 50 	Ltrain: 0.005326 	Lval: 0.005659
Epoch 00054: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 55 	Ltrain: 0.005375 	Lval: 0.005661
Epoch: 60 	Ltrain: 0.005343 	Lval: 0.005660
Epoch: 65 	Ltrain: 0.005335 	Lval: 0.005658
Epoch: 70 	Ltrain: 0.005346 	Lval: 0.005656
Epoch 00072: reducing learning rate of group 0 to 5.0440e-07.
Epoch: 75 	Ltrain: 0.005379 	Lval: 0.005656
Epoch: 80 	Ltrain: 0.005306 	Lval: 0.005656
Epoch 00084: reducing learning rate of group 0 to 5.0440e-08.
Epoch: 85 	Ltrain: 0.005302 	Lval: 0.005656
Epoch: 90 	Ltrain: 0.005243 	Lval: 0.005656
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.005657


	Fold 3/5
Epoch: 1 	Ltrain: 0.025434 	Lval: 0.014766
Epoch: 5 	Ltrain: 0.006699 	Lval: 0.008489
Epoch: 10 	Ltrain: 0.006299 	Lval: 0.007179
Epoch: 15 	Ltrain: 0.005674 	Lval: 0.006364
Epoch: 20 	Ltrain: 0.005515 	Lval: 0.005945
Epoch: 25 	Ltrain: 0.005645 	Lval: 0.006082
Epoch: 30 	Ltrain: 0.004803 	Lval: 0.005285
Epoch: 35 	Ltrain: 0.004767 	Lval: 0.005197
Epoch: 40 	Ltrain: 0.004309 	Lval: 0.004760
Epoch: 45 	Ltrain: 0.003774 	Lval: 0.004167
Epoch: 50 	Ltrain: 0.003302 	Lval: 0.003757
Epoch: 55 	Ltrain: 0.002991 	Lval: 0.003492
Epoch 00056: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 60 	Ltrain: 0.001943 	Lval: 0.001957
Epoch: 65 	Ltrain: 0.001754 	Lval: 0.001771
Epoch: 70 	Ltrain: 0.001628 	Lval: 0.001626
Epoch: 75 	Ltrain: 0.001527 	Lval: 0.001516
Epoch: 80 	Ltrain: 0.001428 	Lval: 0.001400
Epoch: 85 	Ltrain: 0.001332 	Lval: 0.001299
Epoch: 90 	Ltrain: 0.001236 	Lval: 0.001195
Epoch: 95 	Ltrain: 0.001144 	Lval: 0.001107
Epoch: 100 	Ltrain: 0.001072 	Lval: 0.001026
Epoch: 105 	Ltrain: 0.000977 	Lval: 0.000941
Epoch: 110 	Ltrain: 0.000921 	Lval: 0.000884
Epoch: 115 	Ltrain: 0.000865 	Lval: 0.000828
Epoch: 120 	Ltrain: 0.000786 	Lval: 0.000763
Epoch: 125 	Ltrain: 0.000755 	Lval: 0.000752
Epoch: 130 	Ltrain: 0.000690 	Lval: 0.000673
Epoch: 135 	Ltrain: 0.000657 	Lval: 0.000618
Epoch 00139: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 140 	Ltrain: 0.000601 	Lval: 0.000577
Epoch: 145 	Ltrain: 0.000543 	Lval: 0.000544
Epoch: 150 	Ltrain: 0.000535 	Lval: 0.000535
Epoch: 155 	Ltrain: 0.000531 	Lval: 0.000530
Epoch: 160 	Ltrain: 0.000524 	Lval: 0.000524
Epoch: 165 	Ltrain: 0.000520 	Lval: 0.000519
Epoch: 170 	Ltrain: 0.000517 	Lval: 0.000513
Epoch: 175 	Ltrain: 0.000509 	Lval: 0.000508
Epoch: 180 	Ltrain: 0.000502 	Lval: 0.000502
Epoch: 185 	Ltrain: 0.000498 	Lval: 0.000497
Epoch: 190 	Ltrain: 0.000491 	Lval: 0.000491
Epoch: 195 	Ltrain: 0.000487 	Lval: 0.000485
Epoch: 200 	Ltrain: 0.000480 	Lval: 0.000479
Epoch: 205 	Ltrain: 0.000474 	Lval: 0.000473
Epoch: 210 	Ltrain: 0.000468 	Lval: 0.000467
Epoch: 215 	Ltrain: 0.000462 	Lval: 0.000460
Epoch: 220 	Ltrain: 0.000456 	Lval: 0.000454
Epoch: 225 	Ltrain: 0.000453 	Lval: 0.000448
Epoch: 230 	Ltrain: 0.000442 	Lval: 0.000441
Epoch: 235 	Ltrain: 0.000436 	Lval: 0.000434
Epoch: 240 	Ltrain: 0.000430 	Lval: 0.000428
Epoch: 245 	Ltrain: 0.000424 	Lval: 0.000421
Epoch: 250 	Ltrain: 0.000418 	Lval: 0.000414
Epoch: 255 	Ltrain: 0.000410 	Lval: 0.000408
Epoch: 260 	Ltrain: 0.000408 	Lval: 0.000401
Epoch: 265 	Ltrain: 0.000400 	Lval: 0.000395
Epoch: 270 	Ltrain: 0.000391 	Lval: 0.000388
Epoch: 275 	Ltrain: 0.000388 	Lval: 0.000381
Epoch: 280 	Ltrain: 0.000378 	Lval: 0.000375
Epoch: 285 	Ltrain: 0.000378 	Lval: 0.000371
Epoch: 290 	Ltrain: 0.000365 	Lval: 0.000362
Epoch: 295 	Ltrain: 0.000358 	Lval: 0.000356
Epoch: 300 	Ltrain: 0.000353 	Lval: 0.000349
Epoch: 305 	Ltrain: 0.000347 	Lval: 0.000343
Epoch: 310 	Ltrain: 0.000340 	Lval: 0.000338
Epoch: 315 	Ltrain: 0.000334 	Lval: 0.000331
Epoch: 320 	Ltrain: 0.000334 	Lval: 0.000326
Epoch: 325 	Ltrain: 0.000325 	Lval: 0.000320
Epoch: 330 	Ltrain: 0.000317 	Lval: 0.000313
Epoch: 335 	Ltrain: 0.000313 	Lval: 0.000308
Epoch: 340 	Ltrain: 0.000308 	Lval: 0.000304
Epoch: 345 	Ltrain: 0.000302 	Lval: 0.000297
Epoch: 350 	Ltrain: 0.000296 	Lval: 0.000291
Epoch: 355 	Ltrain: 0.000290 	Lval: 0.000286
Epoch: 360 	Ltrain: 0.000287 	Lval: 0.000281
Epoch: 365 	Ltrain: 0.000281 	Lval: 0.000277
Epoch: 370 	Ltrain: 0.000277 	Lval: 0.000273
Epoch: 375 	Ltrain: 0.000272 	Lval: 0.000266
Epoch: 380 	Ltrain: 0.000265 	Lval: 0.000262
Epoch: 385 	Ltrain: 0.000265 	Lval: 0.000257
Epoch: 390 	Ltrain: 0.000258 	Lval: 0.000252
Epoch: 395 	Ltrain: 0.000253 	Lval: 0.000248
Epoch: 400 	Ltrain: 0.000251 	Lval: 0.000246
Epoch: 405 	Ltrain: 0.000245 	Lval: 0.000240
Epoch: 410 	Ltrain: 0.000242 	Lval: 0.000235
Epoch: 415 	Ltrain: 0.000239 	Lval: 0.000232
Epoch: 420 	Ltrain: 0.000236 	Lval: 0.000229
Epoch: 425 	Ltrain: 0.000231 	Lval: 0.000225
Epoch: 430 	Ltrain: 0.000228 	Lval: 0.000222
Epoch: 435 	Ltrain: 0.000225 	Lval: 0.000218
Epoch: 440 	Ltrain: 0.000224 	Lval: 0.000215
Epoch: 445 	Ltrain: 0.000216 	Lval: 0.000209
Epoch: 450 	Ltrain: 0.000215 	Lval: 0.000206
Epoch 00455: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 455 	Ltrain: 0.000216 	Lval: 0.000209
Epoch: 460 	Ltrain: 0.000207 	Lval: 0.000201
Epoch: 465 	Ltrain: 0.000206 	Lval: 0.000200
Epoch: 470 	Ltrain: 0.000204 	Lval: 0.000200
Epoch: 475 	Ltrain: 0.000210 	Lval: 0.000200
Epoch: 480 	Ltrain: 0.000204 	Lval: 0.000199
EarlyStopper: stopping at epoch 480 with best_val_loss = 0.000204


	Fold 4/5
Epoch: 1 	Ltrain: 0.019974 	Lval: 0.009965
Epoch: 5 	Ltrain: 0.006068 	Lval: 0.006883
Epoch 00009: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 10 	Ltrain: 0.005218 	Lval: 0.006568
Epoch: 15 	Ltrain: 0.005075 	Lval: 0.006426
Epoch: 20 	Ltrain: 0.005063 	Lval: 0.006487
Epoch: 25 	Ltrain: 0.004940 	Lval: 0.006246
Epoch: 30 	Ltrain: 0.004902 	Lval: 0.006125
Epoch: 35 	Ltrain: 0.004878 	Lval: 0.006032
Epoch: 40 	Ltrain: 0.004738 	Lval: 0.006034
Epoch 00043: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 45 	Ltrain: 0.004582 	Lval: 0.005835
Epoch: 50 	Ltrain: 0.004571 	Lval: 0.005778
Epoch: 55 	Ltrain: 0.004557 	Lval: 0.005813
Epoch 00058: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 60 	Ltrain: 0.004531 	Lval: 0.005759
Epoch: 65 	Ltrain: 0.004527 	Lval: 0.005765
Epoch 00070: reducing learning rate of group 0 to 5.0440e-07.
Epoch: 70 	Ltrain: 0.004550 	Lval: 0.005769
Epoch: 75 	Ltrain: 0.004530 	Lval: 0.005766
EarlyStopper: stopping at epoch 78 with best_val_loss = 0.005751


	Fold 5/5
Epoch: 1 	Ltrain: 0.020096 	Lval: 0.013960
Epoch: 5 	Ltrain: 0.006029 	Lval: 0.008398
Epoch 00010: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 10 	Ltrain: 0.005625 	Lval: 0.007265
Epoch: 15 	Ltrain: 0.005022 	Lval: 0.006740
Epoch: 20 	Ltrain: 0.004950 	Lval: 0.006579
Epoch: 25 	Ltrain: 0.004933 	Lval: 0.006381
Epoch: 30 	Ltrain: 0.004866 	Lval: 0.006182
Epoch 00034: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 35 	Ltrain: 0.004740 	Lval: 0.006160
Epoch: 40 	Ltrain: 0.004698 	Lval: 0.006156
Epoch: 45 	Ltrain: 0.004695 	Lval: 0.006179
Epoch: 50 	Ltrain: 0.004696 	Lval: 0.006108
Epoch: 55 	Ltrain: 0.004657 	Lval: 0.006079
Epoch: 60 	Ltrain: 0.004642 	Lval: 0.006077
Epoch: 65 	Ltrain: 0.004654 	Lval: 0.006058
Epoch: 70 	Ltrain: 0.004622 	Lval: 0.006024
Epoch: 75 	Ltrain: 0.004622 	Lval: 0.005976
Epoch: 80 	Ltrain: 0.004599 	Lval: 0.005959
Epoch 00082: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 85 	Ltrain: 0.004563 	Lval: 0.005976
Epoch: 90 	Ltrain: 0.004566 	Lval: 0.005972
Epoch 00094: reducing learning rate of group 0 to 5.0440e-07.
Epoch: 95 	Ltrain: 0.004565 	Lval: 0.005971
Epoch: 100 	Ltrain: 0.004552 	Lval: 0.005970
EarlyStopper: stopping at epoch 102 with best_val_loss = 0.005954

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0018960689475363206
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.38232269132976e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.029654 	Lval: 0.017230
Epoch: 5 	Ltrain: 0.011397 	Lval: 0.010362
Epoch: 10 	Ltrain: 0.008494 	Lval: 0.007999
Epoch: 15 	Ltrain: 0.007393 	Lval: 0.007448
Epoch: 20 	Ltrain: 0.007978 	Lval: 0.007062
Epoch 00022: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 25 	Ltrain: 0.006699 	Lval: 0.006797
Epoch: 30 	Ltrain: 0.006842 	Lval: 0.006711
Epoch: 35 	Ltrain: 0.006947 	Lval: 0.006616
Epoch: 40 	Ltrain: 0.007056 	Lval: 0.006552
Epoch: 45 	Ltrain: 0.006897 	Lval: 0.006548
Epoch: 50 	Ltrain: 0.006547 	Lval: 0.006491
Epoch: 55 	Ltrain: 0.006423 	Lval: 0.006434
Epoch: 60 	Ltrain: 0.006384 	Lval: 0.006373
Epoch: 65 	Ltrain: 0.006307 	Lval: 0.006281
Epoch: 70 	Ltrain: 0.006357 	Lval: 0.006230
Epoch: 75 	Ltrain: 0.006381 	Lval: 0.006197
Epoch: 80 	Ltrain: 0.005984 	Lval: 0.006132
Epoch: 85 	Ltrain: 0.005953 	Lval: 0.006091
Epoch: 90 	Ltrain: 0.006278 	Lval: 0.006054
Epoch 00094: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 95 	Ltrain: 0.006120 	Lval: 0.006121
Epoch: 100 	Ltrain: 0.006779 	Lval: 0.006058
Epoch: 105 	Ltrain: 0.005887 	Lval: 0.006075
Epoch 00106: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 110 	Ltrain: 0.006004 	Lval: 0.006056
EarlyStopper: stopping at epoch 109 with best_val_loss = 0.006054


	Fold 2/5
Epoch: 1 	Ltrain: 0.020696 	Lval: 0.014355
Epoch: 5 	Ltrain: 0.008405 	Lval: 0.008560
Epoch: 10 	Ltrain: 0.007158 	Lval: 0.007000
Epoch: 15 	Ltrain: 0.006501 	Lval: 0.006817
Epoch: 20 	Ltrain: 0.006808 	Lval: 0.006583
Epoch: 25 	Ltrain: 0.006436 	Lval: 0.006464
Epoch 00026: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 30 	Ltrain: 0.005993 	Lval: 0.006342
Epoch: 35 	Ltrain: 0.005948 	Lval: 0.006206
Epoch: 40 	Ltrain: 0.005857 	Lval: 0.006116
Epoch: 45 	Ltrain: 0.005910 	Lval: 0.006077
Epoch: 50 	Ltrain: 0.005967 	Lval: 0.005973
Epoch: 55 	Ltrain: 0.005776 	Lval: 0.005889
Epoch 00059: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 60 	Ltrain: 0.005691 	Lval: 0.005904
Epoch: 65 	Ltrain: 0.005765 	Lval: 0.005862
Epoch: 70 	Ltrain: 0.005680 	Lval: 0.005861
Epoch 00071: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 75 	Ltrain: 0.005625 	Lval: 0.005862
Epoch: 80 	Ltrain: 0.005702 	Lval: 0.005858
Epoch: 85 	Ltrain: 0.005550 	Lval: 0.005856
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.005862


	Fold 3/5
Epoch: 1 	Ltrain: 0.022097 	Lval: 0.012003
Epoch: 5 	Ltrain: 0.007078 	Lval: 0.007239
Epoch: 10 	Ltrain: 0.006411 	Lval: 0.006940
Epoch: 15 	Ltrain: 0.006140 	Lval: 0.007034
Epoch: 20 	Ltrain: 0.006261 	Lval: 0.007065
Epoch: 25 	Ltrain: 0.005883 	Lval: 0.006470
Epoch: 30 	Ltrain: 0.005881 	Lval: 0.006339
Epoch 00033: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 35 	Ltrain: 0.005165 	Lval: 0.005834
Epoch: 40 	Ltrain: 0.005132 	Lval: 0.005800
Epoch: 45 	Ltrain: 0.005101 	Lval: 0.005771
Epoch: 50 	Ltrain: 0.005090 	Lval: 0.005706
Epoch: 55 	Ltrain: 0.005030 	Lval: 0.005652
Epoch: 60 	Ltrain: 0.004981 	Lval: 0.005673
Epoch: 65 	Ltrain: 0.004985 	Lval: 0.005651
Epoch: 70 	Ltrain: 0.004929 	Lval: 0.005545
Epoch: 75 	Ltrain: 0.004918 	Lval: 0.005580
Epoch 00076: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 80 	Ltrain: 0.004786 	Lval: 0.005485
Epoch: 85 	Ltrain: 0.004811 	Lval: 0.005459
Epoch 00089: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 90 	Ltrain: 0.004790 	Lval: 0.005471
Epoch: 95 	Ltrain: 0.004794 	Lval: 0.005465
Epoch: 100 	Ltrain: 0.004792 	Lval: 0.005465
Epoch 00101: reducing learning rate of group 0 to 1.8961e-07.
Epoch: 105 	Ltrain: 0.004806 	Lval: 0.005463
EarlyStopper: stopping at epoch 104 with best_val_loss = 0.005459


	Fold 4/5
Epoch: 1 	Ltrain: 0.021205 	Lval: 0.014033
Epoch: 5 	Ltrain: 0.005961 	Lval: 0.007373
Epoch: 10 	Ltrain: 0.005647 	Lval: 0.006773
Epoch 00015: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 15 	Ltrain: 0.005580 	Lval: 0.007627
Epoch: 20 	Ltrain: 0.005141 	Lval: 0.006476
Epoch: 25 	Ltrain: 0.005105 	Lval: 0.006555
Epoch: 30 	Ltrain: 0.005067 	Lval: 0.006617
Epoch 00032: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 35 	Ltrain: 0.004995 	Lval: 0.006338
Epoch: 40 	Ltrain: 0.004995 	Lval: 0.006345
Epoch: 45 	Ltrain: 0.004982 	Lval: 0.006335
Epoch: 50 	Ltrain: 0.004985 	Lval: 0.006335
Epoch 00051: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 55 	Ltrain: 0.004969 	Lval: 0.006315
Epoch: 60 	Ltrain: 0.004988 	Lval: 0.006316
Epoch 00063: reducing learning rate of group 0 to 1.8961e-07.
Epoch: 65 	Ltrain: 0.004968 	Lval: 0.006313
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.006310


	Fold 5/5
Epoch: 1 	Ltrain: 0.017282 	Lval: 0.012424
Epoch: 5 	Ltrain: 0.006419 	Lval: 0.008932
Epoch: 10 	Ltrain: 0.005738 	Lval: 0.006904
Epoch: 15 	Ltrain: 0.005564 	Lval: 0.007539
Epoch 00018: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 20 	Ltrain: 0.005027 	Lval: 0.006511
Epoch: 25 	Ltrain: 0.004996 	Lval: 0.006517
Epoch: 30 	Ltrain: 0.005008 	Lval: 0.006296
Epoch 00032: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 35 	Ltrain: 0.004960 	Lval: 0.006341
Epoch: 40 	Ltrain: 0.004918 	Lval: 0.006326
Epoch 00044: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 45 	Ltrain: 0.004922 	Lval: 0.006343
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.006282

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0007627701732261187
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.0045546161308926e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 25
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.036070 	Lval: 0.020854
Epoch: 5 	Ltrain: 0.018308 	Lval: 0.015876
Epoch: 10 	Ltrain: 0.015617 	Lval: 0.015206
Epoch: 15 	Ltrain: 0.012832 	Lval: 0.017429
Epoch: 20 	Ltrain: 0.011688 	Lval: 0.012190
Epoch: 25 	Ltrain: 0.013637 	Lval: 0.010687
Epoch: 30 	Ltrain: 0.011532 	Lval: 0.009623
Epoch: 35 	Ltrain: 0.010162 	Lval: 0.009936
Epoch: 40 	Ltrain: 0.008283 	Lval: 0.008312
Epoch: 45 	Ltrain: 0.009646 	Lval: 0.008647
Epoch: 50 	Ltrain: 0.007399 	Lval: 0.007501
Epoch: 55 	Ltrain: 0.010021 	Lval: 0.007831
Epoch 00056: reducing learning rate of group 0 to 7.6277e-05.
Epoch: 60 	Ltrain: 0.009536 	Lval: 0.007327
Epoch: 65 	Ltrain: 0.007542 	Lval: 0.007194
Epoch 00070: reducing learning rate of group 0 to 7.6277e-06.
Epoch: 70 	Ltrain: 0.009217 	Lval: 0.007388
Epoch: 75 	Ltrain: 0.007990 	Lval: 0.007296
Epoch: 80 	Ltrain: 0.007913 	Lval: 0.007212
Epoch: 85 	Ltrain: 0.006676 	Lval: 0.007135
Epoch 00090: reducing learning rate of group 0 to 7.6277e-07.
Epoch: 90 	Ltrain: 0.007365 	Lval: 0.007173
Epoch: 95 	Ltrain: 0.009871 	Lval: 0.007174
Epoch: 100 	Ltrain: 0.006945 	Lval: 0.007177
Epoch 00102: reducing learning rate of group 0 to 7.6277e-08.
Epoch: 105 	Ltrain: 0.007289 	Lval: 0.007177
Epoch: 110 	Ltrain: 0.007708 	Lval: 0.007176
EarlyStopper: stopping at epoch 109 with best_val_loss = 0.007135


	Fold 2/5
Epoch: 1 	Ltrain: 0.040851 	Lval: 0.023258
Epoch: 5 	Ltrain: 0.018280 	Lval: 0.015189
Epoch: 10 	Ltrain: 0.013554 	Lval: 0.012943
Epoch: 15 	Ltrain: 0.011247 	Lval: 0.010026
Epoch: 20 	Ltrain: 0.010565 	Lval: 0.014451
Epoch: 25 	Ltrain: 0.008008 	Lval: 0.008338
Epoch: 30 	Ltrain: 0.009202 	Lval: 0.007238
Epoch: 35 	Ltrain: 0.007115 	Lval: 0.007000
Epoch 00038: reducing learning rate of group 0 to 7.6277e-05.
Epoch: 40 	Ltrain: 0.006658 	Lval: 0.006735
Epoch: 45 	Ltrain: 0.006768 	Lval: 0.006843
Epoch 00050: reducing learning rate of group 0 to 7.6277e-06.
Epoch: 50 	Ltrain: 0.006498 	Lval: 0.006742
Epoch: 55 	Ltrain: 0.006245 	Lval: 0.006791
Epoch: 60 	Ltrain: 0.006653 	Lval: 0.006812
Epoch 00062: reducing learning rate of group 0 to 7.6277e-07.
Epoch: 65 	Ltrain: 0.007027 	Lval: 0.006783
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.006711


	Fold 3/5
Epoch: 1 	Ltrain: 0.035151 	Lval: 0.016784
Epoch: 5 	Ltrain: 0.015101 	Lval: 0.013494
Epoch: 10 	Ltrain: 0.011723 	Lval: 0.010390
Epoch: 15 	Ltrain: 0.008652 	Lval: 0.008900
Epoch: 20 	Ltrain: 0.007271 	Lval: 0.007092
Epoch: 25 	Ltrain: 0.006784 	Lval: 0.006967
Epoch 00026: reducing learning rate of group 0 to 7.6277e-05.
Epoch: 30 	Ltrain: 0.006708 	Lval: 0.006650
Epoch: 35 	Ltrain: 0.006432 	Lval: 0.006704
Epoch 00038: reducing learning rate of group 0 to 7.6277e-06.
Epoch: 40 	Ltrain: 0.006278 	Lval: 0.006637
Epoch: 45 	Ltrain: 0.005959 	Lval: 0.006621
Epoch 00050: reducing learning rate of group 0 to 7.6277e-07.
Epoch: 50 	Ltrain: 0.006394 	Lval: 0.006628
Epoch: 55 	Ltrain: 0.006308 	Lval: 0.006624
Epoch: 60 	Ltrain: 0.006157 	Lval: 0.006626
Epoch 00062: reducing learning rate of group 0 to 7.6277e-08.
Epoch: 65 	Ltrain: 0.006093 	Lval: 0.006624
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.006618


	Fold 4/5
Epoch: 1 	Ltrain: 0.038511 	Lval: 0.020076
Epoch: 5 	Ltrain: 0.012521 	Lval: 0.012543
Epoch: 10 	Ltrain: 0.009309 	Lval: 0.009338
Epoch: 15 	Ltrain: 0.006351 	Lval: 0.007406
Epoch: 20 	Ltrain: 0.006055 	Lval: 0.007182
Epoch: 25 	Ltrain: 0.005676 	Lval: 0.006699
Epoch 00028: reducing learning rate of group 0 to 7.6277e-05.
Epoch: 30 	Ltrain: 0.005645 	Lval: 0.006517
Epoch: 35 	Ltrain: 0.005311 	Lval: 0.006512
Epoch: 40 	Ltrain: 0.005391 	Lval: 0.006484
Epoch: 45 	Ltrain: 0.005480 	Lval: 0.006501
Epoch 00050: reducing learning rate of group 0 to 7.6277e-06.
Epoch: 50 	Ltrain: 0.005441 	Lval: 0.006454
Epoch: 55 	Ltrain: 0.005527 	Lval: 0.006455
Epoch: 60 	Ltrain: 0.005667 	Lval: 0.006453
Epoch 00062: reducing learning rate of group 0 to 7.6277e-07.
Epoch: 65 	Ltrain: 0.005498 	Lval: 0.006452
Epoch: 70 	Ltrain: 0.005559 	Lval: 0.006451
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.006448


	Fold 5/5
Epoch: 1 	Ltrain: 0.035516 	Lval: 0.025763
Epoch: 5 	Ltrain: 0.013762 	Lval: 0.015911
Epoch: 10 	Ltrain: 0.008450 	Lval: 0.012184
Epoch: 15 	Ltrain: 0.006311 	Lval: 0.008169
Epoch: 20 	Ltrain: 0.005966 	Lval: 0.007983
Epoch: 25 	Ltrain: 0.006090 	Lval: 0.007268
Epoch: 30 	Ltrain: 0.005356 	Lval: 0.006767
Epoch 00032: reducing learning rate of group 0 to 7.6277e-05.
Epoch: 35 	Ltrain: 0.005235 	Lval: 0.006760
Epoch: 40 	Ltrain: 0.005311 	Lval: 0.006806
Epoch 00044: reducing learning rate of group 0 to 7.6277e-06.
Epoch: 45 	Ltrain: 0.005298 	Lval: 0.006778
Epoch: 50 	Ltrain: 0.005237 	Lval: 0.006745
Epoch: 55 	Ltrain: 0.005322 	Lval: 0.006747
Epoch 00056: reducing learning rate of group 0 to 7.6277e-07.
Epoch: 60 	Ltrain: 0.005263 	Lval: 0.006749
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.006738

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009927915998691006
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.170607548869232e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 30
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.206553 	Lval: 0.082174
Epoch: 5 	Ltrain: 0.016090 	Lval: 0.015512
Epoch: 10 	Ltrain: 0.010158 	Lval: 0.008701
Epoch: 15 	Ltrain: 0.008342 	Lval: 0.007696
Epoch: 20 	Ltrain: 0.007013 	Lval: 0.006782
Epoch: 25 	Ltrain: 0.006458 	Lval: 0.006343
Epoch 00029: reducing learning rate of group 0 to 9.9279e-04.
Epoch: 30 	Ltrain: 0.006253 	Lval: 0.006237
Epoch: 35 	Ltrain: 0.006339 	Lval: 0.006036
Epoch: 40 	Ltrain: 0.005858 	Lval: 0.005990
Epoch: 45 	Ltrain: 0.005697 	Lval: 0.005971
Epoch: 50 	Ltrain: 0.005819 	Lval: 0.005913
Epoch: 55 	Ltrain: 0.005784 	Lval: 0.005867
Epoch: 60 	Ltrain: 0.005643 	Lval: 0.005835
Epoch: 65 	Ltrain: 0.005843 	Lval: 0.005718
Epoch: 70 	Ltrain: 0.005758 	Lval: 0.005658
Epoch: 75 	Ltrain: 0.005516 	Lval: 0.005582
Epoch: 80 	Ltrain: 0.005374 	Lval: 0.005530
Epoch: 85 	Ltrain: 0.005749 	Lval: 0.005458
Epoch: 90 	Ltrain: 0.005393 	Lval: 0.005341
Epoch: 95 	Ltrain: 0.005125 	Lval: 0.005277
Epoch: 100 	Ltrain: 0.005212 	Lval: 0.005161
Epoch: 105 	Ltrain: 0.005074 	Lval: 0.005344
Epoch 00110: reducing learning rate of group 0 to 9.9279e-05.
Epoch: 110 	Ltrain: 0.005782 	Lval: 0.005237
Epoch: 115 	Ltrain: 0.004652 	Lval: 0.004865
Epoch: 120 	Ltrain: 0.004716 	Lval: 0.004855
Epoch: 125 	Ltrain: 0.004581 	Lval: 0.004842
Epoch: 130 	Ltrain: 0.004657 	Lval: 0.004830
Epoch: 135 	Ltrain: 0.004718 	Lval: 0.004823
Epoch: 140 	Ltrain: 0.004613 	Lval: 0.004810
Epoch 00141: reducing learning rate of group 0 to 9.9279e-06.
Epoch: 145 	Ltrain: 0.005081 	Lval: 0.004805
Epoch: 150 	Ltrain: 0.004933 	Lval: 0.004806
Epoch: 155 	Ltrain: 0.004604 	Lval: 0.004803
Epoch: 160 	Ltrain: 0.004847 	Lval: 0.004800
Epoch: 165 	Ltrain: 0.004636 	Lval: 0.004797
Epoch 00168: reducing learning rate of group 0 to 9.9279e-07.
Epoch: 170 	Ltrain: 0.004709 	Lval: 0.004799
Epoch: 175 	Ltrain: 0.004698 	Lval: 0.004799
Epoch 00180: reducing learning rate of group 0 to 9.9279e-08.
Epoch: 180 	Ltrain: 0.004974 	Lval: 0.004798
Epoch: 185 	Ltrain: 0.004905 	Lval: 0.004798
EarlyStopper: stopping at epoch 186 with best_val_loss = 0.004799


	Fold 2/5
Epoch: 1 	Ltrain: 0.134536 	Lval: 0.022378
Epoch: 5 	Ltrain: 0.008875 	Lval: 0.008255
Epoch: 10 	Ltrain: 0.006582 	Lval: 0.007333
Epoch: 15 	Ltrain: 0.006522 	Lval: 0.006351
Epoch: 20 	Ltrain: 0.005893 	Lval: 0.005783
Epoch 00024: reducing learning rate of group 0 to 9.9279e-04.
Epoch: 25 	Ltrain: 0.005460 	Lval: 0.005477
Epoch: 30 	Ltrain: 0.005125 	Lval: 0.005364
Epoch: 35 	Ltrain: 0.005024 	Lval: 0.005288
Epoch 00039: reducing learning rate of group 0 to 9.9279e-05.
Epoch: 40 	Ltrain: 0.004920 	Lval: 0.005227
Epoch: 45 	Ltrain: 0.004900 	Lval: 0.005209
Epoch: 50 	Ltrain: 0.005112 	Lval: 0.005199
Epoch 00052: reducing learning rate of group 0 to 9.9279e-06.
Epoch: 55 	Ltrain: 0.004985 	Lval: 0.005204
Epoch: 60 	Ltrain: 0.004827 	Lval: 0.005203
Epoch 00064: reducing learning rate of group 0 to 9.9279e-07.
Epoch: 65 	Ltrain: 0.004982 	Lval: 0.005200
Epoch: 70 	Ltrain: 0.004864 	Lval: 0.005201
Epoch: 75 	Ltrain: 0.004879 	Lval: 0.005200
Epoch 00076: reducing learning rate of group 0 to 9.9279e-08.
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.005199


	Fold 3/5
Epoch: 1 	Ltrain: 0.100357 	Lval: 0.022457
Epoch: 5 	Ltrain: 0.007151 	Lval: 0.008370
Epoch: 10 	Ltrain: 0.006421 	Lval: 0.006933
Epoch 00015: reducing learning rate of group 0 to 9.9279e-04.
Epoch: 15 	Ltrain: 0.007294 	Lval: 0.007280
Epoch: 20 	Ltrain: 0.005411 	Lval: 0.006076
Epoch: 25 	Ltrain: 0.005324 	Lval: 0.006045
Epoch: 30 	Ltrain: 0.005203 	Lval: 0.005951
Epoch: 35 	Ltrain: 0.005142 	Lval: 0.005852
Epoch: 40 	Ltrain: 0.005067 	Lval: 0.005792
Epoch: 45 	Ltrain: 0.005058 	Lval: 0.005687
Epoch: 50 	Ltrain: 0.005021 	Lval: 0.005589
Epoch: 55 	Ltrain: 0.004906 	Lval: 0.005530
Epoch: 60 	Ltrain: 0.004811 	Lval: 0.005386
Epoch: 65 	Ltrain: 0.004628 	Lval: 0.005267
Epoch: 70 	Ltrain: 0.004499 	Lval: 0.005231
Epoch: 75 	Ltrain: 0.004437 	Lval: 0.005267
Epoch: 80 	Ltrain: 0.004269 	Lval: 0.004755
Epoch: 85 	Ltrain: 0.004142 	Lval: 0.004478
Epoch: 90 	Ltrain: 0.004039 	Lval: 0.004272
Epoch 00095: reducing learning rate of group 0 to 9.9279e-05.
Epoch: 95 	Ltrain: 0.003935 	Lval: 0.004280
Epoch: 100 	Ltrain: 0.003584 	Lval: 0.003962
Epoch: 105 	Ltrain: 0.003525 	Lval: 0.003937
Epoch: 110 	Ltrain: 0.003498 	Lval: 0.003882
Epoch: 115 	Ltrain: 0.003486 	Lval: 0.003835
Epoch: 120 	Ltrain: 0.003442 	Lval: 0.003794
Epoch: 125 	Ltrain: 0.003394 	Lval: 0.003778
Epoch: 130 	Ltrain: 0.003364 	Lval: 0.003724
Epoch: 135 	Ltrain: 0.003350 	Lval: 0.003719
Epoch: 140 	Ltrain: 0.003296 	Lval: 0.003640
Epoch: 145 	Ltrain: 0.003285 	Lval: 0.003619
Epoch: 150 	Ltrain: 0.003240 	Lval: 0.003540
Epoch: 155 	Ltrain: 0.003177 	Lval: 0.003498
Epoch: 160 	Ltrain: 0.003166 	Lval: 0.003451
Epoch: 165 	Ltrain: 0.003132 	Lval: 0.003437
Epoch: 170 	Ltrain: 0.003090 	Lval: 0.003357
Epoch: 175 	Ltrain: 0.003032 	Lval: 0.003298
Epoch: 180 	Ltrain: 0.002992 	Lval: 0.003239
Epoch: 185 	Ltrain: 0.002972 	Lval: 0.003196
Epoch: 190 	Ltrain: 0.002894 	Lval: 0.003104
Epoch: 195 	Ltrain: 0.002856 	Lval: 0.003074
Epoch: 200 	Ltrain: 0.002804 	Lval: 0.003013
Epoch: 205 	Ltrain: 0.002756 	Lval: 0.002945
Epoch: 210 	Ltrain: 0.002723 	Lval: 0.002885
Epoch: 215 	Ltrain: 0.002689 	Lval: 0.002828
Epoch: 220 	Ltrain: 0.002633 	Lval: 0.002775
Epoch: 225 	Ltrain: 0.002583 	Lval: 0.002729
Epoch: 230 	Ltrain: 0.002543 	Lval: 0.002669
Epoch: 235 	Ltrain: 0.002510 	Lval: 0.002658
Epoch: 240 	Ltrain: 0.002452 	Lval: 0.002583
Epoch 00243: reducing learning rate of group 0 to 9.9279e-06.
Epoch: 245 	Ltrain: 0.002363 	Lval: 0.002508
Epoch: 250 	Ltrain: 0.002345 	Lval: 0.002501
Epoch: 255 	Ltrain: 0.002357 	Lval: 0.002495
Epoch: 260 	Ltrain: 0.002340 	Lval: 0.002489
Epoch: 265 	Ltrain: 0.002335 	Lval: 0.002483
Epoch: 270 	Ltrain: 0.002323 	Lval: 0.002477
Epoch: 275 	Ltrain: 0.002327 	Lval: 0.002472
Epoch: 280 	Ltrain: 0.002338 	Lval: 0.002465
Epoch: 285 	Ltrain: 0.002311 	Lval: 0.002461
Epoch: 290 	Ltrain: 0.002325 	Lval: 0.002459
Epoch: 295 	Ltrain: 0.002308 	Lval: 0.002448
Epoch: 300 	Ltrain: 0.002292 	Lval: 0.002445
Epoch: 305 	Ltrain: 0.002286 	Lval: 0.002438
Epoch: 310 	Ltrain: 0.002297 	Lval: 0.002432
Epoch: 315 	Ltrain: 0.002289 	Lval: 0.002424
Epoch: 320 	Ltrain: 0.002288 	Lval: 0.002421
Epoch: 325 	Ltrain: 0.002284 	Lval: 0.002413
Epoch: 330 	Ltrain: 0.002280 	Lval: 0.002407
Epoch: 335 	Ltrain: 0.002261 	Lval: 0.002403
Epoch: 340 	Ltrain: 0.002264 	Lval: 0.002396
Epoch: 345 	Ltrain: 0.002253 	Lval: 0.002389
Epoch: 350 	Ltrain: 0.002237 	Lval: 0.002383
Epoch: 355 	Ltrain: 0.002237 	Lval: 0.002378
Epoch: 360 	Ltrain: 0.002225 	Lval: 0.002372
Epoch: 365 	Ltrain: 0.002227 	Lval: 0.002364
Epoch: 370 	Ltrain: 0.002240 	Lval: 0.002358
Epoch: 375 	Ltrain: 0.002203 	Lval: 0.002353
Epoch: 380 	Ltrain: 0.002224 	Lval: 0.002346
Epoch: 385 	Ltrain: 0.002196 	Lval: 0.002344
Epoch: 390 	Ltrain: 0.002203 	Lval: 0.002333
Epoch: 395 	Ltrain: 0.002191 	Lval: 0.002327
Epoch: 400 	Ltrain: 0.002190 	Lval: 0.002324
Epoch: 405 	Ltrain: 0.002179 	Lval: 0.002316
Epoch: 410 	Ltrain: 0.002176 	Lval: 0.002310
Epoch: 415 	Ltrain: 0.002158 	Lval: 0.002306
Epoch: 420 	Ltrain: 0.002160 	Lval: 0.002297
Epoch: 425 	Ltrain: 0.002161 	Lval: 0.002290
Epoch: 430 	Ltrain: 0.002138 	Lval: 0.002285
Epoch: 435 	Ltrain: 0.002143 	Lval: 0.002280
Epoch: 440 	Ltrain: 0.002139 	Lval: 0.002272
Epoch: 445 	Ltrain: 0.002129 	Lval: 0.002270
Epoch 00448: reducing learning rate of group 0 to 9.9279e-07.
Epoch: 450 	Ltrain: 0.002110 	Lval: 0.002263
Epoch: 455 	Ltrain: 0.002110 	Lval: 0.002261
Epoch: 460 	Ltrain: 0.002138 	Lval: 0.002261
Epoch: 465 	Ltrain: 0.002115 	Lval: 0.002260
Epoch 00470: reducing learning rate of group 0 to 9.9279e-08.
Epoch: 470 	Ltrain: 0.002119 	Lval: 0.002260
EarlyStopper: stopping at epoch 473 with best_val_loss = 0.002267


	Fold 4/5
Epoch: 1 	Ltrain: 0.048556 	Lval: 0.011172
Epoch: 5 	Ltrain: 0.006125 	Lval: 0.008906
Epoch: 10 	Ltrain: 0.005393 	Lval: 0.007157
Epoch: 15 	Ltrain: 0.005279 	Lval: 0.005967
Epoch: 20 	Ltrain: 0.004844 	Lval: 0.006149
Epoch: 25 	Ltrain: 0.004741 	Lval: 0.005101
Epoch: 30 	Ltrain: 0.003912 	Lval: 0.004598
Epoch: 35 	Ltrain: 0.003635 	Lval: 0.003970
Epoch 00036: reducing learning rate of group 0 to 9.9279e-04.
Epoch: 40 	Ltrain: 0.002401 	Lval: 0.002756
Epoch: 45 	Ltrain: 0.002182 	Lval: 0.002478
Epoch: 50 	Ltrain: 0.002006 	Lval: 0.002244
Epoch: 55 	Ltrain: 0.001820 	Lval: 0.001995
Epoch: 60 	Ltrain: 0.001630 	Lval: 0.001828
Epoch: 65 	Ltrain: 0.001455 	Lval: 0.001600
Epoch: 70 	Ltrain: 0.001355 	Lval: 0.001492
Epoch: 75 	Ltrain: 0.001196 	Lval: 0.001322
Epoch: 80 	Ltrain: 0.001108 	Lval: 0.001146
Epoch: 85 	Ltrain: 0.001000 	Lval: 0.001058
Epoch: 90 	Ltrain: 0.000896 	Lval: 0.000926
Epoch: 95 	Ltrain: 0.000945 	Lval: 0.000975
Epoch: 100 	Ltrain: 0.000799 	Lval: 0.000855
Epoch: 105 	Ltrain: 0.000747 	Lval: 0.000736
Epoch 00110: reducing learning rate of group 0 to 9.9279e-05.
Epoch: 110 	Ltrain: 0.000703 	Lval: 0.000722
Epoch: 115 	Ltrain: 0.000541 	Lval: 0.000543
Epoch: 120 	Ltrain: 0.000520 	Lval: 0.000522
Epoch: 125 	Ltrain: 0.000507 	Lval: 0.000510
Epoch: 130 	Ltrain: 0.000499 	Lval: 0.000499
Epoch: 135 	Ltrain: 0.000489 	Lval: 0.000487
Epoch: 140 	Ltrain: 0.000479 	Lval: 0.000477
Epoch: 145 	Ltrain: 0.000470 	Lval: 0.000466
Epoch: 150 	Ltrain: 0.000460 	Lval: 0.000455
Epoch: 155 	Ltrain: 0.000451 	Lval: 0.000444
Epoch: 160 	Ltrain: 0.000440 	Lval: 0.000432
Epoch: 165 	Ltrain: 0.000429 	Lval: 0.000420
Epoch: 170 	Ltrain: 0.000418 	Lval: 0.000408
Epoch: 175 	Ltrain: 0.000407 	Lval: 0.000395
Epoch: 180 	Ltrain: 0.000396 	Lval: 0.000383
Epoch: 185 	Ltrain: 0.000383 	Lval: 0.000370
Epoch: 190 	Ltrain: 0.000372 	Lval: 0.000358
Epoch: 195 	Ltrain: 0.000360 	Lval: 0.000345
Epoch: 200 	Ltrain: 0.000349 	Lval: 0.000332
Epoch: 205 	Ltrain: 0.000335 	Lval: 0.000320
Epoch: 210 	Ltrain: 0.000324 	Lval: 0.000308
Epoch: 215 	Ltrain: 0.000312 	Lval: 0.000296
Epoch: 220 	Ltrain: 0.000300 	Lval: 0.000284
Epoch: 225 	Ltrain: 0.000291 	Lval: 0.000278
Epoch: 230 	Ltrain: 0.000288 	Lval: 0.000273
Epoch: 235 	Ltrain: 0.000273 	Lval: 0.000255
Epoch: 240 	Ltrain: 0.000265 	Lval: 0.000251
Epoch: 245 	Ltrain: 0.000255 	Lval: 0.000240
Epoch 00250: reducing learning rate of group 0 to 9.9279e-06.
Epoch: 250 	Ltrain: 0.000249 	Lval: 0.000237
Epoch: 255 	Ltrain: 0.000230 	Lval: 0.000217
Epoch: 260 	Ltrain: 0.000228 	Lval: 0.000215
Epoch: 265 	Ltrain: 0.000229 	Lval: 0.000214
Epoch: 270 	Ltrain: 0.000226 	Lval: 0.000212
Epoch: 275 	Ltrain: 0.000224 	Lval: 0.000211
Epoch: 280 	Ltrain: 0.000225 	Lval: 0.000210
Epoch: 285 	Ltrain: 0.000223 	Lval: 0.000209
Epoch: 290 	Ltrain: 0.000221 	Lval: 0.000207
Epoch: 295 	Ltrain: 0.000222 	Lval: 0.000206
EarlyStopper: stopping at epoch 298 with best_val_loss = 0.000213


	Fold 5/5
Epoch: 1 	Ltrain: 0.126532 	Lval: 0.021793
Epoch: 5 	Ltrain: 0.006654 	Lval: 0.007605
Epoch: 10 	Ltrain: 0.005658 	Lval: 0.007266
Epoch 00012: reducing learning rate of group 0 to 9.9279e-04.
Epoch: 15 	Ltrain: 0.005121 	Lval: 0.006806
Epoch: 20 	Ltrain: 0.005078 	Lval: 0.006667
Epoch: 25 	Ltrain: 0.005121 	Lval: 0.006877
Epoch 00026: reducing learning rate of group 0 to 9.9279e-05.
Epoch: 30 	Ltrain: 0.005007 	Lval: 0.006572
Epoch: 35 	Ltrain: 0.005009 	Lval: 0.006540
Epoch 00038: reducing learning rate of group 0 to 9.9279e-06.
Epoch: 40 	Ltrain: 0.004984 	Lval: 0.006590
Epoch: 45 	Ltrain: 0.004976 	Lval: 0.006570
Epoch 00050: reducing learning rate of group 0 to 9.9279e-07.
Epoch: 50 	Ltrain: 0.004988 	Lval: 0.006568
EarlyStopper: stopping at epoch 51 with best_val_loss = 0.006523

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007344618313746603
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.393679091139585e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 28
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.094948 	Lval: 0.018447
Epoch: 5 	Ltrain: 0.010742 	Lval: 0.008887
Epoch: 10 	Ltrain: 0.008913 	Lval: 0.007604
Epoch: 15 	Ltrain: 0.007472 	Lval: 0.007110
Epoch: 20 	Ltrain: 0.006476 	Lval: 0.006645
Epoch 00023: reducing learning rate of group 0 to 7.3446e-04.
Epoch: 25 	Ltrain: 0.005983 	Lval: 0.006000
Epoch: 30 	Ltrain: 0.005660 	Lval: 0.005823
Epoch: 35 	Ltrain: 0.005674 	Lval: 0.005672
Epoch: 40 	Ltrain: 0.005317 	Lval: 0.005509
Epoch: 45 	Ltrain: 0.005491 	Lval: 0.005334
Epoch: 50 	Ltrain: 0.005279 	Lval: 0.005212
Epoch: 55 	Ltrain: 0.005147 	Lval: 0.005016
Epoch: 60 	Ltrain: 0.005254 	Lval: 0.004878
Epoch: 65 	Ltrain: 0.004827 	Lval: 0.004734
Epoch 00070: reducing learning rate of group 0 to 7.3446e-05.
Epoch: 70 	Ltrain: 0.004755 	Lval: 0.004683
Epoch: 75 	Ltrain: 0.004325 	Lval: 0.004488
Epoch: 80 	Ltrain: 0.004494 	Lval: 0.004499
Epoch: 85 	Ltrain: 0.004274 	Lval: 0.004444
Epoch: 90 	Ltrain: 0.004360 	Lval: 0.004439
Epoch: 95 	Ltrain: 0.004350 	Lval: 0.004385
Epoch: 100 	Ltrain: 0.004225 	Lval: 0.004385
Epoch: 105 	Ltrain: 0.004406 	Lval: 0.004359
Epoch: 110 	Ltrain: 0.004179 	Lval: 0.004319
Epoch: 115 	Ltrain: 0.004245 	Lval: 0.004307
Epoch: 120 	Ltrain: 0.004313 	Lval: 0.004286
Epoch: 125 	Ltrain: 0.004160 	Lval: 0.004256
Epoch: 130 	Ltrain: 0.004011 	Lval: 0.004242
Epoch: 135 	Ltrain: 0.004083 	Lval: 0.004210
Epoch: 140 	Ltrain: 0.004035 	Lval: 0.004170
Epoch: 145 	Ltrain: 0.004201 	Lval: 0.004150
Epoch: 150 	Ltrain: 0.003983 	Lval: 0.004152
Epoch: 155 	Ltrain: 0.004100 	Lval: 0.004123
Epoch: 160 	Ltrain: 0.004238 	Lval: 0.004080
Epoch: 165 	Ltrain: 0.004060 	Lval: 0.004058
Epoch: 170 	Ltrain: 0.003999 	Lval: 0.004045
Epoch: 175 	Ltrain: 0.003949 	Lval: 0.004024
Epoch: 180 	Ltrain: 0.004064 	Lval: 0.003990
Epoch: 185 	Ltrain: 0.004031 	Lval: 0.003955
Epoch: 190 	Ltrain: 0.003925 	Lval: 0.003940
Epoch: 195 	Ltrain: 0.004146 	Lval: 0.003917
Epoch: 200 	Ltrain: 0.003826 	Lval: 0.003888
Epoch: 205 	Ltrain: 0.003691 	Lval: 0.003842
Epoch: 210 	Ltrain: 0.003994 	Lval: 0.003843
Epoch: 215 	Ltrain: 0.003790 	Lval: 0.003805
Epoch: 220 	Ltrain: 0.003669 	Lval: 0.003766
Epoch: 225 	Ltrain: 0.003675 	Lval: 0.003727
Epoch: 230 	Ltrain: 0.003765 	Lval: 0.003758
Epoch: 235 	Ltrain: 0.004210 	Lval: 0.003677
Epoch 00238: reducing learning rate of group 0 to 7.3446e-06.
Epoch: 240 	Ltrain: 0.003584 	Lval: 0.003686
Epoch: 245 	Ltrain: 0.004013 	Lval: 0.003676
Epoch: 250 	Ltrain: 0.003478 	Lval: 0.003670
Epoch: 255 	Ltrain: 0.003514 	Lval: 0.003658
Epoch: 260 	Ltrain: 0.003768 	Lval: 0.003653
Epoch: 265 	Ltrain: 0.003532 	Lval: 0.003643
Epoch: 270 	Ltrain: 0.003679 	Lval: 0.003635
Epoch: 275 	Ltrain: 0.003544 	Lval: 0.003630
Epoch: 280 	Ltrain: 0.003898 	Lval: 0.003629
Epoch 00281: reducing learning rate of group 0 to 7.3446e-07.
Epoch: 285 	Ltrain: 0.003595 	Lval: 0.003632
Epoch: 290 	Ltrain: 0.003555 	Lval: 0.003631
Epoch 00293: reducing learning rate of group 0 to 7.3446e-08.
Epoch: 295 	Ltrain: 0.003539 	Lval: 0.003630
Epoch: 300 	Ltrain: 0.003486 	Lval: 0.003630
EarlyStopper: stopping at epoch 299 with best_val_loss = 0.003634


	Fold 2/5
Epoch: 1 	Ltrain: 0.070315 	Lval: 0.020202
Epoch: 5 	Ltrain: 0.008196 	Lval: 0.007764
Epoch: 10 	Ltrain: 0.006780 	Lval: 0.007781
Epoch: 15 	Ltrain: 0.006443 	Lval: 0.006734
Epoch: 20 	Ltrain: 0.006289 	Lval: 0.005877
Epoch: 25 	Ltrain: 0.005657 	Lval: 0.005712
Epoch: 30 	Ltrain: 0.005496 	Lval: 0.005476
Epoch: 35 	Ltrain: 0.004973 	Lval: 0.004900
Epoch: 40 	Ltrain: 0.004358 	Lval: 0.004048
Epoch: 45 	Ltrain: 0.003962 	Lval: 0.003686
Epoch: 50 	Ltrain: 0.003711 	Lval: 0.003243
Epoch: 55 	Ltrain: 0.003002 	Lval: 0.003177
Epoch: 60 	Ltrain: 0.002650 	Lval: 0.002652
Epoch 00063: reducing learning rate of group 0 to 7.3446e-04.
Epoch: 65 	Ltrain: 0.001845 	Lval: 0.001720
Epoch: 70 	Ltrain: 0.001435 	Lval: 0.001440
Epoch: 75 	Ltrain: 0.001292 	Lval: 0.001312
Epoch: 80 	Ltrain: 0.001202 	Lval: 0.001212
Epoch: 85 	Ltrain: 0.001110 	Lval: 0.001116
Epoch: 90 	Ltrain: 0.001016 	Lval: 0.001024
Epoch: 95 	Ltrain: 0.000938 	Lval: 0.000941
Epoch: 100 	Ltrain: 0.000868 	Lval: 0.000866
Epoch: 105 	Ltrain: 0.000783 	Lval: 0.000783
Epoch: 110 	Ltrain: 0.000716 	Lval: 0.000712
Epoch: 115 	Ltrain: 0.000671 	Lval: 0.000652
Epoch: 120 	Ltrain: 0.000607 	Lval: 0.000586
Epoch: 125 	Ltrain: 0.000542 	Lval: 0.000535
Epoch: 130 	Ltrain: 0.000507 	Lval: 0.000484
Epoch: 135 	Ltrain: 0.000455 	Lval: 0.000451
Epoch: 140 	Ltrain: 0.000444 	Lval: 0.000423
Epoch: 145 	Ltrain: 0.000376 	Lval: 0.000363
Epoch 00149: reducing learning rate of group 0 to 7.3446e-05.
Epoch: 150 	Ltrain: 0.000369 	Lval: 0.000343
Epoch: 155 	Ltrain: 0.000329 	Lval: 0.000318
Epoch: 160 	Ltrain: 0.000314 	Lval: 0.000313
Epoch: 165 	Ltrain: 0.000309 	Lval: 0.000308
Epoch: 170 	Ltrain: 0.000304 	Lval: 0.000304
Epoch: 175 	Ltrain: 0.000302 	Lval: 0.000300
Epoch: 180 	Ltrain: 0.000298 	Lval: 0.000296
Epoch: 185 	Ltrain: 0.000297 	Lval: 0.000292
Epoch: 190 	Ltrain: 0.000288 	Lval: 0.000288
Epoch: 195 	Ltrain: 0.000294 	Lval: 0.000284
Epoch: 200 	Ltrain: 0.000286 	Lval: 0.000280
Epoch: 205 	Ltrain: 0.000276 	Lval: 0.000276
Epoch: 210 	Ltrain: 0.000272 	Lval: 0.000271
Epoch: 215 	Ltrain: 0.000269 	Lval: 0.000267
Epoch: 220 	Ltrain: 0.000267 	Lval: 0.000263
Epoch: 225 	Ltrain: 0.000262 	Lval: 0.000258
Epoch: 230 	Ltrain: 0.000262 	Lval: 0.000254
Epoch: 235 	Ltrain: 0.000250 	Lval: 0.000249
Epoch: 240 	Ltrain: 0.000247 	Lval: 0.000245
Epoch: 245 	Ltrain: 0.000240 	Lval: 0.000240
Epoch: 250 	Ltrain: 0.000241 	Lval: 0.000236
Epoch: 255 	Ltrain: 0.000233 	Lval: 0.000231
Epoch: 260 	Ltrain: 0.000228 	Lval: 0.000226
Epoch: 265 	Ltrain: 0.000228 	Lval: 0.000222
Epoch: 270 	Ltrain: 0.000226 	Lval: 0.000217
Epoch: 275 	Ltrain: 0.000213 	Lval: 0.000212
Epoch: 280 	Ltrain: 0.000209 	Lval: 0.000208
Epoch: 285 	Ltrain: 0.000202 	Lval: 0.000203
Epoch: 290 	Ltrain: 0.000199 	Lval: 0.000199
Epoch: 295 	Ltrain: 0.000194 	Lval: 0.000194
Epoch: 300 	Ltrain: 0.000189 	Lval: 0.000189
Epoch: 305 	Ltrain: 0.000187 	Lval: 0.000185
Epoch: 310 	Ltrain: 0.000182 	Lval: 0.000180
Epoch: 315 	Ltrain: 0.000177 	Lval: 0.000176
Epoch: 320 	Ltrain: 0.000173 	Lval: 0.000171
Epoch: 325 	Ltrain: 0.000169 	Lval: 0.000167
Epoch: 330 	Ltrain: 0.000164 	Lval: 0.000162
Epoch: 335 	Ltrain: 0.000157 	Lval: 0.000157
Epoch: 340 	Ltrain: 0.000155 	Lval: 0.000153
Epoch: 345 	Ltrain: 0.000152 	Lval: 0.000149
Epoch: 350 	Ltrain: 0.000148 	Lval: 0.000145
Epoch: 355 	Ltrain: 0.000143 	Lval: 0.000141
Epoch: 360 	Ltrain: 0.000142 	Lval: 0.000138
Epoch: 365 	Ltrain: 0.000141 	Lval: 0.000134
Epoch: 370 	Ltrain: 0.000131 	Lval: 0.000129
Epoch: 375 	Ltrain: 0.000129 	Lval: 0.000126
Epoch: 380 	Ltrain: 0.000124 	Lval: 0.000122
Epoch: 385 	Ltrain: 0.000121 	Lval: 0.000118
Epoch: 390 	Ltrain: 0.000116 	Lval: 0.000114
Epoch: 395 	Ltrain: 0.000113 	Lval: 0.000111
Epoch: 400 	Ltrain: 0.000112 	Lval: 0.000108
Epoch: 405 	Ltrain: 0.000109 	Lval: 0.000105
Epoch: 410 	Ltrain: 0.000107 	Lval: 0.000103
Epoch: 415 	Ltrain: 0.000113 	Lval: 0.000105
Epoch 00416: reducing learning rate of group 0 to 7.3446e-06.
Epoch: 420 	Ltrain: 0.000100 	Lval: 0.000096
Epoch: 425 	Ltrain: 0.000102 	Lval: 0.000096
Epoch: 430 	Ltrain: 0.000098 	Lval: 0.000095
Epoch: 435 	Ltrain: 0.000097 	Lval: 0.000095
Epoch: 440 	Ltrain: 0.000096 	Lval: 0.000095
Epoch: 445 	Ltrain: 0.000099 	Lval: 0.000094
EarlyStopper: stopping at epoch 445 with best_val_loss = 0.000097


	Fold 3/5
Epoch: 1 	Ltrain: 0.058093 	Lval: 0.015012
Epoch: 5 	Ltrain: 0.006859 	Lval: 0.007661
Epoch: 10 	Ltrain: 0.006113 	Lval: 0.006724
Epoch 00014: reducing learning rate of group 0 to 7.3446e-04.
Epoch: 15 	Ltrain: 0.005661 	Lval: 0.006072
Epoch: 20 	Ltrain: 0.005282 	Lval: 0.005905
Epoch: 25 	Ltrain: 0.005183 	Lval: 0.005901
Epoch: 30 	Ltrain: 0.005200 	Lval: 0.005710
Epoch: 35 	Ltrain: 0.005076 	Lval: 0.005658
Epoch: 40 	Ltrain: 0.005072 	Lval: 0.005510
Epoch: 45 	Ltrain: 0.004834 	Lval: 0.005307
Epoch: 50 	Ltrain: 0.004714 	Lval: 0.005435
Epoch: 55 	Ltrain: 0.004499 	Lval: 0.005084
Epoch: 60 	Ltrain: 0.004394 	Lval: 0.004707
Epoch: 65 	Ltrain: 0.004151 	Lval: 0.004521
Epoch: 70 	Ltrain: 0.004017 	Lval: 0.004434
Epoch: 75 	Ltrain: 0.003949 	Lval: 0.004275
Epoch: 80 	Ltrain: 0.003724 	Lval: 0.003886
Epoch: 85 	Ltrain: 0.003694 	Lval: 0.003819
Epoch: 90 	Ltrain: 0.003382 	Lval: 0.003530
Epoch 00094: reducing learning rate of group 0 to 7.3446e-05.
Epoch: 95 	Ltrain: 0.003045 	Lval: 0.003214
Epoch: 100 	Ltrain: 0.002861 	Lval: 0.003128
Epoch: 105 	Ltrain: 0.002819 	Lval: 0.003086
Epoch: 110 	Ltrain: 0.002790 	Lval: 0.003014
Epoch: 115 	Ltrain: 0.002754 	Lval: 0.002969
Epoch: 120 	Ltrain: 0.002699 	Lval: 0.002936
Epoch: 125 	Ltrain: 0.002684 	Lval: 0.002897
Epoch: 130 	Ltrain: 0.002650 	Lval: 0.002833
Epoch: 135 	Ltrain: 0.002610 	Lval: 0.002792
Epoch: 140 	Ltrain: 0.002580 	Lval: 0.002738
Epoch: 145 	Ltrain: 0.002542 	Lval: 0.002685
Epoch: 150 	Ltrain: 0.002488 	Lval: 0.002675
Epoch: 155 	Ltrain: 0.002472 	Lval: 0.002609
Epoch: 160 	Ltrain: 0.002423 	Lval: 0.002547
Epoch: 165 	Ltrain: 0.002404 	Lval: 0.002516
Epoch: 170 	Ltrain: 0.002348 	Lval: 0.002494
Epoch: 175 	Ltrain: 0.002319 	Lval: 0.002413
Epoch: 180 	Ltrain: 0.002267 	Lval: 0.002383
Epoch: 185 	Ltrain: 0.002239 	Lval: 0.002327
Epoch: 190 	Ltrain: 0.002194 	Lval: 0.002272
Epoch: 195 	Ltrain: 0.002145 	Lval: 0.002223
Epoch: 200 	Ltrain: 0.002112 	Lval: 0.002193
Epoch: 205 	Ltrain: 0.002074 	Lval: 0.002129
Epoch: 210 	Ltrain: 0.002047 	Lval: 0.002118
Epoch: 215 	Ltrain: 0.002006 	Lval: 0.002052
Epoch: 220 	Ltrain: 0.001955 	Lval: 0.002002
Epoch: 225 	Ltrain: 0.001905 	Lval: 0.001951
Epoch: 230 	Ltrain: 0.001863 	Lval: 0.001921
Epoch: 235 	Ltrain: 0.001851 	Lval: 0.001861
Epoch: 240 	Ltrain: 0.001802 	Lval: 0.001843
Epoch: 245 	Ltrain: 0.001758 	Lval: 0.001787
Epoch: 250 	Ltrain: 0.001738 	Lval: 0.001743
Epoch: 255 	Ltrain: 0.001684 	Lval: 0.001704
Epoch 00260: reducing learning rate of group 0 to 7.3446e-06.
Epoch: 260 	Ltrain: 0.001647 	Lval: 0.001678
Epoch: 265 	Ltrain: 0.001594 	Lval: 0.001619
Epoch: 270 	Ltrain: 0.001591 	Lval: 0.001613
Epoch: 275 	Ltrain: 0.001582 	Lval: 0.001609
Epoch: 280 	Ltrain: 0.001572 	Lval: 0.001603
Epoch: 285 	Ltrain: 0.001576 	Lval: 0.001599
Epoch: 290 	Ltrain: 0.001575 	Lval: 0.001594
Epoch: 295 	Ltrain: 0.001560 	Lval: 0.001588
Epoch: 300 	Ltrain: 0.001559 	Lval: 0.001585
Epoch: 305 	Ltrain: 0.001560 	Lval: 0.001579
Epoch: 310 	Ltrain: 0.001549 	Lval: 0.001575
Epoch: 315 	Ltrain: 0.001551 	Lval: 0.001570
Epoch: 320 	Ltrain: 0.001536 	Lval: 0.001564
Epoch: 325 	Ltrain: 0.001539 	Lval: 0.001558
Epoch: 330 	Ltrain: 0.001526 	Lval: 0.001556
Epoch: 335 	Ltrain: 0.001531 	Lval: 0.001549
Epoch: 340 	Ltrain: 0.001525 	Lval: 0.001542
Epoch: 345 	Ltrain: 0.001518 	Lval: 0.001538
Epoch: 350 	Ltrain: 0.001525 	Lval: 0.001532
Epoch: 355 	Ltrain: 0.001512 	Lval: 0.001529
Epoch: 360 	Ltrain: 0.001504 	Lval: 0.001523
Epoch: 365 	Ltrain: 0.001499 	Lval: 0.001516
Epoch: 370 	Ltrain: 0.001494 	Lval: 0.001513
Epoch: 375 	Ltrain: 0.001493 	Lval: 0.001506
Epoch: 380 	Ltrain: 0.001493 	Lval: 0.001500
Epoch: 385 	Ltrain: 0.001489 	Lval: 0.001495
Epoch: 390 	Ltrain: 0.001479 	Lval: 0.001491
Epoch: 395 	Ltrain: 0.001485 	Lval: 0.001485
Epoch: 400 	Ltrain: 0.001468 	Lval: 0.001481
Epoch: 405 	Ltrain: 0.001464 	Lval: 0.001475
Epoch: 410 	Ltrain: 0.001464 	Lval: 0.001470
Epoch: 415 	Ltrain: 0.001459 	Lval: 0.001465
Epoch: 420 	Ltrain: 0.001454 	Lval: 0.001459
Epoch: 425 	Ltrain: 0.001444 	Lval: 0.001454
Epoch: 430 	Ltrain: 0.001441 	Lval: 0.001449
Epoch: 435 	Ltrain: 0.001443 	Lval: 0.001444
Epoch: 440 	Ltrain: 0.001434 	Lval: 0.001438
Epoch: 445 	Ltrain: 0.001425 	Lval: 0.001434
Epoch: 450 	Ltrain: 0.001430 	Lval: 0.001428
Epoch: 455 	Ltrain: 0.001420 	Lval: 0.001423
Epoch: 460 	Ltrain: 0.001413 	Lval: 0.001418
Epoch: 465 	Ltrain: 0.001409 	Lval: 0.001413
Epoch: 470 	Ltrain: 0.001410 	Lval: 0.001408
Epoch: 475 	Ltrain: 0.001397 	Lval: 0.001403
Epoch: 480 	Ltrain: 0.001402 	Lval: 0.001398
Epoch: 485 	Ltrain: 0.001390 	Lval: 0.001393
Epoch: 490 	Ltrain: 0.001390 	Lval: 0.001388
Epoch: 495 	Ltrain: 0.001387 	Lval: 0.001383
Epoch: 500 	Ltrain: 0.001383 	Lval: 0.001377
Epoch: 505 	Ltrain: 0.001375 	Lval: 0.001372
Epoch: 510 	Ltrain: 0.001371 	Lval: 0.001368
Epoch: 515 	Ltrain: 0.001367 	Lval: 0.001362
Epoch: 520 	Ltrain: 0.001359 	Lval: 0.001359
Epoch: 525 	Ltrain: 0.001354 	Lval: 0.001353
Epoch: 530 	Ltrain: 0.001353 	Lval: 0.001349
Epoch: 535 	Ltrain: 0.001346 	Lval: 0.001344
Epoch: 540 	Ltrain: 0.001343 	Lval: 0.001339
Epoch: 545 	Ltrain: 0.001343 	Lval: 0.001334
Epoch: 550 	Ltrain: 0.001333 	Lval: 0.001333
Epoch: 555 	Ltrain: 0.001326 	Lval: 0.001323
Epoch: 560 	Ltrain: 0.001321 	Lval: 0.001320
Epoch: 565 	Ltrain: 0.001320 	Lval: 0.001314
Epoch: 570 	Ltrain: 0.001319 	Lval: 0.001309
Epoch: 575 	Ltrain: 0.001312 	Lval: 0.001306
Epoch: 580 	Ltrain: 0.001307 	Lval: 0.001301
Epoch: 585 	Ltrain: 0.001299 	Lval: 0.001296
Epoch: 590 	Ltrain: 0.001307 	Lval: 0.001291
Epoch: 595 	Ltrain: 0.001293 	Lval: 0.001287
Epoch: 600 	Ltrain: 0.001289 	Lval: 0.001282
Epoch: 605 	Ltrain: 0.001286 	Lval: 0.001278
Epoch: 610 	Ltrain: 0.001279 	Lval: 0.001276
Epoch: 615 	Ltrain: 0.001282 	Lval: 0.001269
Epoch: 620 	Ltrain: 0.001274 	Lval: 0.001265
Epoch: 625 	Ltrain: 0.001268 	Lval: 0.001259
Epoch: 630 	Ltrain: 0.001263 	Lval: 0.001254
Epoch: 635 	Ltrain: 0.001272 	Lval: 0.001249
Epoch: 640 	Ltrain: 0.001258 	Lval: 0.001246
Epoch: 645 	Ltrain: 0.001259 	Lval: 0.001239
Epoch: 650 	Ltrain: 0.001245 	Lval: 0.001237
Epoch: 655 	Ltrain: 0.001243 	Lval: 0.001231
Epoch: 660 	Ltrain: 0.001246 	Lval: 0.001227
Epoch: 665 	Ltrain: 0.001237 	Lval: 0.001223
Epoch: 670 	Ltrain: 0.001230 	Lval: 0.001219
Epoch: 675 	Ltrain: 0.001226 	Lval: 0.001215
Epoch: 680 	Ltrain: 0.001236 	Lval: 0.001209
Epoch: 685 	Ltrain: 0.001222 	Lval: 0.001205
Epoch: 690 	Ltrain: 0.001213 	Lval: 0.001200
Epoch: 695 	Ltrain: 0.001217 	Lval: 0.001196
Epoch: 700 	Ltrain: 0.001209 	Lval: 0.001192
Epoch: 705 	Ltrain: 0.001208 	Lval: 0.001187
Epoch: 710 	Ltrain: 0.001200 	Lval: 0.001185
Epoch: 715 	Ltrain: 0.001200 	Lval: 0.001180
Epoch: 720 	Ltrain: 0.001185 	Lval: 0.001177
Epoch: 725 	Ltrain: 0.001183 	Lval: 0.001172
Epoch: 730 	Ltrain: 0.001177 	Lval: 0.001167
Epoch: 735 	Ltrain: 0.001177 	Lval: 0.001163
Epoch: 740 	Ltrain: 0.001179 	Lval: 0.001159
Epoch: 745 	Ltrain: 0.001171 	Lval: 0.001155
Epoch: 750 	Ltrain: 0.001169 	Lval: 0.001150
Epoch: 755 	Ltrain: 0.001161 	Lval: 0.001145
Epoch: 760 	Ltrain: 0.001156 	Lval: 0.001141
Epoch: 765 	Ltrain: 0.001158 	Lval: 0.001139
Epoch: 770 	Ltrain: 0.001144 	Lval: 0.001134
Epoch: 775 	Ltrain: 0.001148 	Lval: 0.001131
Epoch: 780 	Ltrain: 0.001147 	Lval: 0.001125
Epoch: 785 	Ltrain: 0.001133 	Lval: 0.001120
Epoch: 790 	Ltrain: 0.001132 	Lval: 0.001118
Epoch: 795 	Ltrain: 0.001146 	Lval: 0.001113
Epoch: 800 	Ltrain: 0.001131 	Lval: 0.001111
Epoch: 805 	Ltrain: 0.001123 	Lval: 0.001105
Epoch: 810 	Ltrain: 0.001118 	Lval: 0.001100
Epoch: 815 	Ltrain: 0.001123 	Lval: 0.001096
Epoch: 820 	Ltrain: 0.001111 	Lval: 0.001094
Epoch: 825 	Ltrain: 0.001114 	Lval: 0.001090
Epoch: 830 	Ltrain: 0.001101 	Lval: 0.001085
Epoch: 835 	Ltrain: 0.001105 	Lval: 0.001081
Epoch: 840 	Ltrain: 0.001093 	Lval: 0.001078
Epoch: 845 	Ltrain: 0.001090 	Lval: 0.001073
Epoch: 850 	Ltrain: 0.001088 	Lval: 0.001070
Epoch: 855 	Ltrain: 0.001081 	Lval: 0.001066
Epoch: 860 	Ltrain: 0.001084 	Lval: 0.001062
Epoch: 865 	Ltrain: 0.001078 	Lval: 0.001058
Epoch: 870 	Ltrain: 0.001077 	Lval: 0.001055
Epoch: 875 	Ltrain: 0.001068 	Lval: 0.001051
Epoch: 880 	Ltrain: 0.001065 	Lval: 0.001047
Epoch: 885 	Ltrain: 0.001059 	Lval: 0.001044
Epoch: 890 	Ltrain: 0.001058 	Lval: 0.001040
Epoch: 895 	Ltrain: 0.001056 	Lval: 0.001036
Epoch: 900 	Ltrain: 0.001052 	Lval: 0.001032
Epoch: 905 	Ltrain: 0.001039 	Lval: 0.001028
Epoch: 910 	Ltrain: 0.001044 	Lval: 0.001024
Epoch: 915 	Ltrain: 0.001047 	Lval: 0.001021
Epoch: 920 	Ltrain: 0.001032 	Lval: 0.001018
Epoch: 925 	Ltrain: 0.001031 	Lval: 0.001014
Epoch: 930 	Ltrain: 0.001030 	Lval: 0.001011
Epoch: 935 	Ltrain: 0.001026 	Lval: 0.001006
Epoch: 940 	Ltrain: 0.001021 	Lval: 0.001002
Epoch: 945 	Ltrain: 0.001017 	Lval: 0.001000
Epoch: 950 	Ltrain: 0.001013 	Lval: 0.000996
Epoch: 955 	Ltrain: 0.001009 	Lval: 0.000991
Epoch: 960 	Ltrain: 0.001010 	Lval: 0.000988
Epoch: 965 	Ltrain: 0.001001 	Lval: 0.000985
Epoch: 970 	Ltrain: 0.001000 	Lval: 0.000980
Epoch: 975 	Ltrain: 0.000996 	Lval: 0.000977
Epoch: 980 	Ltrain: 0.000992 	Lval: 0.000974
Epoch: 985 	Ltrain: 0.000988 	Lval: 0.000971
Epoch: 990 	Ltrain: 0.000986 	Lval: 0.000967
Epoch: 995 	Ltrain: 0.000986 	Lval: 0.000964
Epoch: 1000 	Ltrain: 0.000982 	Lval: 0.000961
Epoch: 1005 	Ltrain: 0.000976 	Lval: 0.000957
Epoch: 1010 	Ltrain: 0.000972 	Lval: 0.000953
Epoch: 1015 	Ltrain: 0.000967 	Lval: 0.000949
Epoch: 1020 	Ltrain: 0.000963 	Lval: 0.000947
Epoch: 1025 	Ltrain: 0.000961 	Lval: 0.000942
Epoch: 1030 	Ltrain: 0.000970 	Lval: 0.000939
Epoch: 1035 	Ltrain: 0.000956 	Lval: 0.000935
Epoch: 1040 	Ltrain: 0.000951 	Lval: 0.000932
Epoch: 1045 	Ltrain: 0.000945 	Lval: 0.000929
Epoch: 1050 	Ltrain: 0.000945 	Lval: 0.000926
Epoch: 1055 	Ltrain: 0.000941 	Lval: 0.000924
Epoch: 1060 	Ltrain: 0.000940 	Lval: 0.000919
Epoch: 1065 	Ltrain: 0.000940 	Lval: 0.000916
Epoch: 1070 	Ltrain: 0.000940 	Lval: 0.000913
Epoch: 1075 	Ltrain: 0.000926 	Lval: 0.000910
Epoch: 1080 	Ltrain: 0.000919 	Lval: 0.000907
Epoch: 1085 	Ltrain: 0.000922 	Lval: 0.000902
Epoch: 1090 	Ltrain: 0.000925 	Lval: 0.000899
Epoch: 1095 	Ltrain: 0.000934 	Lval: 0.000896
Epoch: 1100 	Ltrain: 0.000913 	Lval: 0.000892
Epoch: 1105 	Ltrain: 0.000907 	Lval: 0.000890
Epoch: 1110 	Ltrain: 0.000904 	Lval: 0.000886
Epoch: 1115 	Ltrain: 0.000901 	Lval: 0.000883
Epoch: 1120 	Ltrain: 0.000900 	Lval: 0.000880
Epoch: 1125 	Ltrain: 0.000894 	Lval: 0.000877
Epoch: 1130 	Ltrain: 0.000893 	Lval: 0.000874
Epoch: 1135 	Ltrain: 0.000888 	Lval: 0.000870
Epoch: 1140 	Ltrain: 0.000885 	Lval: 0.000867
Epoch: 1145 	Ltrain: 0.000881 	Lval: 0.000864
Epoch: 1150 	Ltrain: 0.000878 	Lval: 0.000860
Epoch: 1155 	Ltrain: 0.000878 	Lval: 0.000858
Epoch: 1160 	Ltrain: 0.000877 	Lval: 0.000855
Epoch: 1165 	Ltrain: 0.000869 	Lval: 0.000852
Epoch: 1170 	Ltrain: 0.000867 	Lval: 0.000849
Epoch: 1175 	Ltrain: 0.000864 	Lval: 0.000845
Epoch: 1180 	Ltrain: 0.000865 	Lval: 0.000844
Epoch: 1185 	Ltrain: 0.000858 	Lval: 0.000839
Epoch: 1190 	Ltrain: 0.000856 	Lval: 0.000837
Epoch: 1195 	Ltrain: 0.000854 	Lval: 0.000834
Epoch: 1200 	Ltrain: 0.000854 	Lval: 0.000830
Epoch: 1205 	Ltrain: 0.000846 	Lval: 0.000827
Epoch: 1210 	Ltrain: 0.000851 	Lval: 0.000824
Epoch: 1215 	Ltrain: 0.000843 	Lval: 0.000822
Epoch: 1220 	Ltrain: 0.000839 	Lval: 0.000818
Epoch: 1225 	Ltrain: 0.000834 	Lval: 0.000815
Epoch: 1230 	Ltrain: 0.000832 	Lval: 0.000813
Epoch: 1235 	Ltrain: 0.000827 	Lval: 0.000809
Epoch: 1240 	Ltrain: 0.000827 	Lval: 0.000806
Epoch: 1245 	Ltrain: 0.000820 	Lval: 0.000804
Epoch: 1250 	Ltrain: 0.000822 	Lval: 0.000801
Epoch: 1255 	Ltrain: 0.000814 	Lval: 0.000798
Epoch: 1260 	Ltrain: 0.000814 	Lval: 0.000796
Epoch: 1265 	Ltrain: 0.000812 	Lval: 0.000792
Epoch: 1270 	Ltrain: 0.000806 	Lval: 0.000789
Epoch: 1275 	Ltrain: 0.000804 	Lval: 0.000786
Epoch: 1280 	Ltrain: 0.000802 	Lval: 0.000783
Epoch: 1285 	Ltrain: 0.000807 	Lval: 0.000780
Epoch: 1290 	Ltrain: 0.000800 	Lval: 0.000777
Epoch: 1295 	Ltrain: 0.000793 	Lval: 0.000774
Epoch: 1300 	Ltrain: 0.000790 	Lval: 0.000773
Epoch: 1305 	Ltrain: 0.000789 	Lval: 0.000768
Epoch: 1310 	Ltrain: 0.000801 	Lval: 0.000766
Epoch: 1315 	Ltrain: 0.000784 	Lval: 0.000763
Epoch: 1320 	Ltrain: 0.000782 	Lval: 0.000761
Epoch: 1325 	Ltrain: 0.000777 	Lval: 0.000758
Epoch: 1330 	Ltrain: 0.000774 	Lval: 0.000756
Epoch: 1335 	Ltrain: 0.000778 	Lval: 0.000752
Epoch: 1340 	Ltrain: 0.000771 	Lval: 0.000749
Epoch: 1345 	Ltrain: 0.000765 	Lval: 0.000747
Epoch: 1350 	Ltrain: 0.000764 	Lval: 0.000744
Epoch: 1355 	Ltrain: 0.000763 	Lval: 0.000742
Epoch: 1360 	Ltrain: 0.000761 	Lval: 0.000739
Epoch: 1365 	Ltrain: 0.000756 	Lval: 0.000736
Epoch: 1370 	Ltrain: 0.000752 	Lval: 0.000733
Epoch: 1375 	Ltrain: 0.000753 	Lval: 0.000731
Epoch: 1380 	Ltrain: 0.000747 	Lval: 0.000727
Epoch: 1385 	Ltrain: 0.000744 	Lval: 0.000725
Epoch: 1390 	Ltrain: 0.000740 	Lval: 0.000723
Epoch: 1395 	Ltrain: 0.000740 	Lval: 0.000719
Epoch: 1400 	Ltrain: 0.000735 	Lval: 0.000717
Epoch: 1405 	Ltrain: 0.000733 	Lval: 0.000714
Epoch: 1410 	Ltrain: 0.000734 	Lval: 0.000712
Epoch: 1415 	Ltrain: 0.000730 	Lval: 0.000709
Epoch: 1420 	Ltrain: 0.000723 	Lval: 0.000706
Epoch: 1425 	Ltrain: 0.000732 	Lval: 0.000704
Epoch: 1430 	Ltrain: 0.000720 	Lval: 0.000702
Epoch: 1435 	Ltrain: 0.000722 	Lval: 0.000699
Epoch: 1440 	Ltrain: 0.000721 	Lval: 0.000696
Epoch: 1445 	Ltrain: 0.000715 	Lval: 0.000694
Epoch: 1450 	Ltrain: 0.000711 	Lval: 0.000690
Epoch: 1455 	Ltrain: 0.000709 	Lval: 0.000689
Epoch: 1460 	Ltrain: 0.000707 	Lval: 0.000686
Epoch: 1465 	Ltrain: 0.000705 	Lval: 0.000683
Epoch: 1470 	Ltrain: 0.000702 	Lval: 0.000680
Epoch: 1475 	Ltrain: 0.000704 	Lval: 0.000678
Epoch: 1480 	Ltrain: 0.000694 	Lval: 0.000675
Epoch: 1485 	Ltrain: 0.000697 	Lval: 0.000672
Epoch: 1490 	Ltrain: 0.000691 	Lval: 0.000671
Epoch: 1495 	Ltrain: 0.000688 	Lval: 0.000668
Epoch: 1500 	Ltrain: 0.000683 	Lval: 0.000665
Epoch: 1505 	Ltrain: 0.000682 	Lval: 0.000664
Epoch: 1510 	Ltrain: 0.000678 	Lval: 0.000661
Epoch: 1515 	Ltrain: 0.000680 	Lval: 0.000658
Epoch: 1520 	Ltrain: 0.000678 	Lval: 0.000655
Epoch: 1525 	Ltrain: 0.000676 	Lval: 0.000653
Epoch: 1530 	Ltrain: 0.000675 	Lval: 0.000651
Epoch: 1535 	Ltrain: 0.000669 	Lval: 0.000648
Epoch: 1540 	Ltrain: 0.000667 	Lval: 0.000646
Epoch: 1545 	Ltrain: 0.000665 	Lval: 0.000645
Epoch: 1550 	Ltrain: 0.000664 	Lval: 0.000641
Epoch: 1555 	Ltrain: 0.000661 	Lval: 0.000639
Epoch: 1560 	Ltrain: 0.000657 	Lval: 0.000637
Epoch: 1565 	Ltrain: 0.000658 	Lval: 0.000634
Epoch: 1570 	Ltrain: 0.000655 	Lval: 0.000632
Epoch: 1575 	Ltrain: 0.000654 	Lval: 0.000631
Epoch: 1580 	Ltrain: 0.000648 	Lval: 0.000627
Epoch: 1585 	Ltrain: 0.000646 	Lval: 0.000625
Epoch: 1590 	Ltrain: 0.000641 	Lval: 0.000622
Epoch: 1595 	Ltrain: 0.000643 	Lval: 0.000620
Epoch: 1600 	Ltrain: 0.000640 	Lval: 0.000618
Epoch: 1605 	Ltrain: 0.000636 	Lval: 0.000616
Epoch: 1610 	Ltrain: 0.000638 	Lval: 0.000615
Epoch: 1615 	Ltrain: 0.000631 	Lval: 0.000611
Epoch: 1620 	Ltrain: 0.000630 	Lval: 0.000609
Epoch: 1625 	Ltrain: 0.000625 	Lval: 0.000607
Epoch: 1630 	Ltrain: 0.000627 	Lval: 0.000604
Epoch: 1635 	Ltrain: 0.000623 	Lval: 0.000603
Epoch: 1640 	Ltrain: 0.000622 	Lval: 0.000600
Epoch: 1645 	Ltrain: 0.000617 	Lval: 0.000598
Epoch: 1650 	Ltrain: 0.000615 	Lval: 0.000595
Epoch: 1655 	Ltrain: 0.000616 	Lval: 0.000593
Epoch: 1660 	Ltrain: 0.000615 	Lval: 0.000592
Epoch: 1665 	Ltrain: 0.000609 	Lval: 0.000589
Epoch: 1670 	Ltrain: 0.000610 	Lval: 0.000586
Epoch: 1675 	Ltrain: 0.000619 	Lval: 0.000584
Epoch: 1680 	Ltrain: 0.000604 	Lval: 0.000582
Epoch: 1685 	Ltrain: 0.000599 	Lval: 0.000580
Epoch: 1690 	Ltrain: 0.000600 	Lval: 0.000578
Epoch: 1695 	Ltrain: 0.000598 	Lval: 0.000576
Epoch: 1700 	Ltrain: 0.000600 	Lval: 0.000573
Epoch: 1705 	Ltrain: 0.000593 	Lval: 0.000572
Epoch: 1710 	Ltrain: 0.000591 	Lval: 0.000570
Epoch: 1715 	Ltrain: 0.000590 	Lval: 0.000567
Epoch: 1720 	Ltrain: 0.000589 	Lval: 0.000565
Epoch: 1725 	Ltrain: 0.000583 	Lval: 0.000562
Epoch: 1730 	Ltrain: 0.000585 	Lval: 0.000561
Epoch: 1735 	Ltrain: 0.000579 	Lval: 0.000559
Epoch: 1740 	Ltrain: 0.000579 	Lval: 0.000557
Epoch: 1745 	Ltrain: 0.000576 	Lval: 0.000554
Epoch: 1750 	Ltrain: 0.000574 	Lval: 0.000554
Epoch: 1755 	Ltrain: 0.000573 	Lval: 0.000550
Epoch: 1760 	Ltrain: 0.000567 	Lval: 0.000548
Epoch: 1765 	Ltrain: 0.000567 	Lval: 0.000546
Epoch: 1770 	Ltrain: 0.000569 	Lval: 0.000544
Epoch: 1775 	Ltrain: 0.000564 	Lval: 0.000543
Epoch: 1780 	Ltrain: 0.000568 	Lval: 0.000541
Epoch: 1785 	Ltrain: 0.000566 	Lval: 0.000539
Epoch: 1790 	Ltrain: 0.000560 	Lval: 0.000537
Epoch: 1795 	Ltrain: 0.000554 	Lval: 0.000535
Epoch: 1800 	Ltrain: 0.000553 	Lval: 0.000532
Epoch: 1805 	Ltrain: 0.000551 	Lval: 0.000531
Epoch: 1810 	Ltrain: 0.000553 	Lval: 0.000528
Epoch: 1815 	Ltrain: 0.000548 	Lval: 0.000527
Epoch: 1820 	Ltrain: 0.000549 	Lval: 0.000524
Epoch: 1825 	Ltrain: 0.000544 	Lval: 0.000522
Epoch: 1830 	Ltrain: 0.000543 	Lval: 0.000521
Epoch: 1835 	Ltrain: 0.000539 	Lval: 0.000519
Epoch: 1840 	Ltrain: 0.000540 	Lval: 0.000518
Epoch: 1845 	Ltrain: 0.000536 	Lval: 0.000515
Epoch: 1850 	Ltrain: 0.000534 	Lval: 0.000512
Epoch: 1855 	Ltrain: 0.000534 	Lval: 0.000511
Epoch: 1860 	Ltrain: 0.000532 	Lval: 0.000509
Epoch: 1865 	Ltrain: 0.000527 	Lval: 0.000508
Epoch: 1870 	Ltrain: 0.000526 	Lval: 0.000506
Epoch: 1875 	Ltrain: 0.000526 	Lval: 0.000504
Epoch: 1880 	Ltrain: 0.000525 	Lval: 0.000502
Epoch: 1885 	Ltrain: 0.000524 	Lval: 0.000501
Epoch: 1890 	Ltrain: 0.000522 	Lval: 0.000498
Epoch: 1895 	Ltrain: 0.000516 	Lval: 0.000496
Epoch: 1900 	Ltrain: 0.000516 	Lval: 0.000495
Epoch: 1905 	Ltrain: 0.000514 	Lval: 0.000493
EarlyStopper: stopping at epoch 1904 with best_val_loss = 0.000502


	Fold 4/5
Epoch: 1 	Ltrain: 0.042600 	Lval: 0.011656
Epoch: 5 	Ltrain: 0.005704 	Lval: 0.007029
Epoch: 10 	Ltrain: 0.005442 	Lval: 0.006793
Epoch 00013: reducing learning rate of group 0 to 7.3446e-04.
Epoch: 15 	Ltrain: 0.004719 	Lval: 0.005929
Epoch: 20 	Ltrain: 0.004652 	Lval: 0.005851
Epoch: 25 	Ltrain: 0.004583 	Lval: 0.005705
Epoch: 30 	Ltrain: 0.004484 	Lval: 0.005561
Epoch 00034: reducing learning rate of group 0 to 7.3446e-05.
Epoch: 35 	Ltrain: 0.004358 	Lval: 0.005472
Epoch: 40 	Ltrain: 0.004330 	Lval: 0.005470
Epoch: 45 	Ltrain: 0.004312 	Lval: 0.005486
Epoch: 50 	Ltrain: 0.004301 	Lval: 0.005416
Epoch: 55 	Ltrain: 0.004291 	Lval: 0.005409
Epoch 00060: reducing learning rate of group 0 to 7.3446e-06.
Epoch: 60 	Ltrain: 0.004280 	Lval: 0.005419
Epoch: 65 	Ltrain: 0.004267 	Lval: 0.005409
Epoch: 70 	Ltrain: 0.004271 	Lval: 0.005406
Epoch 00072: reducing learning rate of group 0 to 7.3446e-07.
Epoch: 75 	Ltrain: 0.004249 	Lval: 0.005401
Epoch: 80 	Ltrain: 0.004269 	Lval: 0.005402
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.005404


	Fold 5/5
Epoch: 1 	Ltrain: 0.045212 	Lval: 0.014316
Epoch: 5 	Ltrain: 0.006317 	Lval: 0.007314
Epoch: 10 	Ltrain: 0.005556 	Lval: 0.006986
Epoch: 15 	Ltrain: 0.005131 	Lval: 0.005956
Epoch 00019: reducing learning rate of group 0 to 7.3446e-04.
Epoch: 20 	Ltrain: 0.004536 	Lval: 0.005652
Epoch: 25 	Ltrain: 0.004295 	Lval: 0.005313
Epoch: 30 	Ltrain: 0.004172 	Lval: 0.005176
Epoch: 35 	Ltrain: 0.004136 	Lval: 0.004975
Epoch: 40 	Ltrain: 0.003994 	Lval: 0.004855
Epoch: 45 	Ltrain: 0.003876 	Lval: 0.004655
Epoch: 50 	Ltrain: 0.003771 	Lval: 0.004491
Epoch: 55 	Ltrain: 0.003691 	Lval: 0.004349
Epoch: 60 	Ltrain: 0.003493 	Lval: 0.004031
Epoch: 65 	Ltrain: 0.003401 	Lval: 0.003960
Epoch: 70 	Ltrain: 0.003215 	Lval: 0.003649
Epoch: 75 	Ltrain: 0.003086 	Lval: 0.003371
Epoch: 80 	Ltrain: 0.002758 	Lval: 0.003181
Epoch 00082: reducing learning rate of group 0 to 7.3446e-05.
Epoch: 85 	Ltrain: 0.002364 	Lval: 0.002740
Epoch: 90 	Ltrain: 0.002300 	Lval: 0.002637
Epoch: 95 	Ltrain: 0.002266 	Lval: 0.002602
Epoch: 100 	Ltrain: 0.002231 	Lval: 0.002545
Epoch: 105 	Ltrain: 0.002198 	Lval: 0.002504
Epoch: 110 	Ltrain: 0.002151 	Lval: 0.002461
Epoch: 115 	Ltrain: 0.002124 	Lval: 0.002418
Epoch: 120 	Ltrain: 0.002082 	Lval: 0.002387
Epoch: 125 	Ltrain: 0.002057 	Lval: 0.002342
Epoch: 130 	Ltrain: 0.002016 	Lval: 0.002319
Epoch: 135 	Ltrain: 0.001971 	Lval: 0.002245
Epoch: 140 	Ltrain: 0.001929 	Lval: 0.002216
Epoch: 145 	Ltrain: 0.001902 	Lval: 0.002156
Epoch: 150 	Ltrain: 0.001869 	Lval: 0.002137
Epoch: 155 	Ltrain: 0.001829 	Lval: 0.002077
Epoch: 160 	Ltrain: 0.001798 	Lval: 0.002050
Epoch: 165 	Ltrain: 0.001760 	Lval: 0.001979
Epoch: 170 	Ltrain: 0.001723 	Lval: 0.001951
Epoch: 175 	Ltrain: 0.001688 	Lval: 0.001914
Epoch: 180 	Ltrain: 0.001653 	Lval: 0.001850
Epoch: 185 	Ltrain: 0.001627 	Lval: 0.001844
Epoch: 190 	Ltrain: 0.001580 	Lval: 0.001795
Epoch: 195 	Ltrain: 0.001555 	Lval: 0.001730
Epoch: 200 	Ltrain: 0.001518 	Lval: 0.001702
Epoch: 205 	Ltrain: 0.001484 	Lval: 0.001647
Epoch: 210 	Ltrain: 0.001450 	Lval: 0.001627
Epoch: 215 	Ltrain: 0.001424 	Lval: 0.001577
Epoch: 220 	Ltrain: 0.001392 	Lval: 0.001533
Epoch: 225 	Ltrain: 0.001360 	Lval: 0.001500
Epoch: 230 	Ltrain: 0.001342 	Lval: 0.001466
Epoch: 235 	Ltrain: 0.001296 	Lval: 0.001434
Epoch: 240 	Ltrain: 0.001273 	Lval: 0.001427
Epoch: 245 	Ltrain: 0.001240 	Lval: 0.001382
Epoch: 250 	Ltrain: 0.001220 	Lval: 0.001342
Epoch: 255 	Ltrain: 0.001189 	Lval: 0.001336
Epoch: 260 	Ltrain: 0.001163 	Lval: 0.001275
Epoch 00265: reducing learning rate of group 0 to 7.3446e-06.
Epoch: 265 	Ltrain: 0.001139 	Lval: 0.001273
Epoch: 270 	Ltrain: 0.001099 	Lval: 0.001219
Epoch: 275 	Ltrain: 0.001089 	Lval: 0.001214
Epoch: 280 	Ltrain: 0.001088 	Lval: 0.001211
Epoch: 285 	Ltrain: 0.001082 	Lval: 0.001207
Epoch: 290 	Ltrain: 0.001080 	Lval: 0.001203
Epoch: 295 	Ltrain: 0.001076 	Lval: 0.001200
Epoch: 300 	Ltrain: 0.001074 	Lval: 0.001196
Epoch: 305 	Ltrain: 0.001073 	Lval: 0.001193
Epoch: 310 	Ltrain: 0.001073 	Lval: 0.001189
Epoch: 315 	Ltrain: 0.001065 	Lval: 0.001185
Epoch: 320 	Ltrain: 0.001064 	Lval: 0.001181
Epoch: 325 	Ltrain: 0.001060 	Lval: 0.001179
Epoch: 330 	Ltrain: 0.001056 	Lval: 0.001174
Epoch: 335 	Ltrain: 0.001052 	Lval: 0.001170
Epoch: 340 	Ltrain: 0.001047 	Lval: 0.001168
Epoch: 345 	Ltrain: 0.001044 	Lval: 0.001163
Epoch: 350 	Ltrain: 0.001042 	Lval: 0.001159
Epoch: 355 	Ltrain: 0.001038 	Lval: 0.001154
Epoch: 360 	Ltrain: 0.001033 	Lval: 0.001151
Epoch: 365 	Ltrain: 0.001032 	Lval: 0.001147
Epoch: 370 	Ltrain: 0.001027 	Lval: 0.001144
Epoch: 375 	Ltrain: 0.001026 	Lval: 0.001140
Epoch: 380 	Ltrain: 0.001022 	Lval: 0.001136
Epoch: 385 	Ltrain: 0.001017 	Lval: 0.001133
Epoch: 390 	Ltrain: 0.001015 	Lval: 0.001128
Epoch: 395 	Ltrain: 0.001011 	Lval: 0.001125
Epoch: 400 	Ltrain: 0.001007 	Lval: 0.001120
Epoch: 405 	Ltrain: 0.001005 	Lval: 0.001116
Epoch: 410 	Ltrain: 0.001001 	Lval: 0.001114
Epoch: 415 	Ltrain: 0.000998 	Lval: 0.001110
Epoch: 420 	Ltrain: 0.000996 	Lval: 0.001106
Epoch: 425 	Ltrain: 0.000992 	Lval: 0.001102
Epoch: 430 	Ltrain: 0.000988 	Lval: 0.001098
Epoch: 435 	Ltrain: 0.000987 	Lval: 0.001095
Epoch: 440 	Ltrain: 0.000980 	Lval: 0.001091
Epoch: 445 	Ltrain: 0.000977 	Lval: 0.001088
Epoch: 450 	Ltrain: 0.000975 	Lval: 0.001083
Epoch: 455 	Ltrain: 0.000970 	Lval: 0.001079
Epoch: 460 	Ltrain: 0.000969 	Lval: 0.001075
Epoch: 465 	Ltrain: 0.000964 	Lval: 0.001072
Epoch: 470 	Ltrain: 0.000960 	Lval: 0.001068
Epoch: 475 	Ltrain: 0.000956 	Lval: 0.001064
Epoch: 480 	Ltrain: 0.000958 	Lval: 0.001062
Epoch: 485 	Ltrain: 0.000950 	Lval: 0.001056
Epoch: 490 	Ltrain: 0.000949 	Lval: 0.001053
Epoch: 495 	Ltrain: 0.000945 	Lval: 0.001049
Epoch: 500 	Ltrain: 0.000943 	Lval: 0.001046
Epoch: 505 	Ltrain: 0.000939 	Lval: 0.001042
Epoch: 510 	Ltrain: 0.000937 	Lval: 0.001040
Epoch: 515 	Ltrain: 0.000932 	Lval: 0.001037
Epoch: 520 	Ltrain: 0.000930 	Lval: 0.001032
Epoch: 525 	Ltrain: 0.000927 	Lval: 0.001028
Epoch: 530 	Ltrain: 0.000924 	Lval: 0.001025
Epoch: 535 	Ltrain: 0.000920 	Lval: 0.001020
Epoch: 540 	Ltrain: 0.000917 	Lval: 0.001018
Epoch: 545 	Ltrain: 0.000913 	Lval: 0.001015
Epoch: 550 	Ltrain: 0.000911 	Lval: 0.001010
Epoch: 555 	Ltrain: 0.000908 	Lval: 0.001006
Epoch: 560 	Ltrain: 0.000902 	Lval: 0.001003
Epoch: 565 	Ltrain: 0.000902 	Lval: 0.000999
Epoch: 570 	Ltrain: 0.000899 	Lval: 0.000995
Epoch: 575 	Ltrain: 0.000894 	Lval: 0.000993
Epoch: 580 	Ltrain: 0.000892 	Lval: 0.000988
Epoch: 585 	Ltrain: 0.000893 	Lval: 0.000985
Epoch: 590 	Ltrain: 0.000886 	Lval: 0.000982
Epoch: 595 	Ltrain: 0.000882 	Lval: 0.000977
Epoch: 600 	Ltrain: 0.000880 	Lval: 0.000974
Epoch: 605 	Ltrain: 0.000881 	Lval: 0.000972
Epoch: 610 	Ltrain: 0.000872 	Lval: 0.000967
Epoch: 615 	Ltrain: 0.000871 	Lval: 0.000964
Epoch: 620 	Ltrain: 0.000868 	Lval: 0.000961
Epoch: 625 	Ltrain: 0.000863 	Lval: 0.000958
Epoch: 630 	Ltrain: 0.000859 	Lval: 0.000954
Epoch: 635 	Ltrain: 0.000862 	Lval: 0.000950
Epoch: 640 	Ltrain: 0.000856 	Lval: 0.000949
Epoch: 645 	Ltrain: 0.000853 	Lval: 0.000944
Epoch: 650 	Ltrain: 0.000849 	Lval: 0.000940
Epoch: 655 	Ltrain: 0.000847 	Lval: 0.000937
Epoch: 660 	Ltrain: 0.000844 	Lval: 0.000934
Epoch: 665 	Ltrain: 0.000842 	Lval: 0.000930
Epoch: 670 	Ltrain: 0.000837 	Lval: 0.000926
Epoch: 675 	Ltrain: 0.000834 	Lval: 0.000923
Epoch: 680 	Ltrain: 0.000832 	Lval: 0.000920
Epoch: 685 	Ltrain: 0.000833 	Lval: 0.000917
Epoch: 690 	Ltrain: 0.000827 	Lval: 0.000913
Epoch: 695 	Ltrain: 0.000823 	Lval: 0.000910
Epoch: 700 	Ltrain: 0.000820 	Lval: 0.000908
Epoch: 705 	Ltrain: 0.000818 	Lval: 0.000903
Epoch: 710 	Ltrain: 0.000815 	Lval: 0.000900
Epoch: 715 	Ltrain: 0.000811 	Lval: 0.000896
Epoch: 720 	Ltrain: 0.000807 	Lval: 0.000893
Epoch: 725 	Ltrain: 0.000805 	Lval: 0.000890
Epoch: 730 	Ltrain: 0.000802 	Lval: 0.000888
Epoch: 735 	Ltrain: 0.000800 	Lval: 0.000883
Epoch: 740 	Ltrain: 0.000798 	Lval: 0.000881
Epoch: 745 	Ltrain: 0.000795 	Lval: 0.000877
Epoch: 750 	Ltrain: 0.000792 	Lval: 0.000873
Epoch: 755 	Ltrain: 0.000789 	Lval: 0.000871
Epoch: 760 	Ltrain: 0.000785 	Lval: 0.000867
Epoch: 765 	Ltrain: 0.000786 	Lval: 0.000864
Epoch: 770 	Ltrain: 0.000781 	Lval: 0.000861
Epoch: 775 	Ltrain: 0.000778 	Lval: 0.000858
Epoch: 780 	Ltrain: 0.000775 	Lval: 0.000855
Epoch: 785 	Ltrain: 0.000772 	Lval: 0.000852
Epoch: 790 	Ltrain: 0.000770 	Lval: 0.000849
Epoch: 795 	Ltrain: 0.000768 	Lval: 0.000845
Epoch: 800 	Ltrain: 0.000763 	Lval: 0.000843
Epoch: 805 	Ltrain: 0.000762 	Lval: 0.000838
Epoch: 810 	Ltrain: 0.000763 	Lval: 0.000836
Epoch: 815 	Ltrain: 0.000755 	Lval: 0.000832
Epoch: 820 	Ltrain: 0.000752 	Lval: 0.000830
Epoch: 825 	Ltrain: 0.000753 	Lval: 0.000827
Epoch: 830 	Ltrain: 0.000749 	Lval: 0.000824
Epoch: 835 	Ltrain: 0.000745 	Lval: 0.000821
Epoch: 840 	Ltrain: 0.000742 	Lval: 0.000818
Epoch: 845 	Ltrain: 0.000740 	Lval: 0.000815
Epoch: 850 	Ltrain: 0.000738 	Lval: 0.000810
Epoch: 855 	Ltrain: 0.000734 	Lval: 0.000809
Epoch: 860 	Ltrain: 0.000732 	Lval: 0.000805
Epoch: 865 	Ltrain: 0.000729 	Lval: 0.000803
Epoch: 870 	Ltrain: 0.000726 	Lval: 0.000799
Epoch: 875 	Ltrain: 0.000726 	Lval: 0.000797
Epoch: 880 	Ltrain: 0.000722 	Lval: 0.000793
Epoch: 885 	Ltrain: 0.000720 	Lval: 0.000790
Epoch: 890 	Ltrain: 0.000717 	Lval: 0.000787
Epoch: 895 	Ltrain: 0.000714 	Lval: 0.000784
Epoch: 900 	Ltrain: 0.000712 	Lval: 0.000782
Epoch: 905 	Ltrain: 0.000710 	Lval: 0.000778
Epoch: 910 	Ltrain: 0.000705 	Lval: 0.000776
Epoch: 915 	Ltrain: 0.000703 	Lval: 0.000772
Epoch: 920 	Ltrain: 0.000701 	Lval: 0.000769
Epoch: 925 	Ltrain: 0.000699 	Lval: 0.000767
Epoch: 930 	Ltrain: 0.000696 	Lval: 0.000764
Epoch: 935 	Ltrain: 0.000695 	Lval: 0.000761
Epoch: 940 	Ltrain: 0.000690 	Lval: 0.000757
Epoch: 945 	Ltrain: 0.000689 	Lval: 0.000755
Epoch: 950 	Ltrain: 0.000686 	Lval: 0.000752
Epoch: 955 	Ltrain: 0.000685 	Lval: 0.000748
Epoch: 960 	Ltrain: 0.000681 	Lval: 0.000747
Epoch: 965 	Ltrain: 0.000679 	Lval: 0.000744
Epoch: 970 	Ltrain: 0.000676 	Lval: 0.000741
Epoch: 975 	Ltrain: 0.000675 	Lval: 0.000738
Epoch: 980 	Ltrain: 0.000672 	Lval: 0.000735
Epoch: 985 	Ltrain: 0.000669 	Lval: 0.000732
Epoch: 990 	Ltrain: 0.000668 	Lval: 0.000729
Epoch: 995 	Ltrain: 0.000665 	Lval: 0.000727
Epoch: 1000 	Ltrain: 0.000663 	Lval: 0.000723
Epoch: 1005 	Ltrain: 0.000658 	Lval: 0.000720
Epoch: 1010 	Ltrain: 0.000658 	Lval: 0.000718
Epoch: 1015 	Ltrain: 0.000657 	Lval: 0.000715
Epoch: 1020 	Ltrain: 0.000652 	Lval: 0.000712
Epoch: 1025 	Ltrain: 0.000651 	Lval: 0.000709
Epoch: 1030 	Ltrain: 0.000647 	Lval: 0.000707
Epoch: 1035 	Ltrain: 0.000646 	Lval: 0.000704
Epoch: 1040 	Ltrain: 0.000642 	Lval: 0.000701
Epoch: 1045 	Ltrain: 0.000640 	Lval: 0.000698
Epoch: 1050 	Ltrain: 0.000639 	Lval: 0.000695
Epoch: 1055 	Ltrain: 0.000638 	Lval: 0.000693
Epoch: 1060 	Ltrain: 0.000633 	Lval: 0.000690
Epoch: 1065 	Ltrain: 0.000631 	Lval: 0.000687
Epoch: 1070 	Ltrain: 0.000629 	Lval: 0.000685
Epoch: 1075 	Ltrain: 0.000625 	Lval: 0.000683
Epoch: 1080 	Ltrain: 0.000625 	Lval: 0.000681
Epoch: 1085 	Ltrain: 0.000621 	Lval: 0.000677
Epoch: 1090 	Ltrain: 0.000619 	Lval: 0.000674
Epoch: 1095 	Ltrain: 0.000617 	Lval: 0.000671
Epoch: 1100 	Ltrain: 0.000615 	Lval: 0.000669
Epoch: 1105 	Ltrain: 0.000614 	Lval: 0.000666
Epoch: 1110 	Ltrain: 0.000610 	Lval: 0.000663
Epoch: 1115 	Ltrain: 0.000611 	Lval: 0.000662
Epoch: 1120 	Ltrain: 0.000607 	Lval: 0.000659
Epoch: 1125 	Ltrain: 0.000605 	Lval: 0.000656
Epoch: 1130 	Ltrain: 0.000601 	Lval: 0.000654
Epoch: 1135 	Ltrain: 0.000599 	Lval: 0.000650
Epoch: 1140 	Ltrain: 0.000597 	Lval: 0.000648
Epoch: 1145 	Ltrain: 0.000595 	Lval: 0.000646
Epoch: 1150 	Ltrain: 0.000593 	Lval: 0.000643
Epoch: 1155 	Ltrain: 0.000591 	Lval: 0.000641
Epoch: 1160 	Ltrain: 0.000590 	Lval: 0.000639
Epoch: 1165 	Ltrain: 0.000587 	Lval: 0.000635
Epoch: 1170 	Ltrain: 0.000584 	Lval: 0.000633
Epoch: 1175 	Ltrain: 0.000581 	Lval: 0.000630
Epoch: 1180 	Ltrain: 0.000581 	Lval: 0.000628
Epoch: 1185 	Ltrain: 0.000577 	Lval: 0.000626
Epoch: 1190 	Ltrain: 0.000575 	Lval: 0.000624
Epoch: 1195 	Ltrain: 0.000575 	Lval: 0.000622
Epoch: 1200 	Ltrain: 0.000571 	Lval: 0.000619
Epoch: 1205 	Ltrain: 0.000569 	Lval: 0.000616
Epoch: 1210 	Ltrain: 0.000567 	Lval: 0.000614
Epoch: 1215 	Ltrain: 0.000565 	Lval: 0.000612
Epoch: 1220 	Ltrain: 0.000562 	Lval: 0.000608
Epoch: 1225 	Ltrain: 0.000561 	Lval: 0.000607
Epoch: 1230 	Ltrain: 0.000558 	Lval: 0.000604
Epoch: 1235 	Ltrain: 0.000557 	Lval: 0.000601
Epoch: 1240 	Ltrain: 0.000553 	Lval: 0.000599
Epoch: 1245 	Ltrain: 0.000552 	Lval: 0.000597
Epoch: 1250 	Ltrain: 0.000550 	Lval: 0.000595
Epoch: 1255 	Ltrain: 0.000548 	Lval: 0.000592
Epoch: 1260 	Ltrain: 0.000547 	Lval: 0.000590
Epoch: 1265 	Ltrain: 0.000544 	Lval: 0.000587
Epoch: 1270 	Ltrain: 0.000543 	Lval: 0.000585
Epoch: 1275 	Ltrain: 0.000541 	Lval: 0.000583
Epoch: 1280 	Ltrain: 0.000538 	Lval: 0.000581
Epoch: 1285 	Ltrain: 0.000535 	Lval: 0.000578
Epoch: 1290 	Ltrain: 0.000534 	Lval: 0.000576
Epoch: 1295 	Ltrain: 0.000533 	Lval: 0.000574
Epoch: 1300 	Ltrain: 0.000530 	Lval: 0.000572
Epoch: 1305 	Ltrain: 0.000528 	Lval: 0.000570
Epoch: 1310 	Ltrain: 0.000527 	Lval: 0.000568
Epoch: 1315 	Ltrain: 0.000524 	Lval: 0.000565
Epoch: 1320 	Ltrain: 0.000522 	Lval: 0.000562
Epoch: 1325 	Ltrain: 0.000520 	Lval: 0.000560
Epoch: 1330 	Ltrain: 0.000519 	Lval: 0.000558
Epoch: 1335 	Ltrain: 0.000516 	Lval: 0.000555
Epoch: 1340 	Ltrain: 0.000514 	Lval: 0.000555
Epoch: 1345 	Ltrain: 0.000513 	Lval: 0.000551
Epoch: 1350 	Ltrain: 0.000510 	Lval: 0.000549
Epoch: 1355 	Ltrain: 0.000509 	Lval: 0.000549
Epoch: 1360 	Ltrain: 0.000507 	Lval: 0.000545
Epoch: 1365 	Ltrain: 0.000504 	Lval: 0.000542
Epoch: 1370 	Ltrain: 0.000503 	Lval: 0.000540
Epoch: 1375 	Ltrain: 0.000501 	Lval: 0.000538
Epoch: 1380 	Ltrain: 0.000499 	Lval: 0.000537
Epoch: 1385 	Ltrain: 0.000497 	Lval: 0.000536
Epoch: 1390 	Ltrain: 0.000495 	Lval: 0.000532
Epoch: 1395 	Ltrain: 0.000494 	Lval: 0.000530
Epoch: 1400 	Ltrain: 0.000491 	Lval: 0.000528
Epoch: 1405 	Ltrain: 0.000491 	Lval: 0.000526
Epoch: 1410 	Ltrain: 0.000488 	Lval: 0.000523
Epoch: 1415 	Ltrain: 0.000486 	Lval: 0.000522
Epoch: 1420 	Ltrain: 0.000484 	Lval: 0.000520
Epoch: 1425 	Ltrain: 0.000488 	Lval: 0.000518
Epoch: 1430 	Ltrain: 0.000480 	Lval: 0.000516
Epoch: 1435 	Ltrain: 0.000479 	Lval: 0.000514
Epoch: 1440 	Ltrain: 0.000477 	Lval: 0.000511
Epoch: 1445 	Ltrain: 0.000476 	Lval: 0.000510
Epoch: 1450 	Ltrain: 0.000473 	Lval: 0.000507
Epoch: 1455 	Ltrain: 0.000472 	Lval: 0.000505
Epoch: 1460 	Ltrain: 0.000470 	Lval: 0.000503
Epoch: 1465 	Ltrain: 0.000468 	Lval: 0.000502
Epoch: 1470 	Ltrain: 0.000467 	Lval: 0.000499
Epoch: 1475 	Ltrain: 0.000465 	Lval: 0.000497
Epoch: 1480 	Ltrain: 0.000463 	Lval: 0.000495
Epoch: 1485 	Ltrain: 0.000460 	Lval: 0.000493
Epoch 01490: reducing learning rate of group 0 to 7.3446e-07.
Epoch: 1490 	Ltrain: 0.000461 	Lval: 0.000494
Epoch: 1495 	Ltrain: 0.000458 	Lval: 0.000490
EarlyStopper: stopping at epoch 1495 with best_val_loss = 0.000500

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006842813135963306
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.575528824143786e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.105525 	Lval: 0.029404
Epoch: 5 	Ltrain: 0.013933 	Lval: 0.011359
Epoch: 10 	Ltrain: 0.008605 	Lval: 0.007357
Epoch 00014: reducing learning rate of group 0 to 6.8428e-04.
Epoch: 15 	Ltrain: 0.006896 	Lval: 0.006824
Epoch: 20 	Ltrain: 0.006990 	Lval: 0.006720
Epoch: 25 	Ltrain: 0.006642 	Lval: 0.006608
Epoch: 30 	Ltrain: 0.006566 	Lval: 0.006561
Epoch 00035: reducing learning rate of group 0 to 6.8428e-05.
Epoch: 35 	Ltrain: 0.006829 	Lval: 0.006536
Epoch: 40 	Ltrain: 0.006288 	Lval: 0.006448
Epoch: 45 	Ltrain: 0.006833 	Lval: 0.006421
Epoch 00049: reducing learning rate of group 0 to 6.8428e-06.
Epoch: 50 	Ltrain: 0.006464 	Lval: 0.006426
Epoch: 55 	Ltrain: 0.006144 	Lval: 0.006424
Epoch: 60 	Ltrain: 0.006842 	Lval: 0.006420
Epoch 00063: reducing learning rate of group 0 to 6.8428e-07.
Epoch: 65 	Ltrain: 0.006148 	Lval: 0.006424
Epoch: 70 	Ltrain: 0.006340 	Lval: 0.006424
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.006427


	Fold 2/5
Epoch: 1 	Ltrain: 0.080625 	Lval: 0.022987
Epoch: 5 	Ltrain: 0.008071 	Lval: 0.008011
Epoch: 10 	Ltrain: 0.007096 	Lval: 0.008211
Epoch 00012: reducing learning rate of group 0 to 6.8428e-04.
Epoch: 15 	Ltrain: 0.005992 	Lval: 0.006304
Epoch: 20 	Ltrain: 0.005969 	Lval: 0.006250
Epoch: 25 	Ltrain: 0.005954 	Lval: 0.006278
Epoch: 30 	Ltrain: 0.005782 	Lval: 0.006181
Epoch: 35 	Ltrain: 0.005870 	Lval: 0.006142
Epoch 00038: reducing learning rate of group 0 to 6.8428e-05.
Epoch: 40 	Ltrain: 0.005701 	Lval: 0.006021
Epoch: 45 	Ltrain: 0.005664 	Lval: 0.005993
Epoch: 50 	Ltrain: 0.005638 	Lval: 0.006005
Epoch: 55 	Ltrain: 0.005638 	Lval: 0.005979
Epoch: 60 	Ltrain: 0.005739 	Lval: 0.005960
Epoch: 65 	Ltrain: 0.005577 	Lval: 0.005964
Epoch: 70 	Ltrain: 0.005529 	Lval: 0.005924
Epoch: 75 	Ltrain: 0.005648 	Lval: 0.005921
Epoch: 80 	Ltrain: 0.005626 	Lval: 0.005906
Epoch: 85 	Ltrain: 0.005691 	Lval: 0.005895
Epoch: 90 	Ltrain: 0.005619 	Lval: 0.005893
Epoch: 95 	Ltrain: 0.005646 	Lval: 0.005865
Epoch: 100 	Ltrain: 0.005546 	Lval: 0.005874
Epoch 00103: reducing learning rate of group 0 to 6.8428e-06.
Epoch: 105 	Ltrain: 0.005582 	Lval: 0.005864
Epoch: 110 	Ltrain: 0.005579 	Lval: 0.005859
Epoch: 115 	Ltrain: 0.005600 	Lval: 0.005854
Epoch 00119: reducing learning rate of group 0 to 6.8428e-07.
Epoch: 120 	Ltrain: 0.005548 	Lval: 0.005855
Epoch: 125 	Ltrain: 0.005654 	Lval: 0.005854
Epoch: 130 	Ltrain: 0.005632 	Lval: 0.005854
Epoch 00131: reducing learning rate of group 0 to 6.8428e-08.
Epoch: 135 	Ltrain: 0.005510 	Lval: 0.005854
Epoch: 140 	Ltrain: 0.005472 	Lval: 0.005854
EarlyStopper: stopping at epoch 141 with best_val_loss = 0.005854


	Fold 3/5
Epoch: 1 	Ltrain: 0.053114 	Lval: 0.018381
Epoch: 5 	Ltrain: 0.007643 	Lval: 0.007995
Epoch: 10 	Ltrain: 0.006248 	Lval: 0.007105
Epoch: 15 	Ltrain: 0.006218 	Lval: 0.006956
Epoch: 20 	Ltrain: 0.005649 	Lval: 0.006039
Epoch: 25 	Ltrain: 0.005395 	Lval: 0.005794
Epoch: 30 	Ltrain: 0.005115 	Lval: 0.006197
Epoch: 35 	Ltrain: 0.004765 	Lval: 0.005292
Epoch: 40 	Ltrain: 0.004720 	Lval: 0.004791
Epoch: 45 	Ltrain: 0.004589 	Lval: 0.004516
Epoch: 50 	Ltrain: 0.003980 	Lval: 0.004015
Epoch: 55 	Ltrain: 0.003843 	Lval: 0.003645
Epoch 00056: reducing learning rate of group 0 to 6.8428e-04.
Epoch: 60 	Ltrain: 0.002603 	Lval: 0.002697
Epoch: 65 	Ltrain: 0.002387 	Lval: 0.002431
Epoch: 70 	Ltrain: 0.002214 	Lval: 0.002247
Epoch: 75 	Ltrain: 0.002029 	Lval: 0.002045
Epoch: 80 	Ltrain: 0.001870 	Lval: 0.001860
Epoch: 85 	Ltrain: 0.001720 	Lval: 0.001688
Epoch: 90 	Ltrain: 0.001581 	Lval: 0.001553
Epoch: 95 	Ltrain: 0.001466 	Lval: 0.001438
Epoch: 100 	Ltrain: 0.001356 	Lval: 0.001324
Epoch: 105 	Ltrain: 0.001229 	Lval: 0.001229
Epoch: 110 	Ltrain: 0.001142 	Lval: 0.001107
Epoch: 115 	Ltrain: 0.001059 	Lval: 0.001048
Epoch: 120 	Ltrain: 0.001030 	Lval: 0.001021
Epoch: 125 	Ltrain: 0.000916 	Lval: 0.000900
Epoch: 130 	Ltrain: 0.000888 	Lval: 0.000877
Epoch: 135 	Ltrain: 0.000806 	Lval: 0.000809
Epoch: 140 	Ltrain: 0.000775 	Lval: 0.000795
Epoch 00142: reducing learning rate of group 0 to 6.8428e-05.
Epoch: 145 	Ltrain: 0.000634 	Lval: 0.000630
Epoch: 150 	Ltrain: 0.000598 	Lval: 0.000599
Epoch: 155 	Ltrain: 0.000587 	Lval: 0.000586
Epoch: 160 	Ltrain: 0.000577 	Lval: 0.000576
Epoch: 165 	Ltrain: 0.000566 	Lval: 0.000567
Epoch: 170 	Ltrain: 0.000558 	Lval: 0.000558
Epoch: 175 	Ltrain: 0.000551 	Lval: 0.000550
Epoch: 180 	Ltrain: 0.000546 	Lval: 0.000543
Epoch: 185 	Ltrain: 0.000535 	Lval: 0.000533
Epoch: 190 	Ltrain: 0.000531 	Lval: 0.000525
Epoch: 195 	Ltrain: 0.000519 	Lval: 0.000517
Epoch: 200 	Ltrain: 0.000512 	Lval: 0.000508
Epoch: 205 	Ltrain: 0.000503 	Lval: 0.000498
Epoch: 210 	Ltrain: 0.000496 	Lval: 0.000489
Epoch: 215 	Ltrain: 0.000487 	Lval: 0.000481
Epoch: 220 	Ltrain: 0.000480 	Lval: 0.000471
Epoch: 225 	Ltrain: 0.000468 	Lval: 0.000462
Epoch: 230 	Ltrain: 0.000459 	Lval: 0.000453
Epoch: 235 	Ltrain: 0.000450 	Lval: 0.000443
Epoch: 240 	Ltrain: 0.000443 	Lval: 0.000434
Epoch: 245 	Ltrain: 0.000431 	Lval: 0.000423
Epoch: 250 	Ltrain: 0.000423 	Lval: 0.000415
Epoch: 255 	Ltrain: 0.000413 	Lval: 0.000405
Epoch: 260 	Ltrain: 0.000405 	Lval: 0.000397
Epoch: 265 	Ltrain: 0.000395 	Lval: 0.000386
Epoch: 270 	Ltrain: 0.000386 	Lval: 0.000376
Epoch: 275 	Ltrain: 0.000380 	Lval: 0.000366
Epoch: 280 	Ltrain: 0.000367 	Lval: 0.000358
Epoch: 285 	Ltrain: 0.000359 	Lval: 0.000350
Epoch: 290 	Ltrain: 0.000351 	Lval: 0.000342
Epoch: 295 	Ltrain: 0.000345 	Lval: 0.000332
Epoch: 300 	Ltrain: 0.000336 	Lval: 0.000324
Epoch: 305 	Ltrain: 0.000326 	Lval: 0.000313
Epoch: 310 	Ltrain: 0.000318 	Lval: 0.000306
Epoch: 315 	Ltrain: 0.000313 	Lval: 0.000303
Epoch: 320 	Ltrain: 0.000308 	Lval: 0.000291
Epoch: 325 	Ltrain: 0.000301 	Lval: 0.000283
Epoch: 330 	Ltrain: 0.000292 	Lval: 0.000282
Epoch: 335 	Ltrain: 0.000283 	Lval: 0.000272
Epoch: 340 	Ltrain: 0.000275 	Lval: 0.000269
Epoch: 345 	Ltrain: 0.000268 	Lval: 0.000259
Epoch: 350 	Ltrain: 0.000269 	Lval: 0.000251
Epoch: 355 	Ltrain: 0.000258 	Lval: 0.000247
Epoch 00360: reducing learning rate of group 0 to 6.8428e-06.
Epoch: 360 	Ltrain: 0.000256 	Lval: 0.000246
Epoch: 365 	Ltrain: 0.000238 	Lval: 0.000231
Epoch: 370 	Ltrain: 0.000237 	Lval: 0.000230
Epoch: 375 	Ltrain: 0.000237 	Lval: 0.000229
Epoch: 380 	Ltrain: 0.000234 	Lval: 0.000228
Epoch: 385 	Ltrain: 0.000235 	Lval: 0.000228
EarlyStopper: stopping at epoch 388 with best_val_loss = 0.000233


	Fold 4/5
Epoch: 1 	Ltrain: 0.040247 	Lval: 0.014771
Epoch: 5 	Ltrain: 0.005745 	Lval: 0.006927
Epoch: 10 	Ltrain: 0.006191 	Lval: 0.007931
Epoch: 15 	Ltrain: 0.005251 	Lval: 0.006091
Epoch: 20 	Ltrain: 0.005547 	Lval: 0.006785
Epoch 00021: reducing learning rate of group 0 to 6.8428e-04.
Epoch: 25 	Ltrain: 0.004501 	Lval: 0.005631
Epoch: 30 	Ltrain: 0.004399 	Lval: 0.005697
Epoch: 35 	Ltrain: 0.004341 	Lval: 0.005453
Epoch 00036: reducing learning rate of group 0 to 6.8428e-05.
Epoch: 40 	Ltrain: 0.004216 	Lval: 0.005370
Epoch: 45 	Ltrain: 0.004216 	Lval: 0.005324
Epoch: 50 	Ltrain: 0.004186 	Lval: 0.005317
Epoch: 55 	Ltrain: 0.004188 	Lval: 0.005307
Epoch 00057: reducing learning rate of group 0 to 6.8428e-06.
Epoch: 60 	Ltrain: 0.004159 	Lval: 0.005287
Epoch: 65 	Ltrain: 0.004161 	Lval: 0.005287
Epoch 00069: reducing learning rate of group 0 to 6.8428e-07.
Epoch: 70 	Ltrain: 0.004146 	Lval: 0.005289
Epoch: 75 	Ltrain: 0.004139 	Lval: 0.005289
Epoch: 80 	Ltrain: 0.004161 	Lval: 0.005288
EarlyStopper: stopping at epoch 79 with best_val_loss = 0.005282


	Fold 5/5
Epoch: 1 	Ltrain: 0.032478 	Lval: 0.013389
Epoch: 5 	Ltrain: 0.005855 	Lval: 0.007616
Epoch 00010: reducing learning rate of group 0 to 6.8428e-04.
Epoch: 10 	Ltrain: 0.005492 	Lval: 0.008116
Epoch: 15 	Ltrain: 0.004914 	Lval: 0.006184
Epoch: 20 	Ltrain: 0.004871 	Lval: 0.006280
Epoch: 25 	Ltrain: 0.004783 	Lval: 0.006184
Epoch: 30 	Ltrain: 0.004709 	Lval: 0.005806
Epoch: 35 	Ltrain: 0.004642 	Lval: 0.005808
Epoch: 40 	Ltrain: 0.004399 	Lval: 0.005544
Epoch: 45 	Ltrain: 0.004438 	Lval: 0.005239
Epoch: 50 	Ltrain: 0.004210 	Lval: 0.005019
Epoch: 55 	Ltrain: 0.003949 	Lval: 0.004663
Epoch: 60 	Ltrain: 0.003878 	Lval: 0.004652
Epoch: 65 	Ltrain: 0.003651 	Lval: 0.004301
Epoch: 70 	Ltrain: 0.003557 	Lval: 0.004210
Epoch: 75 	Ltrain: 0.003618 	Lval: 0.003957
Epoch: 80 	Ltrain: 0.003233 	Lval: 0.003668
Epoch 00084: reducing learning rate of group 0 to 6.8428e-05.
Epoch: 85 	Ltrain: 0.002884 	Lval: 0.003292
Epoch: 90 	Ltrain: 0.002706 	Lval: 0.003183
Epoch: 95 	Ltrain: 0.002682 	Lval: 0.003133
Epoch: 100 	Ltrain: 0.002638 	Lval: 0.003095
Epoch: 105 	Ltrain: 0.002600 	Lval: 0.003050
Epoch: 110 	Ltrain: 0.002557 	Lval: 0.002977
Epoch: 115 	Ltrain: 0.002528 	Lval: 0.002914
Epoch: 120 	Ltrain: 0.002488 	Lval: 0.002884
Epoch: 125 	Ltrain: 0.002454 	Lval: 0.002808
Epoch: 130 	Ltrain: 0.002419 	Lval: 0.002763
Epoch: 135 	Ltrain: 0.002372 	Lval: 0.002709
Epoch: 140 	Ltrain: 0.002334 	Lval: 0.002654
Epoch: 145 	Ltrain: 0.002287 	Lval: 0.002575
Epoch: 150 	Ltrain: 0.002252 	Lval: 0.002531
Epoch: 155 	Ltrain: 0.002213 	Lval: 0.002466
Epoch: 160 	Ltrain: 0.002172 	Lval: 0.002427
Epoch: 165 	Ltrain: 0.002131 	Lval: 0.002381
Epoch: 170 	Ltrain: 0.002093 	Lval: 0.002337
Epoch: 175 	Ltrain: 0.002045 	Lval: 0.002270
Epoch: 180 	Ltrain: 0.002015 	Lval: 0.002194
Epoch: 185 	Ltrain: 0.001980 	Lval: 0.002168
Epoch: 190 	Ltrain: 0.001932 	Lval: 0.002124
Epoch: 195 	Ltrain: 0.001898 	Lval: 0.002098
Epoch: 200 	Ltrain: 0.001850 	Lval: 0.002007
Epoch: 205 	Ltrain: 0.001810 	Lval: 0.001971
Epoch: 210 	Ltrain: 0.001774 	Lval: 0.001931
Epoch: 215 	Ltrain: 0.001748 	Lval: 0.001866
Epoch: 220 	Ltrain: 0.001701 	Lval: 0.001818
Epoch: 225 	Ltrain: 0.001676 	Lval: 0.001789
Epoch: 230 	Ltrain: 0.001637 	Lval: 0.001780
Epoch: 235 	Ltrain: 0.001605 	Lval: 0.001699
Epoch: 240 	Ltrain: 0.001555 	Lval: 0.001665
Epoch: 245 	Ltrain: 0.001517 	Lval: 0.001606
Epoch: 250 	Ltrain: 0.001483 	Lval: 0.001583
Epoch: 255 	Ltrain: 0.001449 	Lval: 0.001539
Epoch: 260 	Ltrain: 0.001424 	Lval: 0.001493
Epoch: 265 	Ltrain: 0.001385 	Lval: 0.001452
Epoch: 270 	Ltrain: 0.001356 	Lval: 0.001428
Epoch: 275 	Ltrain: 0.001324 	Lval: 0.001390
Epoch: 280 	Ltrain: 0.001300 	Lval: 0.001351
Epoch: 285 	Ltrain: 0.001267 	Lval: 0.001338
Epoch: 290 	Ltrain: 0.001245 	Lval: 0.001291
Epoch: 295 	Ltrain: 0.001212 	Lval: 0.001268
Epoch: 300 	Ltrain: 0.001188 	Lval: 0.001230
Epoch: 305 	Ltrain: 0.001168 	Lval: 0.001217
Epoch: 310 	Ltrain: 0.001128 	Lval: 0.001182
Epoch: 315 	Ltrain: 0.001106 	Lval: 0.001150
Epoch: 320 	Ltrain: 0.001086 	Lval: 0.001123
Epoch: 325 	Ltrain: 0.001068 	Lval: 0.001096
Epoch: 330 	Ltrain: 0.001049 	Lval: 0.001082
Epoch: 335 	Ltrain: 0.001016 	Lval: 0.001041
Epoch: 340 	Ltrain: 0.001002 	Lval: 0.001025
Epoch: 345 	Ltrain: 0.000992 	Lval: 0.001039
Epoch: 350 	Ltrain: 0.000956 	Lval: 0.000984
Epoch: 355 	Ltrain: 0.000947 	Lval: 0.000995
Epoch: 360 	Ltrain: 0.000923 	Lval: 0.000953
Epoch: 365 	Ltrain: 0.000907 	Lval: 0.000932
Epoch: 370 	Ltrain: 0.000890 	Lval: 0.000900
Epoch: 375 	Ltrain: 0.000877 	Lval: 0.000901
Epoch: 380 	Ltrain: 0.000855 	Lval: 0.000880
Epoch: 385 	Ltrain: 0.000836 	Lval: 0.000853
Epoch: 390 	Ltrain: 0.000825 	Lval: 0.000833
Epoch: 395 	Ltrain: 0.000809 	Lval: 0.000825
Epoch: 400 	Ltrain: 0.000790 	Lval: 0.000803
Epoch 00402: reducing learning rate of group 0 to 6.8428e-06.
Epoch: 405 	Ltrain: 0.000755 	Lval: 0.000771
Epoch: 410 	Ltrain: 0.000750 	Lval: 0.000765
Epoch: 415 	Ltrain: 0.000746 	Lval: 0.000762
Epoch: 420 	Ltrain: 0.000744 	Lval: 0.000760
Epoch: 425 	Ltrain: 0.000742 	Lval: 0.000758
Epoch: 430 	Ltrain: 0.000740 	Lval: 0.000756
Epoch: 435 	Ltrain: 0.000737 	Lval: 0.000753
Epoch: 440 	Ltrain: 0.000738 	Lval: 0.000752
Epoch: 445 	Ltrain: 0.000739 	Lval: 0.000750
Epoch: 450 	Ltrain: 0.000731 	Lval: 0.000747
Epoch: 455 	Ltrain: 0.000732 	Lval: 0.000745
Epoch: 460 	Ltrain: 0.000729 	Lval: 0.000743
Epoch: 465 	Ltrain: 0.000727 	Lval: 0.000741
Epoch: 470 	Ltrain: 0.000725 	Lval: 0.000739
Epoch: 475 	Ltrain: 0.000723 	Lval: 0.000737
Epoch: 480 	Ltrain: 0.000720 	Lval: 0.000734
Epoch: 485 	Ltrain: 0.000718 	Lval: 0.000732
Epoch: 490 	Ltrain: 0.000716 	Lval: 0.000730
Epoch: 495 	Ltrain: 0.000714 	Lval: 0.000728
Epoch: 500 	Ltrain: 0.000711 	Lval: 0.000725
Epoch: 505 	Ltrain: 0.000709 	Lval: 0.000723
Epoch: 510 	Ltrain: 0.000707 	Lval: 0.000721
Epoch: 515 	Ltrain: 0.000705 	Lval: 0.000719
Epoch: 520 	Ltrain: 0.000703 	Lval: 0.000716
Epoch: 525 	Ltrain: 0.000701 	Lval: 0.000714
Epoch: 530 	Ltrain: 0.000697 	Lval: 0.000711
Epoch: 535 	Ltrain: 0.000696 	Lval: 0.000708
Epoch: 540 	Ltrain: 0.000694 	Lval: 0.000706
Epoch: 545 	Ltrain: 0.000693 	Lval: 0.000705
Epoch: 550 	Ltrain: 0.000688 	Lval: 0.000701
Epoch: 555 	Ltrain: 0.000688 	Lval: 0.000699
Epoch: 560 	Ltrain: 0.000686 	Lval: 0.000697
Epoch: 565 	Ltrain: 0.000683 	Lval: 0.000695
Epoch: 570 	Ltrain: 0.000681 	Lval: 0.000692
Epoch: 575 	Ltrain: 0.000678 	Lval: 0.000691
Epoch: 580 	Ltrain: 0.000681 	Lval: 0.000687
Epoch: 585 	Ltrain: 0.000674 	Lval: 0.000685
Epoch: 590 	Ltrain: 0.000672 	Lval: 0.000683
Epoch: 595 	Ltrain: 0.000669 	Lval: 0.000681
Epoch: 600 	Ltrain: 0.000668 	Lval: 0.000681
Epoch: 605 	Ltrain: 0.000665 	Lval: 0.000676
Epoch: 610 	Ltrain: 0.000664 	Lval: 0.000675
Epoch: 615 	Ltrain: 0.000662 	Lval: 0.000672
Epoch: 620 	Ltrain: 0.000661 	Lval: 0.000670
Epoch: 625 	Ltrain: 0.000656 	Lval: 0.000668
Epoch: 630 	Ltrain: 0.000656 	Lval: 0.000665
Epoch: 635 	Ltrain: 0.000652 	Lval: 0.000663
Epoch: 640 	Ltrain: 0.000650 	Lval: 0.000661
Epoch: 645 	Ltrain: 0.000648 	Lval: 0.000658
Epoch: 650 	Ltrain: 0.000646 	Lval: 0.000657
Epoch: 655 	Ltrain: 0.000646 	Lval: 0.000654
Epoch: 660 	Ltrain: 0.000642 	Lval: 0.000653
Epoch: 665 	Ltrain: 0.000640 	Lval: 0.000650
Epoch: 670 	Ltrain: 0.000637 	Lval: 0.000648
Epoch: 675 	Ltrain: 0.000636 	Lval: 0.000646
Epoch: 680 	Ltrain: 0.000633 	Lval: 0.000643
Epoch: 685 	Ltrain: 0.000632 	Lval: 0.000641
Epoch: 690 	Ltrain: 0.000630 	Lval: 0.000640
Epoch: 695 	Ltrain: 0.000628 	Lval: 0.000637
Epoch: 700 	Ltrain: 0.000625 	Lval: 0.000635
Epoch: 705 	Ltrain: 0.000626 	Lval: 0.000633
Epoch: 710 	Ltrain: 0.000622 	Lval: 0.000631
Epoch: 715 	Ltrain: 0.000621 	Lval: 0.000629
Epoch: 720 	Ltrain: 0.000617 	Lval: 0.000626
Epoch: 725 	Ltrain: 0.000615 	Lval: 0.000624
Epoch: 730 	Ltrain: 0.000614 	Lval: 0.000623
Epoch: 735 	Ltrain: 0.000612 	Lval: 0.000620
Epoch: 740 	Ltrain: 0.000610 	Lval: 0.000618
Epoch: 745 	Ltrain: 0.000609 	Lval: 0.000616
Epoch: 750 	Ltrain: 0.000606 	Lval: 0.000614
Epoch: 755 	Ltrain: 0.000603 	Lval: 0.000612
Epoch: 760 	Ltrain: 0.000602 	Lval: 0.000610
Epoch: 765 	Ltrain: 0.000600 	Lval: 0.000607
Epoch: 770 	Ltrain: 0.000598 	Lval: 0.000606
Epoch: 775 	Ltrain: 0.000597 	Lval: 0.000604
Epoch: 780 	Ltrain: 0.000593 	Lval: 0.000601
Epoch: 785 	Ltrain: 0.000592 	Lval: 0.000600
Epoch: 790 	Ltrain: 0.000591 	Lval: 0.000598
Epoch: 795 	Ltrain: 0.000592 	Lval: 0.000595
Epoch: 800 	Ltrain: 0.000589 	Lval: 0.000594
Epoch: 805 	Ltrain: 0.000584 	Lval: 0.000591
Epoch: 810 	Ltrain: 0.000582 	Lval: 0.000590
Epoch: 815 	Ltrain: 0.000580 	Lval: 0.000587
Epoch: 820 	Ltrain: 0.000579 	Lval: 0.000586
Epoch: 825 	Ltrain: 0.000577 	Lval: 0.000583
Epoch: 830 	Ltrain: 0.000574 	Lval: 0.000581
Epoch: 835 	Ltrain: 0.000573 	Lval: 0.000579
Epoch: 840 	Ltrain: 0.000569 	Lval: 0.000578
Epoch: 845 	Ltrain: 0.000568 	Lval: 0.000576
Epoch: 850 	Ltrain: 0.000567 	Lval: 0.000573
Epoch: 855 	Ltrain: 0.000565 	Lval: 0.000572
Epoch: 860 	Ltrain: 0.000564 	Lval: 0.000570
EarlyStopper: stopping at epoch 859 with best_val_loss = 0.000580

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00363631498855339
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 7.954899550574365e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.039346 	Lval: 0.016874
Epoch: 5 	Ltrain: 0.011480 	Lval: 0.009648
Epoch: 10 	Ltrain: 0.007684 	Lval: 0.007374
Epoch 00014: reducing learning rate of group 0 to 3.6363e-04.
Epoch: 15 	Ltrain: 0.008117 	Lval: 0.007255
Epoch: 20 	Ltrain: 0.006815 	Lval: 0.006879
Epoch: 25 	Ltrain: 0.007122 	Lval: 0.006825
Epoch: 30 	Ltrain: 0.006591 	Lval: 0.006755
Epoch: 35 	Ltrain: 0.006921 	Lval: 0.006692
Epoch: 40 	Ltrain: 0.006831 	Lval: 0.006644
Epoch: 45 	Ltrain: 0.007535 	Lval: 0.006592
Epoch: 50 	Ltrain: 0.006505 	Lval: 0.006501
Epoch: 55 	Ltrain: 0.006605 	Lval: 0.006446
Epoch: 60 	Ltrain: 0.006367 	Lval: 0.006400
Epoch: 65 	Ltrain: 0.006338 	Lval: 0.006357
Epoch: 70 	Ltrain: 0.006056 	Lval: 0.006301
Epoch: 75 	Ltrain: 0.006487 	Lval: 0.006217
Epoch: 80 	Ltrain: 0.006560 	Lval: 0.006168
Epoch: 85 	Ltrain: 0.005948 	Lval: 0.006035
Epoch: 90 	Ltrain: 0.005963 	Lval: 0.005972
Epoch: 95 	Ltrain: 0.005764 	Lval: 0.005987
Epoch 00098: reducing learning rate of group 0 to 3.6363e-05.
Epoch: 100 	Ltrain: 0.006188 	Lval: 0.005854
Epoch: 105 	Ltrain: 0.005573 	Lval: 0.005847
Epoch: 110 	Ltrain: 0.006060 	Lval: 0.005835
Epoch: 115 	Ltrain: 0.005695 	Lval: 0.005829
Epoch 00118: reducing learning rate of group 0 to 3.6363e-06.
Epoch: 120 	Ltrain: 0.006054 	Lval: 0.005822
Epoch: 125 	Ltrain: 0.006123 	Lval: 0.005816
Epoch 00130: reducing learning rate of group 0 to 3.6363e-07.
Epoch: 130 	Ltrain: 0.005716 	Lval: 0.005817
Epoch: 135 	Ltrain: 0.005675 	Lval: 0.005817
Epoch: 140 	Ltrain: 0.005681 	Lval: 0.005817
Epoch 00142: reducing learning rate of group 0 to 3.6363e-08.
Epoch: 145 	Ltrain: 0.005773 	Lval: 0.005817
EarlyStopper: stopping at epoch 145 with best_val_loss = 0.005818


	Fold 2/5
Epoch: 1 	Ltrain: 0.021662 	Lval: 0.014602
Epoch: 5 	Ltrain: 0.008016 	Lval: 0.007548
Epoch: 10 	Ltrain: 0.006680 	Lval: 0.008460
Epoch: 15 	Ltrain: 0.006600 	Lval: 0.006590
Epoch 00020: reducing learning rate of group 0 to 3.6363e-04.
Epoch: 20 	Ltrain: 0.006603 	Lval: 0.006600
Epoch: 25 	Ltrain: 0.005753 	Lval: 0.006009
Epoch: 30 	Ltrain: 0.005732 	Lval: 0.005950
Epoch: 35 	Ltrain: 0.005547 	Lval: 0.005885
Epoch: 40 	Ltrain: 0.005527 	Lval: 0.005838
Epoch: 45 	Ltrain: 0.005525 	Lval: 0.005797
Epoch: 50 	Ltrain: 0.005472 	Lval: 0.005681
Epoch: 55 	Ltrain: 0.005465 	Lval: 0.005660
Epoch 00060: reducing learning rate of group 0 to 3.6363e-05.
Epoch: 60 	Ltrain: 0.005456 	Lval: 0.005649
Epoch: 65 	Ltrain: 0.005225 	Lval: 0.005577
Epoch: 70 	Ltrain: 0.005235 	Lval: 0.005564
Epoch: 75 	Ltrain: 0.005385 	Lval: 0.005556
Epoch 00078: reducing learning rate of group 0 to 3.6363e-06.
Epoch: 80 	Ltrain: 0.005161 	Lval: 0.005548
Epoch: 85 	Ltrain: 0.005149 	Lval: 0.005544
Epoch: 90 	Ltrain: 0.005201 	Lval: 0.005542
Epoch: 95 	Ltrain: 0.005257 	Lval: 0.005539
EarlyStopper: stopping at epoch 97 with best_val_loss = 0.005547


	Fold 3/5
Epoch: 1 	Ltrain: 0.021501 	Lval: 0.011708
Epoch: 5 	Ltrain: 0.006583 	Lval: 0.007503
Epoch 00008: reducing learning rate of group 0 to 3.6363e-04.
Epoch: 10 	Ltrain: 0.005876 	Lval: 0.006717
Epoch: 15 	Ltrain: 0.005770 	Lval: 0.006589
Epoch: 20 	Ltrain: 0.005816 	Lval: 0.006488
Epoch: 25 	Ltrain: 0.005749 	Lval: 0.006450
Epoch: 30 	Ltrain: 0.005680 	Lval: 0.006308
Epoch: 35 	Ltrain: 0.005593 	Lval: 0.006265
Epoch: 40 	Ltrain: 0.005536 	Lval: 0.006128
Epoch: 45 	Ltrain: 0.005460 	Lval: 0.006124
Epoch 00047: reducing learning rate of group 0 to 3.6363e-05.
Epoch: 50 	Ltrain: 0.005277 	Lval: 0.005935
Epoch: 55 	Ltrain: 0.005281 	Lval: 0.005928
Epoch: 60 	Ltrain: 0.005308 	Lval: 0.005904
Epoch: 65 	Ltrain: 0.005373 	Lval: 0.005899
Epoch 00066: reducing learning rate of group 0 to 3.6363e-06.
Epoch: 70 	Ltrain: 0.005244 	Lval: 0.005894
Epoch: 75 	Ltrain: 0.005250 	Lval: 0.005893
Epoch 00078: reducing learning rate of group 0 to 3.6363e-07.
Epoch: 80 	Ltrain: 0.005217 	Lval: 0.005891
Epoch: 85 	Ltrain: 0.005283 	Lval: 0.005892
EarlyStopper: stopping at epoch 85 with best_val_loss = 0.005893


	Fold 4/5
Epoch: 1 	Ltrain: 0.018631 	Lval: 0.010368
Epoch: 5 	Ltrain: 0.006388 	Lval: 0.007287
Epoch: 10 	Ltrain: 0.005400 	Lval: 0.006653
Epoch: 15 	Ltrain: 0.005261 	Lval: 0.006292
Epoch: 20 	Ltrain: 0.004946 	Lval: 0.006075
Epoch: 25 	Ltrain: 0.004683 	Lval: 0.005268
Epoch: 30 	Ltrain: 0.004382 	Lval: 0.005170
Epoch: 35 	Ltrain: 0.004407 	Lval: 0.004986
Epoch: 40 	Ltrain: 0.004120 	Lval: 0.004571
Epoch 00041: reducing learning rate of group 0 to 3.6363e-04.
Epoch: 45 	Ltrain: 0.003069 	Lval: 0.003543
Epoch: 50 	Ltrain: 0.002879 	Lval: 0.003305
Epoch: 55 	Ltrain: 0.002723 	Lval: 0.003071
Epoch: 60 	Ltrain: 0.002571 	Lval: 0.002881
Epoch: 65 	Ltrain: 0.002397 	Lval: 0.002608
Epoch: 70 	Ltrain: 0.002228 	Lval: 0.002388
Epoch: 75 	Ltrain: 0.002047 	Lval: 0.002179
Epoch: 80 	Ltrain: 0.001916 	Lval: 0.001971
Epoch: 85 	Ltrain: 0.001731 	Lval: 0.001768
Epoch: 90 	Ltrain: 0.001597 	Lval: 0.001634
Epoch: 95 	Ltrain: 0.001485 	Lval: 0.001482
Epoch: 100 	Ltrain: 0.001379 	Lval: 0.001406
Epoch: 105 	Ltrain: 0.001258 	Lval: 0.001254
Epoch: 110 	Ltrain: 0.001162 	Lval: 0.001201
Epoch: 115 	Ltrain: 0.001098 	Lval: 0.001056
Epoch: 120 	Ltrain: 0.001051 	Lval: 0.001049
Epoch: 125 	Ltrain: 0.000912 	Lval: 0.000876
Epoch 00129: reducing learning rate of group 0 to 3.6363e-05.
Epoch: 130 	Ltrain: 0.000833 	Lval: 0.000794
Epoch: 135 	Ltrain: 0.000756 	Lval: 0.000748
Epoch: 140 	Ltrain: 0.000744 	Lval: 0.000736
Epoch: 145 	Ltrain: 0.000734 	Lval: 0.000725
Epoch: 150 	Ltrain: 0.000727 	Lval: 0.000716
Epoch: 155 	Ltrain: 0.000717 	Lval: 0.000706
Epoch: 160 	Ltrain: 0.000708 	Lval: 0.000697
Epoch: 165 	Ltrain: 0.000700 	Lval: 0.000687
Epoch: 170 	Ltrain: 0.000691 	Lval: 0.000677
Epoch: 175 	Ltrain: 0.000682 	Lval: 0.000669
Epoch: 180 	Ltrain: 0.000673 	Lval: 0.000658
Epoch: 185 	Ltrain: 0.000664 	Lval: 0.000649
Epoch: 190 	Ltrain: 0.000655 	Lval: 0.000637
Epoch: 195 	Ltrain: 0.000645 	Lval: 0.000629
Epoch: 200 	Ltrain: 0.000637 	Lval: 0.000620
Epoch: 205 	Ltrain: 0.000627 	Lval: 0.000610
Epoch: 210 	Ltrain: 0.000618 	Lval: 0.000600
Epoch: 215 	Ltrain: 0.000608 	Lval: 0.000590
Epoch: 220 	Ltrain: 0.000598 	Lval: 0.000580
Epoch: 225 	Ltrain: 0.000588 	Lval: 0.000570
Epoch: 230 	Ltrain: 0.000580 	Lval: 0.000559
Epoch: 235 	Ltrain: 0.000572 	Lval: 0.000549
Epoch: 240 	Ltrain: 0.000560 	Lval: 0.000540
Epoch: 245 	Ltrain: 0.000555 	Lval: 0.000530
Epoch: 250 	Ltrain: 0.000542 	Lval: 0.000522
Epoch: 255 	Ltrain: 0.000535 	Lval: 0.000512
Epoch: 260 	Ltrain: 0.000524 	Lval: 0.000503
Epoch: 265 	Ltrain: 0.000516 	Lval: 0.000494
Epoch: 270 	Ltrain: 0.000506 	Lval: 0.000485
Epoch: 275 	Ltrain: 0.000498 	Lval: 0.000477
Epoch: 280 	Ltrain: 0.000490 	Lval: 0.000467
Epoch: 285 	Ltrain: 0.000481 	Lval: 0.000459
Epoch: 290 	Ltrain: 0.000474 	Lval: 0.000452
Epoch: 295 	Ltrain: 0.000467 	Lval: 0.000443
Epoch: 300 	Ltrain: 0.000458 	Lval: 0.000435
Epoch: 305 	Ltrain: 0.000449 	Lval: 0.000429
Epoch: 310 	Ltrain: 0.000442 	Lval: 0.000422
Epoch: 315 	Ltrain: 0.000435 	Lval: 0.000413
Epoch: 320 	Ltrain: 0.000428 	Lval: 0.000405
Epoch: 325 	Ltrain: 0.000420 	Lval: 0.000400
Epoch: 330 	Ltrain: 0.000413 	Lval: 0.000393
Epoch: 335 	Ltrain: 0.000407 	Lval: 0.000386
Epoch: 340 	Ltrain: 0.000401 	Lval: 0.000378
Epoch: 345 	Ltrain: 0.000392 	Lval: 0.000372
Epoch: 350 	Ltrain: 0.000385 	Lval: 0.000365
Epoch: 355 	Ltrain: 0.000380 	Lval: 0.000361
Epoch: 360 	Ltrain: 0.000374 	Lval: 0.000352
Epoch: 365 	Ltrain: 0.000366 	Lval: 0.000346
Epoch: 370 	Ltrain: 0.000360 	Lval: 0.000340
Epoch: 375 	Ltrain: 0.000353 	Lval: 0.000335
Epoch: 380 	Ltrain: 0.000347 	Lval: 0.000327
Epoch: 385 	Ltrain: 0.000343 	Lval: 0.000324
Epoch: 390 	Ltrain: 0.000337 	Lval: 0.000318
Epoch: 395 	Ltrain: 0.000332 	Lval: 0.000311
Epoch: 400 	Ltrain: 0.000325 	Lval: 0.000306
Epoch: 405 	Ltrain: 0.000320 	Lval: 0.000300
Epoch: 410 	Ltrain: 0.000317 	Lval: 0.000297
Epoch: 415 	Ltrain: 0.000311 	Lval: 0.000292
Epoch: 420 	Ltrain: 0.000305 	Lval: 0.000285
Epoch: 425 	Ltrain: 0.000300 	Lval: 0.000281
Epoch: 430 	Ltrain: 0.000294 	Lval: 0.000276
Epoch: 435 	Ltrain: 0.000292 	Lval: 0.000274
Epoch: 440 	Ltrain: 0.000287 	Lval: 0.000267
Epoch: 445 	Ltrain: 0.000281 	Lval: 0.000264
Epoch: 450 	Ltrain: 0.000276 	Lval: 0.000258
Epoch: 455 	Ltrain: 0.000273 	Lval: 0.000254
Epoch: 460 	Ltrain: 0.000269 	Lval: 0.000252
Epoch 00464: reducing learning rate of group 0 to 3.6363e-06.
Epoch: 465 	Ltrain: 0.000265 	Lval: 0.000247
Epoch: 470 	Ltrain: 0.000259 	Lval: 0.000243
Epoch: 475 	Ltrain: 0.000258 	Lval: 0.000242
Epoch: 480 	Ltrain: 0.000259 	Lval: 0.000242
Epoch: 485 	Ltrain: 0.000258 	Lval: 0.000241
EarlyStopper: stopping at epoch 488 with best_val_loss = 0.000247


	Fold 5/5
Epoch: 1 	Ltrain: 0.018775 	Lval: 0.012949
Epoch: 5 	Ltrain: 0.005810 	Lval: 0.008094
Epoch: 10 	Ltrain: 0.005583 	Lval: 0.007089
Epoch: 15 	Ltrain: 0.005353 	Lval: 0.006347
Epoch: 20 	Ltrain: 0.005029 	Lval: 0.006325
Epoch 00023: reducing learning rate of group 0 to 3.6363e-04.
Epoch: 25 	Ltrain: 0.004296 	Lval: 0.005393
Epoch: 30 	Ltrain: 0.004184 	Lval: 0.005304
Epoch: 35 	Ltrain: 0.004091 	Lval: 0.005139
Epoch: 40 	Ltrain: 0.004004 	Lval: 0.005041
Epoch: 45 	Ltrain: 0.003894 	Lval: 0.004831
Epoch: 50 	Ltrain: 0.003794 	Lval: 0.004755
Epoch: 55 	Ltrain: 0.003672 	Lval: 0.004331
Epoch: 60 	Ltrain: 0.003556 	Lval: 0.004071
Epoch: 65 	Ltrain: 0.003457 	Lval: 0.004063
Epoch: 70 	Ltrain: 0.003416 	Lval: 0.004358
Epoch 00071: reducing learning rate of group 0 to 3.6363e-05.
Epoch: 75 	Ltrain: 0.003137 	Lval: 0.003741
Epoch: 80 	Ltrain: 0.003111 	Lval: 0.003698
Epoch: 85 	Ltrain: 0.003096 	Lval: 0.003679
Epoch: 90 	Ltrain: 0.003083 	Lval: 0.003641
Epoch: 95 	Ltrain: 0.003053 	Lval: 0.003659
Epoch 00097: reducing learning rate of group 0 to 3.6363e-06.
Epoch: 100 	Ltrain: 0.003034 	Lval: 0.003631
Epoch: 105 	Ltrain: 0.003030 	Lval: 0.003627
Epoch: 110 	Ltrain: 0.003034 	Lval: 0.003626
Epoch: 115 	Ltrain: 0.003034 	Lval: 0.003620
EarlyStopper: stopping at epoch 116 with best_val_loss = 0.003624

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005077043052252252
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.8923167452868376e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.088159 	Lval: 0.021392
Epoch: 5 	Ltrain: 0.016541 	Lval: 0.014946
Epoch: 10 	Ltrain: 0.009231 	Lval: 0.008359
Epoch: 15 	Ltrain: 0.007627 	Lval: 0.007700
Epoch: 20 	Ltrain: 0.008071 	Lval: 0.009557
Epoch 00022: reducing learning rate of group 0 to 5.0770e-04.
Epoch: 25 	Ltrain: 0.006391 	Lval: 0.006574
Epoch: 30 	Ltrain: 0.006473 	Lval: 0.006399
Epoch: 35 	Ltrain: 0.006597 	Lval: 0.006316
Epoch: 40 	Ltrain: 0.006759 	Lval: 0.006260
Epoch: 45 	Ltrain: 0.006182 	Lval: 0.006177
Epoch: 50 	Ltrain: 0.006146 	Lval: 0.006190
Epoch: 55 	Ltrain: 0.006194 	Lval: 0.006045
Epoch: 60 	Ltrain: 0.005876 	Lval: 0.005971
Epoch: 65 	Ltrain: 0.005756 	Lval: 0.005892
Epoch: 70 	Ltrain: 0.005850 	Lval: 0.005816
Epoch: 75 	Ltrain: 0.005712 	Lval: 0.005826
Epoch: 80 	Ltrain: 0.005726 	Lval: 0.005565
Epoch: 85 	Ltrain: 0.005465 	Lval: 0.005457
Epoch: 90 	Ltrain: 0.005708 	Lval: 0.005344
Epoch: 95 	Ltrain: 0.005492 	Lval: 0.005136
Epoch: 100 	Ltrain: 0.004971 	Lval: 0.004931
Epoch: 105 	Ltrain: 0.004920 	Lval: 0.004745
Epoch: 110 	Ltrain: 0.005094 	Lval: 0.004660
Epoch: 115 	Ltrain: 0.004936 	Lval: 0.004514
Epoch: 120 	Ltrain: 0.004975 	Lval: 0.004621
Epoch: 125 	Ltrain: 0.004368 	Lval: 0.004302
Epoch: 130 	Ltrain: 0.004356 	Lval: 0.004076
Epoch: 135 	Ltrain: 0.004194 	Lval: 0.003972
Epoch: 140 	Ltrain: 0.004075 	Lval: 0.003880
Epoch: 145 	Ltrain: 0.003913 	Lval: 0.003680
Epoch: 150 	Ltrain: 0.003734 	Lval: 0.003429
Epoch: 155 	Ltrain: 0.003410 	Lval: 0.003384
Epoch: 160 	Ltrain: 0.003219 	Lval: 0.003092
Epoch: 165 	Ltrain: 0.002970 	Lval: 0.002931
Epoch 00168: reducing learning rate of group 0 to 5.0770e-05.
Epoch: 170 	Ltrain: 0.002764 	Lval: 0.002756
Epoch: 175 	Ltrain: 0.002532 	Lval: 0.002642
Epoch: 180 	Ltrain: 0.002559 	Lval: 0.002608
Epoch: 185 	Ltrain: 0.002602 	Lval: 0.002582
Epoch: 190 	Ltrain: 0.002511 	Lval: 0.002555
Epoch: 195 	Ltrain: 0.002569 	Lval: 0.002538
Epoch: 200 	Ltrain: 0.002571 	Lval: 0.002505
Epoch: 205 	Ltrain: 0.002594 	Lval: 0.002487
Epoch: 210 	Ltrain: 0.002512 	Lval: 0.002465
Epoch: 215 	Ltrain: 0.002382 	Lval: 0.002442
Epoch: 220 	Ltrain: 0.002470 	Lval: 0.002424
Epoch: 225 	Ltrain: 0.002407 	Lval: 0.002394
Epoch: 230 	Ltrain: 0.002355 	Lval: 0.002378
Epoch: 235 	Ltrain: 0.002324 	Lval: 0.002358
Epoch: 240 	Ltrain: 0.002288 	Lval: 0.002327
Epoch: 245 	Ltrain: 0.002398 	Lval: 0.002302
Epoch: 250 	Ltrain: 0.002202 	Lval: 0.002282
Epoch: 255 	Ltrain: 0.002302 	Lval: 0.002259
Epoch: 260 	Ltrain: 0.002204 	Lval: 0.002251
Epoch: 265 	Ltrain: 0.002239 	Lval: 0.002215
Epoch: 270 	Ltrain: 0.002196 	Lval: 0.002193
Epoch: 275 	Ltrain: 0.002178 	Lval: 0.002178
Epoch: 280 	Ltrain: 0.002128 	Lval: 0.002145
Epoch: 285 	Ltrain: 0.002156 	Lval: 0.002129
Epoch: 290 	Ltrain: 0.002150 	Lval: 0.002101
Epoch: 295 	Ltrain: 0.002149 	Lval: 0.002075
Epoch: 300 	Ltrain: 0.002007 	Lval: 0.002054
Epoch: 305 	Ltrain: 0.002109 	Lval: 0.002034
Epoch: 310 	Ltrain: 0.001962 	Lval: 0.002007
Epoch: 315 	Ltrain: 0.001987 	Lval: 0.001987
Epoch: 320 	Ltrain: 0.001961 	Lval: 0.001960
Epoch: 325 	Ltrain: 0.001965 	Lval: 0.001936
Epoch: 330 	Ltrain: 0.001911 	Lval: 0.001918
Epoch: 335 	Ltrain: 0.001878 	Lval: 0.001896
Epoch: 340 	Ltrain: 0.001918 	Lval: 0.001878
Epoch: 345 	Ltrain: 0.001790 	Lval: 0.001848
Epoch: 350 	Ltrain: 0.001832 	Lval: 0.001832
Epoch: 355 	Ltrain: 0.001779 	Lval: 0.001809
Epoch: 360 	Ltrain: 0.001839 	Lval: 0.001784
Epoch: 365 	Ltrain: 0.001811 	Lval: 0.001770
Epoch: 370 	Ltrain: 0.001712 	Lval: 0.001742
Epoch: 375 	Ltrain: 0.001755 	Lval: 0.001721
Epoch: 380 	Ltrain: 0.001746 	Lval: 0.001706
Epoch: 385 	Ltrain: 0.001697 	Lval: 0.001683
Epoch: 390 	Ltrain: 0.001723 	Lval: 0.001658
Epoch: 395 	Ltrain: 0.001687 	Lval: 0.001641
Epoch: 400 	Ltrain: 0.001637 	Lval: 0.001619
Epoch: 405 	Ltrain: 0.001591 	Lval: 0.001600
Epoch: 410 	Ltrain: 0.001572 	Lval: 0.001575
Epoch: 415 	Ltrain: 0.001657 	Lval: 0.001566
Epoch: 420 	Ltrain: 0.001604 	Lval: 0.001544
Epoch: 425 	Ltrain: 0.001564 	Lval: 0.001525
Epoch: 430 	Ltrain: 0.001530 	Lval: 0.001507
Epoch: 435 	Ltrain: 0.001528 	Lval: 0.001481
Epoch: 440 	Ltrain: 0.001483 	Lval: 0.001463
Epoch: 445 	Ltrain: 0.001484 	Lval: 0.001447
Epoch: 450 	Ltrain: 0.001483 	Lval: 0.001429
Epoch: 455 	Ltrain: 0.001434 	Lval: 0.001412
Epoch: 460 	Ltrain: 0.001466 	Lval: 0.001398
Epoch: 465 	Ltrain: 0.001377 	Lval: 0.001379
Epoch: 470 	Ltrain: 0.001410 	Lval: 0.001360
Epoch: 475 	Ltrain: 0.001357 	Lval: 0.001345
Epoch: 480 	Ltrain: 0.001331 	Lval: 0.001320
Epoch: 485 	Ltrain: 0.001321 	Lval: 0.001305
Epoch: 490 	Ltrain: 0.001310 	Lval: 0.001294
Epoch: 495 	Ltrain: 0.001262 	Lval: 0.001271
Epoch: 500 	Ltrain: 0.001235 	Lval: 0.001255
Epoch: 505 	Ltrain: 0.001356 	Lval: 0.001242
Epoch: 510 	Ltrain: 0.001257 	Lval: 0.001223
Epoch: 515 	Ltrain: 0.001242 	Lval: 0.001216
Epoch: 520 	Ltrain: 0.001207 	Lval: 0.001191
Epoch: 525 	Ltrain: 0.001196 	Lval: 0.001180
Epoch: 530 	Ltrain: 0.001149 	Lval: 0.001157
Epoch: 535 	Ltrain: 0.001157 	Lval: 0.001151
Epoch: 540 	Ltrain: 0.001122 	Lval: 0.001132
Epoch: 545 	Ltrain: 0.001108 	Lval: 0.001115
Epoch: 550 	Ltrain: 0.001138 	Lval: 0.001104
Epoch: 555 	Ltrain: 0.001085 	Lval: 0.001091
Epoch: 560 	Ltrain: 0.001074 	Lval: 0.001080
Epoch: 565 	Ltrain: 0.001071 	Lval: 0.001057
Epoch: 570 	Ltrain: 0.001030 	Lval: 0.001046
Epoch: 575 	Ltrain: 0.001072 	Lval: 0.001027
Epoch: 580 	Ltrain: 0.001079 	Lval: 0.001017
Epoch: 585 	Ltrain: 0.001033 	Lval: 0.001016
Epoch: 590 	Ltrain: 0.000982 	Lval: 0.000987
Epoch: 595 	Ltrain: 0.000985 	Lval: 0.000974
Epoch: 600 	Ltrain: 0.000958 	Lval: 0.000960
Epoch 00604: reducing learning rate of group 0 to 5.0770e-06.
Epoch: 605 	Ltrain: 0.000958 	Lval: 0.000948
Epoch: 610 	Ltrain: 0.000921 	Lval: 0.000943
Epoch: 615 	Ltrain: 0.000946 	Lval: 0.000941
Epoch: 620 	Ltrain: 0.000901 	Lval: 0.000939
Epoch: 625 	Ltrain: 0.000934 	Lval: 0.000938
Epoch: 630 	Ltrain: 0.000922 	Lval: 0.000936
Epoch: 635 	Ltrain: 0.001007 	Lval: 0.000934
Epoch: 640 	Ltrain: 0.000944 	Lval: 0.000933
Epoch: 645 	Ltrain: 0.000902 	Lval: 0.000932
Epoch: 650 	Ltrain: 0.000932 	Lval: 0.000930
EarlyStopper: stopping at epoch 652 with best_val_loss = 0.000938


	Fold 2/5
Epoch: 1 	Ltrain: 0.044126 	Lval: 0.022959
Epoch: 5 	Ltrain: 0.008656 	Lval: 0.008710
Epoch: 10 	Ltrain: 0.007330 	Lval: 0.007860
Epoch: 15 	Ltrain: 0.006558 	Lval: 0.007023
Epoch 00018: reducing learning rate of group 0 to 5.0770e-04.
Epoch: 20 	Ltrain: 0.005890 	Lval: 0.006130
Epoch: 25 	Ltrain: 0.005758 	Lval: 0.006054
Epoch: 30 	Ltrain: 0.005542 	Lval: 0.005905
Epoch: 35 	Ltrain: 0.005686 	Lval: 0.005895
Epoch: 40 	Ltrain: 0.005439 	Lval: 0.005750
Epoch: 45 	Ltrain: 0.005396 	Lval: 0.005612
Epoch: 50 	Ltrain: 0.005262 	Lval: 0.005477
Epoch: 55 	Ltrain: 0.005080 	Lval: 0.005336
Epoch: 60 	Ltrain: 0.004819 	Lval: 0.005158
Epoch: 65 	Ltrain: 0.004989 	Lval: 0.005038
Epoch: 70 	Ltrain: 0.004633 	Lval: 0.004841
Epoch: 75 	Ltrain: 0.004492 	Lval: 0.004643
Epoch: 80 	Ltrain: 0.004391 	Lval: 0.004462
Epoch: 85 	Ltrain: 0.004176 	Lval: 0.004167
Epoch: 90 	Ltrain: 0.004061 	Lval: 0.004018
Epoch: 95 	Ltrain: 0.003806 	Lval: 0.003714
Epoch: 100 	Ltrain: 0.003417 	Lval: 0.003516
Epoch: 105 	Ltrain: 0.003252 	Lval: 0.003248
Epoch: 110 	Ltrain: 0.003091 	Lval: 0.003015
Epoch: 115 	Ltrain: 0.002742 	Lval: 0.002698
Epoch: 120 	Ltrain: 0.002579 	Lval: 0.002544
Epoch: 125 	Ltrain: 0.002432 	Lval: 0.002259
Epoch: 130 	Ltrain: 0.002258 	Lval: 0.002162
Epoch: 135 	Ltrain: 0.002186 	Lval: 0.002383
Epoch: 140 	Ltrain: 0.001942 	Lval: 0.001926
Epoch: 145 	Ltrain: 0.001748 	Lval: 0.001658
Epoch: 150 	Ltrain: 0.001640 	Lval: 0.001548
Epoch: 155 	Ltrain: 0.001779 	Lval: 0.001799
Epoch: 160 	Ltrain: 0.001458 	Lval: 0.001393
Epoch: 165 	Ltrain: 0.001314 	Lval: 0.001251
Epoch 00169: reducing learning rate of group 0 to 5.0770e-05.
Epoch: 170 	Ltrain: 0.001221 	Lval: 0.001113
Epoch: 175 	Ltrain: 0.001003 	Lval: 0.001019
Epoch: 180 	Ltrain: 0.000990 	Lval: 0.001002
Epoch: 185 	Ltrain: 0.000970 	Lval: 0.000988
Epoch: 190 	Ltrain: 0.000961 	Lval: 0.000976
Epoch: 195 	Ltrain: 0.000970 	Lval: 0.000964
Epoch: 200 	Ltrain: 0.000953 	Lval: 0.000954
Epoch: 205 	Ltrain: 0.000936 	Lval: 0.000943
Epoch: 210 	Ltrain: 0.000927 	Lval: 0.000930
Epoch: 215 	Ltrain: 0.000916 	Lval: 0.000919
Epoch: 220 	Ltrain: 0.000897 	Lval: 0.000908
Epoch: 225 	Ltrain: 0.000887 	Lval: 0.000897
Epoch: 230 	Ltrain: 0.000876 	Lval: 0.000884
Epoch: 235 	Ltrain: 0.000863 	Lval: 0.000873
Epoch: 240 	Ltrain: 0.000848 	Lval: 0.000862
Epoch: 245 	Ltrain: 0.000841 	Lval: 0.000850
Epoch: 250 	Ltrain: 0.000830 	Lval: 0.000838
Epoch: 255 	Ltrain: 0.000825 	Lval: 0.000825
Epoch: 260 	Ltrain: 0.000825 	Lval: 0.000813
Epoch: 265 	Ltrain: 0.000790 	Lval: 0.000800
Epoch: 270 	Ltrain: 0.000786 	Lval: 0.000788
Epoch: 275 	Ltrain: 0.000767 	Lval: 0.000777
Epoch: 280 	Ltrain: 0.000754 	Lval: 0.000763
Epoch: 285 	Ltrain: 0.000740 	Lval: 0.000752
Epoch: 290 	Ltrain: 0.000728 	Lval: 0.000742
Epoch: 295 	Ltrain: 0.000723 	Lval: 0.000728
Epoch: 300 	Ltrain: 0.000708 	Lval: 0.000713
Epoch: 305 	Ltrain: 0.000692 	Lval: 0.000702
Epoch: 310 	Ltrain: 0.000681 	Lval: 0.000690
Epoch: 315 	Ltrain: 0.000669 	Lval: 0.000676
Epoch: 320 	Ltrain: 0.000672 	Lval: 0.000665
Epoch: 325 	Ltrain: 0.000648 	Lval: 0.000652
Epoch: 330 	Ltrain: 0.000640 	Lval: 0.000639
Epoch: 335 	Ltrain: 0.000633 	Lval: 0.000626
Epoch: 340 	Ltrain: 0.000624 	Lval: 0.000616
Epoch: 345 	Ltrain: 0.000599 	Lval: 0.000603
Epoch: 350 	Ltrain: 0.000584 	Lval: 0.000591
Epoch: 355 	Ltrain: 0.000576 	Lval: 0.000580
Epoch: 360 	Ltrain: 0.000568 	Lval: 0.000567
Epoch: 365 	Ltrain: 0.000554 	Lval: 0.000554
Epoch: 370 	Ltrain: 0.000544 	Lval: 0.000546
Epoch: 375 	Ltrain: 0.000528 	Lval: 0.000533
Epoch: 380 	Ltrain: 0.000516 	Lval: 0.000522
Epoch: 385 	Ltrain: 0.000512 	Lval: 0.000509
Epoch: 390 	Ltrain: 0.000498 	Lval: 0.000499
Epoch: 395 	Ltrain: 0.000501 	Lval: 0.000487
Epoch: 400 	Ltrain: 0.000476 	Lval: 0.000476
Epoch: 405 	Ltrain: 0.000469 	Lval: 0.000466
Epoch: 410 	Ltrain: 0.000461 	Lval: 0.000458
Epoch: 415 	Ltrain: 0.000456 	Lval: 0.000444
Epoch: 420 	Ltrain: 0.000445 	Lval: 0.000438
Epoch: 425 	Ltrain: 0.000442 	Lval: 0.000452
Epoch: 430 	Ltrain: 0.000426 	Lval: 0.000421
Epoch: 435 	Ltrain: 0.000410 	Lval: 0.000406
Epoch: 440 	Ltrain: 0.000403 	Lval: 0.000394
Epoch: 445 	Ltrain: 0.000392 	Lval: 0.000388
Epoch: 450 	Ltrain: 0.000389 	Lval: 0.000381
Epoch: 455 	Ltrain: 0.000375 	Lval: 0.000370
Epoch: 460 	Ltrain: 0.000363 	Lval: 0.000364
Epoch: 465 	Ltrain: 0.000360 	Lval: 0.000358
Epoch: 470 	Ltrain: 0.000350 	Lval: 0.000344
Epoch: 475 	Ltrain: 0.000345 	Lval: 0.000338
Epoch: 480 	Ltrain: 0.000341 	Lval: 0.000332
Epoch: 485 	Ltrain: 0.000328 	Lval: 0.000321
Epoch: 490 	Ltrain: 0.000326 	Lval: 0.000318
Epoch: 495 	Ltrain: 0.000329 	Lval: 0.000312
Epoch: 500 	Ltrain: 0.000310 	Lval: 0.000306
Epoch: 505 	Ltrain: 0.000304 	Lval: 0.000297
Epoch: 510 	Ltrain: 0.000296 	Lval: 0.000292
Epoch 00513: reducing learning rate of group 0 to 5.0770e-06.
Epoch: 515 	Ltrain: 0.000294 	Lval: 0.000279
Epoch: 520 	Ltrain: 0.000283 	Lval: 0.000277
Epoch: 525 	Ltrain: 0.000285 	Lval: 0.000276
Epoch: 530 	Ltrain: 0.000287 	Lval: 0.000275
Epoch: 535 	Ltrain: 0.000284 	Lval: 0.000275
Epoch: 540 	Ltrain: 0.000280 	Lval: 0.000274
Epoch: 545 	Ltrain: 0.000279 	Lval: 0.000273
Epoch: 550 	Ltrain: 0.000278 	Lval: 0.000272
EarlyStopper: stopping at epoch 551 with best_val_loss = 0.000276


	Fold 3/5
Epoch: 1 	Ltrain: 0.031064 	Lval: 0.016157
Epoch: 5 	Ltrain: 0.007423 	Lval: 0.007496
Epoch: 10 	Ltrain: 0.006353 	Lval: 0.006946
Epoch: 15 	Ltrain: 0.006013 	Lval: 0.006276
Epoch: 20 	Ltrain: 0.005974 	Lval: 0.007268
Epoch 00022: reducing learning rate of group 0 to 5.0770e-04.
Epoch: 25 	Ltrain: 0.004820 	Lval: 0.005506
Epoch: 30 	Ltrain: 0.004746 	Lval: 0.005289
Epoch: 35 	Ltrain: 0.004490 	Lval: 0.005133
Epoch: 40 	Ltrain: 0.004389 	Lval: 0.004881
Epoch: 45 	Ltrain: 0.004193 	Lval: 0.004611
Epoch: 50 	Ltrain: 0.004020 	Lval: 0.004375
Epoch: 55 	Ltrain: 0.003899 	Lval: 0.004190
Epoch: 60 	Ltrain: 0.003775 	Lval: 0.003914
Epoch: 65 	Ltrain: 0.003567 	Lval: 0.003723
Epoch: 70 	Ltrain: 0.003295 	Lval: 0.003486
Epoch: 75 	Ltrain: 0.003117 	Lval: 0.003132
Epoch: 80 	Ltrain: 0.002946 	Lval: 0.003034
Epoch: 85 	Ltrain: 0.002710 	Lval: 0.002679
Epoch: 90 	Ltrain: 0.002577 	Lval: 0.002640
Epoch: 95 	Ltrain: 0.002367 	Lval: 0.002275
Epoch: 100 	Ltrain: 0.002116 	Lval: 0.002153
Epoch: 105 	Ltrain: 0.002016 	Lval: 0.001949
Epoch: 110 	Ltrain: 0.001841 	Lval: 0.001803
Epoch: 115 	Ltrain: 0.001629 	Lval: 0.001702
Epoch: 120 	Ltrain: 0.001523 	Lval: 0.001495
Epoch: 125 	Ltrain: 0.001442 	Lval: 0.001393
Epoch: 130 	Ltrain: 0.001273 	Lval: 0.001282
Epoch: 135 	Ltrain: 0.001181 	Lval: 0.001110
Epoch: 140 	Ltrain: 0.001057 	Lval: 0.001041
Epoch: 145 	Ltrain: 0.001024 	Lval: 0.001074
Epoch: 150 	Ltrain: 0.000942 	Lval: 0.000882
Epoch: 155 	Ltrain: 0.000855 	Lval: 0.000864
Epoch: 160 	Ltrain: 0.000755 	Lval: 0.000722
Epoch: 165 	Ltrain: 0.000680 	Lval: 0.000659
Epoch: 170 	Ltrain: 0.000671 	Lval: 0.000621
Epoch 00174: reducing learning rate of group 0 to 5.0770e-05.
Epoch: 175 	Ltrain: 0.000566 	Lval: 0.000493
Epoch: 180 	Ltrain: 0.000443 	Lval: 0.000425
Epoch: 185 	Ltrain: 0.000428 	Lval: 0.000411
Epoch: 190 	Ltrain: 0.000419 	Lval: 0.000402
Epoch: 195 	Ltrain: 0.000411 	Lval: 0.000395
Epoch: 200 	Ltrain: 0.000404 	Lval: 0.000388
Epoch: 205 	Ltrain: 0.000397 	Lval: 0.000381
Epoch: 210 	Ltrain: 0.000391 	Lval: 0.000374
Epoch: 215 	Ltrain: 0.000392 	Lval: 0.000368
Epoch: 220 	Ltrain: 0.000377 	Lval: 0.000361
Epoch: 225 	Ltrain: 0.000371 	Lval: 0.000355
Epoch: 230 	Ltrain: 0.000362 	Lval: 0.000349
Epoch: 235 	Ltrain: 0.000356 	Lval: 0.000342
Epoch: 240 	Ltrain: 0.000348 	Lval: 0.000335
Epoch: 245 	Ltrain: 0.000342 	Lval: 0.000329
Epoch: 250 	Ltrain: 0.000335 	Lval: 0.000321
Epoch: 255 	Ltrain: 0.000325 	Lval: 0.000316
Epoch: 260 	Ltrain: 0.000318 	Lval: 0.000308
Epoch: 265 	Ltrain: 0.000310 	Lval: 0.000301
Epoch: 270 	Ltrain: 0.000304 	Lval: 0.000294
Epoch: 275 	Ltrain: 0.000295 	Lval: 0.000286
Epoch: 280 	Ltrain: 0.000288 	Lval: 0.000279
Epoch: 285 	Ltrain: 0.000279 	Lval: 0.000273
Epoch: 290 	Ltrain: 0.000271 	Lval: 0.000266
Epoch: 295 	Ltrain: 0.000266 	Lval: 0.000259
Epoch: 300 	Ltrain: 0.000256 	Lval: 0.000252
Epoch: 305 	Ltrain: 0.000249 	Lval: 0.000245
Epoch: 310 	Ltrain: 0.000240 	Lval: 0.000238
Epoch: 315 	Ltrain: 0.000235 	Lval: 0.000231
Epoch: 320 	Ltrain: 0.000226 	Lval: 0.000224
Epoch: 325 	Ltrain: 0.000219 	Lval: 0.000219
Epoch: 330 	Ltrain: 0.000211 	Lval: 0.000213
Epoch: 335 	Ltrain: 0.000206 	Lval: 0.000205
Epoch: 340 	Ltrain: 0.000203 	Lval: 0.000199
Epoch: 345 	Ltrain: 0.000192 	Lval: 0.000193
Epoch: 350 	Ltrain: 0.000186 	Lval: 0.000187
Epoch: 355 	Ltrain: 0.000180 	Lval: 0.000182
Epoch: 360 	Ltrain: 0.000176 	Lval: 0.000176
Epoch: 365 	Ltrain: 0.000168 	Lval: 0.000170
Epoch: 370 	Ltrain: 0.000165 	Lval: 0.000165
Epoch: 375 	Ltrain: 0.000159 	Lval: 0.000161
Epoch: 380 	Ltrain: 0.000155 	Lval: 0.000157
Epoch: 385 	Ltrain: 0.000151 	Lval: 0.000154
Epoch: 390 	Ltrain: 0.000146 	Lval: 0.000146
Epoch: 395 	Ltrain: 0.000141 	Lval: 0.000143
Epoch: 400 	Ltrain: 0.000139 	Lval: 0.000142
Epoch: 405 	Ltrain: 0.000138 	Lval: 0.000139
Epoch: 410 	Ltrain: 0.000132 	Lval: 0.000134
Epoch: 415 	Ltrain: 0.000129 	Lval: 0.000130
Epoch: 420 	Ltrain: 0.000123 	Lval: 0.000125
Epoch: 425 	Ltrain: 0.000121 	Lval: 0.000122
Epoch: 430 	Ltrain: 0.000119 	Lval: 0.000120
Epoch: 435 	Ltrain: 0.000114 	Lval: 0.000114
Epoch: 440 	Ltrain: 0.000111 	Lval: 0.000111
Epoch: 445 	Ltrain: 0.000110 	Lval: 0.000112
Epoch 00448: reducing learning rate of group 0 to 5.0770e-06.
Epoch: 450 	Ltrain: 0.000104 	Lval: 0.000104
Epoch: 455 	Ltrain: 0.000101 	Lval: 0.000102
Epoch: 460 	Ltrain: 0.000100 	Lval: 0.000102
Epoch: 465 	Ltrain: 0.000100 	Lval: 0.000101
Epoch: 470 	Ltrain: 0.000100 	Lval: 0.000101
Epoch: 475 	Ltrain: 0.000099 	Lval: 0.000101
EarlyStopper: stopping at epoch 475 with best_val_loss = 0.000106


	Fold 4/5
Epoch: 1 	Ltrain: 0.028021 	Lval: 0.015206
Epoch: 5 	Ltrain: 0.006822 	Lval: 0.008120
Epoch: 10 	Ltrain: 0.005633 	Lval: 0.006594
Epoch: 15 	Ltrain: 0.005379 	Lval: 0.006198
Epoch: 20 	Ltrain: 0.004947 	Lval: 0.005682
Epoch: 25 	Ltrain: 0.004735 	Lval: 0.005321
Epoch: 30 	Ltrain: 0.004371 	Lval: 0.005306
Epoch: 35 	Ltrain: 0.004004 	Lval: 0.004300
Epoch 00039: reducing learning rate of group 0 to 5.0770e-04.
Epoch: 40 	Ltrain: 0.003391 	Lval: 0.003684
Epoch: 45 	Ltrain: 0.002916 	Lval: 0.003318
Epoch: 50 	Ltrain: 0.002736 	Lval: 0.003100
Epoch: 55 	Ltrain: 0.002550 	Lval: 0.002851
Epoch: 60 	Ltrain: 0.002381 	Lval: 0.002635
Epoch: 65 	Ltrain: 0.002208 	Lval: 0.002412
Epoch: 70 	Ltrain: 0.002042 	Lval: 0.002291
Epoch: 75 	Ltrain: 0.001891 	Lval: 0.002103
Epoch: 80 	Ltrain: 0.001747 	Lval: 0.001870
Epoch: 85 	Ltrain: 0.001590 	Lval: 0.001715
Epoch: 90 	Ltrain: 0.001431 	Lval: 0.001512
Epoch: 95 	Ltrain: 0.001317 	Lval: 0.001367
Epoch: 100 	Ltrain: 0.001221 	Lval: 0.001229
Epoch: 105 	Ltrain: 0.001122 	Lval: 0.001177
Epoch 00108: reducing learning rate of group 0 to 5.0770e-05.
Epoch: 110 	Ltrain: 0.000928 	Lval: 0.000919
Epoch: 115 	Ltrain: 0.000862 	Lval: 0.000861
Epoch: 120 	Ltrain: 0.000840 	Lval: 0.000835
Epoch: 125 	Ltrain: 0.000823 	Lval: 0.000815
Epoch: 130 	Ltrain: 0.000808 	Lval: 0.000797
Epoch: 135 	Ltrain: 0.000791 	Lval: 0.000781
Epoch: 140 	Ltrain: 0.000779 	Lval: 0.000762
Epoch: 145 	Ltrain: 0.000762 	Lval: 0.000747
Epoch: 150 	Ltrain: 0.000747 	Lval: 0.000729
Epoch: 155 	Ltrain: 0.000732 	Lval: 0.000711
Epoch: 160 	Ltrain: 0.000716 	Lval: 0.000697
Epoch: 165 	Ltrain: 0.000701 	Lval: 0.000680
Epoch: 170 	Ltrain: 0.000685 	Lval: 0.000660
Epoch: 175 	Ltrain: 0.000669 	Lval: 0.000643
Epoch: 180 	Ltrain: 0.000655 	Lval: 0.000629
Epoch: 185 	Ltrain: 0.000636 	Lval: 0.000608
Epoch: 190 	Ltrain: 0.000621 	Lval: 0.000593
Epoch: 195 	Ltrain: 0.000606 	Lval: 0.000575
Epoch: 200 	Ltrain: 0.000595 	Lval: 0.000559
Epoch: 205 	Ltrain: 0.000574 	Lval: 0.000546
Epoch: 210 	Ltrain: 0.000558 	Lval: 0.000528
Epoch: 215 	Ltrain: 0.000542 	Lval: 0.000513
Epoch: 220 	Ltrain: 0.000529 	Lval: 0.000498
Epoch: 225 	Ltrain: 0.000516 	Lval: 0.000485
Epoch: 230 	Ltrain: 0.000501 	Lval: 0.000469
Epoch: 235 	Ltrain: 0.000491 	Lval: 0.000455
Epoch: 240 	Ltrain: 0.000474 	Lval: 0.000442
Epoch: 245 	Ltrain: 0.000461 	Lval: 0.000433
Epoch: 250 	Ltrain: 0.000448 	Lval: 0.000417
Epoch: 255 	Ltrain: 0.000436 	Lval: 0.000408
Epoch: 260 	Ltrain: 0.000424 	Lval: 0.000395
Epoch: 265 	Ltrain: 0.000414 	Lval: 0.000384
Epoch: 270 	Ltrain: 0.000401 	Lval: 0.000371
Epoch: 275 	Ltrain: 0.000391 	Lval: 0.000362
Epoch: 280 	Ltrain: 0.000383 	Lval: 0.000353
Epoch: 285 	Ltrain: 0.000371 	Lval: 0.000342
Epoch: 290 	Ltrain: 0.000364 	Lval: 0.000333
Epoch: 295 	Ltrain: 0.000356 	Lval: 0.000337
Epoch: 300 	Ltrain: 0.000344 	Lval: 0.000318
Epoch: 305 	Ltrain: 0.000334 	Lval: 0.000307
Epoch: 310 	Ltrain: 0.000333 	Lval: 0.000304
Epoch: 315 	Ltrain: 0.000320 	Lval: 0.000291
Epoch: 320 	Ltrain: 0.000312 	Lval: 0.000285
Epoch: 325 	Ltrain: 0.000307 	Lval: 0.000281
Epoch: 330 	Ltrain: 0.000298 	Lval: 0.000271
Epoch: 335 	Ltrain: 0.000292 	Lval: 0.000264
Epoch: 340 	Ltrain: 0.000287 	Lval: 0.000258
Epoch: 345 	Ltrain: 0.000278 	Lval: 0.000253
Epoch: 350 	Ltrain: 0.000273 	Lval: 0.000249
Epoch: 355 	Ltrain: 0.000267 	Lval: 0.000245
Epoch: 360 	Ltrain: 0.000266 	Lval: 0.000239
Epoch 00362: reducing learning rate of group 0 to 5.0770e-06.
Epoch: 365 	Ltrain: 0.000249 	Lval: 0.000225
Epoch: 370 	Ltrain: 0.000246 	Lval: 0.000223
Epoch: 375 	Ltrain: 0.000246 	Lval: 0.000222
Epoch: 380 	Ltrain: 0.000245 	Lval: 0.000221
Epoch: 385 	Ltrain: 0.000244 	Lval: 0.000221
Epoch: 390 	Ltrain: 0.000243 	Lval: 0.000220
EarlyStopper: stopping at epoch 389 with best_val_loss = 0.000228


	Fold 5/5
Epoch: 1 	Ltrain: 0.028063 	Lval: 0.014777
Epoch: 5 	Ltrain: 0.006128 	Lval: 0.008050
Epoch: 10 	Ltrain: 0.005433 	Lval: 0.006937
Epoch: 15 	Ltrain: 0.005434 	Lval: 0.006865
Epoch: 20 	Ltrain: 0.005616 	Lval: 0.006040
Epoch 00024: reducing learning rate of group 0 to 5.0770e-04.
Epoch: 25 	Ltrain: 0.004517 	Lval: 0.005560
Epoch: 30 	Ltrain: 0.004209 	Lval: 0.005323
Epoch: 35 	Ltrain: 0.004123 	Lval: 0.005097
Epoch: 40 	Ltrain: 0.004028 	Lval: 0.004900
Epoch: 45 	Ltrain: 0.003908 	Lval: 0.004730
Epoch: 50 	Ltrain: 0.003798 	Lval: 0.004582
Epoch: 55 	Ltrain: 0.003659 	Lval: 0.004315
Epoch: 60 	Ltrain: 0.003504 	Lval: 0.004101
Epoch: 65 	Ltrain: 0.003279 	Lval: 0.003820
Epoch: 70 	Ltrain: 0.003184 	Lval: 0.003823
Epoch: 75 	Ltrain: 0.003005 	Lval: 0.003566
Epoch: 80 	Ltrain: 0.002825 	Lval: 0.003494
Epoch: 85 	Ltrain: 0.002611 	Lval: 0.002944
Epoch: 90 	Ltrain: 0.002429 	Lval: 0.002653
Epoch: 95 	Ltrain: 0.002214 	Lval: 0.002351
Epoch: 100 	Ltrain: 0.002029 	Lval: 0.002309
Epoch: 105 	Ltrain: 0.001879 	Lval: 0.001983
Epoch: 110 	Ltrain: 0.001703 	Lval: 0.002001
Epoch: 115 	Ltrain: 0.001586 	Lval: 0.001600
Epoch 00120: reducing learning rate of group 0 to 5.0770e-05.
Epoch: 120 	Ltrain: 0.001494 	Lval: 0.001668
Epoch: 125 	Ltrain: 0.001142 	Lval: 0.001252
Epoch: 130 	Ltrain: 0.001111 	Lval: 0.001210
Epoch: 135 	Ltrain: 0.001087 	Lval: 0.001185
Epoch: 140 	Ltrain: 0.001067 	Lval: 0.001159
Epoch: 145 	Ltrain: 0.001050 	Lval: 0.001136
Epoch: 150 	Ltrain: 0.001032 	Lval: 0.001110
Epoch: 155 	Ltrain: 0.001007 	Lval: 0.001091
Epoch: 160 	Ltrain: 0.000987 	Lval: 0.001065
Epoch: 165 	Ltrain: 0.000969 	Lval: 0.001047
Epoch: 170 	Ltrain: 0.000950 	Lval: 0.001021
Epoch: 175 	Ltrain: 0.000928 	Lval: 0.000995
Epoch: 180 	Ltrain: 0.000905 	Lval: 0.000973
Epoch: 185 	Ltrain: 0.000891 	Lval: 0.000949
Epoch: 190 	Ltrain: 0.000870 	Lval: 0.000929
Epoch: 195 	Ltrain: 0.000848 	Lval: 0.000902
Epoch: 200 	Ltrain: 0.000824 	Lval: 0.000879
Epoch: 205 	Ltrain: 0.000806 	Lval: 0.000855
Epoch: 210 	Ltrain: 0.000789 	Lval: 0.000829
Epoch: 215 	Ltrain: 0.000767 	Lval: 0.000809
Epoch: 220 	Ltrain: 0.000744 	Lval: 0.000784
Epoch: 225 	Ltrain: 0.000729 	Lval: 0.000763
Epoch: 230 	Ltrain: 0.000706 	Lval: 0.000742
Epoch: 235 	Ltrain: 0.000684 	Lval: 0.000720
Epoch: 240 	Ltrain: 0.000668 	Lval: 0.000702
Epoch: 245 	Ltrain: 0.000649 	Lval: 0.000678
Epoch: 250 	Ltrain: 0.000627 	Lval: 0.000655
Epoch: 255 	Ltrain: 0.000613 	Lval: 0.000637
Epoch: 260 	Ltrain: 0.000595 	Lval: 0.000617
Epoch: 265 	Ltrain: 0.000577 	Lval: 0.000598
Epoch: 270 	Ltrain: 0.000561 	Lval: 0.000580
Epoch: 275 	Ltrain: 0.000545 	Lval: 0.000561
Epoch: 280 	Ltrain: 0.000535 	Lval: 0.000550
Epoch: 285 	Ltrain: 0.000516 	Lval: 0.000527
Epoch: 290 	Ltrain: 0.000501 	Lval: 0.000511
Epoch: 295 	Ltrain: 0.000486 	Lval: 0.000497
Epoch: 300 	Ltrain: 0.000474 	Lval: 0.000482
Epoch: 305 	Ltrain: 0.000463 	Lval: 0.000473
Epoch: 310 	Ltrain: 0.000448 	Lval: 0.000454
Epoch: 315 	Ltrain: 0.000437 	Lval: 0.000444
Epoch: 320 	Ltrain: 0.000425 	Lval: 0.000433
Epoch: 325 	Ltrain: 0.000418 	Lval: 0.000418
Epoch: 330 	Ltrain: 0.000400 	Lval: 0.000401
Epoch: 335 	Ltrain: 0.000394 	Lval: 0.000398
Epoch: 340 	Ltrain: 0.000383 	Lval: 0.000386
Epoch: 345 	Ltrain: 0.000373 	Lval: 0.000373
Epoch: 350 	Ltrain: 0.000364 	Lval: 0.000360
Epoch: 355 	Ltrain: 0.000354 	Lval: 0.000353
Epoch: 360 	Ltrain: 0.000350 	Lval: 0.000345
Epoch: 365 	Ltrain: 0.000340 	Lval: 0.000334
Epoch: 370 	Ltrain: 0.000329 	Lval: 0.000324
Epoch: 375 	Ltrain: 0.000319 	Lval: 0.000316
Epoch: 380 	Ltrain: 0.000315 	Lval: 0.000315
Epoch: 385 	Ltrain: 0.000315 	Lval: 0.000308
Epoch 00386: reducing learning rate of group 0 to 5.0770e-06.
Epoch: 390 	Ltrain: 0.000292 	Lval: 0.000290
Epoch: 395 	Ltrain: 0.000291 	Lval: 0.000289
Epoch: 400 	Ltrain: 0.000289 	Lval: 0.000288
Epoch: 405 	Ltrain: 0.000288 	Lval: 0.000287
Epoch: 410 	Ltrain: 0.000288 	Lval: 0.000286
EarlyStopper: stopping at epoch 413 with best_val_loss = 0.000294

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003952469056122234
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.014857291878612e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.046000 	Lval: 0.030282
Epoch: 5 	Ltrain: 0.013012 	Lval: 0.012622
Epoch: 10 	Ltrain: 0.009077 	Lval: 0.007716
Epoch 00014: reducing learning rate of group 0 to 3.9525e-04.
Epoch: 15 	Ltrain: 0.007400 	Lval: 0.007067
Epoch: 20 	Ltrain: 0.007226 	Lval: 0.006835
Epoch: 25 	Ltrain: 0.006822 	Lval: 0.006738
Epoch: 30 	Ltrain: 0.006853 	Lval: 0.006680
Epoch: 35 	Ltrain: 0.006878 	Lval: 0.006599
Epoch: 40 	Ltrain: 0.006542 	Lval: 0.006504
Epoch: 45 	Ltrain: 0.006436 	Lval: 0.006432
Epoch: 50 	Ltrain: 0.006179 	Lval: 0.006341
Epoch: 55 	Ltrain: 0.006490 	Lval: 0.006353
Epoch 00056: reducing learning rate of group 0 to 3.9525e-05.
Epoch: 60 	Ltrain: 0.006313 	Lval: 0.006201
Epoch: 65 	Ltrain: 0.006744 	Lval: 0.006180
Epoch 00068: reducing learning rate of group 0 to 3.9525e-06.
Epoch: 70 	Ltrain: 0.006043 	Lval: 0.006179
Epoch: 75 	Ltrain: 0.006267 	Lval: 0.006173
Epoch: 80 	Ltrain: 0.006088 	Lval: 0.006168
EarlyStopper: stopping at epoch 83 with best_val_loss = 0.006175


	Fold 2/5
Epoch: 1 	Ltrain: 0.027881 	Lval: 0.016573
Epoch: 5 	Ltrain: 0.007774 	Lval: 0.009148
Epoch: 10 	Ltrain: 0.007419 	Lval: 0.007558
Epoch: 15 	Ltrain: 0.007476 	Lval: 0.007270
Epoch 00016: reducing learning rate of group 0 to 3.9525e-04.
Epoch: 20 	Ltrain: 0.006093 	Lval: 0.006319
Epoch: 25 	Ltrain: 0.006146 	Lval: 0.006276
Epoch: 30 	Ltrain: 0.005989 	Lval: 0.006181
Epoch 00035: reducing learning rate of group 0 to 3.9525e-05.
Epoch: 35 	Ltrain: 0.005881 	Lval: 0.006158
Epoch: 40 	Ltrain: 0.005714 	Lval: 0.006041
Epoch: 45 	Ltrain: 0.005794 	Lval: 0.006041
Epoch: 50 	Ltrain: 0.005821 	Lval: 0.006030
Epoch: 55 	Ltrain: 0.005826 	Lval: 0.006021
Epoch: 60 	Ltrain: 0.005762 	Lval: 0.005990
Epoch: 65 	Ltrain: 0.005803 	Lval: 0.005992
Epoch: 70 	Ltrain: 0.005651 	Lval: 0.005973
Epoch: 75 	Ltrain: 0.005775 	Lval: 0.005960
Epoch 00077: reducing learning rate of group 0 to 3.9525e-06.
Epoch: 80 	Ltrain: 0.005803 	Lval: 0.005960
Epoch: 85 	Ltrain: 0.005602 	Lval: 0.005953
Epoch: 90 	Ltrain: 0.005822 	Lval: 0.005956
Epoch 00091: reducing learning rate of group 0 to 3.9525e-07.
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.005957


	Fold 3/5
Epoch: 1 	Ltrain: 0.023837 	Lval: 0.014677
Epoch: 5 	Ltrain: 0.007281 	Lval: 0.010026
Epoch: 10 	Ltrain: 0.006840 	Lval: 0.007091
Epoch: 15 	Ltrain: 0.006000 	Lval: 0.006964
Epoch 00020: reducing learning rate of group 0 to 3.9525e-04.
Epoch: 20 	Ltrain: 0.006132 	Lval: 0.006603
Epoch: 25 	Ltrain: 0.005438 	Lval: 0.005958
Epoch: 30 	Ltrain: 0.005299 	Lval: 0.005945
Epoch: 35 	Ltrain: 0.005287 	Lval: 0.005836
Epoch 00037: reducing learning rate of group 0 to 3.9525e-05.
Epoch: 40 	Ltrain: 0.005178 	Lval: 0.005767
Epoch: 45 	Ltrain: 0.005152 	Lval: 0.005763
Epoch: 50 	Ltrain: 0.005177 	Lval: 0.005754
Epoch: 55 	Ltrain: 0.005131 	Lval: 0.005743
Epoch: 60 	Ltrain: 0.005139 	Lval: 0.005726
Epoch 00064: reducing learning rate of group 0 to 3.9525e-06.
Epoch: 65 	Ltrain: 0.005103 	Lval: 0.005732
Epoch: 70 	Ltrain: 0.005126 	Lval: 0.005729
Epoch: 75 	Ltrain: 0.005124 	Lval: 0.005728
Epoch 00076: reducing learning rate of group 0 to 3.9525e-07.
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.005732


	Fold 4/5
Epoch: 1 	Ltrain: 0.022109 	Lval: 0.014635
Epoch: 5 	Ltrain: 0.006037 	Lval: 0.007087
Epoch: 10 	Ltrain: 0.005840 	Lval: 0.006642
Epoch: 15 	Ltrain: 0.005488 	Lval: 0.006464
Epoch 00019: reducing learning rate of group 0 to 3.9525e-04.
Epoch: 20 	Ltrain: 0.005398 	Lval: 0.006273
Epoch: 25 	Ltrain: 0.004897 	Lval: 0.006095
Epoch: 30 	Ltrain: 0.004851 	Lval: 0.006143
Epoch: 35 	Ltrain: 0.004815 	Lval: 0.005876
Epoch 00039: reducing learning rate of group 0 to 3.9525e-05.
Epoch: 40 	Ltrain: 0.004746 	Lval: 0.005899
Epoch: 45 	Ltrain: 0.004720 	Lval: 0.005864
Epoch: 50 	Ltrain: 0.004704 	Lval: 0.005861
Epoch 00051: reducing learning rate of group 0 to 3.9525e-06.
Epoch: 55 	Ltrain: 0.004687 	Lval: 0.005861
Epoch: 60 	Ltrain: 0.004707 	Lval: 0.005857
Epoch 00063: reducing learning rate of group 0 to 3.9525e-07.
Epoch: 65 	Ltrain: 0.004689 	Lval: 0.005857
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.005844


	Fold 5/5
Epoch: 1 	Ltrain: 0.021097 	Lval: 0.014738
Epoch: 5 	Ltrain: 0.006166 	Lval: 0.007943
Epoch: 10 	Ltrain: 0.005643 	Lval: 0.007170
Epoch 00013: reducing learning rate of group 0 to 3.9525e-04.
Epoch: 15 	Ltrain: 0.005085 	Lval: 0.006483
Epoch: 20 	Ltrain: 0.005082 	Lval: 0.006910
Epoch: 25 	Ltrain: 0.004955 	Lval: 0.006209
Epoch: 30 	Ltrain: 0.004960 	Lval: 0.006273
Epoch 00033: reducing learning rate of group 0 to 3.9525e-05.
Epoch: 35 	Ltrain: 0.004831 	Lval: 0.006169
Epoch: 40 	Ltrain: 0.004815 	Lval: 0.006144
Epoch: 45 	Ltrain: 0.004803 	Lval: 0.006123
Epoch: 50 	Ltrain: 0.004795 	Lval: 0.006124
Epoch 00052: reducing learning rate of group 0 to 3.9525e-06.
Epoch: 55 	Ltrain: 0.004782 	Lval: 0.006117
Epoch: 60 	Ltrain: 0.004777 	Lval: 0.006112
Epoch 00064: reducing learning rate of group 0 to 3.9525e-07.
Epoch: 65 	Ltrain: 0.004786 	Lval: 0.006116
EarlyStopper: stopping at epoch 67 with best_val_loss = 0.006087

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006549397293513188
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.459722776272002e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.099802 	Lval: 0.022611
Epoch: 5 	Ltrain: 0.010238 	Lval: 0.009947
Epoch: 10 	Ltrain: 0.008471 	Lval: 0.007894
Epoch: 15 	Ltrain: 0.007319 	Lval: 0.007066
Epoch: 20 	Ltrain: 0.007447 	Lval: 0.007808
Epoch 00023: reducing learning rate of group 0 to 6.5494e-04.
Epoch: 25 	Ltrain: 0.006790 	Lval: 0.006080
Epoch: 30 	Ltrain: 0.005837 	Lval: 0.005827
Epoch: 35 	Ltrain: 0.006270 	Lval: 0.005741
Epoch: 40 	Ltrain: 0.005721 	Lval: 0.005619
Epoch: 45 	Ltrain: 0.005617 	Lval: 0.005502
Epoch: 50 	Ltrain: 0.005437 	Lval: 0.005400
Epoch: 55 	Ltrain: 0.005253 	Lval: 0.005271
Epoch: 60 	Ltrain: 0.004925 	Lval: 0.005141
Epoch: 65 	Ltrain: 0.004917 	Lval: 0.005060
Epoch: 70 	Ltrain: 0.004971 	Lval: 0.004883
Epoch: 75 	Ltrain: 0.004770 	Lval: 0.004946
Epoch: 80 	Ltrain: 0.004602 	Lval: 0.004654
Epoch: 85 	Ltrain: 0.004663 	Lval: 0.004630
Epoch: 90 	Ltrain: 0.004686 	Lval: 0.004403
Epoch: 95 	Ltrain: 0.004220 	Lval: 0.004204
Epoch: 100 	Ltrain: 0.004076 	Lval: 0.004178
Epoch: 105 	Ltrain: 0.003892 	Lval: 0.003891
Epoch: 110 	Ltrain: 0.003823 	Lval: 0.003681
Epoch: 115 	Ltrain: 0.003782 	Lval: 0.003538
Epoch: 120 	Ltrain: 0.003485 	Lval: 0.003510
Epoch: 125 	Ltrain: 0.003527 	Lval: 0.003526
Epoch: 130 	Ltrain: 0.003155 	Lval: 0.003014
Epoch: 135 	Ltrain: 0.003137 	Lval: 0.002887
Epoch: 140 	Ltrain: 0.002734 	Lval: 0.002653
Epoch: 145 	Ltrain: 0.002485 	Lval: 0.002467
Epoch: 150 	Ltrain: 0.002527 	Lval: 0.002410
Epoch: 155 	Ltrain: 0.002242 	Lval: 0.002248
Epoch 00157: reducing learning rate of group 0 to 6.5494e-05.
Epoch: 160 	Ltrain: 0.002065 	Lval: 0.002065
Epoch: 165 	Ltrain: 0.001945 	Lval: 0.001991
Epoch: 170 	Ltrain: 0.002014 	Lval: 0.001963
Epoch: 175 	Ltrain: 0.001891 	Lval: 0.001948
Epoch: 180 	Ltrain: 0.001864 	Lval: 0.001924
Epoch: 185 	Ltrain: 0.001925 	Lval: 0.001907
Epoch: 190 	Ltrain: 0.001940 	Lval: 0.001890
Epoch: 195 	Ltrain: 0.001897 	Lval: 0.001873
Epoch: 200 	Ltrain: 0.001839 	Lval: 0.001851
Epoch: 205 	Ltrain: 0.001762 	Lval: 0.001830
Epoch: 210 	Ltrain: 0.001836 	Lval: 0.001811
Epoch: 215 	Ltrain: 0.001746 	Lval: 0.001794
Epoch: 220 	Ltrain: 0.001772 	Lval: 0.001776
Epoch: 225 	Ltrain: 0.001697 	Lval: 0.001755
Epoch: 230 	Ltrain: 0.001683 	Lval: 0.001741
Epoch: 235 	Ltrain: 0.001700 	Lval: 0.001717
Epoch: 240 	Ltrain: 0.001747 	Lval: 0.001696
Epoch: 245 	Ltrain: 0.001660 	Lval: 0.001684
Epoch: 250 	Ltrain: 0.001618 	Lval: 0.001662
Epoch: 255 	Ltrain: 0.001633 	Lval: 0.001638
Epoch: 260 	Ltrain: 0.001630 	Lval: 0.001616
Epoch: 265 	Ltrain: 0.001598 	Lval: 0.001605
Epoch: 270 	Ltrain: 0.001644 	Lval: 0.001589
Epoch: 275 	Ltrain: 0.001553 	Lval: 0.001566
Epoch: 280 	Ltrain: 0.001551 	Lval: 0.001548
Epoch: 285 	Ltrain: 0.001552 	Lval: 0.001530
Epoch: 290 	Ltrain: 0.001520 	Lval: 0.001508
Epoch: 295 	Ltrain: 0.001487 	Lval: 0.001493
Epoch: 300 	Ltrain: 0.001489 	Lval: 0.001472
Epoch: 305 	Ltrain: 0.001493 	Lval: 0.001454
Epoch: 310 	Ltrain: 0.001449 	Lval: 0.001437
Epoch: 315 	Ltrain: 0.001442 	Lval: 0.001418
Epoch: 320 	Ltrain: 0.001477 	Lval: 0.001400
Epoch: 325 	Ltrain: 0.001435 	Lval: 0.001386
Epoch: 330 	Ltrain: 0.001459 	Lval: 0.001369
Epoch: 335 	Ltrain: 0.001359 	Lval: 0.001347
Epoch: 340 	Ltrain: 0.001359 	Lval: 0.001330
Epoch: 345 	Ltrain: 0.001376 	Lval: 0.001313
Epoch: 350 	Ltrain: 0.001324 	Lval: 0.001294
Epoch: 355 	Ltrain: 0.001325 	Lval: 0.001284
Epoch: 360 	Ltrain: 0.001292 	Lval: 0.001259
Epoch: 365 	Ltrain: 0.001251 	Lval: 0.001248
Epoch: 370 	Ltrain: 0.001309 	Lval: 0.001225
Epoch: 375 	Ltrain: 0.001261 	Lval: 0.001210
Epoch: 380 	Ltrain: 0.001281 	Lval: 0.001196
Epoch: 385 	Ltrain: 0.001228 	Lval: 0.001174
Epoch: 390 	Ltrain: 0.001195 	Lval: 0.001161
Epoch: 395 	Ltrain: 0.001184 	Lval: 0.001145
Epoch: 400 	Ltrain: 0.001191 	Lval: 0.001125
Epoch: 405 	Ltrain: 0.001134 	Lval: 0.001109
Epoch: 410 	Ltrain: 0.001111 	Lval: 0.001094
Epoch: 415 	Ltrain: 0.001123 	Lval: 0.001076
Epoch: 420 	Ltrain: 0.001117 	Lval: 0.001064
Epoch: 425 	Ltrain: 0.001072 	Lval: 0.001053
Epoch: 430 	Ltrain: 0.001080 	Lval: 0.001050
Epoch: 435 	Ltrain: 0.001065 	Lval: 0.001023
Epoch: 440 	Ltrain: 0.001034 	Lval: 0.001002
Epoch: 445 	Ltrain: 0.001012 	Lval: 0.000987
Epoch: 450 	Ltrain: 0.001034 	Lval: 0.000985
Epoch: 455 	Ltrain: 0.000999 	Lval: 0.000962
Epoch: 460 	Ltrain: 0.001056 	Lval: 0.000943
Epoch: 465 	Ltrain: 0.000954 	Lval: 0.000934
Epoch: 470 	Ltrain: 0.000991 	Lval: 0.000917
Epoch: 475 	Ltrain: 0.000992 	Lval: 0.000908
Epoch: 480 	Ltrain: 0.000915 	Lval: 0.000888
Epoch: 485 	Ltrain: 0.000906 	Lval: 0.000881
Epoch: 490 	Ltrain: 0.000863 	Lval: 0.000859
Epoch: 495 	Ltrain: 0.000885 	Lval: 0.000845
Epoch: 500 	Ltrain: 0.000877 	Lval: 0.000832
Epoch: 505 	Ltrain: 0.000856 	Lval: 0.000820
Epoch: 510 	Ltrain: 0.000865 	Lval: 0.000809
Epoch: 515 	Ltrain: 0.000837 	Lval: 0.000800
Epoch: 520 	Ltrain: 0.000823 	Lval: 0.000788
Epoch 00523: reducing learning rate of group 0 to 6.5494e-06.
Epoch: 525 	Ltrain: 0.000809 	Lval: 0.000774
Epoch: 530 	Ltrain: 0.000767 	Lval: 0.000769
Epoch: 535 	Ltrain: 0.000822 	Lval: 0.000767
Epoch: 540 	Ltrain: 0.000795 	Lval: 0.000765
Epoch: 545 	Ltrain: 0.000787 	Lval: 0.000763
Epoch: 550 	Ltrain: 0.000787 	Lval: 0.000762
Epoch: 555 	Ltrain: 0.000802 	Lval: 0.000761
Epoch: 560 	Ltrain: 0.000755 	Lval: 0.000759
Epoch: 565 	Ltrain: 0.000826 	Lval: 0.000758
Epoch: 570 	Ltrain: 0.000791 	Lval: 0.000756
EarlyStopper: stopping at epoch 570 with best_val_loss = 0.000764


	Fold 2/5
Epoch: 1 	Ltrain: 0.050088 	Lval: 0.021879
Epoch: 5 	Ltrain: 0.007859 	Lval: 0.007585
Epoch: 10 	Ltrain: 0.006652 	Lval: 0.006988
Epoch 00011: reducing learning rate of group 0 to 6.5494e-04.
Epoch: 15 	Ltrain: 0.006080 	Lval: 0.006268
Epoch: 20 	Ltrain: 0.005863 	Lval: 0.006171
Epoch: 25 	Ltrain: 0.005850 	Lval: 0.006067
Epoch: 30 	Ltrain: 0.005761 	Lval: 0.006067
Epoch: 35 	Ltrain: 0.005743 	Lval: 0.005908
Epoch: 40 	Ltrain: 0.005615 	Lval: 0.005762
Epoch: 45 	Ltrain: 0.005560 	Lval: 0.005806
Epoch: 50 	Ltrain: 0.005443 	Lval: 0.005772
Epoch: 55 	Ltrain: 0.005274 	Lval: 0.005337
Epoch: 60 	Ltrain: 0.005115 	Lval: 0.005334
Epoch: 65 	Ltrain: 0.004831 	Lval: 0.005113
Epoch: 70 	Ltrain: 0.004762 	Lval: 0.004766
Epoch: 75 	Ltrain: 0.004521 	Lval: 0.004606
Epoch: 80 	Ltrain: 0.004244 	Lval: 0.004526
Epoch: 85 	Ltrain: 0.004035 	Lval: 0.004103
Epoch: 90 	Ltrain: 0.003837 	Lval: 0.003943
Epoch: 95 	Ltrain: 0.003821 	Lval: 0.004006
Epoch: 100 	Ltrain: 0.003341 	Lval: 0.003275
Epoch: 105 	Ltrain: 0.003334 	Lval: 0.003219
Epoch: 110 	Ltrain: 0.003006 	Lval: 0.002909
Epoch: 115 	Ltrain: 0.002544 	Lval: 0.002509
Epoch: 120 	Ltrain: 0.002548 	Lval: 0.002301
Epoch: 125 	Ltrain: 0.002184 	Lval: 0.002190
Epoch: 130 	Ltrain: 0.001954 	Lval: 0.001864
Epoch: 135 	Ltrain: 0.001803 	Lval: 0.001787
Epoch: 140 	Ltrain: 0.001682 	Lval: 0.001558
Epoch: 145 	Ltrain: 0.001619 	Lval: 0.001633
Epoch: 150 	Ltrain: 0.001427 	Lval: 0.001438
Epoch: 155 	Ltrain: 0.001200 	Lval: 0.001124
Epoch 00160: reducing learning rate of group 0 to 6.5494e-05.
Epoch: 160 	Ltrain: 0.001349 	Lval: 0.001340
Epoch: 165 	Ltrain: 0.000912 	Lval: 0.000881
Epoch: 170 	Ltrain: 0.000870 	Lval: 0.000848
Epoch: 175 	Ltrain: 0.000841 	Lval: 0.000828
Epoch: 180 	Ltrain: 0.000832 	Lval: 0.000813
Epoch: 185 	Ltrain: 0.000815 	Lval: 0.000797
Epoch: 190 	Ltrain: 0.000792 	Lval: 0.000783
Epoch: 195 	Ltrain: 0.000778 	Lval: 0.000769
Epoch: 200 	Ltrain: 0.000777 	Lval: 0.000757
Epoch: 205 	Ltrain: 0.000767 	Lval: 0.000744
Epoch: 210 	Ltrain: 0.000745 	Lval: 0.000730
Epoch: 215 	Ltrain: 0.000739 	Lval: 0.000717
Epoch: 220 	Ltrain: 0.000710 	Lval: 0.000704
Epoch: 225 	Ltrain: 0.000704 	Lval: 0.000691
Epoch: 230 	Ltrain: 0.000704 	Lval: 0.000678
Epoch: 235 	Ltrain: 0.000680 	Lval: 0.000664
Epoch: 240 	Ltrain: 0.000665 	Lval: 0.000650
Epoch: 245 	Ltrain: 0.000654 	Lval: 0.000636
Epoch: 250 	Ltrain: 0.000631 	Lval: 0.000624
Epoch: 255 	Ltrain: 0.000629 	Lval: 0.000609
Epoch: 260 	Ltrain: 0.000610 	Lval: 0.000595
Epoch: 265 	Ltrain: 0.000603 	Lval: 0.000581
Epoch: 270 	Ltrain: 0.000584 	Lval: 0.000570
Epoch: 275 	Ltrain: 0.000567 	Lval: 0.000556
Epoch: 280 	Ltrain: 0.000554 	Lval: 0.000541
Epoch: 285 	Ltrain: 0.000542 	Lval: 0.000528
Epoch: 290 	Ltrain: 0.000527 	Lval: 0.000516
Epoch: 295 	Ltrain: 0.000514 	Lval: 0.000501
Epoch: 300 	Ltrain: 0.000498 	Lval: 0.000489
Epoch: 305 	Ltrain: 0.000484 	Lval: 0.000478
Epoch: 310 	Ltrain: 0.000477 	Lval: 0.000464
Epoch: 315 	Ltrain: 0.000460 	Lval: 0.000450
Epoch: 320 	Ltrain: 0.000459 	Lval: 0.000439
Epoch: 325 	Ltrain: 0.000434 	Lval: 0.000428
Epoch: 330 	Ltrain: 0.000425 	Lval: 0.000415
Epoch: 335 	Ltrain: 0.000410 	Lval: 0.000401
Epoch: 340 	Ltrain: 0.000400 	Lval: 0.000392
Epoch: 345 	Ltrain: 0.000395 	Lval: 0.000382
Epoch: 350 	Ltrain: 0.000375 	Lval: 0.000368
Epoch: 355 	Ltrain: 0.000369 	Lval: 0.000358
Epoch: 360 	Ltrain: 0.000350 	Lval: 0.000346
Epoch: 365 	Ltrain: 0.000342 	Lval: 0.000340
Epoch: 370 	Ltrain: 0.000329 	Lval: 0.000325
Epoch: 375 	Ltrain: 0.000329 	Lval: 0.000317
Epoch: 380 	Ltrain: 0.000312 	Lval: 0.000309
Epoch: 385 	Ltrain: 0.000300 	Lval: 0.000298
Epoch: 390 	Ltrain: 0.000296 	Lval: 0.000287
Epoch: 395 	Ltrain: 0.000284 	Lval: 0.000278
Epoch: 400 	Ltrain: 0.000271 	Lval: 0.000267
Epoch: 405 	Ltrain: 0.000261 	Lval: 0.000259
Epoch: 410 	Ltrain: 0.000258 	Lval: 0.000257
Epoch 00411: reducing learning rate of group 0 to 6.5494e-06.
Epoch: 415 	Ltrain: 0.000241 	Lval: 0.000243
Epoch: 420 	Ltrain: 0.000238 	Lval: 0.000242
Epoch: 425 	Ltrain: 0.000237 	Lval: 0.000241
Epoch: 430 	Ltrain: 0.000236 	Lval: 0.000240
Epoch: 435 	Ltrain: 0.000239 	Lval: 0.000238
Epoch: 440 	Ltrain: 0.000237 	Lval: 0.000237
EarlyStopper: stopping at epoch 439 with best_val_loss = 0.000245


	Fold 3/5
Epoch: 1 	Ltrain: 0.054434 	Lval: 0.016985
Epoch: 5 	Ltrain: 0.007407 	Lval: 0.008036
Epoch: 10 	Ltrain: 0.006574 	Lval: 0.006700
Epoch: 15 	Ltrain: 0.005880 	Lval: 0.006326
Epoch: 20 	Ltrain: 0.005328 	Lval: 0.005576
Epoch: 25 	Ltrain: 0.005407 	Lval: 0.005763
Epoch: 30 	Ltrain: 0.004647 	Lval: 0.005013
Epoch: 35 	Ltrain: 0.004157 	Lval: 0.004758
Epoch: 40 	Ltrain: 0.003954 	Lval: 0.004312
Epoch: 45 	Ltrain: 0.003104 	Lval: 0.003224
Epoch: 50 	Ltrain: 0.002878 	Lval: 0.002696
Epoch: 55 	Ltrain: 0.002208 	Lval: 0.002255
Epoch 00060: reducing learning rate of group 0 to 6.5494e-04.
Epoch: 60 	Ltrain: 0.002331 	Lval: 0.002309
Epoch: 65 	Ltrain: 0.001160 	Lval: 0.001158
Epoch: 70 	Ltrain: 0.000999 	Lval: 0.000990
Epoch: 75 	Ltrain: 0.000904 	Lval: 0.000872
Epoch: 80 	Ltrain: 0.000800 	Lval: 0.000774
Epoch: 85 	Ltrain: 0.000722 	Lval: 0.000691
Epoch: 90 	Ltrain: 0.000653 	Lval: 0.000616
Epoch: 95 	Ltrain: 0.000578 	Lval: 0.000549
Epoch: 100 	Ltrain: 0.000517 	Lval: 0.000486
Epoch: 105 	Ltrain: 0.000457 	Lval: 0.000431
Epoch: 110 	Ltrain: 0.000404 	Lval: 0.000378
Epoch: 115 	Ltrain: 0.000357 	Lval: 0.000333
Epoch: 120 	Ltrain: 0.000311 	Lval: 0.000293
Epoch: 125 	Ltrain: 0.000286 	Lval: 0.000267
Epoch: 130 	Ltrain: 0.000259 	Lval: 0.000243
Epoch: 135 	Ltrain: 0.000245 	Lval: 0.000221
Epoch: 140 	Ltrain: 0.000228 	Lval: 0.000215
Epoch: 145 	Ltrain: 0.000210 	Lval: 0.000197
Epoch: 150 	Ltrain: 0.000197 	Lval: 0.000183
Epoch: 155 	Ltrain: 0.000174 	Lval: 0.000146
Epoch 00159: reducing learning rate of group 0 to 6.5494e-05.
Epoch: 160 	Ltrain: 0.000169 	Lval: 0.000136
Epoch: 165 	Ltrain: 0.000124 	Lval: 0.000109
Epoch: 170 	Ltrain: 0.000120 	Lval: 0.000104
Epoch: 175 	Ltrain: 0.000116 	Lval: 0.000102
Epoch: 180 	Ltrain: 0.000115 	Lval: 0.000100
Epoch: 185 	Ltrain: 0.000113 	Lval: 0.000098
Epoch: 190 	Ltrain: 0.000111 	Lval: 0.000096
Epoch: 195 	Ltrain: 0.000110 	Lval: 0.000095
Epoch: 200 	Ltrain: 0.000108 	Lval: 0.000093
Epoch: 205 	Ltrain: 0.000106 	Lval: 0.000091
EarlyStopper: stopping at epoch 206 with best_val_loss = 0.000100


	Fold 4/5
Epoch: 1 	Ltrain: 0.055310 	Lval: 0.018754
Epoch: 5 	Ltrain: 0.005939 	Lval: 0.006992
Epoch: 10 	Ltrain: 0.005390 	Lval: 0.006639
Epoch: 15 	Ltrain: 0.005469 	Lval: 0.006397
Epoch: 20 	Ltrain: 0.004783 	Lval: 0.005786
Epoch: 25 	Ltrain: 0.004330 	Lval: 0.004932
Epoch: 30 	Ltrain: 0.003830 	Lval: 0.004303
Epoch: 35 	Ltrain: 0.003624 	Lval: 0.003838
Epoch: 40 	Ltrain: 0.003148 	Lval: 0.002998
Epoch: 45 	Ltrain: 0.002732 	Lval: 0.003037
Epoch: 50 	Ltrain: 0.002194 	Lval: 0.002271
Epoch: 55 	Ltrain: 0.001812 	Lval: 0.002003
Epoch: 60 	Ltrain: 0.001537 	Lval: 0.001579
Epoch: 65 	Ltrain: 0.001258 	Lval: 0.001312
Epoch 00066: reducing learning rate of group 0 to 6.5494e-04.
Epoch: 70 	Ltrain: 0.000667 	Lval: 0.000639
Epoch: 75 	Ltrain: 0.000556 	Lval: 0.000528
Epoch: 80 	Ltrain: 0.000496 	Lval: 0.000467
Epoch: 85 	Ltrain: 0.000451 	Lval: 0.000418
Epoch: 90 	Ltrain: 0.000410 	Lval: 0.000378
Epoch: 95 	Ltrain: 0.000372 	Lval: 0.000339
Epoch: 100 	Ltrain: 0.000337 	Lval: 0.000302
Epoch: 105 	Ltrain: 0.000304 	Lval: 0.000267
Epoch: 110 	Ltrain: 0.000273 	Lval: 0.000236
Epoch: 115 	Ltrain: 0.000244 	Lval: 0.000207
Epoch: 120 	Ltrain: 0.000220 	Lval: 0.000187
Epoch: 125 	Ltrain: 0.000200 	Lval: 0.000167
Epoch: 130 	Ltrain: 0.000175 	Lval: 0.000146
Epoch: 135 	Ltrain: 0.000158 	Lval: 0.000133
Epoch: 140 	Ltrain: 0.000157 	Lval: 0.000140
Epoch: 145 	Ltrain: 0.000145 	Lval: 0.000120
Epoch: 150 	Ltrain: 0.000133 	Lval: 0.000113
Epoch 00154: reducing learning rate of group 0 to 6.5494e-05.
Epoch: 155 	Ltrain: 0.000124 	Lval: 0.000094
Epoch: 160 	Ltrain: 0.000093 	Lval: 0.000074
Epoch: 165 	Ltrain: 0.000089 	Lval: 0.000071
Epoch: 170 	Ltrain: 0.000087 	Lval: 0.000069
Epoch: 175 	Ltrain: 0.000085 	Lval: 0.000067
Epoch: 180 	Ltrain: 0.000084 	Lval: 0.000066
Epoch: 185 	Ltrain: 0.000082 	Lval: 0.000065
Epoch: 190 	Ltrain: 0.000080 	Lval: 0.000063
Epoch: 195 	Ltrain: 0.000078 	Lval: 0.000062
Epoch: 200 	Ltrain: 0.000077 	Lval: 0.000060
Epoch: 205 	Ltrain: 0.000075 	Lval: 0.000059
Epoch: 210 	Ltrain: 0.000073 	Lval: 0.000057
EarlyStopper: stopping at epoch 212 with best_val_loss = 0.000064


	Fold 5/5
Epoch: 1 	Ltrain: 0.047449 	Lval: 0.018499
Epoch: 5 	Ltrain: 0.005975 	Lval: 0.007325
Epoch: 10 	Ltrain: 0.005564 	Lval: 0.006307
Epoch: 15 	Ltrain: 0.004887 	Lval: 0.006283
Epoch: 20 	Ltrain: 0.004809 	Lval: 0.005489
Epoch: 25 	Ltrain: 0.004437 	Lval: 0.005333
Epoch: 30 	Ltrain: 0.004181 	Lval: 0.004663
Epoch: 35 	Ltrain: 0.003798 	Lval: 0.004468
Epoch: 40 	Ltrain: 0.003270 	Lval: 0.003854
Epoch: 45 	Ltrain: 0.002698 	Lval: 0.003066
Epoch: 50 	Ltrain: 0.002571 	Lval: 0.002882
Epoch 00051: reducing learning rate of group 0 to 6.5494e-04.
Epoch: 55 	Ltrain: 0.001626 	Lval: 0.001751
Epoch: 60 	Ltrain: 0.001413 	Lval: 0.001492
Epoch: 65 	Ltrain: 0.001268 	Lval: 0.001323
Epoch: 70 	Ltrain: 0.001153 	Lval: 0.001175
Epoch: 75 	Ltrain: 0.001035 	Lval: 0.001044
Epoch: 80 	Ltrain: 0.000929 	Lval: 0.000912
Epoch: 85 	Ltrain: 0.000828 	Lval: 0.000796
Epoch: 90 	Ltrain: 0.000747 	Lval: 0.000732
Epoch: 95 	Ltrain: 0.000671 	Lval: 0.000646
Epoch: 100 	Ltrain: 0.000610 	Lval: 0.000593
Epoch: 105 	Ltrain: 0.000574 	Lval: 0.000582
Epoch: 110 	Ltrain: 0.000513 	Lval: 0.000511
Epoch: 115 	Ltrain: 0.000481 	Lval: 0.000492
Epoch: 120 	Ltrain: 0.000425 	Lval: 0.000401
Epoch: 125 	Ltrain: 0.000404 	Lval: 0.000399
Epoch: 130 	Ltrain: 0.000373 	Lval: 0.000347
Epoch: 135 	Ltrain: 0.000365 	Lval: 0.000356
Epoch 00140: reducing learning rate of group 0 to 6.5494e-05.
Epoch: 140 	Ltrain: 0.000333 	Lval: 0.000341
Epoch: 145 	Ltrain: 0.000232 	Lval: 0.000211
Epoch: 150 	Ltrain: 0.000222 	Lval: 0.000201
Epoch: 155 	Ltrain: 0.000217 	Lval: 0.000196
Epoch: 160 	Ltrain: 0.000212 	Lval: 0.000192
Epoch: 165 	Ltrain: 0.000208 	Lval: 0.000188
Epoch: 170 	Ltrain: 0.000204 	Lval: 0.000184
Epoch: 175 	Ltrain: 0.000200 	Lval: 0.000180
Epoch: 180 	Ltrain: 0.000196 	Lval: 0.000175
Epoch: 185 	Ltrain: 0.000192 	Lval: 0.000171
Epoch: 190 	Ltrain: 0.000188 	Lval: 0.000167
Epoch: 195 	Ltrain: 0.000183 	Lval: 0.000163
Epoch: 200 	Ltrain: 0.000179 	Lval: 0.000159
Epoch: 205 	Ltrain: 0.000175 	Lval: 0.000154
Epoch: 210 	Ltrain: 0.000170 	Lval: 0.000149
Epoch: 215 	Ltrain: 0.000166 	Lval: 0.000144
Epoch: 220 	Ltrain: 0.000161 	Lval: 0.000140
Epoch: 225 	Ltrain: 0.000156 	Lval: 0.000135
Epoch: 230 	Ltrain: 0.000151 	Lval: 0.000130
Epoch: 235 	Ltrain: 0.000146 	Lval: 0.000126
Epoch: 240 	Ltrain: 0.000142 	Lval: 0.000122
Epoch: 245 	Ltrain: 0.000137 	Lval: 0.000117
Epoch: 250 	Ltrain: 0.000132 	Lval: 0.000112
Epoch: 255 	Ltrain: 0.000127 	Lval: 0.000108
Epoch: 260 	Ltrain: 0.000123 	Lval: 0.000104
Epoch: 265 	Ltrain: 0.000119 	Lval: 0.000102
Epoch: 270 	Ltrain: 0.000115 	Lval: 0.000096
Epoch: 275 	Ltrain: 0.000111 	Lval: 0.000093
Epoch: 280 	Ltrain: 0.000108 	Lval: 0.000090
Epoch: 285 	Ltrain: 0.000106 	Lval: 0.000090
Epoch 00288: reducing learning rate of group 0 to 6.5494e-06.
Epoch: 290 	Ltrain: 0.000098 	Lval: 0.000081
Epoch: 295 	Ltrain: 0.000096 	Lval: 0.000080
Epoch: 300 	Ltrain: 0.000095 	Lval: 0.000079
Epoch: 305 	Ltrain: 0.000095 	Lval: 0.000079
Epoch: 310 	Ltrain: 0.000094 	Lval: 0.000078
Epoch: 315 	Ltrain: 0.000094 	Lval: 0.000078
Epoch: 320 	Ltrain: 0.000093 	Lval: 0.000077
Epoch: 325 	Ltrain: 0.000093 	Lval: 0.000077
Epoch: 330 	Ltrain: 0.000092 	Lval: 0.000076
Epoch: 335 	Ltrain: 0.000092 	Lval: 0.000076
EarlyStopper: stopping at epoch 336 with best_val_loss = 0.000078

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0025736876893316597
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.9630149917573906e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 28
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.025774 	Lval: 0.019763
Epoch: 5 	Ltrain: 0.014316 	Lval: 0.013218
Epoch: 10 	Ltrain: 0.009163 	Lval: 0.008314
Epoch: 15 	Ltrain: 0.007908 	Lval: 0.008258
Epoch: 20 	Ltrain: 0.006852 	Lval: 0.006990
Epoch: 25 	Ltrain: 0.007036 	Lval: 0.007201
Epoch 00027: reducing learning rate of group 0 to 2.5737e-04.
Epoch: 30 	Ltrain: 0.005987 	Lval: 0.006054
Epoch: 35 	Ltrain: 0.005835 	Lval: 0.005896
Epoch: 40 	Ltrain: 0.005727 	Lval: 0.005778
Epoch: 45 	Ltrain: 0.005692 	Lval: 0.005715
Epoch: 50 	Ltrain: 0.005766 	Lval: 0.005621
Epoch: 55 	Ltrain: 0.005436 	Lval: 0.005473
Epoch: 60 	Ltrain: 0.005414 	Lval: 0.005337
Epoch: 65 	Ltrain: 0.005473 	Lval: 0.005256
Epoch: 70 	Ltrain: 0.005365 	Lval: 0.005115
Epoch: 75 	Ltrain: 0.005117 	Lval: 0.004967
Epoch: 80 	Ltrain: 0.004930 	Lval: 0.004838
Epoch: 85 	Ltrain: 0.004872 	Lval: 0.004715
Epoch: 90 	Ltrain: 0.004663 	Lval: 0.004612
Epoch: 95 	Ltrain: 0.004456 	Lval: 0.004468
Epoch: 100 	Ltrain: 0.004561 	Lval: 0.004382
Epoch: 105 	Ltrain: 0.004583 	Lval: 0.004249
Epoch: 110 	Ltrain: 0.004238 	Lval: 0.004083
Epoch: 115 	Ltrain: 0.004103 	Lval: 0.004043
Epoch: 120 	Ltrain: 0.004130 	Lval: 0.003822
Epoch: 125 	Ltrain: 0.003737 	Lval: 0.003640
Epoch: 130 	Ltrain: 0.003752 	Lval: 0.003550
Epoch: 135 	Ltrain: 0.003344 	Lval: 0.003361
Epoch: 140 	Ltrain: 0.003446 	Lval: 0.003511
Epoch: 145 	Ltrain: 0.003112 	Lval: 0.002989
Epoch: 150 	Ltrain: 0.003495 	Lval: 0.003551
Epoch 00151: reducing learning rate of group 0 to 2.5737e-05.
Epoch: 155 	Ltrain: 0.002885 	Lval: 0.002795
Epoch: 160 	Ltrain: 0.002815 	Lval: 0.002744
Epoch: 165 	Ltrain: 0.002830 	Lval: 0.002691
Epoch: 170 	Ltrain: 0.002751 	Lval: 0.002662
Epoch: 175 	Ltrain: 0.002662 	Lval: 0.002647
Epoch: 180 	Ltrain: 0.002810 	Lval: 0.002616
Epoch: 185 	Ltrain: 0.002576 	Lval: 0.002593
Epoch: 190 	Ltrain: 0.002582 	Lval: 0.002570
Epoch: 195 	Ltrain: 0.002577 	Lval: 0.002555
Epoch: 200 	Ltrain: 0.002560 	Lval: 0.002532
Epoch: 205 	Ltrain: 0.002720 	Lval: 0.002512
Epoch: 210 	Ltrain: 0.002581 	Lval: 0.002493
Epoch: 215 	Ltrain: 0.002548 	Lval: 0.002475
Epoch: 220 	Ltrain: 0.002571 	Lval: 0.002452
Epoch: 225 	Ltrain: 0.002456 	Lval: 0.002437
Epoch: 230 	Ltrain: 0.002466 	Lval: 0.002425
Epoch: 235 	Ltrain: 0.002522 	Lval: 0.002402
Epoch: 240 	Ltrain: 0.002387 	Lval: 0.002383
Epoch: 245 	Ltrain: 0.002401 	Lval: 0.002362
Epoch: 250 	Ltrain: 0.002405 	Lval: 0.002357
Epoch: 255 	Ltrain: 0.002490 	Lval: 0.002338
Epoch: 260 	Ltrain: 0.002564 	Lval: 0.002316
Epoch: 265 	Ltrain: 0.002297 	Lval: 0.002301
Epoch: 270 	Ltrain: 0.002439 	Lval: 0.002283
Epoch: 275 	Ltrain: 0.002362 	Lval: 0.002267
Epoch: 280 	Ltrain: 0.002387 	Lval: 0.002246
Epoch: 285 	Ltrain: 0.002333 	Lval: 0.002229
Epoch: 290 	Ltrain: 0.002330 	Lval: 0.002210
Epoch: 295 	Ltrain: 0.002211 	Lval: 0.002194
Epoch: 300 	Ltrain: 0.002151 	Lval: 0.002181
Epoch: 305 	Ltrain: 0.002297 	Lval: 0.002162
Epoch: 310 	Ltrain: 0.002211 	Lval: 0.002145
Epoch: 315 	Ltrain: 0.002223 	Lval: 0.002122
Epoch: 320 	Ltrain: 0.002192 	Lval: 0.002106
Epoch: 325 	Ltrain: 0.002191 	Lval: 0.002090
Epoch: 330 	Ltrain: 0.002073 	Lval: 0.002074
Epoch: 335 	Ltrain: 0.002081 	Lval: 0.002058
Epoch: 340 	Ltrain: 0.002098 	Lval: 0.002051
Epoch: 345 	Ltrain: 0.002071 	Lval: 0.002024
Epoch: 350 	Ltrain: 0.002128 	Lval: 0.002016
Epoch: 355 	Ltrain: 0.002050 	Lval: 0.001996
Epoch: 360 	Ltrain: 0.002092 	Lval: 0.001978
Epoch: 365 	Ltrain: 0.002094 	Lval: 0.001964
Epoch: 370 	Ltrain: 0.001993 	Lval: 0.001947
Epoch: 375 	Ltrain: 0.001975 	Lval: 0.001937
Epoch: 380 	Ltrain: 0.001976 	Lval: 0.001920
Epoch: 385 	Ltrain: 0.002013 	Lval: 0.001902
Epoch: 390 	Ltrain: 0.001950 	Lval: 0.001892
Epoch: 395 	Ltrain: 0.001862 	Lval: 0.001876
Epoch: 400 	Ltrain: 0.001838 	Lval: 0.001854
Epoch: 405 	Ltrain: 0.001854 	Lval: 0.001840
Epoch: 410 	Ltrain: 0.001884 	Lval: 0.001827
Epoch: 415 	Ltrain: 0.001855 	Lval: 0.001816
Epoch: 420 	Ltrain: 0.001861 	Lval: 0.001801
Epoch: 425 	Ltrain: 0.001798 	Lval: 0.001784
Epoch: 430 	Ltrain: 0.001803 	Lval: 0.001771
Epoch: 435 	Ltrain: 0.001785 	Lval: 0.001760
Epoch: 440 	Ltrain: 0.001817 	Lval: 0.001740
Epoch: 445 	Ltrain: 0.001744 	Lval: 0.001732
Epoch: 450 	Ltrain: 0.001713 	Lval: 0.001713
Epoch: 455 	Ltrain: 0.001731 	Lval: 0.001705
Epoch: 460 	Ltrain: 0.001783 	Lval: 0.001688
Epoch: 465 	Ltrain: 0.001767 	Lval: 0.001677
Epoch: 470 	Ltrain: 0.001694 	Lval: 0.001659
Epoch: 475 	Ltrain: 0.001741 	Lval: 0.001649
Epoch: 480 	Ltrain: 0.001641 	Lval: 0.001641
Epoch: 485 	Ltrain: 0.001736 	Lval: 0.001624
Epoch: 490 	Ltrain: 0.001709 	Lval: 0.001610
Epoch: 495 	Ltrain: 0.001679 	Lval: 0.001602
Epoch: 500 	Ltrain: 0.001625 	Lval: 0.001583
Epoch: 505 	Ltrain: 0.001607 	Lval: 0.001570
Epoch: 510 	Ltrain: 0.001605 	Lval: 0.001557
Epoch: 515 	Ltrain: 0.001575 	Lval: 0.001549
Epoch: 520 	Ltrain: 0.001602 	Lval: 0.001534
Epoch: 525 	Ltrain: 0.001602 	Lval: 0.001519
Epoch: 530 	Ltrain: 0.001544 	Lval: 0.001508
Epoch: 535 	Ltrain: 0.001569 	Lval: 0.001507
Epoch 00536: reducing learning rate of group 0 to 2.5737e-06.
Epoch: 540 	Ltrain: 0.001491 	Lval: 0.001491
Epoch: 545 	Ltrain: 0.001562 	Lval: 0.001489
Epoch: 550 	Ltrain: 0.001504 	Lval: 0.001487
Epoch: 555 	Ltrain: 0.001497 	Lval: 0.001486
Epoch: 560 	Ltrain: 0.001552 	Lval: 0.001484
Epoch: 565 	Ltrain: 0.001502 	Lval: 0.001484
Epoch: 570 	Ltrain: 0.001471 	Lval: 0.001482
Epoch: 575 	Ltrain: 0.001486 	Lval: 0.001481
Epoch: 580 	Ltrain: 0.001536 	Lval: 0.001480
Epoch: 585 	Ltrain: 0.001552 	Lval: 0.001479
Epoch: 590 	Ltrain: 0.001471 	Lval: 0.001477
EarlyStopper: stopping at epoch 593 with best_val_loss = 0.001483


	Fold 2/5
Epoch: 1 	Ltrain: 0.022967 	Lval: 0.016372
Epoch: 5 	Ltrain: 0.008731 	Lval: 0.007725
Epoch: 10 	Ltrain: 0.007535 	Lval: 0.007521
Epoch: 15 	Ltrain: 0.006551 	Lval: 0.006576
Epoch: 20 	Ltrain: 0.006623 	Lval: 0.006591
Epoch: 25 	Ltrain: 0.006799 	Lval: 0.006502
Epoch 00026: reducing learning rate of group 0 to 2.5737e-04.
Epoch: 30 	Ltrain: 0.005556 	Lval: 0.005648
Epoch: 35 	Ltrain: 0.005328 	Lval: 0.005560
Epoch: 40 	Ltrain: 0.005300 	Lval: 0.005469
Epoch: 45 	Ltrain: 0.005164 	Lval: 0.005328
Epoch: 50 	Ltrain: 0.005133 	Lval: 0.005271
Epoch: 55 	Ltrain: 0.005143 	Lval: 0.005079
Epoch: 60 	Ltrain: 0.004788 	Lval: 0.004891
Epoch: 65 	Ltrain: 0.004670 	Lval: 0.004658
Epoch: 70 	Ltrain: 0.004511 	Lval: 0.004518
Epoch: 75 	Ltrain: 0.004432 	Lval: 0.004323
Epoch: 80 	Ltrain: 0.004127 	Lval: 0.004017
Epoch: 85 	Ltrain: 0.003942 	Lval: 0.003761
Epoch: 90 	Ltrain: 0.003640 	Lval: 0.003549
Epoch: 95 	Ltrain: 0.003465 	Lval: 0.003365
Epoch: 100 	Ltrain: 0.003336 	Lval: 0.003192
Epoch: 105 	Ltrain: 0.003076 	Lval: 0.003145
Epoch 00107: reducing learning rate of group 0 to 2.5737e-05.
Epoch: 110 	Ltrain: 0.002853 	Lval: 0.002783
Epoch: 115 	Ltrain: 0.002712 	Lval: 0.002729
Epoch: 120 	Ltrain: 0.002750 	Lval: 0.002690
Epoch: 125 	Ltrain: 0.002670 	Lval: 0.002661
Epoch: 130 	Ltrain: 0.002635 	Lval: 0.002642
Epoch: 135 	Ltrain: 0.002614 	Lval: 0.002619
Epoch: 140 	Ltrain: 0.002610 	Lval: 0.002596
Epoch: 145 	Ltrain: 0.002606 	Lval: 0.002571
Epoch: 150 	Ltrain: 0.002578 	Lval: 0.002546
Epoch: 155 	Ltrain: 0.002546 	Lval: 0.002532
Epoch: 160 	Ltrain: 0.002531 	Lval: 0.002504
Epoch: 165 	Ltrain: 0.002516 	Lval: 0.002485
Epoch: 170 	Ltrain: 0.002465 	Lval: 0.002453
Epoch: 175 	Ltrain: 0.002425 	Lval: 0.002437
Epoch: 180 	Ltrain: 0.002443 	Lval: 0.002417
Epoch: 185 	Ltrain: 0.002397 	Lval: 0.002394
Epoch: 190 	Ltrain: 0.002411 	Lval: 0.002372
Epoch: 195 	Ltrain: 0.002355 	Lval: 0.002346
Epoch: 200 	Ltrain: 0.002362 	Lval: 0.002327
Epoch: 205 	Ltrain: 0.002328 	Lval: 0.002298
Epoch: 210 	Ltrain: 0.002306 	Lval: 0.002289
Epoch: 215 	Ltrain: 0.002310 	Lval: 0.002270
Epoch: 220 	Ltrain: 0.002269 	Lval: 0.002251
Epoch: 225 	Ltrain: 0.002251 	Lval: 0.002223
Epoch: 230 	Ltrain: 0.002233 	Lval: 0.002197
Epoch: 235 	Ltrain: 0.002201 	Lval: 0.002177
Epoch: 240 	Ltrain: 0.002183 	Lval: 0.002157
Epoch: 245 	Ltrain: 0.002184 	Lval: 0.002137
Epoch: 250 	Ltrain: 0.002136 	Lval: 0.002117
Epoch: 255 	Ltrain: 0.002121 	Lval: 0.002095
Epoch: 260 	Ltrain: 0.002094 	Lval: 0.002075
Epoch: 265 	Ltrain: 0.002075 	Lval: 0.002062
Epoch: 270 	Ltrain: 0.002066 	Lval: 0.002033
Epoch: 275 	Ltrain: 0.002050 	Lval: 0.002022
Epoch: 280 	Ltrain: 0.002020 	Lval: 0.001998
Epoch: 285 	Ltrain: 0.002008 	Lval: 0.001979
Epoch: 290 	Ltrain: 0.001994 	Lval: 0.001958
Epoch: 295 	Ltrain: 0.001958 	Lval: 0.001940
Epoch: 300 	Ltrain: 0.001934 	Lval: 0.001919
Epoch: 305 	Ltrain: 0.001922 	Lval: 0.001900
Epoch: 310 	Ltrain: 0.001924 	Lval: 0.001886
Epoch: 315 	Ltrain: 0.001892 	Lval: 0.001865
Epoch: 320 	Ltrain: 0.001894 	Lval: 0.001848
Epoch: 325 	Ltrain: 0.001847 	Lval: 0.001838
Epoch: 330 	Ltrain: 0.001817 	Lval: 0.001809
Epoch: 335 	Ltrain: 0.001842 	Lval: 0.001793
Epoch: 340 	Ltrain: 0.001792 	Lval: 0.001776
Epoch: 345 	Ltrain: 0.001793 	Lval: 0.001763
Epoch: 350 	Ltrain: 0.001785 	Lval: 0.001740
Epoch: 355 	Ltrain: 0.001744 	Lval: 0.001729
Epoch: 360 	Ltrain: 0.001746 	Lval: 0.001714
Epoch: 365 	Ltrain: 0.001712 	Lval: 0.001702
Epoch: 370 	Ltrain: 0.001672 	Lval: 0.001674
Epoch: 375 	Ltrain: 0.001675 	Lval: 0.001660
Epoch: 380 	Ltrain: 0.001687 	Lval: 0.001646
Epoch: 385 	Ltrain: 0.001667 	Lval: 0.001628
Epoch: 390 	Ltrain: 0.001621 	Lval: 0.001612
Epoch: 395 	Ltrain: 0.001638 	Lval: 0.001602
Epoch: 400 	Ltrain: 0.001602 	Lval: 0.001581
Epoch: 405 	Ltrain: 0.001569 	Lval: 0.001570
Epoch: 410 	Ltrain: 0.001555 	Lval: 0.001558
Epoch: 415 	Ltrain: 0.001563 	Lval: 0.001539
Epoch: 420 	Ltrain: 0.001556 	Lval: 0.001530
Epoch: 425 	Ltrain: 0.001531 	Lval: 0.001510
Epoch: 430 	Ltrain: 0.001515 	Lval: 0.001496
Epoch: 435 	Ltrain: 0.001498 	Lval: 0.001480
Epoch: 440 	Ltrain: 0.001491 	Lval: 0.001465
Epoch: 445 	Ltrain: 0.001466 	Lval: 0.001450
Epoch: 450 	Ltrain: 0.001451 	Lval: 0.001439
Epoch: 455 	Ltrain: 0.001426 	Lval: 0.001422
Epoch: 460 	Ltrain: 0.001428 	Lval: 0.001413
Epoch: 465 	Ltrain: 0.001404 	Lval: 0.001397
Epoch: 470 	Ltrain: 0.001425 	Lval: 0.001387
Epoch: 475 	Ltrain: 0.001381 	Lval: 0.001373
Epoch: 480 	Ltrain: 0.001367 	Lval: 0.001355
Epoch: 485 	Ltrain: 0.001356 	Lval: 0.001344
Epoch: 490 	Ltrain: 0.001332 	Lval: 0.001334
Epoch: 495 	Ltrain: 0.001331 	Lval: 0.001315
Epoch: 500 	Ltrain: 0.001313 	Lval: 0.001311
Epoch 00503: reducing learning rate of group 0 to 2.5737e-06.
Epoch: 505 	Ltrain: 0.001293 	Lval: 0.001294
Epoch: 510 	Ltrain: 0.001281 	Lval: 0.001291
Epoch: 515 	Ltrain: 0.001310 	Lval: 0.001290
Epoch: 520 	Ltrain: 0.001283 	Lval: 0.001288
Epoch: 525 	Ltrain: 0.001289 	Lval: 0.001287
Epoch: 530 	Ltrain: 0.001282 	Lval: 0.001286
Epoch: 535 	Ltrain: 0.001298 	Lval: 0.001284
Epoch: 540 	Ltrain: 0.001289 	Lval: 0.001283
Epoch: 545 	Ltrain: 0.001286 	Lval: 0.001282
Epoch: 550 	Ltrain: 0.001292 	Lval: 0.001280
EarlyStopper: stopping at epoch 550 with best_val_loss = 0.001287


	Fold 3/5
Epoch: 1 	Ltrain: 0.020593 	Lval: 0.012977
Epoch: 5 	Ltrain: 0.007071 	Lval: 0.007968
Epoch: 10 	Ltrain: 0.006698 	Lval: 0.007462
Epoch: 15 	Ltrain: 0.006153 	Lval: 0.006642
Epoch: 20 	Ltrain: 0.005815 	Lval: 0.006475
Epoch: 25 	Ltrain: 0.006076 	Lval: 0.006500
Epoch 00030: reducing learning rate of group 0 to 2.5737e-04.
Epoch: 30 	Ltrain: 0.005300 	Lval: 0.006155
Epoch: 35 	Ltrain: 0.004606 	Lval: 0.005271
Epoch: 40 	Ltrain: 0.004488 	Lval: 0.005140
Epoch: 45 	Ltrain: 0.004352 	Lval: 0.004880
Epoch: 50 	Ltrain: 0.004236 	Lval: 0.004616
Epoch: 55 	Ltrain: 0.004110 	Lval: 0.004413
Epoch: 60 	Ltrain: 0.003930 	Lval: 0.004118
Epoch: 65 	Ltrain: 0.003827 	Lval: 0.004012
Epoch: 70 	Ltrain: 0.003623 	Lval: 0.003816
Epoch: 75 	Ltrain: 0.003462 	Lval: 0.003519
Epoch: 80 	Ltrain: 0.003331 	Lval: 0.003419
Epoch: 85 	Ltrain: 0.003149 	Lval: 0.003185
Epoch: 90 	Ltrain: 0.003019 	Lval: 0.003085
Epoch: 95 	Ltrain: 0.002755 	Lval: 0.002761
Epoch: 100 	Ltrain: 0.002580 	Lval: 0.002609
Epoch: 105 	Ltrain: 0.002537 	Lval: 0.002498
Epoch: 110 	Ltrain: 0.002333 	Lval: 0.002303
Epoch: 115 	Ltrain: 0.002206 	Lval: 0.002116
Epoch: 120 	Ltrain: 0.001946 	Lval: 0.001999
Epoch: 125 	Ltrain: 0.001792 	Lval: 0.001772
Epoch: 130 	Ltrain: 0.001704 	Lval: 0.001769
Epoch: 135 	Ltrain: 0.001555 	Lval: 0.001541
Epoch: 140 	Ltrain: 0.001479 	Lval: 0.001472
Epoch: 145 	Ltrain: 0.001393 	Lval: 0.001414
Epoch: 150 	Ltrain: 0.001252 	Lval: 0.001289
Epoch: 155 	Ltrain: 0.001177 	Lval: 0.001182
Epoch: 160 	Ltrain: 0.001092 	Lval: 0.001068
Epoch: 165 	Ltrain: 0.001011 	Lval: 0.000984
Epoch 00169: reducing learning rate of group 0 to 2.5737e-05.
Epoch: 170 	Ltrain: 0.000954 	Lval: 0.000904
Epoch: 175 	Ltrain: 0.000851 	Lval: 0.000857
Epoch: 180 	Ltrain: 0.000840 	Lval: 0.000843
Epoch: 185 	Ltrain: 0.000831 	Lval: 0.000832
Epoch: 190 	Ltrain: 0.000824 	Lval: 0.000823
Epoch: 195 	Ltrain: 0.000813 	Lval: 0.000814
Epoch: 200 	Ltrain: 0.000802 	Lval: 0.000804
Epoch: 205 	Ltrain: 0.000797 	Lval: 0.000796
Epoch: 210 	Ltrain: 0.000785 	Lval: 0.000786
Epoch: 215 	Ltrain: 0.000777 	Lval: 0.000777
Epoch: 220 	Ltrain: 0.000771 	Lval: 0.000767
Epoch: 225 	Ltrain: 0.000763 	Lval: 0.000757
Epoch: 230 	Ltrain: 0.000761 	Lval: 0.000750
Epoch: 235 	Ltrain: 0.000745 	Lval: 0.000739
Epoch: 240 	Ltrain: 0.000735 	Lval: 0.000730
Epoch: 245 	Ltrain: 0.000723 	Lval: 0.000720
Epoch: 250 	Ltrain: 0.000715 	Lval: 0.000710
Epoch: 255 	Ltrain: 0.000706 	Lval: 0.000700
Epoch: 260 	Ltrain: 0.000698 	Lval: 0.000690
Epoch: 265 	Ltrain: 0.000689 	Lval: 0.000680
Epoch: 270 	Ltrain: 0.000694 	Lval: 0.000670
Epoch: 275 	Ltrain: 0.000669 	Lval: 0.000660
Epoch: 280 	Ltrain: 0.000659 	Lval: 0.000651
Epoch: 285 	Ltrain: 0.000648 	Lval: 0.000640
Epoch: 290 	Ltrain: 0.000641 	Lval: 0.000632
Epoch: 295 	Ltrain: 0.000634 	Lval: 0.000621
Epoch: 300 	Ltrain: 0.000621 	Lval: 0.000611
Epoch: 305 	Ltrain: 0.000609 	Lval: 0.000605
Epoch: 310 	Ltrain: 0.000601 	Lval: 0.000592
Epoch: 315 	Ltrain: 0.000595 	Lval: 0.000584
Epoch: 320 	Ltrain: 0.000580 	Lval: 0.000573
Epoch: 325 	Ltrain: 0.000574 	Lval: 0.000563
Epoch: 330 	Ltrain: 0.000564 	Lval: 0.000555
Epoch: 335 	Ltrain: 0.000574 	Lval: 0.000546
Epoch: 340 	Ltrain: 0.000552 	Lval: 0.000537
Epoch: 345 	Ltrain: 0.000540 	Lval: 0.000528
Epoch: 350 	Ltrain: 0.000532 	Lval: 0.000518
Epoch: 355 	Ltrain: 0.000524 	Lval: 0.000510
Epoch: 360 	Ltrain: 0.000515 	Lval: 0.000501
Epoch: 365 	Ltrain: 0.000506 	Lval: 0.000491
Epoch: 370 	Ltrain: 0.000497 	Lval: 0.000482
Epoch: 375 	Ltrain: 0.000499 	Lval: 0.000474
Epoch: 380 	Ltrain: 0.000484 	Lval: 0.000467
Epoch: 385 	Ltrain: 0.000472 	Lval: 0.000460
Epoch: 390 	Ltrain: 0.000464 	Lval: 0.000452
Epoch: 395 	Ltrain: 0.000458 	Lval: 0.000445
Epoch: 400 	Ltrain: 0.000453 	Lval: 0.000435
Epoch: 405 	Ltrain: 0.000454 	Lval: 0.000426
Epoch: 410 	Ltrain: 0.000437 	Lval: 0.000420
Epoch: 415 	Ltrain: 0.000430 	Lval: 0.000413
Epoch: 420 	Ltrain: 0.000424 	Lval: 0.000406
Epoch: 425 	Ltrain: 0.000419 	Lval: 0.000398
Epoch: 430 	Ltrain: 0.000410 	Lval: 0.000393
Epoch: 435 	Ltrain: 0.000402 	Lval: 0.000386
Epoch: 440 	Ltrain: 0.000395 	Lval: 0.000380
Epoch: 445 	Ltrain: 0.000390 	Lval: 0.000373
Epoch: 450 	Ltrain: 0.000385 	Lval: 0.000367
Epoch: 455 	Ltrain: 0.000375 	Lval: 0.000359
Epoch: 460 	Ltrain: 0.000369 	Lval: 0.000352
Epoch: 465 	Ltrain: 0.000363 	Lval: 0.000347
Epoch: 470 	Ltrain: 0.000357 	Lval: 0.000342
Epoch: 475 	Ltrain: 0.000352 	Lval: 0.000336
Epoch: 480 	Ltrain: 0.000345 	Lval: 0.000328
Epoch: 485 	Ltrain: 0.000340 	Lval: 0.000323
Epoch: 490 	Ltrain: 0.000339 	Lval: 0.000320
Epoch: 495 	Ltrain: 0.000328 	Lval: 0.000312
Epoch: 500 	Ltrain: 0.000326 	Lval: 0.000309
Epoch: 505 	Ltrain: 0.000319 	Lval: 0.000303
Epoch: 510 	Ltrain: 0.000314 	Lval: 0.000299
Epoch: 515 	Ltrain: 0.000308 	Lval: 0.000293
Epoch: 520 	Ltrain: 0.000304 	Lval: 0.000289
Epoch: 525 	Ltrain: 0.000299 	Lval: 0.000283
Epoch: 530 	Ltrain: 0.000296 	Lval: 0.000282
Epoch: 535 	Ltrain: 0.000289 	Lval: 0.000274
Epoch: 540 	Ltrain: 0.000284 	Lval: 0.000270
Epoch: 545 	Ltrain: 0.000282 	Lval: 0.000267
Epoch: 550 	Ltrain: 0.000275 	Lval: 0.000262
Epoch: 555 	Ltrain: 0.000271 	Lval: 0.000257
Epoch: 560 	Ltrain: 0.000266 	Lval: 0.000253
Epoch: 565 	Ltrain: 0.000267 	Lval: 0.000250
Epoch: 570 	Ltrain: 0.000257 	Lval: 0.000247
Epoch: 575 	Ltrain: 0.000256 	Lval: 0.000244
Epoch: 580 	Ltrain: 0.000252 	Lval: 0.000240
Epoch: 585 	Ltrain: 0.000246 	Lval: 0.000235
Epoch: 590 	Ltrain: 0.000242 	Lval: 0.000231
Epoch: 595 	Ltrain: 0.000242 	Lval: 0.000229
Epoch: 600 	Ltrain: 0.000235 	Lval: 0.000225
Epoch: 605 	Ltrain: 0.000233 	Lval: 0.000223
Epoch: 610 	Ltrain: 0.000229 	Lval: 0.000218
Epoch: 615 	Ltrain: 0.000232 	Lval: 0.000215
Epoch: 620 	Ltrain: 0.000221 	Lval: 0.000212
Epoch: 625 	Ltrain: 0.000217 	Lval: 0.000208
Epoch: 630 	Ltrain: 0.000214 	Lval: 0.000206
Epoch: 635 	Ltrain: 0.000214 	Lval: 0.000203
Epoch: 640 	Ltrain: 0.000210 	Lval: 0.000202
Epoch: 645 	Ltrain: 0.000207 	Lval: 0.000198
Epoch: 650 	Ltrain: 0.000203 	Lval: 0.000196
Epoch: 655 	Ltrain: 0.000199 	Lval: 0.000191
Epoch: 660 	Ltrain: 0.000196 	Lval: 0.000188
Epoch: 665 	Ltrain: 0.000196 	Lval: 0.000189
Epoch 00666: reducing learning rate of group 0 to 2.5737e-06.
Epoch: 670 	Ltrain: 0.000189 	Lval: 0.000183
Epoch: 675 	Ltrain: 0.000188 	Lval: 0.000182
Epoch: 680 	Ltrain: 0.000187 	Lval: 0.000182
Epoch: 685 	Ltrain: 0.000187 	Lval: 0.000182
Epoch: 690 	Ltrain: 0.000187 	Lval: 0.000181
Epoch: 695 	Ltrain: 0.000186 	Lval: 0.000181
EarlyStopper: stopping at epoch 695 with best_val_loss = 0.000184


	Fold 4/5
Epoch: 1 	Ltrain: 0.019348 	Lval: 0.012969
Epoch: 5 	Ltrain: 0.006100 	Lval: 0.009461
Epoch: 10 	Ltrain: 0.005460 	Lval: 0.007796
Epoch 00013: reducing learning rate of group 0 to 2.5737e-04.
Epoch: 15 	Ltrain: 0.004951 	Lval: 0.006271
Epoch: 20 	Ltrain: 0.004843 	Lval: 0.006103
Epoch: 25 	Ltrain: 0.004756 	Lval: 0.005852
Epoch: 30 	Ltrain: 0.004665 	Lval: 0.005783
Epoch: 35 	Ltrain: 0.004599 	Lval: 0.005639
Epoch: 40 	Ltrain: 0.004648 	Lval: 0.005791
Epoch: 45 	Ltrain: 0.004477 	Lval: 0.005469
Epoch: 50 	Ltrain: 0.004415 	Lval: 0.005359
Epoch: 55 	Ltrain: 0.004290 	Lval: 0.005206
Epoch: 60 	Ltrain: 0.004208 	Lval: 0.004840
Epoch 00064: reducing learning rate of group 0 to 2.5737e-05.
Epoch: 65 	Ltrain: 0.003924 	Lval: 0.004772
Epoch: 70 	Ltrain: 0.003809 	Lval: 0.004635
Epoch: 75 	Ltrain: 0.003790 	Lval: 0.004609
Epoch: 80 	Ltrain: 0.003751 	Lval: 0.004590
Epoch 00081: reducing learning rate of group 0 to 2.5737e-06.
Epoch: 85 	Ltrain: 0.003723 	Lval: 0.004582
Epoch: 90 	Ltrain: 0.003730 	Lval: 0.004571
Epoch: 95 	Ltrain: 0.003721 	Lval: 0.004570
Epoch 00096: reducing learning rate of group 0 to 2.5737e-07.
Epoch: 100 	Ltrain: 0.003727 	Lval: 0.004567
Epoch: 105 	Ltrain: 0.003727 	Lval: 0.004567
EarlyStopper: stopping at epoch 104 with best_val_loss = 0.004571


	Fold 5/5
Epoch: 1 	Ltrain: 0.018681 	Lval: 0.014624
Epoch: 5 	Ltrain: 0.006473 	Lval: 0.007610
Epoch: 10 	Ltrain: 0.005516 	Lval: 0.007234
Epoch 00013: reducing learning rate of group 0 to 2.5737e-04.
Epoch: 15 	Ltrain: 0.004956 	Lval: 0.006718
Epoch: 20 	Ltrain: 0.004902 	Lval: 0.006161
Epoch: 25 	Ltrain: 0.004841 	Lval: 0.006147
Epoch 00030: reducing learning rate of group 0 to 2.5737e-05.
Epoch: 30 	Ltrain: 0.004763 	Lval: 0.006023
Epoch: 35 	Ltrain: 0.004700 	Lval: 0.006012
Epoch: 40 	Ltrain: 0.004678 	Lval: 0.005956
Epoch 00043: reducing learning rate of group 0 to 2.5737e-06.
Epoch: 45 	Ltrain: 0.004666 	Lval: 0.005960
Epoch: 50 	Ltrain: 0.004660 	Lval: 0.005951
Epoch 00055: reducing learning rate of group 0 to 2.5737e-07.
Epoch: 55 	Ltrain: 0.004661 	Lval: 0.005950
Epoch: 60 	Ltrain: 0.004678 	Lval: 0.005950
Epoch: 65 	Ltrain: 0.004676 	Lval: 0.005949
Epoch 00067: reducing learning rate of group 0 to 2.5737e-08.
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.005950

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006570510181281101
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.150307270783105e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.192531 	Lval: 0.029275
Epoch: 5 	Ltrain: 0.020734 	Lval: 0.021005
Epoch: 10 	Ltrain: 0.017979 	Lval: 0.015078
Epoch: 15 	Ltrain: 0.011952 	Lval: 0.014984
Epoch 00020: reducing learning rate of group 0 to 6.5705e-04.
Epoch: 20 	Ltrain: 0.010531 	Lval: 0.011915
Epoch: 25 	Ltrain: 0.009215 	Lval: 0.008853
Epoch: 30 	Ltrain: 0.009054 	Lval: 0.008836
Epoch 00032: reducing learning rate of group 0 to 6.5705e-05.
Epoch: 35 	Ltrain: 0.008583 	Lval: 0.008894
Epoch: 40 	Ltrain: 0.008123 	Lval: 0.008484
Epoch: 45 	Ltrain: 0.008130 	Lval: 0.008275
Epoch: 50 	Ltrain: 0.010051 	Lval: 0.008246
Epoch 00054: reducing learning rate of group 0 to 6.5705e-06.
Epoch: 55 	Ltrain: 0.008554 	Lval: 0.008283
Epoch: 60 	Ltrain: 0.010671 	Lval: 0.008288
Epoch: 65 	Ltrain: 0.011277 	Lval: 0.008272
Epoch 00066: reducing learning rate of group 0 to 6.5705e-07.
Epoch: 70 	Ltrain: 0.007632 	Lval: 0.008268
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.008247


	Fold 2/5
Epoch: 1 	Ltrain: 0.051777 	Lval: 0.016818
Epoch: 5 	Ltrain: 0.019783 	Lval: 0.016701
Epoch: 10 	Ltrain: 0.010186 	Lval: 0.008966
Epoch: 15 	Ltrain: 0.007929 	Lval: 0.007421
Epoch: 20 	Ltrain: 0.007145 	Lval: 0.007312
Epoch: 25 	Ltrain: 0.007430 	Lval: 0.006704
Epoch: 30 	Ltrain: 0.006622 	Lval: 0.006527
Epoch 00033: reducing learning rate of group 0 to 6.5705e-04.
Epoch: 35 	Ltrain: 0.006389 	Lval: 0.006448
Epoch: 40 	Ltrain: 0.005780 	Lval: 0.006199
Epoch 00045: reducing learning rate of group 0 to 6.5705e-05.
Epoch: 45 	Ltrain: 0.006156 	Lval: 0.006367
Epoch: 50 	Ltrain: 0.006009 	Lval: 0.006217
Epoch: 55 	Ltrain: 0.005938 	Lval: 0.006261
Epoch 00057: reducing learning rate of group 0 to 6.5705e-06.
Epoch: 60 	Ltrain: 0.006122 	Lval: 0.006257
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.006199


	Fold 3/5
Epoch: 1 	Ltrain: 0.047153 	Lval: 0.018296
Epoch: 5 	Ltrain: 0.014674 	Lval: 0.011635
Epoch: 10 	Ltrain: 0.007373 	Lval: 0.007503
Epoch: 15 	Ltrain: 0.006669 	Lval: 0.006741
Epoch 00020: reducing learning rate of group 0 to 6.5705e-04.
Epoch: 20 	Ltrain: 0.006320 	Lval: 0.006877
Epoch: 25 	Ltrain: 0.006202 	Lval: 0.006366
Epoch: 30 	Ltrain: 0.005554 	Lval: 0.006268
Epoch: 35 	Ltrain: 0.005614 	Lval: 0.006225
Epoch: 40 	Ltrain: 0.005812 	Lval: 0.006170
Epoch 00043: reducing learning rate of group 0 to 6.5705e-05.
Epoch: 45 	Ltrain: 0.005509 	Lval: 0.006178
Epoch: 50 	Ltrain: 0.005510 	Lval: 0.006165
Epoch 00055: reducing learning rate of group 0 to 6.5705e-06.
Epoch: 55 	Ltrain: 0.005401 	Lval: 0.006173
Epoch: 60 	Ltrain: 0.005531 	Lval: 0.006167
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.006155


	Fold 4/5
Epoch: 1 	Ltrain: 0.029489 	Lval: 0.020048
Epoch: 5 	Ltrain: 0.008353 	Lval: 0.008347
Epoch: 10 	Ltrain: 0.005679 	Lval: 0.007190
Epoch 00013: reducing learning rate of group 0 to 6.5705e-04.
Epoch: 15 	Ltrain: 0.005334 	Lval: 0.006508
Epoch: 20 	Ltrain: 0.005380 	Lval: 0.006321
Epoch: 25 	Ltrain: 0.005258 	Lval: 0.006254
Epoch: 30 	Ltrain: 0.005207 	Lval: 0.006255
Epoch 00033: reducing learning rate of group 0 to 6.5705e-05.
Epoch: 35 	Ltrain: 0.004987 	Lval: 0.006295
Epoch: 40 	Ltrain: 0.005174 	Lval: 0.006267
Epoch: 45 	Ltrain: 0.005122 	Lval: 0.006245
Epoch: 50 	Ltrain: 0.005070 	Lval: 0.006223
Epoch: 55 	Ltrain: 0.005066 	Lval: 0.006207
Epoch: 60 	Ltrain: 0.005265 	Lval: 0.006198
Epoch: 65 	Ltrain: 0.005030 	Lval: 0.006191
Epoch: 70 	Ltrain: 0.005226 	Lval: 0.006177
Epoch 00072: reducing learning rate of group 0 to 6.5705e-06.
Epoch: 75 	Ltrain: 0.005048 	Lval: 0.006177
Epoch: 80 	Ltrain: 0.005348 	Lval: 0.006181
Epoch 00084: reducing learning rate of group 0 to 6.5705e-07.
Epoch: 85 	Ltrain: 0.005096 	Lval: 0.006178
Epoch: 90 	Ltrain: 0.005121 	Lval: 0.006178
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.006177


	Fold 5/5
Epoch: 1 	Ltrain: 0.032641 	Lval: 0.018914
Epoch: 5 	Ltrain: 0.008821 	Lval: 0.009356
Epoch: 10 	Ltrain: 0.006082 	Lval: 0.007628
Epoch: 15 	Ltrain: 0.005705 	Lval: 0.007365
Epoch 00016: reducing learning rate of group 0 to 6.5705e-04.
Epoch: 20 	Ltrain: 0.005267 	Lval: 0.006630
Epoch: 25 	Ltrain: 0.005236 	Lval: 0.006665
Epoch 00028: reducing learning rate of group 0 to 6.5705e-05.
Epoch: 30 	Ltrain: 0.005091 	Lval: 0.006632
Epoch: 35 	Ltrain: 0.005106 	Lval: 0.006582
Epoch 00040: reducing learning rate of group 0 to 6.5705e-06.
Epoch: 40 	Ltrain: 0.005029 	Lval: 0.006628
Epoch: 45 	Ltrain: 0.005194 	Lval: 0.006608
Epoch: 50 	Ltrain: 0.005017 	Lval: 0.006600
Epoch 00052: reducing learning rate of group 0 to 6.5705e-07.
Epoch: 55 	Ltrain: 0.005381 	Lval: 0.006596
EarlyStopper: stopping at epoch 56 with best_val_loss = 0.006582

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0018568993557497484
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.2210068658558907e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.052051 	Lval: 0.026827
Epoch: 5 	Ltrain: 0.016888 	Lval: 0.015618
Epoch: 10 	Ltrain: 0.013784 	Lval: 0.012338
Epoch: 15 	Ltrain: 0.010827 	Lval: 0.010915
Epoch: 20 	Ltrain: 0.008622 	Lval: 0.008289
Epoch: 25 	Ltrain: 0.007829 	Lval: 0.007617
Epoch 00028: reducing learning rate of group 0 to 1.8569e-04.
Epoch: 30 	Ltrain: 0.007829 	Lval: 0.007463
Epoch: 35 	Ltrain: 0.006784 	Lval: 0.007290
Epoch 00040: reducing learning rate of group 0 to 1.8569e-05.
Epoch: 40 	Ltrain: 0.008571 	Lval: 0.007348
Epoch: 45 	Ltrain: 0.007198 	Lval: 0.007262
Epoch: 50 	Ltrain: 0.006964 	Lval: 0.007169
Epoch 00055: reducing learning rate of group 0 to 1.8569e-06.
Epoch: 55 	Ltrain: 0.007300 	Lval: 0.007180
Epoch: 60 	Ltrain: 0.007165 	Lval: 0.007186
Epoch: 65 	Ltrain: 0.006584 	Lval: 0.007188
Epoch 00067: reducing learning rate of group 0 to 1.8569e-07.
Epoch: 70 	Ltrain: 0.009436 	Lval: 0.007188
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.007159


	Fold 2/5
Epoch: 1 	Ltrain: 0.039820 	Lval: 0.017911
Epoch: 5 	Ltrain: 0.012916 	Lval: 0.011098
Epoch: 10 	Ltrain: 0.008037 	Lval: 0.008213
Epoch: 15 	Ltrain: 0.006769 	Lval: 0.007422
Epoch 00018: reducing learning rate of group 0 to 1.8569e-04.
Epoch: 20 	Ltrain: 0.006579 	Lval: 0.006892
Epoch: 25 	Ltrain: 0.006614 	Lval: 0.006815
Epoch: 30 	Ltrain: 0.006435 	Lval: 0.006773
Epoch 00035: reducing learning rate of group 0 to 1.8569e-05.
Epoch: 35 	Ltrain: 0.006540 	Lval: 0.006734
Epoch: 40 	Ltrain: 0.006559 	Lval: 0.006686
Epoch: 45 	Ltrain: 0.006294 	Lval: 0.006690
Epoch 00047: reducing learning rate of group 0 to 1.8569e-06.
Epoch: 50 	Ltrain: 0.006467 	Lval: 0.006690
EarlyStopper: stopping at epoch 51 with best_val_loss = 0.006680


	Fold 3/5
Epoch: 1 	Ltrain: 0.021289 	Lval: 0.023908
Epoch: 5 	Ltrain: 0.009148 	Lval: 0.008784
Epoch: 10 	Ltrain: 0.007084 	Lval: 0.007338
Epoch: 15 	Ltrain: 0.006495 	Lval: 0.007746
Epoch 00020: reducing learning rate of group 0 to 1.8569e-04.
Epoch: 20 	Ltrain: 0.006203 	Lval: 0.006707
Epoch: 25 	Ltrain: 0.005791 	Lval: 0.006372
Epoch: 30 	Ltrain: 0.005646 	Lval: 0.006311
Epoch: 35 	Ltrain: 0.005668 	Lval: 0.006270
Epoch: 40 	Ltrain: 0.005649 	Lval: 0.006253
Epoch 00042: reducing learning rate of group 0 to 1.8569e-05.
Epoch: 45 	Ltrain: 0.005595 	Lval: 0.006219
Epoch: 50 	Ltrain: 0.005697 	Lval: 0.006218
Epoch: 55 	Ltrain: 0.005694 	Lval: 0.006210
Epoch: 60 	Ltrain: 0.005914 	Lval: 0.006208
Epoch 00063: reducing learning rate of group 0 to 1.8569e-06.
Epoch: 65 	Ltrain: 0.005565 	Lval: 0.006208
Epoch: 70 	Ltrain: 0.005595 	Lval: 0.006207
Epoch 00075: reducing learning rate of group 0 to 1.8569e-07.
Epoch: 75 	Ltrain: 0.005576 	Lval: 0.006206
Epoch: 80 	Ltrain: 0.005571 	Lval: 0.006206
EarlyStopper: stopping at epoch 79 with best_val_loss = 0.006207


	Fold 4/5
Epoch: 1 	Ltrain: 0.020867 	Lval: 0.016092
Epoch: 5 	Ltrain: 0.006471 	Lval: 0.007671
Epoch: 10 	Ltrain: 0.005908 	Lval: 0.006982
Epoch: 15 	Ltrain: 0.005800 	Lval: 0.006594
Epoch: 20 	Ltrain: 0.005282 	Lval: 0.006670
Epoch 00021: reducing learning rate of group 0 to 1.8569e-04.
Epoch: 25 	Ltrain: 0.004946 	Lval: 0.006085
Epoch: 30 	Ltrain: 0.004898 	Lval: 0.006035
Epoch: 35 	Ltrain: 0.004908 	Lval: 0.005965
Epoch: 40 	Ltrain: 0.004879 	Lval: 0.005963
Epoch: 45 	Ltrain: 0.004852 	Lval: 0.005899
Epoch: 50 	Ltrain: 0.004844 	Lval: 0.005842
Epoch 00052: reducing learning rate of group 0 to 1.8569e-05.
Epoch: 55 	Ltrain: 0.004711 	Lval: 0.005828
Epoch: 60 	Ltrain: 0.004711 	Lval: 0.005821
Epoch: 65 	Ltrain: 0.004817 	Lval: 0.005816
Epoch 00068: reducing learning rate of group 0 to 1.8569e-06.
Epoch: 70 	Ltrain: 0.004705 	Lval: 0.005815
Epoch: 75 	Ltrain: 0.004711 	Lval: 0.005813
Epoch 00080: reducing learning rate of group 0 to 1.8569e-07.
Epoch: 80 	Ltrain: 0.004708 	Lval: 0.005812
Epoch: 85 	Ltrain: 0.004787 	Lval: 0.005812
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.005810


	Fold 5/5
Epoch: 1 	Ltrain: 0.020849 	Lval: 0.015215
Epoch: 5 	Ltrain: 0.006686 	Lval: 0.008646
Epoch: 10 	Ltrain: 0.006321 	Lval: 0.007927
Epoch: 15 	Ltrain: 0.005423 	Lval: 0.006708
Epoch: 20 	Ltrain: 0.005737 	Lval: 0.006843
Epoch 00022: reducing learning rate of group 0 to 1.8569e-04.
Epoch: 25 	Ltrain: 0.005092 	Lval: 0.006388
Epoch: 30 	Ltrain: 0.005001 	Lval: 0.006464
Epoch: 35 	Ltrain: 0.004947 	Lval: 0.006301
Epoch 00037: reducing learning rate of group 0 to 1.8569e-05.
Epoch: 40 	Ltrain: 0.004878 	Lval: 0.006332
Epoch: 45 	Ltrain: 0.004902 	Lval: 0.006297
Epoch 00049: reducing learning rate of group 0 to 1.8569e-06.
Epoch: 50 	Ltrain: 0.004954 	Lval: 0.006321
EarlyStopper: stopping at epoch 53 with best_val_loss = 0.006280

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006801023213316978
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.889409632825133e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.145259 	Lval: 0.031889
Epoch: 5 	Ltrain: 0.011941 	Lval: 0.009950
Epoch: 10 	Ltrain: 0.007919 	Lval: 0.007766
Epoch: 15 	Ltrain: 0.007252 	Lval: 0.006957
Epoch: 20 	Ltrain: 0.006907 	Lval: 0.006165
Epoch: 25 	Ltrain: 0.006273 	Lval: 0.006447
Epoch: 30 	Ltrain: 0.007087 	Lval: 0.006908
Epoch 00031: reducing learning rate of group 0 to 6.8010e-04.
Epoch: 35 	Ltrain: 0.005356 	Lval: 0.005325
Epoch: 40 	Ltrain: 0.004970 	Lval: 0.005099
Epoch: 45 	Ltrain: 0.005075 	Lval: 0.004953
Epoch: 50 	Ltrain: 0.004628 	Lval: 0.004803
Epoch: 55 	Ltrain: 0.004687 	Lval: 0.004668
Epoch: 60 	Ltrain: 0.004346 	Lval: 0.004523
Epoch: 65 	Ltrain: 0.004856 	Lval: 0.004407
Epoch: 70 	Ltrain: 0.004196 	Lval: 0.004269
Epoch: 75 	Ltrain: 0.004216 	Lval: 0.004175
Epoch: 80 	Ltrain: 0.003890 	Lval: 0.004020
Epoch: 85 	Ltrain: 0.003990 	Lval: 0.003761
Epoch: 90 	Ltrain: 0.003513 	Lval: 0.003616
Epoch: 95 	Ltrain: 0.003417 	Lval: 0.003445
Epoch: 100 	Ltrain: 0.003370 	Lval: 0.003560
Epoch: 105 	Ltrain: 0.003041 	Lval: 0.003156
Epoch: 110 	Ltrain: 0.002803 	Lval: 0.002910
Epoch: 115 	Ltrain: 0.002712 	Lval: 0.002770
Epoch: 120 	Ltrain: 0.002684 	Lval: 0.002707
Epoch: 125 	Ltrain: 0.002487 	Lval: 0.002502
Epoch: 130 	Ltrain: 0.002274 	Lval: 0.002237
Epoch: 135 	Ltrain: 0.002208 	Lval: 0.002172
Epoch: 140 	Ltrain: 0.001996 	Lval: 0.001956
Epoch: 145 	Ltrain: 0.001843 	Lval: 0.001861
Epoch: 150 	Ltrain: 0.001778 	Lval: 0.001732
Epoch: 155 	Ltrain: 0.001583 	Lval: 0.001613
Epoch: 160 	Ltrain: 0.001559 	Lval: 0.001617
Epoch: 165 	Ltrain: 0.001446 	Lval: 0.001460
Epoch: 170 	Ltrain: 0.001395 	Lval: 0.001348
Epoch: 175 	Ltrain: 0.001355 	Lval: 0.001367
Epoch 00177: reducing learning rate of group 0 to 6.8010e-05.
Epoch: 180 	Ltrain: 0.001202 	Lval: 0.001214
Epoch: 185 	Ltrain: 0.001117 	Lval: 0.001144
Epoch: 190 	Ltrain: 0.001132 	Lval: 0.001117
Epoch: 195 	Ltrain: 0.001104 	Lval: 0.001098
Epoch: 200 	Ltrain: 0.001043 	Lval: 0.001084
Epoch: 205 	Ltrain: 0.001043 	Lval: 0.001069
Epoch: 210 	Ltrain: 0.001066 	Lval: 0.001056
Epoch: 215 	Ltrain: 0.001052 	Lval: 0.001041
Epoch: 220 	Ltrain: 0.001052 	Lval: 0.001031
Epoch: 225 	Ltrain: 0.001016 	Lval: 0.001020
Epoch: 230 	Ltrain: 0.001008 	Lval: 0.001008
Epoch: 235 	Ltrain: 0.000977 	Lval: 0.000998
Epoch: 240 	Ltrain: 0.000962 	Lval: 0.000988
Epoch: 245 	Ltrain: 0.000991 	Lval: 0.000974
Epoch: 250 	Ltrain: 0.000945 	Lval: 0.000964
Epoch: 255 	Ltrain: 0.000930 	Lval: 0.000953
Epoch: 260 	Ltrain: 0.000973 	Lval: 0.000941
Epoch: 265 	Ltrain: 0.000930 	Lval: 0.000931
Epoch: 270 	Ltrain: 0.000958 	Lval: 0.000921
Epoch: 275 	Ltrain: 0.000897 	Lval: 0.000910
Epoch: 280 	Ltrain: 0.000923 	Lval: 0.000898
Epoch: 285 	Ltrain: 0.000890 	Lval: 0.000889
Epoch: 290 	Ltrain: 0.000890 	Lval: 0.000878
Epoch: 295 	Ltrain: 0.000863 	Lval: 0.000867
Epoch: 300 	Ltrain: 0.000868 	Lval: 0.000856
Epoch: 305 	Ltrain: 0.000841 	Lval: 0.000844
Epoch: 310 	Ltrain: 0.000849 	Lval: 0.000834
Epoch: 315 	Ltrain: 0.000828 	Lval: 0.000823
Epoch: 320 	Ltrain: 0.000813 	Lval: 0.000813
Epoch: 325 	Ltrain: 0.000778 	Lval: 0.000803
Epoch: 330 	Ltrain: 0.000812 	Lval: 0.000793
Epoch: 335 	Ltrain: 0.000796 	Lval: 0.000780
Epoch: 340 	Ltrain: 0.000762 	Lval: 0.000771
Epoch: 345 	Ltrain: 0.000773 	Lval: 0.000760
Epoch: 350 	Ltrain: 0.000768 	Lval: 0.000749
Epoch: 355 	Ltrain: 0.000779 	Lval: 0.000737
Epoch: 360 	Ltrain: 0.000736 	Lval: 0.000727
Epoch: 365 	Ltrain: 0.000708 	Lval: 0.000716
Epoch: 370 	Ltrain: 0.000719 	Lval: 0.000706
Epoch: 375 	Ltrain: 0.000722 	Lval: 0.000695
Epoch: 380 	Ltrain: 0.000676 	Lval: 0.000686
Epoch: 385 	Ltrain: 0.000695 	Lval: 0.000677
Epoch: 390 	Ltrain: 0.000686 	Lval: 0.000666
Epoch: 395 	Ltrain: 0.000660 	Lval: 0.000657
Epoch: 400 	Ltrain: 0.000675 	Lval: 0.000647
Epoch: 405 	Ltrain: 0.000665 	Lval: 0.000635
Epoch: 410 	Ltrain: 0.000652 	Lval: 0.000624
Epoch: 415 	Ltrain: 0.000635 	Lval: 0.000617
Epoch: 420 	Ltrain: 0.000635 	Lval: 0.000607
Epoch: 425 	Ltrain: 0.000647 	Lval: 0.000596
Epoch: 430 	Ltrain: 0.000613 	Lval: 0.000586
Epoch: 435 	Ltrain: 0.000588 	Lval: 0.000579
Epoch: 440 	Ltrain: 0.000602 	Lval: 0.000571
Epoch: 445 	Ltrain: 0.000585 	Lval: 0.000563
Epoch: 450 	Ltrain: 0.000569 	Lval: 0.000559
Epoch: 455 	Ltrain: 0.000554 	Lval: 0.000543
Epoch: 460 	Ltrain: 0.000547 	Lval: 0.000533
Epoch: 465 	Ltrain: 0.000563 	Lval: 0.000531
Epoch: 470 	Ltrain: 0.000521 	Lval: 0.000519
Epoch: 475 	Ltrain: 0.000534 	Lval: 0.000507
Epoch: 480 	Ltrain: 0.000517 	Lval: 0.000498
Epoch: 485 	Ltrain: 0.000501 	Lval: 0.000493
Epoch: 490 	Ltrain: 0.000495 	Lval: 0.000493
Epoch: 495 	Ltrain: 0.000496 	Lval: 0.000481
Epoch: 500 	Ltrain: 0.000499 	Lval: 0.000479
Epoch: 505 	Ltrain: 0.000475 	Lval: 0.000463
Epoch: 510 	Ltrain: 0.000462 	Lval: 0.000453
Epoch: 515 	Ltrain: 0.000467 	Lval: 0.000446
Epoch 00520: reducing learning rate of group 0 to 6.8010e-06.
Epoch: 520 	Ltrain: 0.000469 	Lval: 0.000451
Epoch: 525 	Ltrain: 0.000431 	Lval: 0.000434
Epoch: 530 	Ltrain: 0.000435 	Lval: 0.000432
Epoch: 535 	Ltrain: 0.000461 	Lval: 0.000431
Epoch: 540 	Ltrain: 0.000420 	Lval: 0.000430
Epoch: 545 	Ltrain: 0.000446 	Lval: 0.000429
EarlyStopper: stopping at epoch 548 with best_val_loss = 0.000437


	Fold 2/5
Epoch: 1 	Ltrain: 0.129242 	Lval: 0.020295
Epoch: 5 	Ltrain: 0.009932 	Lval: 0.009256
Epoch: 10 	Ltrain: 0.006993 	Lval: 0.007628
Epoch: 15 	Ltrain: 0.006857 	Lval: 0.006490
Epoch: 20 	Ltrain: 0.005967 	Lval: 0.006549
Epoch: 25 	Ltrain: 0.006063 	Lval: 0.006165
Epoch 00026: reducing learning rate of group 0 to 6.8010e-04.
Epoch: 30 	Ltrain: 0.005171 	Lval: 0.005444
Epoch: 35 	Ltrain: 0.004993 	Lval: 0.005244
Epoch: 40 	Ltrain: 0.004869 	Lval: 0.005117
Epoch: 45 	Ltrain: 0.004829 	Lval: 0.005016
Epoch: 50 	Ltrain: 0.004632 	Lval: 0.004876
Epoch: 55 	Ltrain: 0.004417 	Lval: 0.004697
Epoch: 60 	Ltrain: 0.004344 	Lval: 0.004666
Epoch: 65 	Ltrain: 0.004211 	Lval: 0.004422
Epoch: 70 	Ltrain: 0.004130 	Lval: 0.004222
Epoch: 75 	Ltrain: 0.003987 	Lval: 0.004094
Epoch: 80 	Ltrain: 0.003774 	Lval: 0.003904
Epoch: 85 	Ltrain: 0.003745 	Lval: 0.003713
Epoch: 90 	Ltrain: 0.003504 	Lval: 0.003451
Epoch: 95 	Ltrain: 0.003318 	Lval: 0.003353
Epoch: 100 	Ltrain: 0.003097 	Lval: 0.003028
Epoch: 105 	Ltrain: 0.002932 	Lval: 0.002846
Epoch: 110 	Ltrain: 0.002648 	Lval: 0.002608
Epoch: 115 	Ltrain: 0.002579 	Lval: 0.002465
Epoch: 120 	Ltrain: 0.002303 	Lval: 0.002205
Epoch: 125 	Ltrain: 0.002145 	Lval: 0.002065
Epoch 00128: reducing learning rate of group 0 to 6.8010e-05.
Epoch: 130 	Ltrain: 0.001904 	Lval: 0.001792
Epoch: 135 	Ltrain: 0.001803 	Lval: 0.001738
Epoch: 140 	Ltrain: 0.001766 	Lval: 0.001709
Epoch: 145 	Ltrain: 0.001756 	Lval: 0.001684
Epoch: 150 	Ltrain: 0.001714 	Lval: 0.001663
Epoch: 155 	Ltrain: 0.001699 	Lval: 0.001637
Epoch: 160 	Ltrain: 0.001694 	Lval: 0.001618
Epoch: 165 	Ltrain: 0.001649 	Lval: 0.001592
Epoch: 170 	Ltrain: 0.001647 	Lval: 0.001581
Epoch: 175 	Ltrain: 0.001615 	Lval: 0.001546
Epoch: 180 	Ltrain: 0.001590 	Lval: 0.001532
Epoch: 185 	Ltrain: 0.001572 	Lval: 0.001507
Epoch: 190 	Ltrain: 0.001538 	Lval: 0.001481
Epoch: 195 	Ltrain: 0.001518 	Lval: 0.001463
Epoch: 200 	Ltrain: 0.001493 	Lval: 0.001436
Epoch: 205 	Ltrain: 0.001502 	Lval: 0.001413
Epoch: 210 	Ltrain: 0.001480 	Lval: 0.001391
Epoch: 215 	Ltrain: 0.001444 	Lval: 0.001371
Epoch: 220 	Ltrain: 0.001416 	Lval: 0.001343
Epoch: 225 	Ltrain: 0.001383 	Lval: 0.001319
Epoch: 230 	Ltrain: 0.001383 	Lval: 0.001298
Epoch: 235 	Ltrain: 0.001339 	Lval: 0.001278
Epoch: 240 	Ltrain: 0.001336 	Lval: 0.001255
Epoch: 245 	Ltrain: 0.001308 	Lval: 0.001234
Epoch: 250 	Ltrain: 0.001267 	Lval: 0.001207
Epoch: 255 	Ltrain: 0.001263 	Lval: 0.001186
Epoch: 260 	Ltrain: 0.001216 	Lval: 0.001158
Epoch: 265 	Ltrain: 0.001210 	Lval: 0.001137
Epoch: 270 	Ltrain: 0.001187 	Lval: 0.001116
Epoch: 275 	Ltrain: 0.001165 	Lval: 0.001091
Epoch: 280 	Ltrain: 0.001141 	Lval: 0.001070
Epoch: 285 	Ltrain: 0.001129 	Lval: 0.001051
Epoch: 290 	Ltrain: 0.001118 	Lval: 0.001024
Epoch: 295 	Ltrain: 0.001067 	Lval: 0.001008
Epoch: 300 	Ltrain: 0.001056 	Lval: 0.000982
Epoch: 305 	Ltrain: 0.001031 	Lval: 0.000967
Epoch: 310 	Ltrain: 0.001004 	Lval: 0.000936
Epoch: 315 	Ltrain: 0.000980 	Lval: 0.000921
Epoch: 320 	Ltrain: 0.000972 	Lval: 0.000904
Epoch: 325 	Ltrain: 0.000937 	Lval: 0.000887
Epoch: 330 	Ltrain: 0.000909 	Lval: 0.000870
Epoch: 335 	Ltrain: 0.000906 	Lval: 0.000843
Epoch: 340 	Ltrain: 0.000894 	Lval: 0.000829
Epoch: 345 	Ltrain: 0.000865 	Lval: 0.000811
Epoch: 350 	Ltrain: 0.000840 	Lval: 0.000796
Epoch: 355 	Ltrain: 0.000822 	Lval: 0.000770
Epoch: 360 	Ltrain: 0.000800 	Lval: 0.000752
Epoch 00365: reducing learning rate of group 0 to 6.8010e-06.
Epoch: 365 	Ltrain: 0.000784 	Lval: 0.000751
Epoch: 370 	Ltrain: 0.000756 	Lval: 0.000727
Epoch: 375 	Ltrain: 0.000760 	Lval: 0.000725
Epoch: 380 	Ltrain: 0.000765 	Lval: 0.000723
Epoch: 385 	Ltrain: 0.000758 	Lval: 0.000721
Epoch: 390 	Ltrain: 0.000758 	Lval: 0.000719
Epoch: 395 	Ltrain: 0.000747 	Lval: 0.000717
Epoch: 400 	Ltrain: 0.000738 	Lval: 0.000715
Epoch: 405 	Ltrain: 0.000753 	Lval: 0.000713
Epoch: 410 	Ltrain: 0.000743 	Lval: 0.000711
Epoch: 415 	Ltrain: 0.000740 	Lval: 0.000708
Epoch: 420 	Ltrain: 0.000736 	Lval: 0.000707
Epoch: 425 	Ltrain: 0.000738 	Lval: 0.000705
Epoch: 430 	Ltrain: 0.000733 	Lval: 0.000702
Epoch: 435 	Ltrain: 0.000739 	Lval: 0.000701
Epoch: 440 	Ltrain: 0.000732 	Lval: 0.000698
Epoch: 445 	Ltrain: 0.000749 	Lval: 0.000696
Epoch: 450 	Ltrain: 0.000727 	Lval: 0.000694
Epoch: 455 	Ltrain: 0.000727 	Lval: 0.000691
Epoch: 460 	Ltrain: 0.000722 	Lval: 0.000689
Epoch: 465 	Ltrain: 0.000718 	Lval: 0.000687
Epoch: 470 	Ltrain: 0.000714 	Lval: 0.000684
Epoch: 475 	Ltrain: 0.000719 	Lval: 0.000682
Epoch: 480 	Ltrain: 0.000697 	Lval: 0.000680
Epoch: 485 	Ltrain: 0.000703 	Lval: 0.000677
Epoch: 490 	Ltrain: 0.000703 	Lval: 0.000675
Epoch: 495 	Ltrain: 0.000704 	Lval: 0.000672
Epoch: 500 	Ltrain: 0.000695 	Lval: 0.000670
Epoch: 505 	Ltrain: 0.000693 	Lval: 0.000667
Epoch: 510 	Ltrain: 0.000694 	Lval: 0.000665
Epoch: 515 	Ltrain: 0.000690 	Lval: 0.000662
Epoch: 520 	Ltrain: 0.000693 	Lval: 0.000660
Epoch: 525 	Ltrain: 0.000685 	Lval: 0.000657
Epoch: 530 	Ltrain: 0.000680 	Lval: 0.000655
Epoch: 535 	Ltrain: 0.000682 	Lval: 0.000652
Epoch: 540 	Ltrain: 0.000670 	Lval: 0.000649
Epoch: 545 	Ltrain: 0.000679 	Lval: 0.000646
Epoch: 550 	Ltrain: 0.000672 	Lval: 0.000645
Epoch: 555 	Ltrain: 0.000671 	Lval: 0.000641
Epoch: 560 	Ltrain: 0.000671 	Lval: 0.000639
Epoch: 565 	Ltrain: 0.000657 	Lval: 0.000636
Epoch: 570 	Ltrain: 0.000657 	Lval: 0.000634
Epoch: 575 	Ltrain: 0.000651 	Lval: 0.000630
Epoch: 580 	Ltrain: 0.000648 	Lval: 0.000628
Epoch: 585 	Ltrain: 0.000642 	Lval: 0.000625
Epoch: 590 	Ltrain: 0.000647 	Lval: 0.000623
Epoch: 595 	Ltrain: 0.000655 	Lval: 0.000620
Epoch: 600 	Ltrain: 0.000641 	Lval: 0.000618
Epoch: 605 	Ltrain: 0.000641 	Lval: 0.000615
Epoch: 610 	Ltrain: 0.000645 	Lval: 0.000612
Epoch: 615 	Ltrain: 0.000633 	Lval: 0.000609
Epoch: 620 	Ltrain: 0.000633 	Lval: 0.000607
Epoch: 625 	Ltrain: 0.000626 	Lval: 0.000604
Epoch: 630 	Ltrain: 0.000622 	Lval: 0.000601
Epoch: 635 	Ltrain: 0.000621 	Lval: 0.000600
Epoch: 640 	Ltrain: 0.000626 	Lval: 0.000596
Epoch: 645 	Ltrain: 0.000608 	Lval: 0.000593
Epoch: 650 	Ltrain: 0.000614 	Lval: 0.000591
Epoch: 655 	Ltrain: 0.000618 	Lval: 0.000588
Epoch: 660 	Ltrain: 0.000611 	Lval: 0.000585
Epoch: 665 	Ltrain: 0.000599 	Lval: 0.000583
Epoch: 670 	Ltrain: 0.000603 	Lval: 0.000580
Epoch: 675 	Ltrain: 0.000605 	Lval: 0.000577
Epoch: 680 	Ltrain: 0.000599 	Lval: 0.000576
Epoch: 685 	Ltrain: 0.000584 	Lval: 0.000573
Epoch: 690 	Ltrain: 0.000587 	Lval: 0.000570
Epoch: 695 	Ltrain: 0.000586 	Lval: 0.000567
Epoch: 700 	Ltrain: 0.000584 	Lval: 0.000565
Epoch: 705 	Ltrain: 0.000586 	Lval: 0.000562
Epoch: 710 	Ltrain: 0.000590 	Lval: 0.000559
Epoch: 715 	Ltrain: 0.000577 	Lval: 0.000557
Epoch: 720 	Ltrain: 0.000572 	Lval: 0.000554
Epoch: 725 	Ltrain: 0.000568 	Lval: 0.000553
Epoch: 730 	Ltrain: 0.000570 	Lval: 0.000549
Epoch: 735 	Ltrain: 0.000564 	Lval: 0.000547
Epoch: 740 	Ltrain: 0.000567 	Lval: 0.000544
Epoch: 745 	Ltrain: 0.000557 	Lval: 0.000542
Epoch: 750 	Ltrain: 0.000556 	Lval: 0.000539
Epoch: 755 	Ltrain: 0.000552 	Lval: 0.000537
Epoch: 760 	Ltrain: 0.000547 	Lval: 0.000534
Epoch: 765 	Ltrain: 0.000550 	Lval: 0.000531
Epoch: 770 	Ltrain: 0.000544 	Lval: 0.000529
Epoch: 775 	Ltrain: 0.000546 	Lval: 0.000527
Epoch: 780 	Ltrain: 0.000546 	Lval: 0.000524
Epoch: 785 	Ltrain: 0.000532 	Lval: 0.000521
Epoch: 790 	Ltrain: 0.000537 	Lval: 0.000519
Epoch: 795 	Ltrain: 0.000531 	Lval: 0.000516
Epoch: 800 	Ltrain: 0.000532 	Lval: 0.000514
Epoch: 805 	Ltrain: 0.000528 	Lval: 0.000512
Epoch: 810 	Ltrain: 0.000529 	Lval: 0.000509
Epoch: 815 	Ltrain: 0.000519 	Lval: 0.000508
Epoch: 820 	Ltrain: 0.000521 	Lval: 0.000504
Epoch: 825 	Ltrain: 0.000517 	Lval: 0.000502
Epoch: 830 	Ltrain: 0.000518 	Lval: 0.000500
Epoch: 835 	Ltrain: 0.000511 	Lval: 0.000497
Epoch: 840 	Ltrain: 0.000515 	Lval: 0.000495
Epoch: 845 	Ltrain: 0.000520 	Lval: 0.000493
Epoch: 850 	Ltrain: 0.000521 	Lval: 0.000490
Epoch: 855 	Ltrain: 0.000504 	Lval: 0.000488
Epoch: 860 	Ltrain: 0.000501 	Lval: 0.000486
Epoch: 865 	Ltrain: 0.000492 	Lval: 0.000484
Epoch: 870 	Ltrain: 0.000499 	Lval: 0.000481
Epoch: 875 	Ltrain: 0.000491 	Lval: 0.000479
Epoch: 880 	Ltrain: 0.000503 	Lval: 0.000478
Epoch: 885 	Ltrain: 0.000490 	Lval: 0.000475
Epoch: 890 	Ltrain: 0.000487 	Lval: 0.000472
Epoch: 895 	Ltrain: 0.000483 	Lval: 0.000469
Epoch: 900 	Ltrain: 0.000482 	Lval: 0.000467
Epoch: 905 	Ltrain: 0.000481 	Lval: 0.000465
Epoch: 910 	Ltrain: 0.000472 	Lval: 0.000463
Epoch: 915 	Ltrain: 0.000470 	Lval: 0.000461
Epoch: 920 	Ltrain: 0.000474 	Lval: 0.000458
Epoch: 925 	Ltrain: 0.000468 	Lval: 0.000456
Epoch: 930 	Ltrain: 0.000462 	Lval: 0.000454
Epoch: 935 	Ltrain: 0.000464 	Lval: 0.000452
Epoch: 940 	Ltrain: 0.000456 	Lval: 0.000449
Epoch: 945 	Ltrain: 0.000458 	Lval: 0.000447
Epoch: 950 	Ltrain: 0.000456 	Lval: 0.000445
Epoch: 955 	Ltrain: 0.000452 	Lval: 0.000443
Epoch: 960 	Ltrain: 0.000452 	Lval: 0.000441
Epoch: 965 	Ltrain: 0.000451 	Lval: 0.000439
Epoch: 970 	Ltrain: 0.000445 	Lval: 0.000437
Epoch: 975 	Ltrain: 0.000447 	Lval: 0.000434
Epoch: 980 	Ltrain: 0.000446 	Lval: 0.000432
Epoch: 985 	Ltrain: 0.000447 	Lval: 0.000430
Epoch: 990 	Ltrain: 0.000442 	Lval: 0.000428
Epoch: 995 	Ltrain: 0.000435 	Lval: 0.000426
Epoch: 1000 	Ltrain: 0.000441 	Lval: 0.000424
Epoch: 1005 	Ltrain: 0.000431 	Lval: 0.000423
Epoch: 1010 	Ltrain: 0.000431 	Lval: 0.000420
Epoch: 1015 	Ltrain: 0.000426 	Lval: 0.000418
Epoch: 1020 	Ltrain: 0.000422 	Lval: 0.000415
Epoch: 1025 	Ltrain: 0.000422 	Lval: 0.000413
Epoch: 1030 	Ltrain: 0.000424 	Lval: 0.000411
Epoch: 1035 	Ltrain: 0.000418 	Lval: 0.000409
Epoch: 1040 	Ltrain: 0.000418 	Lval: 0.000408
Epoch: 1045 	Ltrain: 0.000417 	Lval: 0.000405
Epoch: 1050 	Ltrain: 0.000416 	Lval: 0.000403
Epoch: 1055 	Ltrain: 0.000413 	Lval: 0.000401
Epoch: 1060 	Ltrain: 0.000408 	Lval: 0.000399
Epoch: 1065 	Ltrain: 0.000411 	Lval: 0.000397
Epoch: 1070 	Ltrain: 0.000403 	Lval: 0.000395
Epoch: 1075 	Ltrain: 0.000402 	Lval: 0.000393
Epoch: 1080 	Ltrain: 0.000404 	Lval: 0.000391
Epoch: 1085 	Ltrain: 0.000396 	Lval: 0.000389
Epoch: 1090 	Ltrain: 0.000401 	Lval: 0.000387
Epoch: 1095 	Ltrain: 0.000396 	Lval: 0.000385
Epoch: 1100 	Ltrain: 0.000392 	Lval: 0.000384
Epoch: 1105 	Ltrain: 0.000389 	Lval: 0.000381
Epoch: 1110 	Ltrain: 0.000385 	Lval: 0.000379
Epoch: 1115 	Ltrain: 0.000388 	Lval: 0.000378
Epoch: 1120 	Ltrain: 0.000389 	Lval: 0.000376
Epoch: 1125 	Ltrain: 0.000380 	Lval: 0.000374
Epoch: 1130 	Ltrain: 0.000380 	Lval: 0.000372
Epoch: 1135 	Ltrain: 0.000383 	Lval: 0.000370
Epoch: 1140 	Ltrain: 0.000387 	Lval: 0.000368
Epoch: 1145 	Ltrain: 0.000374 	Lval: 0.000366
Epoch: 1150 	Ltrain: 0.000375 	Lval: 0.000364
Epoch: 1155 	Ltrain: 0.000374 	Lval: 0.000362
Epoch: 1160 	Ltrain: 0.000372 	Lval: 0.000360
Epoch: 1165 	Ltrain: 0.000367 	Lval: 0.000359
Epoch: 1170 	Ltrain: 0.000365 	Lval: 0.000357
Epoch: 1175 	Ltrain: 0.000372 	Lval: 0.000355
Epoch: 1180 	Ltrain: 0.000359 	Lval: 0.000353
Epoch: 1185 	Ltrain: 0.000360 	Lval: 0.000351
EarlyStopper: stopping at epoch 1184 with best_val_loss = 0.000361


	Fold 3/5
Epoch: 1 	Ltrain: 0.105066 	Lval: 0.020360
Epoch: 5 	Ltrain: 0.010689 	Lval: 0.010131
Epoch: 10 	Ltrain: 0.006936 	Lval: 0.007669
Epoch: 15 	Ltrain: 0.006587 	Lval: 0.006885
Epoch: 20 	Ltrain: 0.006154 	Lval: 0.006332
Epoch: 25 	Ltrain: 0.005541 	Lval: 0.005780
Epoch: 30 	Ltrain: 0.005121 	Lval: 0.006134
Epoch: 35 	Ltrain: 0.005401 	Lval: 0.005550
Epoch: 40 	Ltrain: 0.004574 	Lval: 0.004637
Epoch: 45 	Ltrain: 0.004203 	Lval: 0.004137
Epoch: 50 	Ltrain: 0.003571 	Lval: 0.003505
Epoch: 55 	Ltrain: 0.002817 	Lval: 0.003416
Epoch: 60 	Ltrain: 0.002736 	Lval: 0.002730
Epoch: 65 	Ltrain: 0.002308 	Lval: 0.002333
Epoch: 70 	Ltrain: 0.001642 	Lval: 0.002057
Epoch: 75 	Ltrain: 0.001696 	Lval: 0.001628
Epoch: 80 	Ltrain: 0.001131 	Lval: 0.001123
Epoch 00084: reducing learning rate of group 0 to 6.8010e-04.
Epoch: 85 	Ltrain: 0.000990 	Lval: 0.000837
Epoch: 90 	Ltrain: 0.000577 	Lval: 0.000570
Epoch: 95 	Ltrain: 0.000517 	Lval: 0.000504
Epoch: 100 	Ltrain: 0.000465 	Lval: 0.000459
Epoch: 105 	Ltrain: 0.000430 	Lval: 0.000425
Epoch: 110 	Ltrain: 0.000399 	Lval: 0.000394
Epoch: 115 	Ltrain: 0.000372 	Lval: 0.000367
Epoch: 120 	Ltrain: 0.000346 	Lval: 0.000339
Epoch: 125 	Ltrain: 0.000322 	Lval: 0.000315
Epoch: 130 	Ltrain: 0.000300 	Lval: 0.000292
Epoch: 135 	Ltrain: 0.000277 	Lval: 0.000270
Epoch: 140 	Ltrain: 0.000255 	Lval: 0.000249
Epoch: 145 	Ltrain: 0.000238 	Lval: 0.000228
Epoch: 150 	Ltrain: 0.000218 	Lval: 0.000211
Epoch: 155 	Ltrain: 0.000200 	Lval: 0.000192
Epoch: 160 	Ltrain: 0.000184 	Lval: 0.000176
Epoch: 165 	Ltrain: 0.000168 	Lval: 0.000160
Epoch: 170 	Ltrain: 0.000156 	Lval: 0.000149
Epoch: 175 	Ltrain: 0.000144 	Lval: 0.000136
Epoch: 180 	Ltrain: 0.000135 	Lval: 0.000127
Epoch: 185 	Ltrain: 0.000136 	Lval: 0.000126
Epoch: 190 	Ltrain: 0.000130 	Lval: 0.000121
Epoch: 195 	Ltrain: 0.000116 	Lval: 0.000111
Epoch: 200 	Ltrain: 0.000111 	Lval: 0.000106
Epoch 00202: reducing learning rate of group 0 to 6.8010e-05.
Epoch: 205 	Ltrain: 0.000089 	Lval: 0.000081
Epoch: 210 	Ltrain: 0.000085 	Lval: 0.000078
Epoch: 215 	Ltrain: 0.000083 	Lval: 0.000077
Epoch: 220 	Ltrain: 0.000082 	Lval: 0.000076
Epoch: 225 	Ltrain: 0.000081 	Lval: 0.000075
Epoch: 230 	Ltrain: 0.000080 	Lval: 0.000074
Epoch: 235 	Ltrain: 0.000079 	Lval: 0.000073
EarlyStopper: stopping at epoch 238 with best_val_loss = 0.000078


	Fold 4/5
Epoch: 1 	Ltrain: 0.043442 	Lval: 0.011734
Epoch: 5 	Ltrain: 0.006011 	Lval: 0.006921
Epoch: 10 	Ltrain: 0.005540 	Lval: 0.006866
Epoch: 15 	Ltrain: 0.005154 	Lval: 0.005892
Epoch: 20 	Ltrain: 0.005185 	Lval: 0.006017
Epoch 00025: reducing learning rate of group 0 to 6.8010e-04.
Epoch: 25 	Ltrain: 0.004367 	Lval: 0.005283
Epoch: 30 	Ltrain: 0.003422 	Lval: 0.004120
Epoch: 35 	Ltrain: 0.003223 	Lval: 0.003815
Epoch: 40 	Ltrain: 0.003029 	Lval: 0.003530
Epoch: 45 	Ltrain: 0.002835 	Lval: 0.003179
Epoch: 50 	Ltrain: 0.002584 	Lval: 0.002846
Epoch: 55 	Ltrain: 0.002292 	Lval: 0.002462
Epoch: 60 	Ltrain: 0.002076 	Lval: 0.002234
Epoch: 65 	Ltrain: 0.001856 	Lval: 0.002055
Epoch: 70 	Ltrain: 0.001656 	Lval: 0.001683
Epoch: 75 	Ltrain: 0.001459 	Lval: 0.001451
Epoch: 80 	Ltrain: 0.001300 	Lval: 0.001397
Epoch: 85 	Ltrain: 0.001159 	Lval: 0.001128
Epoch: 90 	Ltrain: 0.001066 	Lval: 0.001105
Epoch: 95 	Ltrain: 0.000910 	Lval: 0.001017
Epoch: 100 	Ltrain: 0.000823 	Lval: 0.000867
Epoch: 105 	Ltrain: 0.000796 	Lval: 0.000863
Epoch 00106: reducing learning rate of group 0 to 6.8010e-05.
Epoch: 110 	Ltrain: 0.000576 	Lval: 0.000574
Epoch: 115 	Ltrain: 0.000549 	Lval: 0.000548
Epoch: 120 	Ltrain: 0.000535 	Lval: 0.000533
Epoch: 125 	Ltrain: 0.000523 	Lval: 0.000521
Epoch: 130 	Ltrain: 0.000511 	Lval: 0.000508
Epoch: 135 	Ltrain: 0.000501 	Lval: 0.000496
Epoch: 140 	Ltrain: 0.000489 	Lval: 0.000485
Epoch: 145 	Ltrain: 0.000477 	Lval: 0.000474
Epoch: 150 	Ltrain: 0.000467 	Lval: 0.000462
Epoch: 155 	Ltrain: 0.000456 	Lval: 0.000449
Epoch: 160 	Ltrain: 0.000443 	Lval: 0.000438
Epoch: 165 	Ltrain: 0.000431 	Lval: 0.000425
Epoch: 170 	Ltrain: 0.000419 	Lval: 0.000413
Epoch: 175 	Ltrain: 0.000408 	Lval: 0.000402
Epoch: 180 	Ltrain: 0.000395 	Lval: 0.000390
Epoch: 185 	Ltrain: 0.000382 	Lval: 0.000376
Epoch: 190 	Ltrain: 0.000370 	Lval: 0.000366
Epoch: 195 	Ltrain: 0.000358 	Lval: 0.000352
Epoch: 200 	Ltrain: 0.000347 	Lval: 0.000341
Epoch: 205 	Ltrain: 0.000335 	Lval: 0.000330
Epoch: 210 	Ltrain: 0.000323 	Lval: 0.000317
Epoch: 215 	Ltrain: 0.000312 	Lval: 0.000306
Epoch: 220 	Ltrain: 0.000300 	Lval: 0.000295
Epoch: 225 	Ltrain: 0.000290 	Lval: 0.000285
Epoch: 230 	Ltrain: 0.000280 	Lval: 0.000275
Epoch: 235 	Ltrain: 0.000269 	Lval: 0.000264
Epoch: 240 	Ltrain: 0.000259 	Lval: 0.000254
Epoch: 245 	Ltrain: 0.000250 	Lval: 0.000245
Epoch: 250 	Ltrain: 0.000241 	Lval: 0.000237
Epoch: 255 	Ltrain: 0.000234 	Lval: 0.000231
Epoch: 260 	Ltrain: 0.000229 	Lval: 0.000219
Epoch: 265 	Ltrain: 0.000220 	Lval: 0.000213
Epoch: 270 	Ltrain: 0.000209 	Lval: 0.000201
Epoch: 275 	Ltrain: 0.000203 	Lval: 0.000198
Epoch: 280 	Ltrain: 0.000194 	Lval: 0.000187
Epoch: 285 	Ltrain: 0.000186 	Lval: 0.000178
Epoch: 290 	Ltrain: 0.000181 	Lval: 0.000178
Epoch: 295 	Ltrain: 0.000178 	Lval: 0.000174
Epoch 00300: reducing learning rate of group 0 to 6.8010e-06.
Epoch: 300 	Ltrain: 0.000183 	Lval: 0.000177
Epoch: 305 	Ltrain: 0.000160 	Lval: 0.000155
Epoch: 310 	Ltrain: 0.000159 	Lval: 0.000154
Epoch: 315 	Ltrain: 0.000158 	Lval: 0.000152
Epoch: 320 	Ltrain: 0.000157 	Lval: 0.000152
Epoch: 325 	Ltrain: 0.000156 	Lval: 0.000151
EarlyStopper: stopping at epoch 327 with best_val_loss = 0.000160


	Fold 5/5
Epoch: 1 	Ltrain: 0.052761 	Lval: 0.017268
Epoch: 5 	Ltrain: 0.006254 	Lval: 0.007348
Epoch 00010: reducing learning rate of group 0 to 6.8010e-04.
Epoch: 10 	Ltrain: 0.005710 	Lval: 0.007240
Epoch: 15 	Ltrain: 0.005031 	Lval: 0.006418
Epoch: 20 	Ltrain: 0.004972 	Lval: 0.006468
Epoch: 25 	Ltrain: 0.004903 	Lval: 0.006254
Epoch: 30 	Ltrain: 0.004898 	Lval: 0.006266
Epoch 00031: reducing learning rate of group 0 to 6.8010e-05.
Epoch: 35 	Ltrain: 0.004753 	Lval: 0.006157
Epoch: 40 	Ltrain: 0.004758 	Lval: 0.006156
Epoch: 45 	Ltrain: 0.004757 	Lval: 0.006145
Epoch: 50 	Ltrain: 0.004723 	Lval: 0.006124
Epoch 00052: reducing learning rate of group 0 to 6.8010e-06.
Epoch: 55 	Ltrain: 0.004712 	Lval: 0.006105
Epoch: 60 	Ltrain: 0.004716 	Lval: 0.006100
Epoch 00064: reducing learning rate of group 0 to 6.8010e-07.
Epoch: 65 	Ltrain: 0.004726 	Lval: 0.006104
Epoch: 70 	Ltrain: 0.004710 	Lval: 0.006104
Epoch: 75 	Ltrain: 0.004720 	Lval: 0.006105
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.006085

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006859390024172842
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 7.440326763879891e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.237477 	Lval: 0.022988
Epoch: 5 	Ltrain: 0.017471 	Lval: 0.013116
Epoch: 10 	Ltrain: 0.010061 	Lval: 0.010202
Epoch: 15 	Ltrain: 0.007751 	Lval: 0.007098
Epoch: 20 	Ltrain: 0.007127 	Lval: 0.006798
Epoch: 25 	Ltrain: 0.007108 	Lval: 0.007059
Epoch: 30 	Ltrain: 0.006727 	Lval: 0.005984
Epoch 00034: reducing learning rate of group 0 to 6.8594e-04.
Epoch: 35 	Ltrain: 0.005989 	Lval: 0.005804
Epoch: 40 	Ltrain: 0.005466 	Lval: 0.005372
Epoch: 45 	Ltrain: 0.005025 	Lval: 0.005162
Epoch: 50 	Ltrain: 0.005215 	Lval: 0.005053
Epoch: 55 	Ltrain: 0.005005 	Lval: 0.004933
Epoch: 60 	Ltrain: 0.004883 	Lval: 0.004937
Epoch: 65 	Ltrain: 0.005191 	Lval: 0.004748
Epoch: 70 	Ltrain: 0.004616 	Lval: 0.004658
Epoch: 75 	Ltrain: 0.004535 	Lval: 0.004530
Epoch 00078: reducing learning rate of group 0 to 6.8594e-05.
Epoch: 80 	Ltrain: 0.004398 	Lval: 0.004424
Epoch: 85 	Ltrain: 0.004354 	Lval: 0.004404
Epoch: 90 	Ltrain: 0.004507 	Lval: 0.004392
Epoch 00093: reducing learning rate of group 0 to 6.8594e-06.
Epoch: 95 	Ltrain: 0.004497 	Lval: 0.004392
Epoch: 100 	Ltrain: 0.004279 	Lval: 0.004390
Epoch: 105 	Ltrain: 0.004460 	Lval: 0.004387
Epoch: 110 	Ltrain: 0.004323 	Lval: 0.004387
Epoch: 115 	Ltrain: 0.004154 	Lval: 0.004385
Epoch: 120 	Ltrain: 0.004474 	Lval: 0.004383
Epoch: 125 	Ltrain: 0.004523 	Lval: 0.004382
Epoch 00126: reducing learning rate of group 0 to 6.8594e-07.
Epoch: 130 	Ltrain: 0.004331 	Lval: 0.004381
EarlyStopper: stopping at epoch 129 with best_val_loss = 0.004388


	Fold 2/5
Epoch: 1 	Ltrain: 0.117547 	Lval: 0.024778
Epoch: 5 	Ltrain: 0.008284 	Lval: 0.008510
Epoch: 10 	Ltrain: 0.006905 	Lval: 0.006761
Epoch: 15 	Ltrain: 0.007413 	Lval: 0.006581
Epoch: 20 	Ltrain: 0.006251 	Lval: 0.006240
Epoch 00022: reducing learning rate of group 0 to 6.8594e-04.
Epoch: 25 	Ltrain: 0.005531 	Lval: 0.005571
Epoch: 30 	Ltrain: 0.005205 	Lval: 0.005505
Epoch: 35 	Ltrain: 0.005150 	Lval: 0.005520
Epoch: 40 	Ltrain: 0.005069 	Lval: 0.005285
Epoch: 45 	Ltrain: 0.004977 	Lval: 0.005193
Epoch: 50 	Ltrain: 0.004937 	Lval: 0.005026
Epoch: 55 	Ltrain: 0.004724 	Lval: 0.004984
Epoch: 60 	Ltrain: 0.004607 	Lval: 0.004713
Epoch: 65 	Ltrain: 0.004476 	Lval: 0.004557
Epoch: 70 	Ltrain: 0.004423 	Lval: 0.004437
Epoch: 75 	Ltrain: 0.004087 	Lval: 0.004173
Epoch: 80 	Ltrain: 0.003953 	Lval: 0.004065
Epoch: 85 	Ltrain: 0.003764 	Lval: 0.003833
Epoch: 90 	Ltrain: 0.003655 	Lval: 0.003615
Epoch: 95 	Ltrain: 0.003432 	Lval: 0.003451
Epoch: 100 	Ltrain: 0.003273 	Lval: 0.003346
Epoch: 105 	Ltrain: 0.002954 	Lval: 0.002940
Epoch 00109: reducing learning rate of group 0 to 6.8594e-05.
Epoch: 110 	Ltrain: 0.002739 	Lval: 0.002768
Epoch: 115 	Ltrain: 0.002615 	Lval: 0.002676
Epoch: 120 	Ltrain: 0.002545 	Lval: 0.002650
Epoch: 125 	Ltrain: 0.002576 	Lval: 0.002626
Epoch: 130 	Ltrain: 0.002507 	Lval: 0.002606
Epoch: 135 	Ltrain: 0.002482 	Lval: 0.002579
Epoch: 140 	Ltrain: 0.002458 	Lval: 0.002546
Epoch: 145 	Ltrain: 0.002447 	Lval: 0.002521
Epoch: 150 	Ltrain: 0.002446 	Lval: 0.002500
Epoch: 155 	Ltrain: 0.002391 	Lval: 0.002468
Epoch: 160 	Ltrain: 0.002385 	Lval: 0.002446
Epoch: 165 	Ltrain: 0.002328 	Lval: 0.002415
Epoch: 170 	Ltrain: 0.002332 	Lval: 0.002388
Epoch: 175 	Ltrain: 0.002300 	Lval: 0.002356
Epoch: 180 	Ltrain: 0.002248 	Lval: 0.002337
Epoch: 185 	Ltrain: 0.002190 	Lval: 0.002311
Epoch: 190 	Ltrain: 0.002190 	Lval: 0.002285
Epoch: 195 	Ltrain: 0.002184 	Lval: 0.002242
Epoch: 200 	Ltrain: 0.002136 	Lval: 0.002221
Epoch: 205 	Ltrain: 0.002118 	Lval: 0.002190
Epoch: 210 	Ltrain: 0.002110 	Lval: 0.002161
Epoch: 215 	Ltrain: 0.002076 	Lval: 0.002120
Epoch: 220 	Ltrain: 0.002038 	Lval: 0.002091
Epoch: 225 	Ltrain: 0.001986 	Lval: 0.002064
Epoch: 230 	Ltrain: 0.001956 	Lval: 0.002030
Epoch: 235 	Ltrain: 0.001945 	Lval: 0.002000
Epoch: 240 	Ltrain: 0.001903 	Lval: 0.001966
Epoch: 245 	Ltrain: 0.001869 	Lval: 0.001932
Epoch: 250 	Ltrain: 0.001848 	Lval: 0.001903
Epoch: 255 	Ltrain: 0.001832 	Lval: 0.001879
Epoch: 260 	Ltrain: 0.001795 	Lval: 0.001835
Epoch: 265 	Ltrain: 0.001764 	Lval: 0.001814
Epoch: 270 	Ltrain: 0.001730 	Lval: 0.001774
Epoch: 275 	Ltrain: 0.001691 	Lval: 0.001759
Epoch: 280 	Ltrain: 0.001654 	Lval: 0.001713
Epoch: 285 	Ltrain: 0.001634 	Lval: 0.001681
Epoch: 290 	Ltrain: 0.001603 	Lval: 0.001645
Epoch: 295 	Ltrain: 0.001570 	Lval: 0.001620
Epoch: 300 	Ltrain: 0.001549 	Lval: 0.001590
Epoch: 305 	Ltrain: 0.001548 	Lval: 0.001568
Epoch: 310 	Ltrain: 0.001480 	Lval: 0.001533
Epoch: 315 	Ltrain: 0.001471 	Lval: 0.001493
Epoch: 320 	Ltrain: 0.001425 	Lval: 0.001478
Epoch: 325 	Ltrain: 0.001389 	Lval: 0.001446
Epoch: 330 	Ltrain: 0.001380 	Lval: 0.001416
Epoch: 335 	Ltrain: 0.001352 	Lval: 0.001394
Epoch: 340 	Ltrain: 0.001343 	Lval: 0.001373
Epoch: 345 	Ltrain: 0.001301 	Lval: 0.001358
Epoch: 350 	Ltrain: 0.001279 	Lval: 0.001312
Epoch: 355 	Ltrain: 0.001250 	Lval: 0.001295
Epoch: 360 	Ltrain: 0.001243 	Lval: 0.001275
Epoch: 365 	Ltrain: 0.001214 	Lval: 0.001248
Epoch: 370 	Ltrain: 0.001196 	Lval: 0.001224
Epoch: 375 	Ltrain: 0.001158 	Lval: 0.001223
Epoch: 380 	Ltrain: 0.001128 	Lval: 0.001169
Epoch: 385 	Ltrain: 0.001121 	Lval: 0.001142
Epoch: 390 	Ltrain: 0.001093 	Lval: 0.001132
Epoch: 395 	Ltrain: 0.001065 	Lval: 0.001098
Epoch: 400 	Ltrain: 0.001053 	Lval: 0.001104
Epoch: 405 	Ltrain: 0.001041 	Lval: 0.001078
Epoch: 410 	Ltrain: 0.001016 	Lval: 0.001035
Epoch: 415 	Ltrain: 0.000993 	Lval: 0.001046
Epoch 00417: reducing learning rate of group 0 to 6.8594e-06.
Epoch: 420 	Ltrain: 0.000960 	Lval: 0.000994
Epoch: 425 	Ltrain: 0.000949 	Lval: 0.000989
Epoch: 430 	Ltrain: 0.000952 	Lval: 0.000985
Epoch: 435 	Ltrain: 0.000950 	Lval: 0.000983
Epoch: 440 	Ltrain: 0.000939 	Lval: 0.000981
Epoch: 445 	Ltrain: 0.000950 	Lval: 0.000979
Epoch: 450 	Ltrain: 0.000946 	Lval: 0.000976
Epoch: 455 	Ltrain: 0.000932 	Lval: 0.000974
Epoch: 460 	Ltrain: 0.000942 	Lval: 0.000972
Epoch: 465 	Ltrain: 0.000941 	Lval: 0.000969
Epoch: 470 	Ltrain: 0.000930 	Lval: 0.000966
Epoch: 475 	Ltrain: 0.000923 	Lval: 0.000964
Epoch: 480 	Ltrain: 0.000930 	Lval: 0.000962
Epoch: 485 	Ltrain: 0.000917 	Lval: 0.000959
Epoch: 490 	Ltrain: 0.000924 	Lval: 0.000956
Epoch: 495 	Ltrain: 0.000912 	Lval: 0.000953
Epoch: 500 	Ltrain: 0.000916 	Lval: 0.000950
Epoch: 505 	Ltrain: 0.000923 	Lval: 0.000948
Epoch: 510 	Ltrain: 0.000912 	Lval: 0.000945
Epoch: 515 	Ltrain: 0.000900 	Lval: 0.000942
Epoch: 520 	Ltrain: 0.000903 	Lval: 0.000939
Epoch: 525 	Ltrain: 0.000907 	Lval: 0.000937
Epoch: 530 	Ltrain: 0.000908 	Lval: 0.000933
Epoch: 535 	Ltrain: 0.000891 	Lval: 0.000930
Epoch: 540 	Ltrain: 0.000893 	Lval: 0.000926
Epoch: 545 	Ltrain: 0.000888 	Lval: 0.000924
Epoch: 550 	Ltrain: 0.000892 	Lval: 0.000921
Epoch: 555 	Ltrain: 0.000890 	Lval: 0.000918
Epoch: 560 	Ltrain: 0.000877 	Lval: 0.000915
Epoch: 565 	Ltrain: 0.000890 	Lval: 0.000912
Epoch: 570 	Ltrain: 0.000863 	Lval: 0.000909
Epoch: 575 	Ltrain: 0.000867 	Lval: 0.000906
Epoch: 580 	Ltrain: 0.000874 	Lval: 0.000903
Epoch: 585 	Ltrain: 0.000862 	Lval: 0.000899
Epoch: 590 	Ltrain: 0.000867 	Lval: 0.000896
Epoch: 595 	Ltrain: 0.000857 	Lval: 0.000894
Epoch: 600 	Ltrain: 0.000852 	Lval: 0.000890
Epoch: 605 	Ltrain: 0.000859 	Lval: 0.000887
Epoch: 610 	Ltrain: 0.000852 	Lval: 0.000884
Epoch: 615 	Ltrain: 0.000847 	Lval: 0.000881
Epoch: 620 	Ltrain: 0.000844 	Lval: 0.000878
Epoch: 625 	Ltrain: 0.000843 	Lval: 0.000875
Epoch: 630 	Ltrain: 0.000845 	Lval: 0.000871
Epoch: 635 	Ltrain: 0.000842 	Lval: 0.000868
Epoch: 640 	Ltrain: 0.000835 	Lval: 0.000865
Epoch: 645 	Ltrain: 0.000831 	Lval: 0.000862
Epoch: 650 	Ltrain: 0.000823 	Lval: 0.000858
Epoch: 655 	Ltrain: 0.000829 	Lval: 0.000855
Epoch: 660 	Ltrain: 0.000819 	Lval: 0.000853
Epoch: 665 	Ltrain: 0.000822 	Lval: 0.000850
Epoch: 670 	Ltrain: 0.000826 	Lval: 0.000846
Epoch: 675 	Ltrain: 0.000812 	Lval: 0.000844
Epoch: 680 	Ltrain: 0.000809 	Lval: 0.000840
Epoch: 685 	Ltrain: 0.000811 	Lval: 0.000837
Epoch: 690 	Ltrain: 0.000798 	Lval: 0.000834
Epoch: 695 	Ltrain: 0.000794 	Lval: 0.000831
Epoch: 700 	Ltrain: 0.000792 	Lval: 0.000828
Epoch: 705 	Ltrain: 0.000802 	Lval: 0.000826
Epoch: 710 	Ltrain: 0.000800 	Lval: 0.000822
Epoch: 715 	Ltrain: 0.000814 	Lval: 0.000819
Epoch: 720 	Ltrain: 0.000788 	Lval: 0.000816
Epoch: 725 	Ltrain: 0.000787 	Lval: 0.000813
Epoch: 730 	Ltrain: 0.000782 	Lval: 0.000810
Epoch: 735 	Ltrain: 0.000785 	Lval: 0.000807
Epoch: 740 	Ltrain: 0.000771 	Lval: 0.000803
Epoch: 745 	Ltrain: 0.000772 	Lval: 0.000801
Epoch: 750 	Ltrain: 0.000765 	Lval: 0.000797
Epoch: 755 	Ltrain: 0.000764 	Lval: 0.000795
Epoch: 760 	Ltrain: 0.000763 	Lval: 0.000792
Epoch: 765 	Ltrain: 0.000758 	Lval: 0.000789
Epoch: 770 	Ltrain: 0.000765 	Lval: 0.000786
Epoch: 775 	Ltrain: 0.000752 	Lval: 0.000783
Epoch: 780 	Ltrain: 0.000759 	Lval: 0.000780
Epoch: 785 	Ltrain: 0.000752 	Lval: 0.000777
Epoch: 790 	Ltrain: 0.000744 	Lval: 0.000774
Epoch: 795 	Ltrain: 0.000755 	Lval: 0.000771
Epoch: 800 	Ltrain: 0.000744 	Lval: 0.000767
Epoch: 805 	Ltrain: 0.000740 	Lval: 0.000765
Epoch: 810 	Ltrain: 0.000732 	Lval: 0.000763
Epoch: 815 	Ltrain: 0.000738 	Lval: 0.000759
Epoch: 820 	Ltrain: 0.000728 	Lval: 0.000757
Epoch: 825 	Ltrain: 0.000735 	Lval: 0.000753
Epoch: 830 	Ltrain: 0.000729 	Lval: 0.000750
Epoch: 835 	Ltrain: 0.000746 	Lval: 0.000748
Epoch: 840 	Ltrain: 0.000726 	Lval: 0.000745
Epoch: 845 	Ltrain: 0.000720 	Lval: 0.000743
Epoch: 850 	Ltrain: 0.000706 	Lval: 0.000739
Epoch: 855 	Ltrain: 0.000718 	Lval: 0.000737
Epoch: 860 	Ltrain: 0.000718 	Lval: 0.000733
Epoch: 865 	Ltrain: 0.000722 	Lval: 0.000730
Epoch: 870 	Ltrain: 0.000702 	Lval: 0.000728
Epoch: 875 	Ltrain: 0.000711 	Lval: 0.000725
Epoch: 880 	Ltrain: 0.000698 	Lval: 0.000722
Epoch: 885 	Ltrain: 0.000710 	Lval: 0.000719
Epoch: 890 	Ltrain: 0.000703 	Lval: 0.000716
Epoch: 895 	Ltrain: 0.000687 	Lval: 0.000713
Epoch: 900 	Ltrain: 0.000689 	Lval: 0.000710
Epoch: 905 	Ltrain: 0.000685 	Lval: 0.000708
Epoch: 910 	Ltrain: 0.000687 	Lval: 0.000706
Epoch: 915 	Ltrain: 0.000695 	Lval: 0.000702
Epoch: 920 	Ltrain: 0.000679 	Lval: 0.000700
Epoch: 925 	Ltrain: 0.000677 	Lval: 0.000697
Epoch: 930 	Ltrain: 0.000677 	Lval: 0.000695
Epoch: 935 	Ltrain: 0.000672 	Lval: 0.000692
Epoch: 940 	Ltrain: 0.000675 	Lval: 0.000689
Epoch: 945 	Ltrain: 0.000661 	Lval: 0.000687
Epoch: 950 	Ltrain: 0.000657 	Lval: 0.000683
Epoch: 955 	Ltrain: 0.000660 	Lval: 0.000681
Epoch: 960 	Ltrain: 0.000661 	Lval: 0.000679
Epoch: 965 	Ltrain: 0.000655 	Lval: 0.000676
Epoch: 970 	Ltrain: 0.000651 	Lval: 0.000674
Epoch: 975 	Ltrain: 0.000653 	Lval: 0.000671
Epoch: 980 	Ltrain: 0.000657 	Lval: 0.000667
Epoch: 985 	Ltrain: 0.000654 	Lval: 0.000665
Epoch: 990 	Ltrain: 0.000648 	Lval: 0.000663
Epoch: 995 	Ltrain: 0.000647 	Lval: 0.000660
Epoch: 1000 	Ltrain: 0.000645 	Lval: 0.000657
Epoch: 1005 	Ltrain: 0.000637 	Lval: 0.000655
Epoch: 1010 	Ltrain: 0.000639 	Lval: 0.000653
Epoch: 1015 	Ltrain: 0.000653 	Lval: 0.000650
Epoch: 1020 	Ltrain: 0.000628 	Lval: 0.000647
Epoch: 1025 	Ltrain: 0.000632 	Lval: 0.000645
Epoch: 1030 	Ltrain: 0.000636 	Lval: 0.000642
Epoch: 1035 	Ltrain: 0.000629 	Lval: 0.000640
Epoch: 1040 	Ltrain: 0.000619 	Lval: 0.000637
Epoch: 1045 	Ltrain: 0.000629 	Lval: 0.000634
Epoch: 1050 	Ltrain: 0.000629 	Lval: 0.000632
Epoch: 1055 	Ltrain: 0.000614 	Lval: 0.000630
Epoch: 1060 	Ltrain: 0.000606 	Lval: 0.000628
Epoch: 1065 	Ltrain: 0.000606 	Lval: 0.000624
Epoch: 1070 	Ltrain: 0.000610 	Lval: 0.000622
Epoch: 1075 	Ltrain: 0.000602 	Lval: 0.000619
Epoch: 1080 	Ltrain: 0.000600 	Lval: 0.000617
Epoch: 1085 	Ltrain: 0.000602 	Lval: 0.000614
Epoch: 1090 	Ltrain: 0.000593 	Lval: 0.000612
Epoch: 1095 	Ltrain: 0.000600 	Lval: 0.000610
Epoch: 1100 	Ltrain: 0.000598 	Lval: 0.000607
Epoch: 1105 	Ltrain: 0.000584 	Lval: 0.000605
Epoch: 1110 	Ltrain: 0.000594 	Lval: 0.000603
Epoch: 1115 	Ltrain: 0.000584 	Lval: 0.000599
Epoch: 1120 	Ltrain: 0.000585 	Lval: 0.000598
Epoch: 1125 	Ltrain: 0.000590 	Lval: 0.000595
Epoch: 1130 	Ltrain: 0.000599 	Lval: 0.000593
Epoch: 1135 	Ltrain: 0.000572 	Lval: 0.000590
Epoch: 1140 	Ltrain: 0.000575 	Lval: 0.000588
Epoch: 1145 	Ltrain: 0.000572 	Lval: 0.000586
Epoch: 1150 	Ltrain: 0.000574 	Lval: 0.000584
Epoch: 1155 	Ltrain: 0.000564 	Lval: 0.000581
Epoch: 1160 	Ltrain: 0.000563 	Lval: 0.000579
Epoch: 1165 	Ltrain: 0.000563 	Lval: 0.000576
Epoch: 1170 	Ltrain: 0.000560 	Lval: 0.000574
Epoch: 1175 	Ltrain: 0.000557 	Lval: 0.000572
Epoch: 1180 	Ltrain: 0.000550 	Lval: 0.000569
Epoch: 1185 	Ltrain: 0.000561 	Lval: 0.000567
Epoch: 1190 	Ltrain: 0.000553 	Lval: 0.000566
Epoch: 1195 	Ltrain: 0.000556 	Lval: 0.000563
Epoch: 1200 	Ltrain: 0.000555 	Lval: 0.000561
Epoch: 1205 	Ltrain: 0.000543 	Lval: 0.000559
Epoch: 1210 	Ltrain: 0.000545 	Lval: 0.000557
Epoch: 1215 	Ltrain: 0.000551 	Lval: 0.000554
Epoch: 1220 	Ltrain: 0.000540 	Lval: 0.000552
Epoch: 1225 	Ltrain: 0.000543 	Lval: 0.000550
Epoch: 1230 	Ltrain: 0.000534 	Lval: 0.000548
Epoch: 1235 	Ltrain: 0.000529 	Lval: 0.000546
Epoch: 1240 	Ltrain: 0.000544 	Lval: 0.000544
Epoch: 1245 	Ltrain: 0.000534 	Lval: 0.000542
Epoch: 1250 	Ltrain: 0.000537 	Lval: 0.000540
Epoch: 1255 	Ltrain: 0.000526 	Lval: 0.000537
Epoch: 1260 	Ltrain: 0.000526 	Lval: 0.000535
Epoch: 1265 	Ltrain: 0.000528 	Lval: 0.000533
Epoch: 1270 	Ltrain: 0.000516 	Lval: 0.000530
Epoch: 1275 	Ltrain: 0.000517 	Lval: 0.000528
Epoch: 1280 	Ltrain: 0.000515 	Lval: 0.000526
Epoch: 1285 	Ltrain: 0.000514 	Lval: 0.000524
Epoch: 1290 	Ltrain: 0.000515 	Lval: 0.000523
Epoch: 1295 	Ltrain: 0.000510 	Lval: 0.000520
Epoch: 1300 	Ltrain: 0.000509 	Lval: 0.000518
Epoch: 1305 	Ltrain: 0.000506 	Lval: 0.000516
Epoch: 1310 	Ltrain: 0.000502 	Lval: 0.000514
Epoch: 1315 	Ltrain: 0.000498 	Lval: 0.000512
Epoch: 1320 	Ltrain: 0.000501 	Lval: 0.000510
Epoch: 1325 	Ltrain: 0.000497 	Lval: 0.000508
Epoch: 1330 	Ltrain: 0.000499 	Lval: 0.000506
Epoch: 1335 	Ltrain: 0.000494 	Lval: 0.000503
Epoch: 1340 	Ltrain: 0.000490 	Lval: 0.000501
Epoch: 1345 	Ltrain: 0.000489 	Lval: 0.000499
Epoch: 1350 	Ltrain: 0.000492 	Lval: 0.000497
Epoch: 1355 	Ltrain: 0.000490 	Lval: 0.000496
Epoch: 1360 	Ltrain: 0.000489 	Lval: 0.000493
Epoch: 1365 	Ltrain: 0.000486 	Lval: 0.000491
Epoch: 1370 	Ltrain: 0.000485 	Lval: 0.000490
Epoch: 1375 	Ltrain: 0.000481 	Lval: 0.000487
Epoch: 1380 	Ltrain: 0.000473 	Lval: 0.000486
Epoch: 1385 	Ltrain: 0.000483 	Lval: 0.000483
Epoch: 1390 	Ltrain: 0.000473 	Lval: 0.000482
Epoch: 1395 	Ltrain: 0.000467 	Lval: 0.000480
Epoch: 1400 	Ltrain: 0.000469 	Lval: 0.000478
Epoch: 1405 	Ltrain: 0.000471 	Lval: 0.000476
Epoch: 1410 	Ltrain: 0.000474 	Lval: 0.000475
Epoch: 1415 	Ltrain: 0.000469 	Lval: 0.000473
Epoch: 1420 	Ltrain: 0.000471 	Lval: 0.000471
EarlyStopper: stopping at epoch 1419 with best_val_loss = 0.000480


	Fold 3/5
Epoch: 1 	Ltrain: 0.061390 	Lval: 0.015294
Epoch: 5 	Ltrain: 0.007132 	Lval: 0.007410
Epoch: 10 	Ltrain: 0.006481 	Lval: 0.006957
Epoch: 15 	Ltrain: 0.006143 	Lval: 0.006380
Epoch: 20 	Ltrain: 0.005824 	Lval: 0.006098
Epoch: 25 	Ltrain: 0.005558 	Lval: 0.006038
Epoch 00030: reducing learning rate of group 0 to 6.8594e-04.
Epoch: 30 	Ltrain: 0.005102 	Lval: 0.005665
Epoch: 35 	Ltrain: 0.004118 	Lval: 0.004623
Epoch: 40 	Ltrain: 0.003967 	Lval: 0.004390
Epoch: 45 	Ltrain: 0.003814 	Lval: 0.004204
Epoch: 50 	Ltrain: 0.003702 	Lval: 0.004034
Epoch: 55 	Ltrain: 0.003517 	Lval: 0.003872
Epoch: 60 	Ltrain: 0.003383 	Lval: 0.003685
Epoch: 65 	Ltrain: 0.003217 	Lval: 0.003403
Epoch: 70 	Ltrain: 0.003001 	Lval: 0.003038
Epoch: 75 	Ltrain: 0.002759 	Lval: 0.002837
Epoch: 80 	Ltrain: 0.002529 	Lval: 0.002583
Epoch: 85 	Ltrain: 0.002279 	Lval: 0.002324
Epoch: 90 	Ltrain: 0.002156 	Lval: 0.002120
Epoch: 95 	Ltrain: 0.002038 	Lval: 0.002296
Epoch: 100 	Ltrain: 0.001815 	Lval: 0.001824
Epoch: 105 	Ltrain: 0.001609 	Lval: 0.001614
Epoch: 110 	Ltrain: 0.001458 	Lval: 0.001455
Epoch: 115 	Ltrain: 0.001488 	Lval: 0.001694
Epoch: 120 	Ltrain: 0.001284 	Lval: 0.001300
Epoch: 125 	Ltrain: 0.001277 	Lval: 0.001235
Epoch: 130 	Ltrain: 0.001161 	Lval: 0.001141
Epoch 00133: reducing learning rate of group 0 to 6.8594e-05.
Epoch: 135 	Ltrain: 0.000880 	Lval: 0.000870
Epoch: 140 	Ltrain: 0.000823 	Lval: 0.000820
Epoch: 145 	Ltrain: 0.000802 	Lval: 0.000800
Epoch: 150 	Ltrain: 0.000789 	Lval: 0.000783
Epoch: 155 	Ltrain: 0.000774 	Lval: 0.000768
Epoch: 160 	Ltrain: 0.000760 	Lval: 0.000754
Epoch: 165 	Ltrain: 0.000749 	Lval: 0.000742
Epoch: 170 	Ltrain: 0.000739 	Lval: 0.000728
Epoch: 175 	Ltrain: 0.000723 	Lval: 0.000713
Epoch: 180 	Ltrain: 0.000711 	Lval: 0.000700
Epoch: 185 	Ltrain: 0.000712 	Lval: 0.000688
Epoch: 190 	Ltrain: 0.000687 	Lval: 0.000671
Epoch: 195 	Ltrain: 0.000673 	Lval: 0.000657
Epoch: 200 	Ltrain: 0.000662 	Lval: 0.000642
Epoch: 205 	Ltrain: 0.000649 	Lval: 0.000627
Epoch: 210 	Ltrain: 0.000632 	Lval: 0.000613
Epoch: 215 	Ltrain: 0.000617 	Lval: 0.000597
Epoch: 220 	Ltrain: 0.000606 	Lval: 0.000584
Epoch: 225 	Ltrain: 0.000591 	Lval: 0.000568
Epoch: 230 	Ltrain: 0.000574 	Lval: 0.000555
Epoch: 235 	Ltrain: 0.000561 	Lval: 0.000541
Epoch: 240 	Ltrain: 0.000549 	Lval: 0.000524
Epoch: 245 	Ltrain: 0.000534 	Lval: 0.000510
Epoch: 250 	Ltrain: 0.000520 	Lval: 0.000495
Epoch: 255 	Ltrain: 0.000504 	Lval: 0.000481
Epoch: 260 	Ltrain: 0.000495 	Lval: 0.000467
Epoch: 265 	Ltrain: 0.000480 	Lval: 0.000451
Epoch: 270 	Ltrain: 0.000466 	Lval: 0.000439
Epoch: 275 	Ltrain: 0.000453 	Lval: 0.000425
Epoch: 280 	Ltrain: 0.000439 	Lval: 0.000413
Epoch: 285 	Ltrain: 0.000433 	Lval: 0.000407
Epoch: 290 	Ltrain: 0.000414 	Lval: 0.000388
Epoch: 295 	Ltrain: 0.000402 	Lval: 0.000374
Epoch: 300 	Ltrain: 0.000394 	Lval: 0.000366
Epoch: 305 	Ltrain: 0.000385 	Lval: 0.000355
Epoch: 310 	Ltrain: 0.000369 	Lval: 0.000340
Epoch: 315 	Ltrain: 0.000361 	Lval: 0.000334
Epoch: 320 	Ltrain: 0.000351 	Lval: 0.000323
Epoch: 325 	Ltrain: 0.000347 	Lval: 0.000319
Epoch: 330 	Ltrain: 0.000341 	Lval: 0.000320
Epoch: 335 	Ltrain: 0.000325 	Lval: 0.000296
Epoch: 340 	Ltrain: 0.000319 	Lval: 0.000294
Epoch: 345 	Ltrain: 0.000306 	Lval: 0.000279
Epoch: 350 	Ltrain: 0.000301 	Lval: 0.000277
Epoch: 355 	Ltrain: 0.000293 	Lval: 0.000263
Epoch: 360 	Ltrain: 0.000284 	Lval: 0.000262
Epoch: 365 	Ltrain: 0.000280 	Lval: 0.000258
Epoch 00367: reducing learning rate of group 0 to 6.8594e-06.
Epoch: 370 	Ltrain: 0.000259 	Lval: 0.000237
Epoch: 375 	Ltrain: 0.000256 	Lval: 0.000235
Epoch: 380 	Ltrain: 0.000254 	Lval: 0.000234
Epoch: 385 	Ltrain: 0.000255 	Lval: 0.000233
Epoch: 390 	Ltrain: 0.000254 	Lval: 0.000232
Epoch: 395 	Ltrain: 0.000252 	Lval: 0.000231
Epoch: 400 	Ltrain: 0.000251 	Lval: 0.000230
Epoch: 405 	Ltrain: 0.000252 	Lval: 0.000229
Epoch: 410 	Ltrain: 0.000250 	Lval: 0.000228
Epoch: 415 	Ltrain: 0.000248 	Lval: 0.000227
EarlyStopper: stopping at epoch 417 with best_val_loss = 0.000232


	Fold 4/5
Epoch: 1 	Ltrain: 0.047424 	Lval: 0.010306
Epoch: 5 	Ltrain: 0.006118 	Lval: 0.007803
Epoch: 10 	Ltrain: 0.005582 	Lval: 0.006563
Epoch: 15 	Ltrain: 0.005424 	Lval: 0.006129
Epoch: 20 	Ltrain: 0.004785 	Lval: 0.005911
Epoch: 25 	Ltrain: 0.004791 	Lval: 0.005948
Epoch: 30 	Ltrain: 0.004064 	Lval: 0.005220
Epoch: 35 	Ltrain: 0.003713 	Lval: 0.003986
Epoch: 40 	Ltrain: 0.003518 	Lval: 0.003891
Epoch: 45 	Ltrain: 0.002893 	Lval: 0.002820
Epoch: 50 	Ltrain: 0.002382 	Lval: 0.002503
Epoch 00052: reducing learning rate of group 0 to 6.8594e-04.
Epoch: 55 	Ltrain: 0.001603 	Lval: 0.001664
Epoch: 60 	Ltrain: 0.001356 	Lval: 0.001390
Epoch: 65 	Ltrain: 0.001215 	Lval: 0.001223
Epoch: 70 	Ltrain: 0.001095 	Lval: 0.001083
Epoch: 75 	Ltrain: 0.000984 	Lval: 0.000962
Epoch: 80 	Ltrain: 0.000886 	Lval: 0.000850
Epoch: 85 	Ltrain: 0.000791 	Lval: 0.000753
Epoch: 90 	Ltrain: 0.000708 	Lval: 0.000671
Epoch: 95 	Ltrain: 0.000665 	Lval: 0.000624
Epoch: 100 	Ltrain: 0.000565 	Lval: 0.000528
Epoch: 105 	Ltrain: 0.000518 	Lval: 0.000477
Epoch: 110 	Ltrain: 0.000489 	Lval: 0.000474
Epoch: 115 	Ltrain: 0.000445 	Lval: 0.000444
Epoch: 120 	Ltrain: 0.000426 	Lval: 0.000391
Epoch 00122: reducing learning rate of group 0 to 6.8594e-05.
Epoch: 125 	Ltrain: 0.000339 	Lval: 0.000310
Epoch: 130 	Ltrain: 0.000314 	Lval: 0.000285
Epoch: 135 	Ltrain: 0.000305 	Lval: 0.000276
Epoch: 140 	Ltrain: 0.000299 	Lval: 0.000270
Epoch: 145 	Ltrain: 0.000293 	Lval: 0.000264
Epoch: 150 	Ltrain: 0.000287 	Lval: 0.000258
Epoch: 155 	Ltrain: 0.000281 	Lval: 0.000253
Epoch: 160 	Ltrain: 0.000275 	Lval: 0.000247
Epoch: 165 	Ltrain: 0.000270 	Lval: 0.000242
Epoch: 170 	Ltrain: 0.000265 	Lval: 0.000236
Epoch: 175 	Ltrain: 0.000259 	Lval: 0.000230
Epoch: 180 	Ltrain: 0.000252 	Lval: 0.000224
Epoch: 185 	Ltrain: 0.000245 	Lval: 0.000218
Epoch: 190 	Ltrain: 0.000240 	Lval: 0.000212
Epoch: 195 	Ltrain: 0.000233 	Lval: 0.000206
Epoch: 200 	Ltrain: 0.000226 	Lval: 0.000200
Epoch: 205 	Ltrain: 0.000219 	Lval: 0.000194
Epoch: 210 	Ltrain: 0.000213 	Lval: 0.000187
Epoch: 215 	Ltrain: 0.000205 	Lval: 0.000181
Epoch: 220 	Ltrain: 0.000199 	Lval: 0.000175
Epoch: 225 	Ltrain: 0.000192 	Lval: 0.000168
Epoch: 230 	Ltrain: 0.000185 	Lval: 0.000162
Epoch: 235 	Ltrain: 0.000179 	Lval: 0.000155
Epoch: 240 	Ltrain: 0.000172 	Lval: 0.000149
Epoch: 245 	Ltrain: 0.000166 	Lval: 0.000144
Epoch: 250 	Ltrain: 0.000160 	Lval: 0.000137
Epoch: 255 	Ltrain: 0.000154 	Lval: 0.000132
Epoch: 260 	Ltrain: 0.000148 	Lval: 0.000129
Epoch: 265 	Ltrain: 0.000143 	Lval: 0.000123
Epoch: 270 	Ltrain: 0.000140 	Lval: 0.000123
Epoch 00272: reducing learning rate of group 0 to 6.8594e-06.
Epoch: 275 	Ltrain: 0.000131 	Lval: 0.000112
Epoch: 280 	Ltrain: 0.000129 	Lval: 0.000111
Epoch: 285 	Ltrain: 0.000128 	Lval: 0.000110
Epoch: 290 	Ltrain: 0.000128 	Lval: 0.000109
Epoch: 295 	Ltrain: 0.000127 	Lval: 0.000109
Epoch: 300 	Ltrain: 0.000126 	Lval: 0.000108
EarlyStopper: stopping at epoch 301 with best_val_loss = 0.000112


	Fold 5/5
Epoch: 1 	Ltrain: 0.058823 	Lval: 0.018263
Epoch: 5 	Ltrain: 0.006385 	Lval: 0.007386
Epoch: 10 	Ltrain: 0.005617 	Lval: 0.008259
Epoch: 15 	Ltrain: 0.005360 	Lval: 0.006436
Epoch: 20 	Ltrain: 0.004985 	Lval: 0.006725
Epoch 00023: reducing learning rate of group 0 to 6.8594e-04.
Epoch: 25 	Ltrain: 0.004252 	Lval: 0.005245
Epoch: 30 	Ltrain: 0.004055 	Lval: 0.005032
Epoch: 35 	Ltrain: 0.003929 	Lval: 0.004895
Epoch: 40 	Ltrain: 0.003810 	Lval: 0.004670
Epoch: 45 	Ltrain: 0.003714 	Lval: 0.004397
Epoch: 50 	Ltrain: 0.003563 	Lval: 0.004175
Epoch: 55 	Ltrain: 0.003416 	Lval: 0.003981
Epoch: 60 	Ltrain: 0.003294 	Lval: 0.003797
Epoch 00065: reducing learning rate of group 0 to 6.8594e-05.
Epoch: 65 	Ltrain: 0.003193 	Lval: 0.003781
Epoch: 70 	Ltrain: 0.002918 	Lval: 0.003380
Epoch: 75 	Ltrain: 0.002893 	Lval: 0.003348
Epoch: 80 	Ltrain: 0.002872 	Lval: 0.003301
Epoch: 85 	Ltrain: 0.002848 	Lval: 0.003284
Epoch: 90 	Ltrain: 0.002822 	Lval: 0.003227
Epoch: 95 	Ltrain: 0.002787 	Lval: 0.003181
Epoch: 100 	Ltrain: 0.002763 	Lval: 0.003148
Epoch: 105 	Ltrain: 0.002727 	Lval: 0.003097
Epoch: 110 	Ltrain: 0.002695 	Lval: 0.003054
Epoch: 115 	Ltrain: 0.002663 	Lval: 0.003007
Epoch: 120 	Ltrain: 0.002628 	Lval: 0.002965
Epoch: 125 	Ltrain: 0.002586 	Lval: 0.002920
Epoch: 130 	Ltrain: 0.002559 	Lval: 0.002878
Epoch: 135 	Ltrain: 0.002514 	Lval: 0.002828
Epoch: 140 	Ltrain: 0.002482 	Lval: 0.002755
Epoch: 145 	Ltrain: 0.002435 	Lval: 0.002693
Epoch: 150 	Ltrain: 0.002391 	Lval: 0.002641
Epoch: 155 	Ltrain: 0.002353 	Lval: 0.002591
Epoch: 160 	Ltrain: 0.002312 	Lval: 0.002578
Epoch: 165 	Ltrain: 0.002267 	Lval: 0.002496
Epoch: 170 	Ltrain: 0.002213 	Lval: 0.002412
Epoch: 175 	Ltrain: 0.002162 	Lval: 0.002369
Epoch: 180 	Ltrain: 0.002125 	Lval: 0.002311
Epoch: 185 	Ltrain: 0.002077 	Lval: 0.002275
Epoch 00187: reducing learning rate of group 0 to 6.8594e-06.
Epoch: 190 	Ltrain: 0.002005 	Lval: 0.002209
Epoch: 195 	Ltrain: 0.001994 	Lval: 0.002187
Epoch: 200 	Ltrain: 0.001989 	Lval: 0.002178
Epoch: 205 	Ltrain: 0.001981 	Lval: 0.002169
Epoch: 210 	Ltrain: 0.001984 	Lval: 0.002166
Epoch: 215 	Ltrain: 0.001968 	Lval: 0.002158
Epoch: 220 	Ltrain: 0.001964 	Lval: 0.002153
Epoch: 225 	Ltrain: 0.001960 	Lval: 0.002146
Epoch: 230 	Ltrain: 0.001956 	Lval: 0.002138
Epoch: 235 	Ltrain: 0.001955 	Lval: 0.002130
Epoch: 240 	Ltrain: 0.001950 	Lval: 0.002126
Epoch: 245 	Ltrain: 0.001936 	Lval: 0.002121
Epoch: 250 	Ltrain: 0.001940 	Lval: 0.002114
Epoch: 255 	Ltrain: 0.001930 	Lval: 0.002108
Epoch: 260 	Ltrain: 0.001921 	Lval: 0.002098
Epoch: 265 	Ltrain: 0.001915 	Lval: 0.002094
Epoch: 270 	Ltrain: 0.001913 	Lval: 0.002086
Epoch: 275 	Ltrain: 0.001905 	Lval: 0.002079
Epoch: 280 	Ltrain: 0.001903 	Lval: 0.002070
Epoch: 285 	Ltrain: 0.001899 	Lval: 0.002066
Epoch: 290 	Ltrain: 0.001887 	Lval: 0.002058
Epoch: 295 	Ltrain: 0.001887 	Lval: 0.002049
Epoch: 300 	Ltrain: 0.001878 	Lval: 0.002043
Epoch: 305 	Ltrain: 0.001868 	Lval: 0.002036
Epoch: 310 	Ltrain: 0.001862 	Lval: 0.002028
Epoch: 315 	Ltrain: 0.001858 	Lval: 0.002022
Epoch: 320 	Ltrain: 0.001856 	Lval: 0.002015
Epoch: 325 	Ltrain: 0.001850 	Lval: 0.002006
Epoch: 330 	Ltrain: 0.001842 	Lval: 0.002001
Epoch: 335 	Ltrain: 0.001839 	Lval: 0.001996
Epoch: 340 	Ltrain: 0.001832 	Lval: 0.001988
Epoch: 345 	Ltrain: 0.001826 	Lval: 0.001982
Epoch: 350 	Ltrain: 0.001818 	Lval: 0.001975
Epoch: 355 	Ltrain: 0.001813 	Lval: 0.001969
Epoch: 360 	Ltrain: 0.001810 	Lval: 0.001963
Epoch: 365 	Ltrain: 0.001802 	Lval: 0.001958
Epoch: 370 	Ltrain: 0.001793 	Lval: 0.001950
Epoch: 375 	Ltrain: 0.001788 	Lval: 0.001941
Epoch: 380 	Ltrain: 0.001785 	Lval: 0.001935
Epoch: 385 	Ltrain: 0.001782 	Lval: 0.001927
Epoch: 390 	Ltrain: 0.001769 	Lval: 0.001922
Epoch: 395 	Ltrain: 0.001769 	Lval: 0.001918
Epoch: 400 	Ltrain: 0.001761 	Lval: 0.001908
Epoch: 405 	Ltrain: 0.001761 	Lval: 0.001901
Epoch: 410 	Ltrain: 0.001750 	Lval: 0.001896
Epoch: 415 	Ltrain: 0.001744 	Lval: 0.001889
Epoch: 420 	Ltrain: 0.001740 	Lval: 0.001881
Epoch: 425 	Ltrain: 0.001729 	Lval: 0.001881
Epoch: 430 	Ltrain: 0.001730 	Lval: 0.001869
Epoch: 435 	Ltrain: 0.001723 	Lval: 0.001865
Epoch: 440 	Ltrain: 0.001716 	Lval: 0.001856
Epoch: 445 	Ltrain: 0.001708 	Lval: 0.001848
Epoch: 450 	Ltrain: 0.001714 	Lval: 0.001845
Epoch: 455 	Ltrain: 0.001699 	Lval: 0.001835
Epoch: 460 	Ltrain: 0.001694 	Lval: 0.001831
Epoch: 465 	Ltrain: 0.001688 	Lval: 0.001822
Epoch: 470 	Ltrain: 0.001683 	Lval: 0.001819
Epoch: 475 	Ltrain: 0.001679 	Lval: 0.001812
Epoch: 480 	Ltrain: 0.001680 	Lval: 0.001803
Epoch: 485 	Ltrain: 0.001665 	Lval: 0.001799
Epoch: 490 	Ltrain: 0.001663 	Lval: 0.001793
Epoch: 495 	Ltrain: 0.001656 	Lval: 0.001787
Epoch: 500 	Ltrain: 0.001655 	Lval: 0.001778
Epoch: 505 	Ltrain: 0.001646 	Lval: 0.001776
Epoch: 510 	Ltrain: 0.001638 	Lval: 0.001766
Epoch: 515 	Ltrain: 0.001635 	Lval: 0.001760
Epoch: 520 	Ltrain: 0.001631 	Lval: 0.001755
Epoch: 525 	Ltrain: 0.001624 	Lval: 0.001749
Epoch: 530 	Ltrain: 0.001622 	Lval: 0.001742
Epoch: 535 	Ltrain: 0.001618 	Lval: 0.001736
Epoch: 540 	Ltrain: 0.001614 	Lval: 0.001731
Epoch: 545 	Ltrain: 0.001601 	Lval: 0.001723
Epoch: 550 	Ltrain: 0.001597 	Lval: 0.001715
Epoch: 555 	Ltrain: 0.001596 	Lval: 0.001709
Epoch: 560 	Ltrain: 0.001587 	Lval: 0.001702
Epoch: 565 	Ltrain: 0.001580 	Lval: 0.001697
Epoch: 570 	Ltrain: 0.001576 	Lval: 0.001691
Epoch: 575 	Ltrain: 0.001574 	Lval: 0.001685
Epoch: 580 	Ltrain: 0.001566 	Lval: 0.001680
Epoch: 585 	Ltrain: 0.001559 	Lval: 0.001673
Epoch: 590 	Ltrain: 0.001554 	Lval: 0.001666
Epoch: 595 	Ltrain: 0.001554 	Lval: 0.001663
Epoch: 600 	Ltrain: 0.001545 	Lval: 0.001655
Epoch: 605 	Ltrain: 0.001541 	Lval: 0.001647
Epoch: 610 	Ltrain: 0.001539 	Lval: 0.001643
Epoch: 615 	Ltrain: 0.001531 	Lval: 0.001637
Epoch: 620 	Ltrain: 0.001525 	Lval: 0.001629
Epoch: 625 	Ltrain: 0.001520 	Lval: 0.001625
Epoch: 630 	Ltrain: 0.001515 	Lval: 0.001616
Epoch: 635 	Ltrain: 0.001510 	Lval: 0.001613
Epoch: 640 	Ltrain: 0.001507 	Lval: 0.001607
Epoch: 645 	Ltrain: 0.001502 	Lval: 0.001601
Epoch: 650 	Ltrain: 0.001494 	Lval: 0.001595
Epoch: 655 	Ltrain: 0.001489 	Lval: 0.001588
Epoch: 660 	Ltrain: 0.001487 	Lval: 0.001584
Epoch: 665 	Ltrain: 0.001482 	Lval: 0.001579
Epoch: 670 	Ltrain: 0.001478 	Lval: 0.001570
Epoch: 675 	Ltrain: 0.001472 	Lval: 0.001566
Epoch: 680 	Ltrain: 0.001467 	Lval: 0.001561
Epoch: 685 	Ltrain: 0.001463 	Lval: 0.001557
Epoch: 690 	Ltrain: 0.001454 	Lval: 0.001549
Epoch: 695 	Ltrain: 0.001452 	Lval: 0.001544
Epoch: 700 	Ltrain: 0.001446 	Lval: 0.001538
Epoch: 705 	Ltrain: 0.001438 	Lval: 0.001533
Epoch: 710 	Ltrain: 0.001437 	Lval: 0.001533
Epoch: 715 	Ltrain: 0.001432 	Lval: 0.001521
Epoch: 720 	Ltrain: 0.001431 	Lval: 0.001517
Epoch: 725 	Ltrain: 0.001421 	Lval: 0.001509
Epoch: 730 	Ltrain: 0.001417 	Lval: 0.001505
Epoch: 735 	Ltrain: 0.001413 	Lval: 0.001499
Epoch: 740 	Ltrain: 0.001408 	Lval: 0.001496
Epoch: 745 	Ltrain: 0.001405 	Lval: 0.001490
Epoch: 750 	Ltrain: 0.001397 	Lval: 0.001485
Epoch: 755 	Ltrain: 0.001393 	Lval: 0.001479
Epoch: 760 	Ltrain: 0.001389 	Lval: 0.001472
Epoch: 765 	Ltrain: 0.001386 	Lval: 0.001468
Epoch: 770 	Ltrain: 0.001380 	Lval: 0.001462
Epoch: 775 	Ltrain: 0.001374 	Lval: 0.001457
Epoch: 780 	Ltrain: 0.001368 	Lval: 0.001452
Epoch: 785 	Ltrain: 0.001372 	Lval: 0.001447
Epoch: 790 	Ltrain: 0.001362 	Lval: 0.001442
Epoch: 795 	Ltrain: 0.001356 	Lval: 0.001436
Epoch: 800 	Ltrain: 0.001355 	Lval: 0.001432
Epoch: 805 	Ltrain: 0.001352 	Lval: 0.001430
Epoch: 810 	Ltrain: 0.001344 	Lval: 0.001423
Epoch: 815 	Ltrain: 0.001340 	Lval: 0.001417
Epoch: 820 	Ltrain: 0.001335 	Lval: 0.001411
Epoch: 825 	Ltrain: 0.001330 	Lval: 0.001406
Epoch: 830 	Ltrain: 0.001331 	Lval: 0.001402
Epoch: 835 	Ltrain: 0.001326 	Lval: 0.001396
Epoch: 840 	Ltrain: 0.001324 	Lval: 0.001393
Epoch: 845 	Ltrain: 0.001314 	Lval: 0.001386
Epoch: 850 	Ltrain: 0.001308 	Lval: 0.001382
Epoch: 855 	Ltrain: 0.001309 	Lval: 0.001376
Epoch: 860 	Ltrain: 0.001299 	Lval: 0.001371
Epoch: 865 	Ltrain: 0.001297 	Lval: 0.001367
Epoch: 870 	Ltrain: 0.001292 	Lval: 0.001361
Epoch: 875 	Ltrain: 0.001288 	Lval: 0.001357
Epoch: 880 	Ltrain: 0.001283 	Lval: 0.001354
Epoch: 885 	Ltrain: 0.001279 	Lval: 0.001345
Epoch: 890 	Ltrain: 0.001274 	Lval: 0.001342
Epoch: 895 	Ltrain: 0.001271 	Lval: 0.001336
Epoch: 900 	Ltrain: 0.001264 	Lval: 0.001334
Epoch: 905 	Ltrain: 0.001260 	Lval: 0.001328
Epoch: 910 	Ltrain: 0.001256 	Lval: 0.001324
Epoch: 915 	Ltrain: 0.001255 	Lval: 0.001319
Epoch: 920 	Ltrain: 0.001250 	Lval: 0.001315
Epoch: 925 	Ltrain: 0.001248 	Lval: 0.001310
Epoch: 930 	Ltrain: 0.001242 	Lval: 0.001305
Epoch: 935 	Ltrain: 0.001237 	Lval: 0.001301
Epoch: 940 	Ltrain: 0.001231 	Lval: 0.001295
Epoch: 945 	Ltrain: 0.001228 	Lval: 0.001290
Epoch: 950 	Ltrain: 0.001227 	Lval: 0.001287
Epoch: 955 	Ltrain: 0.001222 	Lval: 0.001282
Epoch: 960 	Ltrain: 0.001218 	Lval: 0.001279
Epoch: 965 	Ltrain: 0.001210 	Lval: 0.001273
Epoch: 970 	Ltrain: 0.001208 	Lval: 0.001269
Epoch: 975 	Ltrain: 0.001206 	Lval: 0.001266
Epoch: 980 	Ltrain: 0.001201 	Lval: 0.001258
Epoch: 985 	Ltrain: 0.001198 	Lval: 0.001256
Epoch: 990 	Ltrain: 0.001191 	Lval: 0.001252
Epoch: 995 	Ltrain: 0.001186 	Lval: 0.001246
Epoch: 1000 	Ltrain: 0.001181 	Lval: 0.001241
Epoch: 1005 	Ltrain: 0.001179 	Lval: 0.001237
Epoch: 1010 	Ltrain: 0.001177 	Lval: 0.001232
Epoch: 1015 	Ltrain: 0.001175 	Lval: 0.001228
Epoch: 1020 	Ltrain: 0.001169 	Lval: 0.001226
Epoch: 1025 	Ltrain: 0.001165 	Lval: 0.001220
Epoch: 1030 	Ltrain: 0.001160 	Lval: 0.001215
Epoch: 1035 	Ltrain: 0.001157 	Lval: 0.001211
Epoch: 1040 	Ltrain: 0.001155 	Lval: 0.001209
Epoch: 1045 	Ltrain: 0.001149 	Lval: 0.001203
Epoch: 1050 	Ltrain: 0.001146 	Lval: 0.001199
Epoch: 1055 	Ltrain: 0.001140 	Lval: 0.001195
Epoch: 1060 	Ltrain: 0.001135 	Lval: 0.001192
Epoch: 1065 	Ltrain: 0.001133 	Lval: 0.001187
Epoch: 1070 	Ltrain: 0.001131 	Lval: 0.001182
Epoch: 1075 	Ltrain: 0.001126 	Lval: 0.001179
Epoch: 1080 	Ltrain: 0.001119 	Lval: 0.001175
Epoch: 1085 	Ltrain: 0.001117 	Lval: 0.001171
Epoch: 1090 	Ltrain: 0.001113 	Lval: 0.001168
Epoch: 1095 	Ltrain: 0.001112 	Lval: 0.001162
Epoch: 1100 	Ltrain: 0.001112 	Lval: 0.001157
Epoch: 1105 	Ltrain: 0.001101 	Lval: 0.001153
Epoch: 1110 	Ltrain: 0.001100 	Lval: 0.001151
Epoch: 1115 	Ltrain: 0.001094 	Lval: 0.001145
Epoch: 1120 	Ltrain: 0.001093 	Lval: 0.001142
Epoch: 1125 	Ltrain: 0.001087 	Lval: 0.001138
Epoch: 1130 	Ltrain: 0.001087 	Lval: 0.001135
Epoch: 1135 	Ltrain: 0.001081 	Lval: 0.001130
Epoch: 1140 	Ltrain: 0.001076 	Lval: 0.001126
Epoch: 1145 	Ltrain: 0.001072 	Lval: 0.001122
Epoch: 1150 	Ltrain: 0.001073 	Lval: 0.001119
Epoch: 1155 	Ltrain: 0.001067 	Lval: 0.001115
Epoch: 1160 	Ltrain: 0.001066 	Lval: 0.001111
Epoch: 1165 	Ltrain: 0.001061 	Lval: 0.001107
Epoch: 1170 	Ltrain: 0.001056 	Lval: 0.001102
Epoch: 1175 	Ltrain: 0.001052 	Lval: 0.001099
Epoch: 1180 	Ltrain: 0.001051 	Lval: 0.001096
Epoch: 1185 	Ltrain: 0.001044 	Lval: 0.001091
Epoch: 1190 	Ltrain: 0.001041 	Lval: 0.001088
Epoch: 1195 	Ltrain: 0.001038 	Lval: 0.001083
Epoch: 1200 	Ltrain: 0.001035 	Lval: 0.001081
Epoch: 1205 	Ltrain: 0.001034 	Lval: 0.001078
Epoch: 1210 	Ltrain: 0.001029 	Lval: 0.001072
Epoch: 1215 	Ltrain: 0.001025 	Lval: 0.001068
Epoch: 1220 	Ltrain: 0.001021 	Lval: 0.001065
Epoch: 1225 	Ltrain: 0.001017 	Lval: 0.001061
Epoch: 1230 	Ltrain: 0.001013 	Lval: 0.001057
Epoch: 1235 	Ltrain: 0.001012 	Lval: 0.001054
Epoch: 1240 	Ltrain: 0.001007 	Lval: 0.001050
Epoch: 1245 	Ltrain: 0.001005 	Lval: 0.001049
Epoch: 1250 	Ltrain: 0.001001 	Lval: 0.001042
Epoch: 1255 	Ltrain: 0.000995 	Lval: 0.001038
Epoch: 1260 	Ltrain: 0.000993 	Lval: 0.001033
Epoch: 1265 	Ltrain: 0.000989 	Lval: 0.001032
Epoch: 1270 	Ltrain: 0.000987 	Lval: 0.001028
Epoch: 1275 	Ltrain: 0.000983 	Lval: 0.001027
Epoch: 1280 	Ltrain: 0.000980 	Lval: 0.001021
Epoch: 1285 	Ltrain: 0.000978 	Lval: 0.001016
Epoch: 1290 	Ltrain: 0.000975 	Lval: 0.001014
Epoch: 1295 	Ltrain: 0.000971 	Lval: 0.001009
Epoch: 1300 	Ltrain: 0.000969 	Lval: 0.001007
Epoch: 1305 	Ltrain: 0.000964 	Lval: 0.001004
Epoch: 1310 	Ltrain: 0.000960 	Lval: 0.001001
Epoch: 1315 	Ltrain: 0.000957 	Lval: 0.000996
Epoch: 1320 	Ltrain: 0.000955 	Lval: 0.000994
Epoch: 1325 	Ltrain: 0.000950 	Lval: 0.000989
Epoch: 1330 	Ltrain: 0.000950 	Lval: 0.000987
Epoch: 1335 	Ltrain: 0.000944 	Lval: 0.000982
Epoch: 1340 	Ltrain: 0.000941 	Lval: 0.000980
Epoch: 1345 	Ltrain: 0.000939 	Lval: 0.000978
Epoch: 1350 	Ltrain: 0.000935 	Lval: 0.000972
Epoch: 1355 	Ltrain: 0.000933 	Lval: 0.000970
Epoch: 1360 	Ltrain: 0.000928 	Lval: 0.000965
Epoch: 1365 	Ltrain: 0.000927 	Lval: 0.000962
Epoch: 1370 	Ltrain: 0.000921 	Lval: 0.000958
Epoch: 1375 	Ltrain: 0.000919 	Lval: 0.000955
Epoch: 1380 	Ltrain: 0.000918 	Lval: 0.000952
Epoch: 1385 	Ltrain: 0.000914 	Lval: 0.000948
Epoch: 1390 	Ltrain: 0.000910 	Lval: 0.000945
Epoch: 1395 	Ltrain: 0.000906 	Lval: 0.000942
Epoch: 1400 	Ltrain: 0.000904 	Lval: 0.000939
Epoch: 1405 	Ltrain: 0.000903 	Lval: 0.000944
Epoch 01406: reducing learning rate of group 0 to 6.8594e-07.
Epoch: 1410 	Ltrain: 0.000895 	Lval: 0.000933
Epoch: 1415 	Ltrain: 0.000895 	Lval: 0.000932
Epoch: 1420 	Ltrain: 0.000897 	Lval: 0.000931
Epoch: 1425 	Ltrain: 0.000895 	Lval: 0.000931
Epoch 01427: reducing learning rate of group 0 to 6.8594e-08.
EarlyStopper: stopping at epoch 1428 with best_val_loss = 0.000936

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004859929398928176
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.9236911054952745e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 25
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.042275 	Lval: 0.021130
Epoch: 5 	Ltrain: 0.013144 	Lval: 0.012043
Epoch: 10 	Ltrain: 0.008623 	Lval: 0.009779
Epoch: 15 	Ltrain: 0.007925 	Lval: 0.008712
Epoch: 20 	Ltrain: 0.006955 	Lval: 0.007227
Epoch 00023: reducing learning rate of group 0 to 4.8599e-04.
Epoch: 25 	Ltrain: 0.006340 	Lval: 0.006064
Epoch: 30 	Ltrain: 0.005940 	Lval: 0.005825
Epoch: 35 	Ltrain: 0.006179 	Lval: 0.005687
Epoch: 40 	Ltrain: 0.005784 	Lval: 0.005539
Epoch: 45 	Ltrain: 0.005246 	Lval: 0.005406
Epoch: 50 	Ltrain: 0.005550 	Lval: 0.005306
Epoch: 55 	Ltrain: 0.005764 	Lval: 0.005276
Epoch: 60 	Ltrain: 0.005090 	Lval: 0.005129
Epoch: 65 	Ltrain: 0.005149 	Lval: 0.004936
Epoch: 70 	Ltrain: 0.004737 	Lval: 0.004781
Epoch: 75 	Ltrain: 0.004939 	Lval: 0.004811
Epoch: 80 	Ltrain: 0.004641 	Lval: 0.004557
Epoch: 85 	Ltrain: 0.004525 	Lval: 0.004483
Epoch 00087: reducing learning rate of group 0 to 4.8599e-05.
Epoch: 90 	Ltrain: 0.004168 	Lval: 0.004258
Epoch: 95 	Ltrain: 0.004326 	Lval: 0.004213
Epoch: 100 	Ltrain: 0.004013 	Lval: 0.004178
Epoch: 105 	Ltrain: 0.004297 	Lval: 0.004180
Epoch 00106: reducing learning rate of group 0 to 4.8599e-06.
Epoch: 110 	Ltrain: 0.004113 	Lval: 0.004167
Epoch: 115 	Ltrain: 0.004005 	Lval: 0.004160
Epoch: 120 	Ltrain: 0.004056 	Lval: 0.004153
Epoch: 125 	Ltrain: 0.004230 	Lval: 0.004148
Epoch 00130: reducing learning rate of group 0 to 4.8599e-07.
Epoch: 130 	Ltrain: 0.004432 	Lval: 0.004148
Epoch: 135 	Ltrain: 0.004068 	Lval: 0.004147
Epoch: 140 	Ltrain: 0.004164 	Lval: 0.004146
Epoch: 145 	Ltrain: 0.004076 	Lval: 0.004146
Epoch: 150 	Ltrain: 0.004128 	Lval: 0.004145
Epoch 00152: reducing learning rate of group 0 to 4.8599e-08.
Epoch: 155 	Ltrain: 0.004418 	Lval: 0.004145
Epoch: 160 	Ltrain: 0.004341 	Lval: 0.004145
EarlyStopper: stopping at epoch 160 with best_val_loss = 0.004147


	Fold 2/5
Epoch: 1 	Ltrain: 0.035719 	Lval: 0.020336
Epoch: 5 	Ltrain: 0.007965 	Lval: 0.007920
Epoch: 10 	Ltrain: 0.007168 	Lval: 0.007226
Epoch 00012: reducing learning rate of group 0 to 4.8599e-04.
Epoch: 15 	Ltrain: 0.006045 	Lval: 0.006352
Epoch: 20 	Ltrain: 0.006014 	Lval: 0.006270
Epoch: 25 	Ltrain: 0.005914 	Lval: 0.006191
Epoch: 30 	Ltrain: 0.005753 	Lval: 0.006137
Epoch: 35 	Ltrain: 0.005737 	Lval: 0.005986
Epoch: 40 	Ltrain: 0.005594 	Lval: 0.005873
Epoch 00044: reducing learning rate of group 0 to 4.8599e-05.
Epoch: 45 	Ltrain: 0.005512 	Lval: 0.005771
Epoch: 50 	Ltrain: 0.005422 	Lval: 0.005744
Epoch: 55 	Ltrain: 0.005394 	Lval: 0.005717
Epoch: 60 	Ltrain: 0.005398 	Lval: 0.005705
Epoch: 65 	Ltrain: 0.005419 	Lval: 0.005670
Epoch: 70 	Ltrain: 0.005453 	Lval: 0.005646
Epoch: 75 	Ltrain: 0.005316 	Lval: 0.005644
Epoch: 80 	Ltrain: 0.005360 	Lval: 0.005607
Epoch: 85 	Ltrain: 0.005267 	Lval: 0.005589
Epoch: 90 	Ltrain: 0.005356 	Lval: 0.005571
Epoch: 95 	Ltrain: 0.005350 	Lval: 0.005535
Epoch: 100 	Ltrain: 0.005411 	Lval: 0.005539
Epoch: 105 	Ltrain: 0.005428 	Lval: 0.005495
Epoch: 110 	Ltrain: 0.005150 	Lval: 0.005477
Epoch: 115 	Ltrain: 0.005210 	Lval: 0.005444
Epoch: 120 	Ltrain: 0.005085 	Lval: 0.005421
Epoch: 125 	Ltrain: 0.005165 	Lval: 0.005403
Epoch: 130 	Ltrain: 0.005087 	Lval: 0.005372
Epoch: 135 	Ltrain: 0.005069 	Lval: 0.005336
Epoch: 140 	Ltrain: 0.005160 	Lval: 0.005305
Epoch: 145 	Ltrain: 0.005130 	Lval: 0.005263
Epoch: 150 	Ltrain: 0.004975 	Lval: 0.005217
Epoch: 155 	Ltrain: 0.004982 	Lval: 0.005187
Epoch: 160 	Ltrain: 0.004857 	Lval: 0.005138
Epoch: 165 	Ltrain: 0.004835 	Lval: 0.005110
Epoch: 170 	Ltrain: 0.004813 	Lval: 0.005061
Epoch: 175 	Ltrain: 0.004757 	Lval: 0.005044
Epoch: 180 	Ltrain: 0.004758 	Lval: 0.004974
Epoch: 185 	Ltrain: 0.004845 	Lval: 0.004947
Epoch: 190 	Ltrain: 0.004773 	Lval: 0.004912
Epoch: 195 	Ltrain: 0.004650 	Lval: 0.004864
Epoch: 200 	Ltrain: 0.004647 	Lval: 0.004821
Epoch: 205 	Ltrain: 0.004588 	Lval: 0.004815
Epoch: 210 	Ltrain: 0.004610 	Lval: 0.004795
Epoch: 215 	Ltrain: 0.004499 	Lval: 0.004702
Epoch: 220 	Ltrain: 0.004585 	Lval: 0.004654
Epoch: 225 	Ltrain: 0.004461 	Lval: 0.004636
Epoch: 230 	Ltrain: 0.004395 	Lval: 0.004585
Epoch: 235 	Ltrain: 0.004391 	Lval: 0.004530
Epoch: 240 	Ltrain: 0.004339 	Lval: 0.004504
Epoch: 245 	Ltrain: 0.004314 	Lval: 0.004481
Epoch: 250 	Ltrain: 0.004298 	Lval: 0.004401
Epoch: 255 	Ltrain: 0.004265 	Lval: 0.004365
Epoch: 260 	Ltrain: 0.004257 	Lval: 0.004344
Epoch: 265 	Ltrain: 0.004163 	Lval: 0.004308
Epoch: 270 	Ltrain: 0.004177 	Lval: 0.004254
Epoch: 275 	Ltrain: 0.004084 	Lval: 0.004202
Epoch: 280 	Ltrain: 0.004120 	Lval: 0.004169
Epoch: 285 	Ltrain: 0.004040 	Lval: 0.004170
Epoch: 290 	Ltrain: 0.003939 	Lval: 0.004099
Epoch: 295 	Ltrain: 0.003993 	Lval: 0.004054
Epoch: 300 	Ltrain: 0.003884 	Lval: 0.003982
Epoch: 305 	Ltrain: 0.003825 	Lval: 0.003964
Epoch: 310 	Ltrain: 0.003864 	Lval: 0.003905
Epoch: 315 	Ltrain: 0.003793 	Lval: 0.003837
Epoch: 320 	Ltrain: 0.003833 	Lval: 0.003818
Epoch: 325 	Ltrain: 0.003704 	Lval: 0.003776
Epoch: 330 	Ltrain: 0.003660 	Lval: 0.003715
Epoch: 335 	Ltrain: 0.003669 	Lval: 0.003657
Epoch: 340 	Ltrain: 0.003645 	Lval: 0.003627
Epoch: 345 	Ltrain: 0.003658 	Lval: 0.003591
Epoch: 350 	Ltrain: 0.003549 	Lval: 0.003545
Epoch: 355 	Ltrain: 0.003474 	Lval: 0.003507
Epoch: 360 	Ltrain: 0.003409 	Lval: 0.003454
Epoch: 365 	Ltrain: 0.003423 	Lval: 0.003416
Epoch: 370 	Ltrain: 0.003321 	Lval: 0.003375
Epoch: 375 	Ltrain: 0.003360 	Lval: 0.003347
Epoch: 380 	Ltrain: 0.003285 	Lval: 0.003293
Epoch: 385 	Ltrain: 0.003267 	Lval: 0.003251
Epoch: 390 	Ltrain: 0.003185 	Lval: 0.003223
Epoch: 395 	Ltrain: 0.003183 	Lval: 0.003172
Epoch: 400 	Ltrain: 0.003148 	Lval: 0.003125
Epoch: 405 	Ltrain: 0.003087 	Lval: 0.003077
Epoch: 410 	Ltrain: 0.003015 	Lval: 0.003048
Epoch: 415 	Ltrain: 0.002995 	Lval: 0.002994
Epoch: 420 	Ltrain: 0.003018 	Lval: 0.003001
Epoch 00422: reducing learning rate of group 0 to 4.8599e-06.
Epoch: 425 	Ltrain: 0.002855 	Lval: 0.002929
Epoch: 430 	Ltrain: 0.002864 	Lval: 0.002923
Epoch: 435 	Ltrain: 0.002869 	Lval: 0.002920
Epoch: 440 	Ltrain: 0.002887 	Lval: 0.002918
Epoch: 445 	Ltrain: 0.002871 	Lval: 0.002913
Epoch: 450 	Ltrain: 0.002849 	Lval: 0.002908
Epoch: 455 	Ltrain: 0.002874 	Lval: 0.002903
Epoch: 460 	Ltrain: 0.002882 	Lval: 0.002898
Epoch: 465 	Ltrain: 0.002840 	Lval: 0.002894
Epoch: 470 	Ltrain: 0.002920 	Lval: 0.002888
Epoch 00474: reducing learning rate of group 0 to 4.8599e-07.
Epoch: 475 	Ltrain: 0.002929 	Lval: 0.002888
Epoch: 480 	Ltrain: 0.002867 	Lval: 0.002886
Epoch: 485 	Ltrain: 0.002840 	Lval: 0.002886
Epoch 00486: reducing learning rate of group 0 to 4.8599e-08.
Epoch: 490 	Ltrain: 0.002825 	Lval: 0.002886
Epoch: 495 	Ltrain: 0.002833 	Lval: 0.002886
EarlyStopper: stopping at epoch 494 with best_val_loss = 0.002888


	Fold 3/5
Epoch: 1 	Ltrain: 0.027776 	Lval: 0.015449
Epoch: 5 	Ltrain: 0.006720 	Lval: 0.007251
Epoch: 10 	Ltrain: 0.006581 	Lval: 0.007542
Epoch: 15 	Ltrain: 0.006407 	Lval: 0.007019
Epoch: 20 	Ltrain: 0.005761 	Lval: 0.006113
Epoch: 25 	Ltrain: 0.005316 	Lval: 0.005257
Epoch: 30 	Ltrain: 0.004875 	Lval: 0.005048
Epoch: 35 	Ltrain: 0.004296 	Lval: 0.004713
Epoch: 40 	Ltrain: 0.003372 	Lval: 0.003473
Epoch: 45 	Ltrain: 0.002961 	Lval: 0.002586
Epoch 00050: reducing learning rate of group 0 to 4.8599e-04.
Epoch: 50 	Ltrain: 0.002625 	Lval: 0.002557
Epoch: 55 	Ltrain: 0.001411 	Lval: 0.001386
Epoch: 60 	Ltrain: 0.001217 	Lval: 0.001196
Epoch: 65 	Ltrain: 0.001081 	Lval: 0.001040
Epoch: 70 	Ltrain: 0.000952 	Lval: 0.000915
Epoch: 75 	Ltrain: 0.000843 	Lval: 0.000800
Epoch: 80 	Ltrain: 0.000745 	Lval: 0.000694
Epoch: 85 	Ltrain: 0.000669 	Lval: 0.000602
Epoch: 90 	Ltrain: 0.000571 	Lval: 0.000531
Epoch: 95 	Ltrain: 0.000501 	Lval: 0.000457
Epoch: 100 	Ltrain: 0.000457 	Lval: 0.000420
Epoch: 105 	Ltrain: 0.000415 	Lval: 0.000377
Epoch: 110 	Ltrain: 0.000361 	Lval: 0.000324
Epoch: 115 	Ltrain: 0.000337 	Lval: 0.000315
Epoch: 120 	Ltrain: 0.000300 	Lval: 0.000292
Epoch 00121: reducing learning rate of group 0 to 4.8599e-05.
Epoch: 125 	Ltrain: 0.000236 	Lval: 0.000225
Epoch: 130 	Ltrain: 0.000226 	Lval: 0.000215
Epoch: 135 	Ltrain: 0.000221 	Lval: 0.000210
Epoch: 140 	Ltrain: 0.000218 	Lval: 0.000206
Epoch: 145 	Ltrain: 0.000213 	Lval: 0.000202
Epoch: 150 	Ltrain: 0.000209 	Lval: 0.000199
Epoch: 155 	Ltrain: 0.000206 	Lval: 0.000195
Epoch: 160 	Ltrain: 0.000201 	Lval: 0.000191
Epoch: 165 	Ltrain: 0.000200 	Lval: 0.000187
Epoch: 170 	Ltrain: 0.000193 	Lval: 0.000183
Epoch: 175 	Ltrain: 0.000190 	Lval: 0.000179
Epoch: 180 	Ltrain: 0.000187 	Lval: 0.000175
Epoch: 185 	Ltrain: 0.000183 	Lval: 0.000171
Epoch: 190 	Ltrain: 0.000178 	Lval: 0.000166
Epoch: 195 	Ltrain: 0.000174 	Lval: 0.000162
Epoch: 200 	Ltrain: 0.000169 	Lval: 0.000158
Epoch: 205 	Ltrain: 0.000166 	Lval: 0.000153
Epoch: 210 	Ltrain: 0.000160 	Lval: 0.000149
Epoch: 215 	Ltrain: 0.000156 	Lval: 0.000144
Epoch: 220 	Ltrain: 0.000151 	Lval: 0.000140
Epoch: 225 	Ltrain: 0.000147 	Lval: 0.000136
Epoch: 230 	Ltrain: 0.000142 	Lval: 0.000131
Epoch: 235 	Ltrain: 0.000139 	Lval: 0.000127
Epoch: 240 	Ltrain: 0.000134 	Lval: 0.000123
Epoch: 245 	Ltrain: 0.000130 	Lval: 0.000118
Epoch: 250 	Ltrain: 0.000127 	Lval: 0.000114
Epoch: 255 	Ltrain: 0.000121 	Lval: 0.000110
Epoch: 260 	Ltrain: 0.000119 	Lval: 0.000106
Epoch: 265 	Ltrain: 0.000115 	Lval: 0.000102
Epoch: 270 	Ltrain: 0.000110 	Lval: 0.000098
Epoch: 275 	Ltrain: 0.000107 	Lval: 0.000094
Epoch: 280 	Ltrain: 0.000104 	Lval: 0.000090
Epoch: 285 	Ltrain: 0.000101 	Lval: 0.000087
Epoch: 290 	Ltrain: 0.000096 	Lval: 0.000083
Epoch: 295 	Ltrain: 0.000093 	Lval: 0.000080
Epoch: 300 	Ltrain: 0.000091 	Lval: 0.000078
Epoch: 305 	Ltrain: 0.000090 	Lval: 0.000078
Epoch: 310 	Ltrain: 0.000085 	Lval: 0.000073
Epoch: 315 	Ltrain: 0.000082 	Lval: 0.000069
Epoch: 320 	Ltrain: 0.000081 	Lval: 0.000067
Epoch: 325 	Ltrain: 0.000077 	Lval: 0.000066
Epoch: 330 	Ltrain: 0.000075 	Lval: 0.000063
Epoch: 335 	Ltrain: 0.000072 	Lval: 0.000060
Epoch: 340 	Ltrain: 0.000070 	Lval: 0.000058
Epoch: 345 	Ltrain: 0.000068 	Lval: 0.000056
Epoch: 350 	Ltrain: 0.000066 	Lval: 0.000055
Epoch: 355 	Ltrain: 0.000065 	Lval: 0.000053
Epoch: 360 	Ltrain: 0.000064 	Lval: 0.000052
Epoch: 365 	Ltrain: 0.000061 	Lval: 0.000050
EarlyStopper: stopping at epoch 364 with best_val_loss = 0.000058


	Fold 4/5
Epoch: 1 	Ltrain: 0.028772 	Lval: 0.014227
Epoch: 5 	Ltrain: 0.006928 	Lval: 0.008124
Epoch: 10 	Ltrain: 0.005635 	Lval: 0.007729
Epoch 00011: reducing learning rate of group 0 to 4.8599e-04.
Epoch: 15 	Ltrain: 0.004877 	Lval: 0.006140
Epoch: 20 	Ltrain: 0.004814 	Lval: 0.005948
Epoch: 25 	Ltrain: 0.004755 	Lval: 0.005875
Epoch: 30 	Ltrain: 0.004682 	Lval: 0.005732
Epoch: 35 	Ltrain: 0.004563 	Lval: 0.005826
Epoch: 40 	Ltrain: 0.004522 	Lval: 0.005544
Epoch: 45 	Ltrain: 0.004367 	Lval: 0.005466
Epoch: 50 	Ltrain: 0.004397 	Lval: 0.005292
Epoch 00051: reducing learning rate of group 0 to 4.8599e-05.
Epoch: 55 	Ltrain: 0.004038 	Lval: 0.005000
Epoch: 60 	Ltrain: 0.003991 	Lval: 0.004973
Epoch: 65 	Ltrain: 0.003976 	Lval: 0.004867
Epoch: 70 	Ltrain: 0.003934 	Lval: 0.004875
Epoch: 75 	Ltrain: 0.003912 	Lval: 0.004829
Epoch: 80 	Ltrain: 0.003885 	Lval: 0.004781
Epoch: 85 	Ltrain: 0.003860 	Lval: 0.004769
Epoch: 90 	Ltrain: 0.003850 	Lval: 0.004687
Epoch: 95 	Ltrain: 0.003805 	Lval: 0.004686
Epoch: 100 	Ltrain: 0.003764 	Lval: 0.004586
Epoch: 105 	Ltrain: 0.003731 	Lval: 0.004566
Epoch: 110 	Ltrain: 0.003706 	Lval: 0.004497
Epoch 00111: reducing learning rate of group 0 to 4.8599e-06.
Epoch: 115 	Ltrain: 0.003670 	Lval: 0.004501
Epoch: 120 	Ltrain: 0.003654 	Lval: 0.004493
Epoch: 125 	Ltrain: 0.003649 	Lval: 0.004486
Epoch: 130 	Ltrain: 0.003638 	Lval: 0.004487
Epoch 00133: reducing learning rate of group 0 to 4.8599e-07.
Epoch: 135 	Ltrain: 0.003640 	Lval: 0.004479
Epoch: 140 	Ltrain: 0.003641 	Lval: 0.004480
Epoch 00145: reducing learning rate of group 0 to 4.8599e-08.
Epoch: 145 	Ltrain: 0.003650 	Lval: 0.004479
Epoch: 150 	Ltrain: 0.003647 	Lval: 0.004479
EarlyStopper: stopping at epoch 151 with best_val_loss = 0.004484


	Fold 5/5
Epoch: 1 	Ltrain: 0.025260 	Lval: 0.011975
Epoch: 5 	Ltrain: 0.006516 	Lval: 0.008656
Epoch: 10 	Ltrain: 0.005744 	Lval: 0.006813
Epoch 00011: reducing learning rate of group 0 to 4.8599e-04.
Epoch: 15 	Ltrain: 0.004793 	Lval: 0.006054
Epoch: 20 	Ltrain: 0.004693 	Lval: 0.005909
Epoch: 25 	Ltrain: 0.004692 	Lval: 0.005842
Epoch: 30 	Ltrain: 0.004588 	Lval: 0.005804
Epoch: 35 	Ltrain: 0.004451 	Lval: 0.005460
Epoch: 40 	Ltrain: 0.004349 	Lval: 0.005262
Epoch: 45 	Ltrain: 0.004135 	Lval: 0.005111
Epoch: 50 	Ltrain: 0.003996 	Lval: 0.005098
Epoch: 55 	Ltrain: 0.003830 	Lval: 0.004383
Epoch: 60 	Ltrain: 0.003599 	Lval: 0.004500
Epoch 00065: reducing learning rate of group 0 to 4.8599e-05.
Epoch: 65 	Ltrain: 0.003457 	Lval: 0.004067
Epoch: 70 	Ltrain: 0.003145 	Lval: 0.003766
Epoch: 75 	Ltrain: 0.003105 	Lval: 0.003708
Epoch: 80 	Ltrain: 0.003076 	Lval: 0.003648
Epoch: 85 	Ltrain: 0.003042 	Lval: 0.003650
Epoch: 90 	Ltrain: 0.003013 	Lval: 0.003642
Epoch: 95 	Ltrain: 0.002972 	Lval: 0.003548
Epoch: 100 	Ltrain: 0.002949 	Lval: 0.003473
Epoch: 105 	Ltrain: 0.002928 	Lval: 0.003442
Epoch: 110 	Ltrain: 0.002889 	Lval: 0.003401
Epoch: 115 	Ltrain: 0.002869 	Lval: 0.003345
Epoch: 120 	Ltrain: 0.002816 	Lval: 0.003298
Epoch: 125 	Ltrain: 0.002788 	Lval: 0.003263
Epoch: 130 	Ltrain: 0.002748 	Lval: 0.003222
Epoch: 135 	Ltrain: 0.002716 	Lval: 0.003183
Epoch: 140 	Ltrain: 0.002692 	Lval: 0.003119
Epoch: 145 	Ltrain: 0.002634 	Lval: 0.003058
Epoch: 150 	Ltrain: 0.002598 	Lval: 0.003018
Epoch: 155 	Ltrain: 0.002566 	Lval: 0.002966
Epoch: 160 	Ltrain: 0.002518 	Lval: 0.002906
Epoch: 165 	Ltrain: 0.002483 	Lval: 0.002865
Epoch: 170 	Ltrain: 0.002446 	Lval: 0.002788
Epoch: 175 	Ltrain: 0.002392 	Lval: 0.002774
Epoch: 180 	Ltrain: 0.002356 	Lval: 0.002690
Epoch: 185 	Ltrain: 0.002329 	Lval: 0.002640
Epoch: 190 	Ltrain: 0.002273 	Lval: 0.002605
Epoch: 195 	Ltrain: 0.002245 	Lval: 0.002564
Epoch: 200 	Ltrain: 0.002199 	Lval: 0.002490
Epoch: 205 	Ltrain: 0.002162 	Lval: 0.002439
Epoch: 210 	Ltrain: 0.002122 	Lval: 0.002393
Epoch: 215 	Ltrain: 0.002072 	Lval: 0.002331
Epoch: 220 	Ltrain: 0.002039 	Lval: 0.002301
Epoch: 225 	Ltrain: 0.001998 	Lval: 0.002220
Epoch: 230 	Ltrain: 0.001976 	Lval: 0.002194
Epoch: 235 	Ltrain: 0.001929 	Lval: 0.002122
Epoch: 240 	Ltrain: 0.001886 	Lval: 0.002108
Epoch: 245 	Ltrain: 0.001853 	Lval: 0.002047
Epoch: 250 	Ltrain: 0.001814 	Lval: 0.002000
Epoch: 255 	Ltrain: 0.001789 	Lval: 0.001984
Epoch: 260 	Ltrain: 0.001753 	Lval: 0.001927
Epoch: 265 	Ltrain: 0.001717 	Lval: 0.001915
Epoch: 270 	Ltrain: 0.001682 	Lval: 0.001864
Epoch: 275 	Ltrain: 0.001653 	Lval: 0.001839
Epoch: 280 	Ltrain: 0.001618 	Lval: 0.001773
Epoch: 285 	Ltrain: 0.001590 	Lval: 0.001757
Epoch: 290 	Ltrain: 0.001564 	Lval: 0.001723
Epoch: 295 	Ltrain: 0.001533 	Lval: 0.001676
Epoch: 300 	Ltrain: 0.001504 	Lval: 0.001647
Epoch: 305 	Ltrain: 0.001487 	Lval: 0.001623
Epoch: 310 	Ltrain: 0.001450 	Lval: 0.001593
Epoch: 315 	Ltrain: 0.001426 	Lval: 0.001542
Epoch: 320 	Ltrain: 0.001404 	Lval: 0.001532
Epoch: 325 	Ltrain: 0.001386 	Lval: 0.001495
Epoch: 330 	Ltrain: 0.001355 	Lval: 0.001478
Epoch: 335 	Ltrain: 0.001330 	Lval: 0.001441
Epoch: 340 	Ltrain: 0.001317 	Lval: 0.001438
Epoch: 345 	Ltrain: 0.001290 	Lval: 0.001396
Epoch: 350 	Ltrain: 0.001267 	Lval: 0.001350
Epoch: 355 	Ltrain: 0.001253 	Lval: 0.001343
Epoch: 360 	Ltrain: 0.001231 	Lval: 0.001307
Epoch: 365 	Ltrain: 0.001206 	Lval: 0.001281
Epoch: 370 	Ltrain: 0.001187 	Lval: 0.001262
Epoch: 375 	Ltrain: 0.001165 	Lval: 0.001247
Epoch: 380 	Ltrain: 0.001146 	Lval: 0.001220
Epoch: 385 	Ltrain: 0.001126 	Lval: 0.001196
Epoch: 390 	Ltrain: 0.001120 	Lval: 0.001182
Epoch: 395 	Ltrain: 0.001088 	Lval: 0.001154
Epoch: 400 	Ltrain: 0.001075 	Lval: 0.001172
Epoch: 405 	Ltrain: 0.001053 	Lval: 0.001109
Epoch: 410 	Ltrain: 0.001040 	Lval: 0.001103
Epoch: 415 	Ltrain: 0.001022 	Lval: 0.001066
Epoch: 420 	Ltrain: 0.001002 	Lval: 0.001066
Epoch: 425 	Ltrain: 0.001005 	Lval: 0.001048
Epoch: 430 	Ltrain: 0.000977 	Lval: 0.001039
Epoch: 435 	Ltrain: 0.000955 	Lval: 0.001000
Epoch: 440 	Ltrain: 0.000941 	Lval: 0.000983
Epoch 00444: reducing learning rate of group 0 to 4.8599e-06.
Epoch: 445 	Ltrain: 0.000914 	Lval: 0.000958
Epoch: 450 	Ltrain: 0.000896 	Lval: 0.000948
Epoch: 455 	Ltrain: 0.000894 	Lval: 0.000945
Epoch: 460 	Ltrain: 0.000894 	Lval: 0.000944
Epoch: 465 	Ltrain: 0.000890 	Lval: 0.000941
Epoch: 470 	Ltrain: 0.000889 	Lval: 0.000939
Epoch: 475 	Ltrain: 0.000886 	Lval: 0.000937
Epoch: 480 	Ltrain: 0.000884 	Lval: 0.000935
Epoch: 485 	Ltrain: 0.000882 	Lval: 0.000933
Epoch: 490 	Ltrain: 0.000880 	Lval: 0.000930
Epoch: 495 	Ltrain: 0.000880 	Lval: 0.000928
Epoch: 500 	Ltrain: 0.000877 	Lval: 0.000926
Epoch: 505 	Ltrain: 0.000875 	Lval: 0.000924
Epoch: 510 	Ltrain: 0.000874 	Lval: 0.000921
Epoch: 515 	Ltrain: 0.000874 	Lval: 0.000919
Epoch: 520 	Ltrain: 0.000868 	Lval: 0.000916
Epoch: 525 	Ltrain: 0.000866 	Lval: 0.000914
Epoch: 530 	Ltrain: 0.000864 	Lval: 0.000911
Epoch: 535 	Ltrain: 0.000864 	Lval: 0.000909
Epoch: 540 	Ltrain: 0.000864 	Lval: 0.000907
Epoch: 545 	Ltrain: 0.000856 	Lval: 0.000904
Epoch: 550 	Ltrain: 0.000858 	Lval: 0.000903
Epoch: 555 	Ltrain: 0.000853 	Lval: 0.000900
Epoch: 560 	Ltrain: 0.000853 	Lval: 0.000897
Epoch: 565 	Ltrain: 0.000849 	Lval: 0.000894
Epoch: 570 	Ltrain: 0.000847 	Lval: 0.000892
Epoch: 575 	Ltrain: 0.000844 	Lval: 0.000890
Epoch: 580 	Ltrain: 0.000843 	Lval: 0.000888
Epoch: 585 	Ltrain: 0.000841 	Lval: 0.000885
Epoch: 590 	Ltrain: 0.000838 	Lval: 0.000882
Epoch: 595 	Ltrain: 0.000834 	Lval: 0.000880
Epoch: 600 	Ltrain: 0.000832 	Lval: 0.000878
Epoch: 605 	Ltrain: 0.000832 	Lval: 0.000876
Epoch: 610 	Ltrain: 0.000828 	Lval: 0.000873
Epoch: 615 	Ltrain: 0.000831 	Lval: 0.000870
Epoch: 620 	Ltrain: 0.000823 	Lval: 0.000869
Epoch: 625 	Ltrain: 0.000823 	Lval: 0.000866
Epoch: 630 	Ltrain: 0.000820 	Lval: 0.000864
Epoch: 635 	Ltrain: 0.000817 	Lval: 0.000861
Epoch: 640 	Ltrain: 0.000817 	Lval: 0.000859
Epoch: 645 	Ltrain: 0.000812 	Lval: 0.000857
Epoch: 650 	Ltrain: 0.000813 	Lval: 0.000854
Epoch: 655 	Ltrain: 0.000809 	Lval: 0.000854
Epoch: 660 	Ltrain: 0.000807 	Lval: 0.000849
Epoch: 665 	Ltrain: 0.000806 	Lval: 0.000848
Epoch: 670 	Ltrain: 0.000805 	Lval: 0.000846
Epoch: 675 	Ltrain: 0.000801 	Lval: 0.000843
Epoch: 680 	Ltrain: 0.000803 	Lval: 0.000840
Epoch: 685 	Ltrain: 0.000797 	Lval: 0.000838
Epoch: 690 	Ltrain: 0.000794 	Lval: 0.000835
Epoch: 695 	Ltrain: 0.000796 	Lval: 0.000834
Epoch: 700 	Ltrain: 0.000792 	Lval: 0.000831
Epoch: 705 	Ltrain: 0.000788 	Lval: 0.000829
Epoch: 710 	Ltrain: 0.000787 	Lval: 0.000827
Epoch: 715 	Ltrain: 0.000787 	Lval: 0.000825
Epoch: 720 	Ltrain: 0.000784 	Lval: 0.000822
Epoch: 725 	Ltrain: 0.000782 	Lval: 0.000820
Epoch: 730 	Ltrain: 0.000780 	Lval: 0.000817
Epoch: 735 	Ltrain: 0.000777 	Lval: 0.000816
Epoch: 740 	Ltrain: 0.000774 	Lval: 0.000814
Epoch: 745 	Ltrain: 0.000771 	Lval: 0.000811
Epoch: 750 	Ltrain: 0.000771 	Lval: 0.000808
Epoch: 755 	Ltrain: 0.000768 	Lval: 0.000806
Epoch: 760 	Ltrain: 0.000765 	Lval: 0.000805
Epoch: 765 	Ltrain: 0.000765 	Lval: 0.000803
Epoch: 770 	Ltrain: 0.000762 	Lval: 0.000799
Epoch: 775 	Ltrain: 0.000759 	Lval: 0.000798
Epoch: 780 	Ltrain: 0.000757 	Lval: 0.000796
Epoch: 785 	Ltrain: 0.000757 	Lval: 0.000793
Epoch: 790 	Ltrain: 0.000752 	Lval: 0.000791
Epoch: 795 	Ltrain: 0.000752 	Lval: 0.000789
Epoch: 800 	Ltrain: 0.000753 	Lval: 0.000786
Epoch: 805 	Ltrain: 0.000748 	Lval: 0.000784
Epoch: 810 	Ltrain: 0.000745 	Lval: 0.000782
Epoch: 815 	Ltrain: 0.000742 	Lval: 0.000780
Epoch: 820 	Ltrain: 0.000741 	Lval: 0.000778
Epoch: 825 	Ltrain: 0.000739 	Lval: 0.000776
Epoch: 830 	Ltrain: 0.000740 	Lval: 0.000773
Epoch: 835 	Ltrain: 0.000735 	Lval: 0.000771
Epoch: 840 	Ltrain: 0.000734 	Lval: 0.000770
Epoch: 845 	Ltrain: 0.000731 	Lval: 0.000767
Epoch: 850 	Ltrain: 0.000731 	Lval: 0.000766
Epoch: 855 	Ltrain: 0.000728 	Lval: 0.000763
Epoch: 860 	Ltrain: 0.000729 	Lval: 0.000761
Epoch: 865 	Ltrain: 0.000723 	Lval: 0.000759
Epoch: 870 	Ltrain: 0.000722 	Lval: 0.000757
Epoch: 875 	Ltrain: 0.000719 	Lval: 0.000754
Epoch: 880 	Ltrain: 0.000717 	Lval: 0.000752
Epoch: 885 	Ltrain: 0.000716 	Lval: 0.000751
Epoch: 890 	Ltrain: 0.000713 	Lval: 0.000748
Epoch: 895 	Ltrain: 0.000712 	Lval: 0.000746
Epoch: 900 	Ltrain: 0.000713 	Lval: 0.000744
Epoch: 905 	Ltrain: 0.000708 	Lval: 0.000742
Epoch: 910 	Ltrain: 0.000707 	Lval: 0.000741
Epoch: 915 	Ltrain: 0.000705 	Lval: 0.000738
Epoch: 920 	Ltrain: 0.000702 	Lval: 0.000736
Epoch: 925 	Ltrain: 0.000702 	Lval: 0.000734
Epoch: 930 	Ltrain: 0.000699 	Lval: 0.000731
EarlyStopper: stopping at epoch 930 with best_val_loss = 0.000741

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00011789199787394103
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.1567893144012026e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.041703 	Lval: 0.029327
Epoch: 5 	Ltrain: 0.017921 	Lval: 0.017384
Epoch: 10 	Ltrain: 0.017172 	Lval: 0.016878
Epoch: 15 	Ltrain: 0.016386 	Lval: 0.016100
Epoch: 20 	Ltrain: 0.014120 	Lval: 0.014343
Epoch: 25 	Ltrain: 0.013771 	Lval: 0.013579
Epoch: 30 	Ltrain: 0.013880 	Lval: 0.012824
Epoch: 35 	Ltrain: 0.012510 	Lval: 0.012226
Epoch: 40 	Ltrain: 0.012209 	Lval: 0.011751
Epoch: 45 	Ltrain: 0.011898 	Lval: 0.011284
Epoch: 50 	Ltrain: 0.010933 	Lval: 0.011001
Epoch: 55 	Ltrain: 0.010512 	Lval: 0.010376
Epoch: 60 	Ltrain: 0.010532 	Lval: 0.010037
Epoch: 65 	Ltrain: 0.011243 	Lval: 0.009619
Epoch: 70 	Ltrain: 0.009372 	Lval: 0.009292
Epoch: 75 	Ltrain: 0.009087 	Lval: 0.009256
Epoch: 80 	Ltrain: 0.009622 	Lval: 0.008789
Epoch 00082: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 85 	Ltrain: 0.008505 	Lval: 0.008424
Epoch: 90 	Ltrain: 0.008476 	Lval: 0.008499
Epoch: 95 	Ltrain: 0.008705 	Lval: 0.008406
Epoch: 100 	Ltrain: 0.008701 	Lval: 0.008353
Epoch: 105 	Ltrain: 0.008444 	Lval: 0.008329
Epoch 00108: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 110 	Ltrain: 0.008580 	Lval: 0.008319
Epoch: 115 	Ltrain: 0.009372 	Lval: 0.008306
Epoch: 120 	Ltrain: 0.008528 	Lval: 0.008306
Epoch 00122: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 125 	Ltrain: 0.008657 	Lval: 0.008306
Epoch: 130 	Ltrain: 0.008211 	Lval: 0.008305
Epoch 00134: reducing learning rate of group 0 to 1.1789e-08.
Epoch: 135 	Ltrain: 0.008918 	Lval: 0.008305
Epoch: 140 	Ltrain: 0.009023 	Lval: 0.008305
EarlyStopper: stopping at epoch 140 with best_val_loss = 0.008308


	Fold 2/5
Epoch: 1 	Ltrain: 0.029320 	Lval: 0.017226
Epoch: 5 	Ltrain: 0.018224 	Lval: 0.016668
Epoch: 10 	Ltrain: 0.015993 	Lval: 0.014840
Epoch: 15 	Ltrain: 0.013695 	Lval: 0.013140
Epoch: 20 	Ltrain: 0.011984 	Lval: 0.012005
Epoch: 25 	Ltrain: 0.010803 	Lval: 0.010852
Epoch: 30 	Ltrain: 0.010132 	Lval: 0.010125
Epoch: 35 	Ltrain: 0.009277 	Lval: 0.009348
Epoch: 40 	Ltrain: 0.008748 	Lval: 0.008836
Epoch: 45 	Ltrain: 0.008383 	Lval: 0.008304
Epoch: 50 	Ltrain: 0.007833 	Lval: 0.008081
Epoch: 55 	Ltrain: 0.007688 	Lval: 0.007760
Epoch 00059: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 60 	Ltrain: 0.007558 	Lval: 0.007615
Epoch: 65 	Ltrain: 0.007355 	Lval: 0.007593
Epoch: 70 	Ltrain: 0.007465 	Lval: 0.007575
Epoch: 75 	Ltrain: 0.007332 	Lval: 0.007563
Epoch: 80 	Ltrain: 0.007257 	Lval: 0.007553
Epoch: 85 	Ltrain: 0.007254 	Lval: 0.007520
Epoch: 90 	Ltrain: 0.007413 	Lval: 0.007507
Epoch: 95 	Ltrain: 0.007442 	Lval: 0.007495
Epoch: 100 	Ltrain: 0.007305 	Lval: 0.007471
Epoch: 105 	Ltrain: 0.007261 	Lval: 0.007453
Epoch 00109: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 110 	Ltrain: 0.007196 	Lval: 0.007459
Epoch: 115 	Ltrain: 0.007258 	Lval: 0.007447
Epoch: 120 	Ltrain: 0.007159 	Lval: 0.007446
Epoch 00121: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 125 	Ltrain: 0.007156 	Lval: 0.007446
Epoch: 130 	Ltrain: 0.007160 	Lval: 0.007446
EarlyStopper: stopping at epoch 131 with best_val_loss = 0.007453


	Fold 3/5
Epoch: 1 	Ltrain: 0.042401 	Lval: 0.020295
Epoch: 5 	Ltrain: 0.018477 	Lval: 0.016466
Epoch: 10 	Ltrain: 0.013255 	Lval: 0.013284
Epoch: 15 	Ltrain: 0.010962 	Lval: 0.011630
Epoch: 20 	Ltrain: 0.009853 	Lval: 0.010503
Epoch: 25 	Ltrain: 0.008887 	Lval: 0.009285
Epoch: 30 	Ltrain: 0.008117 	Lval: 0.008625
Epoch: 35 	Ltrain: 0.007455 	Lval: 0.007989
Epoch: 40 	Ltrain: 0.007104 	Lval: 0.007667
Epoch: 45 	Ltrain: 0.006856 	Lval: 0.007719
Epoch 00047: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 50 	Ltrain: 0.006565 	Lval: 0.007393
Epoch: 55 	Ltrain: 0.006578 	Lval: 0.007363
Epoch: 60 	Ltrain: 0.006550 	Lval: 0.007329
Epoch 00062: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 65 	Ltrain: 0.006520 	Lval: 0.007338
Epoch: 70 	Ltrain: 0.006614 	Lval: 0.007334
Epoch 00074: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 75 	Ltrain: 0.006490 	Lval: 0.007336
Epoch: 80 	Ltrain: 0.006531 	Lval: 0.007336
Epoch: 85 	Ltrain: 0.006490 	Lval: 0.007335
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.007329


	Fold 4/5
Epoch: 1 	Ltrain: 0.030468 	Lval: 0.018969
Epoch: 5 	Ltrain: 0.015946 	Lval: 0.015529
Epoch: 10 	Ltrain: 0.010966 	Lval: 0.012384
Epoch: 15 	Ltrain: 0.009110 	Lval: 0.010877
Epoch: 20 	Ltrain: 0.007986 	Lval: 0.009448
Epoch: 25 	Ltrain: 0.007142 	Lval: 0.008417
Epoch: 30 	Ltrain: 0.006311 	Lval: 0.007760
Epoch: 35 	Ltrain: 0.005955 	Lval: 0.007750
Epoch: 40 	Ltrain: 0.005825 	Lval: 0.007350
Epoch 00041: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 45 	Ltrain: 0.005643 	Lval: 0.007242
Epoch: 50 	Ltrain: 0.005645 	Lval: 0.007216
Epoch 00053: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 55 	Ltrain: 0.005632 	Lval: 0.007177
Epoch: 60 	Ltrain: 0.005622 	Lval: 0.007164
Epoch 00065: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 65 	Ltrain: 0.005610 	Lval: 0.007178
Epoch: 70 	Ltrain: 0.005610 	Lval: 0.007174
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.007119


	Fold 5/5
Epoch: 1 	Ltrain: 0.035781 	Lval: 0.019352
Epoch: 5 	Ltrain: 0.016308 	Lval: 0.018281
Epoch: 10 	Ltrain: 0.010838 	Lval: 0.012907
Epoch: 15 	Ltrain: 0.009161 	Lval: 0.011397
Epoch: 20 	Ltrain: 0.008028 	Lval: 0.009859
Epoch: 25 	Ltrain: 0.006952 	Lval: 0.008855
Epoch: 30 	Ltrain: 0.006402 	Lval: 0.007808
Epoch: 35 	Ltrain: 0.006033 	Lval: 0.007425
Epoch 00039: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 40 	Ltrain: 0.005702 	Lval: 0.007345
Epoch: 45 	Ltrain: 0.005707 	Lval: 0.007374
Epoch: 50 	Ltrain: 0.005675 	Lval: 0.007412
Epoch 00051: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 55 	Ltrain: 0.005680 	Lval: 0.007382
Epoch: 60 	Ltrain: 0.005670 	Lval: 0.007379
Epoch 00063: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 65 	Ltrain: 0.005666 	Lval: 0.007372
Epoch: 70 	Ltrain: 0.005650 	Lval: 0.007371
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.007299

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003646712063919765
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.892450311696089e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.059434 	Lval: 0.023633
Epoch: 5 	Ltrain: 0.015340 	Lval: 0.015339
Epoch: 10 	Ltrain: 0.010731 	Lval: 0.011311
Epoch: 15 	Ltrain: 0.010921 	Lval: 0.010403
Epoch: 20 	Ltrain: 0.007515 	Lval: 0.007875
Epoch: 25 	Ltrain: 0.006733 	Lval: 0.006936
Epoch: 30 	Ltrain: 0.006884 	Lval: 0.006668
Epoch: 35 	Ltrain: 0.007411 	Lval: 0.006519
Epoch 00037: reducing learning rate of group 0 to 3.6467e-04.
Epoch: 40 	Ltrain: 0.006740 	Lval: 0.006761
Epoch: 45 	Ltrain: 0.006170 	Lval: 0.006406
Epoch: 50 	Ltrain: 0.005993 	Lval: 0.006112
Epoch: 55 	Ltrain: 0.006034 	Lval: 0.006105
Epoch 00058: reducing learning rate of group 0 to 3.6467e-05.
Epoch: 60 	Ltrain: 0.006286 	Lval: 0.006101
Epoch: 65 	Ltrain: 0.006308 	Lval: 0.006093
Epoch 00070: reducing learning rate of group 0 to 3.6467e-06.
Epoch: 70 	Ltrain: 0.005895 	Lval: 0.006108
Epoch: 75 	Ltrain: 0.006017 	Lval: 0.006111
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.006071


	Fold 2/5
Epoch: 1 	Ltrain: 0.033530 	Lval: 0.016505
Epoch: 5 	Ltrain: 0.010236 	Lval: 0.009175
Epoch: 10 	Ltrain: 0.007678 	Lval: 0.007889
Epoch 00015: reducing learning rate of group 0 to 3.6467e-04.
Epoch: 15 	Ltrain: 0.007330 	Lval: 0.007357
Epoch: 20 	Ltrain: 0.006112 	Lval: 0.006503
Epoch: 25 	Ltrain: 0.006258 	Lval: 0.006455
Epoch 00028: reducing learning rate of group 0 to 3.6467e-05.
Epoch: 30 	Ltrain: 0.006050 	Lval: 0.006407
Epoch: 35 	Ltrain: 0.006154 	Lval: 0.006414
Epoch 00040: reducing learning rate of group 0 to 3.6467e-06.
Epoch: 40 	Ltrain: 0.006074 	Lval: 0.006405
Epoch: 45 	Ltrain: 0.006623 	Lval: 0.006400
Epoch: 50 	Ltrain: 0.006418 	Lval: 0.006400
Epoch 00052: reducing learning rate of group 0 to 3.6467e-07.
Epoch: 55 	Ltrain: 0.006383 	Lval: 0.006401
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.006391


	Fold 3/5
Epoch: 1 	Ltrain: 0.026092 	Lval: 0.025621
Epoch: 5 	Ltrain: 0.008142 	Lval: 0.009577
Epoch: 10 	Ltrain: 0.006558 	Lval: 0.007239
Epoch 00015: reducing learning rate of group 0 to 3.6467e-04.
Epoch: 15 	Ltrain: 0.006223 	Lval: 0.006882
Epoch: 20 	Ltrain: 0.005538 	Lval: 0.006190
Epoch: 25 	Ltrain: 0.005551 	Lval: 0.006116
Epoch: 30 	Ltrain: 0.005493 	Lval: 0.006215
Epoch: 35 	Ltrain: 0.005344 	Lval: 0.006027
Epoch 00040: reducing learning rate of group 0 to 3.6467e-05.
Epoch: 40 	Ltrain: 0.005409 	Lval: 0.006033
Epoch: 45 	Ltrain: 0.005381 	Lval: 0.005935
Epoch: 50 	Ltrain: 0.005375 	Lval: 0.005913
Epoch: 55 	Ltrain: 0.005302 	Lval: 0.005899
Epoch: 60 	Ltrain: 0.005365 	Lval: 0.005890
Epoch: 65 	Ltrain: 0.005238 	Lval: 0.005880
Epoch: 70 	Ltrain: 0.005353 	Lval: 0.005867
Epoch: 75 	Ltrain: 0.005365 	Lval: 0.005849
Epoch 00079: reducing learning rate of group 0 to 3.6467e-06.
Epoch: 80 	Ltrain: 0.005377 	Lval: 0.005855
Epoch: 85 	Ltrain: 0.005311 	Lval: 0.005852
Epoch: 90 	Ltrain: 0.005325 	Lval: 0.005849
Epoch 00091: reducing learning rate of group 0 to 3.6467e-07.
Epoch: 95 	Ltrain: 0.005477 	Lval: 0.005849
EarlyStopper: stopping at epoch 98 with best_val_loss = 0.005849


	Fold 4/5
Epoch: 1 	Ltrain: 0.030804 	Lval: 0.017777
Epoch: 5 	Ltrain: 0.006411 	Lval: 0.008188
Epoch: 10 	Ltrain: 0.005856 	Lval: 0.008284
Epoch 00011: reducing learning rate of group 0 to 3.6467e-04.
Epoch: 15 	Ltrain: 0.005222 	Lval: 0.006383
Epoch: 20 	Ltrain: 0.005089 	Lval: 0.006582
Epoch: 25 	Ltrain: 0.005050 	Lval: 0.006267
Epoch 00026: reducing learning rate of group 0 to 3.6467e-05.
Epoch: 30 	Ltrain: 0.005007 	Lval: 0.006232
Epoch: 35 	Ltrain: 0.005051 	Lval: 0.006217
Epoch: 40 	Ltrain: 0.005001 	Lval: 0.006210
Epoch: 45 	Ltrain: 0.004972 	Lval: 0.006199
Epoch: 50 	Ltrain: 0.004961 	Lval: 0.006176
Epoch: 55 	Ltrain: 0.004955 	Lval: 0.006174
Epoch: 60 	Ltrain: 0.004957 	Lval: 0.006163
Epoch: 65 	Ltrain: 0.004869 	Lval: 0.006162
Epoch 00070: reducing learning rate of group 0 to 3.6467e-06.
Epoch: 70 	Ltrain: 0.004961 	Lval: 0.006146
Epoch: 75 	Ltrain: 0.004897 	Lval: 0.006141
Epoch: 80 	Ltrain: 0.004920 	Lval: 0.006146
Epoch 00082: reducing learning rate of group 0 to 3.6467e-07.
Epoch: 85 	Ltrain: 0.004934 	Lval: 0.006141
Epoch: 90 	Ltrain: 0.004919 	Lval: 0.006141
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.006140


	Fold 5/5
Epoch: 1 	Ltrain: 0.027549 	Lval: 0.017616
Epoch: 5 	Ltrain: 0.006183 	Lval: 0.008692
Epoch: 10 	Ltrain: 0.006062 	Lval: 0.007250
Epoch 00011: reducing learning rate of group 0 to 3.6467e-04.
Epoch: 15 	Ltrain: 0.005303 	Lval: 0.006619
Epoch: 20 	Ltrain: 0.005148 	Lval: 0.006570
Epoch: 25 	Ltrain: 0.005113 	Lval: 0.006729
Epoch: 30 	Ltrain: 0.005148 	Lval: 0.006468
Epoch 00033: reducing learning rate of group 0 to 3.6467e-05.
Epoch: 35 	Ltrain: 0.005048 	Lval: 0.006456
Epoch: 40 	Ltrain: 0.004992 	Lval: 0.006467
Epoch 00045: reducing learning rate of group 0 to 3.6467e-06.
Epoch: 45 	Ltrain: 0.005009 	Lval: 0.006482
Epoch: 50 	Ltrain: 0.004999 	Lval: 0.006470
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.006425

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0026055249560641737
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.333980876661893e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.028467 	Lval: 0.018014
Epoch: 5 	Ltrain: 0.014142 	Lval: 0.013333
Epoch: 10 	Ltrain: 0.010111 	Lval: 0.009194
Epoch: 15 	Ltrain: 0.008757 	Lval: 0.008565
Epoch 00017: reducing learning rate of group 0 to 2.6055e-04.
Epoch: 20 	Ltrain: 0.007550 	Lval: 0.007545
Epoch: 25 	Ltrain: 0.008127 	Lval: 0.007380
Epoch: 30 	Ltrain: 0.007248 	Lval: 0.007363
Epoch 00035: reducing learning rate of group 0 to 2.6055e-05.
Epoch: 35 	Ltrain: 0.007117 	Lval: 0.007296
Epoch: 40 	Ltrain: 0.008476 	Lval: 0.007264
Epoch: 45 	Ltrain: 0.007129 	Lval: 0.007264
Epoch: 50 	Ltrain: 0.007245 	Lval: 0.007255
Epoch: 55 	Ltrain: 0.007249 	Lval: 0.007259
Epoch 00056: reducing learning rate of group 0 to 2.6055e-06.
Epoch: 60 	Ltrain: 0.007129 	Lval: 0.007251
Epoch: 65 	Ltrain: 0.007330 	Lval: 0.007251
Epoch 00068: reducing learning rate of group 0 to 2.6055e-07.
Epoch: 70 	Ltrain: 0.007488 	Lval: 0.007253
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.007250


	Fold 2/5
Epoch: 1 	Ltrain: 0.022758 	Lval: 0.019212
Epoch: 5 	Ltrain: 0.010761 	Lval: 0.009141
Epoch: 10 	Ltrain: 0.007415 	Lval: 0.007428
Epoch: 15 	Ltrain: 0.007431 	Lval: 0.007034
Epoch: 20 	Ltrain: 0.006836 	Lval: 0.007683
Epoch: 25 	Ltrain: 0.006810 	Lval: 0.007122
Epoch 00027: reducing learning rate of group 0 to 2.6055e-04.
Epoch: 30 	Ltrain: 0.006063 	Lval: 0.006369
Epoch: 35 	Ltrain: 0.006090 	Lval: 0.006330
Epoch: 40 	Ltrain: 0.005992 	Lval: 0.006288
Epoch: 45 	Ltrain: 0.005927 	Lval: 0.006243
Epoch: 50 	Ltrain: 0.005897 	Lval: 0.006188
Epoch: 55 	Ltrain: 0.006048 	Lval: 0.006145
Epoch: 60 	Ltrain: 0.005832 	Lval: 0.006125
Epoch: 65 	Ltrain: 0.005864 	Lval: 0.006078
Epoch: 70 	Ltrain: 0.005857 	Lval: 0.006034
Epoch: 75 	Ltrain: 0.005783 	Lval: 0.006013
Epoch: 80 	Ltrain: 0.005796 	Lval: 0.006078
Epoch 00081: reducing learning rate of group 0 to 2.6055e-05.
Epoch: 85 	Ltrain: 0.005684 	Lval: 0.005991
Epoch: 90 	Ltrain: 0.005738 	Lval: 0.005987
Epoch 00095: reducing learning rate of group 0 to 2.6055e-06.
Epoch: 95 	Ltrain: 0.005634 	Lval: 0.005983
Epoch: 100 	Ltrain: 0.005678 	Lval: 0.005982
Epoch: 105 	Ltrain: 0.005659 	Lval: 0.005983
Epoch: 110 	Ltrain: 0.005675 	Lval: 0.005981
Epoch 00114: reducing learning rate of group 0 to 2.6055e-07.
Epoch: 115 	Ltrain: 0.005588 	Lval: 0.005980
EarlyStopper: stopping at epoch 114 with best_val_loss = 0.005986


	Fold 3/5
Epoch: 1 	Ltrain: 0.027158 	Lval: 0.017024
Epoch: 5 	Ltrain: 0.008400 	Lval: 0.008314
Epoch: 10 	Ltrain: 0.006821 	Lval: 0.007672
Epoch: 15 	Ltrain: 0.006474 	Lval: 0.006904
Epoch: 20 	Ltrain: 0.006067 	Lval: 0.007090
Epoch: 25 	Ltrain: 0.005908 	Lval: 0.006747
Epoch 00027: reducing learning rate of group 0 to 2.6055e-04.
Epoch: 30 	Ltrain: 0.005684 	Lval: 0.006487
Epoch: 35 	Ltrain: 0.005643 	Lval: 0.006416
Epoch: 40 	Ltrain: 0.005691 	Lval: 0.006406
Epoch: 45 	Ltrain: 0.005715 	Lval: 0.006395
Epoch 00050: reducing learning rate of group 0 to 2.6055e-05.
Epoch: 50 	Ltrain: 0.005611 	Lval: 0.006360
Epoch: 55 	Ltrain: 0.005568 	Lval: 0.006349
Epoch: 60 	Ltrain: 0.005576 	Lval: 0.006344
Epoch 00063: reducing learning rate of group 0 to 2.6055e-06.
Epoch: 65 	Ltrain: 0.005614 	Lval: 0.006344
Epoch: 70 	Ltrain: 0.005575 	Lval: 0.006342
Epoch: 75 	Ltrain: 0.005546 	Lval: 0.006341
Epoch 00079: reducing learning rate of group 0 to 2.6055e-07.
Epoch: 80 	Ltrain: 0.005570 	Lval: 0.006341
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.006343


	Fold 4/5
Epoch: 1 	Ltrain: 0.017858 	Lval: 0.015037
Epoch: 5 	Ltrain: 0.006713 	Lval: 0.008137
Epoch: 10 	Ltrain: 0.005883 	Lval: 0.007129
Epoch: 15 	Ltrain: 0.005362 	Lval: 0.006908
Epoch: 20 	Ltrain: 0.005344 	Lval: 0.006587
Epoch: 25 	Ltrain: 0.005274 	Lval: 0.006622
Epoch: 30 	Ltrain: 0.005064 	Lval: 0.006225
Epoch: 35 	Ltrain: 0.004998 	Lval: 0.006177
Epoch: 40 	Ltrain: 0.005062 	Lval: 0.005963
Epoch: 45 	Ltrain: 0.004934 	Lval: 0.006575
Epoch 00047: reducing learning rate of group 0 to 2.6055e-04.
Epoch: 50 	Ltrain: 0.004457 	Lval: 0.005522
Epoch: 55 	Ltrain: 0.004398 	Lval: 0.005632
Epoch: 60 	Ltrain: 0.004399 	Lval: 0.005492
Epoch: 65 	Ltrain: 0.004368 	Lval: 0.005391
Epoch 00067: reducing learning rate of group 0 to 2.6055e-05.
Epoch: 70 	Ltrain: 0.004310 	Lval: 0.005379
Epoch: 75 	Ltrain: 0.004295 	Lval: 0.005360
Epoch: 80 	Ltrain: 0.004285 	Lval: 0.005355
Epoch 00084: reducing learning rate of group 0 to 2.6055e-06.
Epoch: 85 	Ltrain: 0.004288 	Lval: 0.005356
Epoch: 90 	Ltrain: 0.004283 	Lval: 0.005357
Epoch: 95 	Ltrain: 0.004281 	Lval: 0.005357
Epoch 00096: reducing learning rate of group 0 to 2.6055e-07.
Epoch: 100 	Ltrain: 0.004285 	Lval: 0.005355
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.005360


	Fold 5/5
Epoch: 1 	Ltrain: 0.017764 	Lval: 0.016340
Epoch: 5 	Ltrain: 0.006578 	Lval: 0.007788
Epoch: 10 	Ltrain: 0.005652 	Lval: 0.007791
Epoch: 15 	Ltrain: 0.005456 	Lval: 0.006707
Epoch: 20 	Ltrain: 0.005414 	Lval: 0.006755
Epoch 00022: reducing learning rate of group 0 to 2.6055e-04.
Epoch: 25 	Ltrain: 0.004941 	Lval: 0.006456
Epoch: 30 	Ltrain: 0.004939 	Lval: 0.006387
Epoch: 35 	Ltrain: 0.004941 	Lval: 0.006283
Epoch 00037: reducing learning rate of group 0 to 2.6055e-05.
Epoch: 40 	Ltrain: 0.004848 	Lval: 0.006313
Epoch: 45 	Ltrain: 0.004854 	Lval: 0.006299
Epoch 00049: reducing learning rate of group 0 to 2.6055e-06.
Epoch: 50 	Ltrain: 0.004864 	Lval: 0.006328
Epoch: 55 	Ltrain: 0.004848 	Lval: 0.006317
Epoch: 60 	Ltrain: 0.004866 	Lval: 0.006311
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.006273

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006538443684663064
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.6330166694239005e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 11
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.126880 	Lval: 0.042056
Epoch: 5 	Ltrain: 0.032162 	Lval: 0.016914
Epoch 00009: reducing learning rate of group 0 to 6.5384e-04.
Epoch: 10 	Ltrain: 0.023155 	Lval: 0.018343
Epoch: 15 	Ltrain: 0.017413 	Lval: 0.016129
Epoch: 20 	Ltrain: 0.017326 	Lval: 0.015775
Epoch 00022: reducing learning rate of group 0 to 6.5384e-05.
Epoch: 25 	Ltrain: 0.018178 	Lval: 0.016686
EarlyStopper: stopping at epoch 28 with best_val_loss = 0.015438


	Fold 2/5
Epoch: 1 	Ltrain: 0.267789 	Lval: 0.055060
Epoch: 5 	Ltrain: 0.020374 	Lval: 0.019476
Epoch: 10 	Ltrain: 0.012594 	Lval: 0.016577
Epoch: 15 	Ltrain: 0.009524 	Lval: 0.008019
Epoch: 20 	Ltrain: 0.007450 	Lval: 0.009814
Epoch: 25 	Ltrain: 0.007341 	Lval: 0.006946
Epoch 00029: reducing learning rate of group 0 to 6.5384e-04.
Epoch: 30 	Ltrain: 0.007421 	Lval: 0.006736
Epoch: 35 	Ltrain: 0.006275 	Lval: 0.006553
Epoch: 40 	Ltrain: 0.006468 	Lval: 0.006491
Epoch 00045: reducing learning rate of group 0 to 6.5384e-05.
Epoch: 45 	Ltrain: 0.006098 	Lval: 0.006543
Epoch: 50 	Ltrain: 0.006876 	Lval: 0.006526
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.006491


	Fold 3/5
Epoch: 1 	Ltrain: 0.122508 	Lval: 0.047841
Epoch: 5 	Ltrain: 0.011340 	Lval: 0.009087
Epoch: 10 	Ltrain: 0.009294 	Lval: 0.007578
Epoch: 15 	Ltrain: 0.006709 	Lval: 0.006820
Epoch: 20 	Ltrain: 0.006439 	Lval: 0.007069
Epoch: 25 	Ltrain: 0.006175 	Lval: 0.006415
Epoch: 30 	Ltrain: 0.005715 	Lval: 0.005905
Epoch 00032: reducing learning rate of group 0 to 6.5384e-04.
Epoch: 35 	Ltrain: 0.005472 	Lval: 0.005761
Epoch: 40 	Ltrain: 0.005072 	Lval: 0.005650
Epoch 00044: reducing learning rate of group 0 to 6.5384e-05.
Epoch: 45 	Ltrain: 0.005144 	Lval: 0.005649
Epoch: 50 	Ltrain: 0.004992 	Lval: 0.005633
EarlyStopper: stopping at epoch 49 with best_val_loss = 0.005621


	Fold 4/5
Epoch: 1 	Ltrain: 0.147626 	Lval: 0.041971
Epoch: 5 	Ltrain: 0.009503 	Lval: 0.008965
Epoch: 10 	Ltrain: 0.007033 	Lval: 0.007470
Epoch: 15 	Ltrain: 0.006215 	Lval: 0.006676
Epoch 00019: reducing learning rate of group 0 to 6.5384e-04.
Epoch: 20 	Ltrain: 0.005604 	Lval: 0.006548
Epoch: 25 	Ltrain: 0.005356 	Lval: 0.006357
Epoch: 30 	Ltrain: 0.005166 	Lval: 0.006293
Epoch: 35 	Ltrain: 0.005217 	Lval: 0.006277
Epoch 00038: reducing learning rate of group 0 to 6.5384e-05.
Epoch: 40 	Ltrain: 0.005098 	Lval: 0.006276
Epoch: 45 	Ltrain: 0.005014 	Lval: 0.006269
EarlyStopper: stopping at epoch 44 with best_val_loss = 0.006239


	Fold 5/5
Epoch: 1 	Ltrain: 0.127821 	Lval: 0.018171
Epoch: 5 	Ltrain: 0.010306 	Lval: 0.011682
Epoch: 10 	Ltrain: 0.006205 	Lval: 0.007158
Epoch: 15 	Ltrain: 0.005346 	Lval: 0.007506
Epoch: 20 	Ltrain: 0.005582 	Lval: 0.007110
Epoch: 25 	Ltrain: 0.005627 	Lval: 0.006905
Epoch 00028: reducing learning rate of group 0 to 6.5384e-04.
Epoch: 30 	Ltrain: 0.005132 	Lval: 0.006336
Epoch: 35 	Ltrain: 0.005031 	Lval: 0.006266
Epoch 00040: reducing learning rate of group 0 to 6.5384e-05.
Epoch: 40 	Ltrain: 0.005003 	Lval: 0.006283
Epoch: 45 	Ltrain: 0.005118 	Lval: 0.006273
EarlyStopper: stopping at epoch 45 with best_val_loss = 0.006266

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0031400995421147922
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.2600367870595595e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.020374 	Lval: 0.017056
Epoch: 5 	Ltrain: 0.014919 	Lval: 0.013791
Epoch: 10 	Ltrain: 0.010839 	Lval: 0.008721
Epoch: 15 	Ltrain: 0.008008 	Lval: 0.008183
Epoch: 20 	Ltrain: 0.008335 	Lval: 0.007728
Epoch: 25 	Ltrain: 0.007540 	Lval: 0.007318
Epoch: 30 	Ltrain: 0.007130 	Lval: 0.008056
Epoch: 35 	Ltrain: 0.007591 	Lval: 0.007051
Epoch: 40 	Ltrain: 0.007323 	Lval: 0.006750
Epoch 00045: reducing learning rate of group 0 to 3.1401e-04.
Epoch: 45 	Ltrain: 0.006806 	Lval: 0.006935
Epoch: 50 	Ltrain: 0.006984 	Lval: 0.006411
Epoch: 55 	Ltrain: 0.006446 	Lval: 0.006340
Epoch: 60 	Ltrain: 0.006655 	Lval: 0.006318
Epoch 00064: reducing learning rate of group 0 to 3.1401e-05.
Epoch: 65 	Ltrain: 0.006186 	Lval: 0.006341
Epoch: 70 	Ltrain: 0.006376 	Lval: 0.006297
Epoch: 75 	Ltrain: 0.006503 	Lval: 0.006286
Epoch: 80 	Ltrain: 0.006133 	Lval: 0.006271
Epoch 00083: reducing learning rate of group 0 to 3.1401e-06.
Epoch: 85 	Ltrain: 0.006024 	Lval: 0.006270
Epoch: 90 	Ltrain: 0.006441 	Lval: 0.006269
Epoch: 95 	Ltrain: 0.006242 	Lval: 0.006268
Epoch: 100 	Ltrain: 0.006014 	Lval: 0.006266
EarlyStopper: stopping at epoch 102 with best_val_loss = 0.006270


	Fold 2/5
Epoch: 1 	Ltrain: 0.025675 	Lval: 0.018905
Epoch: 5 	Ltrain: 0.010762 	Lval: 0.009531
Epoch: 10 	Ltrain: 0.008012 	Lval: 0.007613
Epoch: 15 	Ltrain: 0.006957 	Lval: 0.007503
Epoch 00016: reducing learning rate of group 0 to 3.1401e-04.
Epoch: 20 	Ltrain: 0.006688 	Lval: 0.006949
Epoch: 25 	Ltrain: 0.006635 	Lval: 0.006922
Epoch: 30 	Ltrain: 0.006736 	Lval: 0.006970
Epoch: 35 	Ltrain: 0.006590 	Lval: 0.006897
Epoch: 40 	Ltrain: 0.006709 	Lval: 0.006891
Epoch: 45 	Ltrain: 0.006591 	Lval: 0.006854
Epoch: 50 	Ltrain: 0.006560 	Lval: 0.006882
Epoch 00051: reducing learning rate of group 0 to 3.1401e-05.
Epoch: 55 	Ltrain: 0.006490 	Lval: 0.006826
Epoch: 60 	Ltrain: 0.006452 	Lval: 0.006824
Epoch: 65 	Ltrain: 0.006481 	Lval: 0.006821
Epoch 00070: reducing learning rate of group 0 to 3.1401e-06.
Epoch: 70 	Ltrain: 0.006646 	Lval: 0.006816
Epoch: 75 	Ltrain: 0.006466 	Lval: 0.006818
Epoch: 80 	Ltrain: 0.006522 	Lval: 0.006817
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.006822


	Fold 3/5
Epoch: 1 	Ltrain: 0.026446 	Lval: 0.017102
Epoch: 5 	Ltrain: 0.008652 	Lval: 0.008469
Epoch: 10 	Ltrain: 0.006653 	Lval: 0.007922
Epoch: 15 	Ltrain: 0.006382 	Lval: 0.006961
Epoch 00019: reducing learning rate of group 0 to 3.1401e-04.
Epoch: 20 	Ltrain: 0.006132 	Lval: 0.006883
Epoch: 25 	Ltrain: 0.005995 	Lval: 0.006737
Epoch: 30 	Ltrain: 0.006034 	Lval: 0.006733
Epoch 00031: reducing learning rate of group 0 to 3.1401e-05.
Epoch: 35 	Ltrain: 0.005945 	Lval: 0.006725
Epoch: 40 	Ltrain: 0.005879 	Lval: 0.006723
Epoch: 45 	Ltrain: 0.005897 	Lval: 0.006716
Epoch 00047: reducing learning rate of group 0 to 3.1401e-06.
Epoch: 50 	Ltrain: 0.005899 	Lval: 0.006715
Epoch: 55 	Ltrain: 0.005910 	Lval: 0.006715
Epoch 00059: reducing learning rate of group 0 to 3.1401e-07.
Epoch: 60 	Ltrain: 0.005952 	Lval: 0.006714
Epoch: 65 	Ltrain: 0.005941 	Lval: 0.006714
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.006710


	Fold 4/5
Epoch: 1 	Ltrain: 0.024664 	Lval: 0.016712
Epoch: 5 	Ltrain: 0.007015 	Lval: 0.008191
Epoch: 10 	Ltrain: 0.005917 	Lval: 0.007259
Epoch: 15 	Ltrain: 0.005710 	Lval: 0.007099
Epoch 00018: reducing learning rate of group 0 to 3.1401e-04.
Epoch: 20 	Ltrain: 0.005330 	Lval: 0.007086
Epoch: 25 	Ltrain: 0.005345 	Lval: 0.006900
Epoch: 30 	Ltrain: 0.005284 	Lval: 0.006787
Epoch: 35 	Ltrain: 0.005258 	Lval: 0.006648
Epoch: 40 	Ltrain: 0.005284 	Lval: 0.006894
Epoch 00042: reducing learning rate of group 0 to 3.1401e-05.
Epoch: 45 	Ltrain: 0.005222 	Lval: 0.006662
Epoch: 50 	Ltrain: 0.005223 	Lval: 0.006679
Epoch 00054: reducing learning rate of group 0 to 3.1401e-06.
Epoch: 55 	Ltrain: 0.005230 	Lval: 0.006671
Epoch: 60 	Ltrain: 0.005219 	Lval: 0.006671
EarlyStopper: stopping at epoch 61 with best_val_loss = 0.006628


	Fold 5/5
Epoch: 1 	Ltrain: 0.018725 	Lval: 0.017719
Epoch: 5 	Ltrain: 0.006390 	Lval: 0.008674
Epoch: 10 	Ltrain: 0.005759 	Lval: 0.007496
Epoch 00012: reducing learning rate of group 0 to 3.1401e-04.
Epoch: 15 	Ltrain: 0.005401 	Lval: 0.007002
Epoch: 20 	Ltrain: 0.005425 	Lval: 0.007053
Epoch 00024: reducing learning rate of group 0 to 3.1401e-05.
Epoch: 25 	Ltrain: 0.005340 	Lval: 0.007007
Epoch: 30 	Ltrain: 0.005323 	Lval: 0.006960
Epoch: 35 	Ltrain: 0.005331 	Lval: 0.006952
Epoch 00036: reducing learning rate of group 0 to 3.1401e-06.
Epoch: 40 	Ltrain: 0.005332 	Lval: 0.006960
EarlyStopper: stopping at epoch 39 with best_val_loss = 0.006915

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0045164063138236
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.8415296021351108e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.041313 	Lval: 0.030374
Epoch: 5 	Ltrain: 0.016915 	Lval: 0.014203
Epoch: 10 	Ltrain: 0.014512 	Lval: 0.017811
Epoch 00012: reducing learning rate of group 0 to 4.5164e-04.
Epoch: 15 	Ltrain: 0.009420 	Lval: 0.009567
Epoch: 20 	Ltrain: 0.009554 	Lval: 0.008778
Epoch: 25 	Ltrain: 0.009423 	Lval: 0.008497
Epoch: 30 	Ltrain: 0.008401 	Lval: 0.008225
Epoch: 35 	Ltrain: 0.008549 	Lval: 0.007961
Epoch: 40 	Ltrain: 0.009806 	Lval: 0.007791
Epoch: 45 	Ltrain: 0.007502 	Lval: 0.007562
Epoch: 50 	Ltrain: 0.008133 	Lval: 0.007520
Epoch 00051: reducing learning rate of group 0 to 4.5164e-05.
Epoch: 55 	Ltrain: 0.007084 	Lval: 0.007437
Epoch: 60 	Ltrain: 0.007601 	Lval: 0.007437
Epoch 00063: reducing learning rate of group 0 to 4.5164e-06.
Epoch: 65 	Ltrain: 0.007843 	Lval: 0.007396
Epoch: 70 	Ltrain: 0.006899 	Lval: 0.007388
Epoch: 75 	Ltrain: 0.007205 	Lval: 0.007381
Epoch 00079: reducing learning rate of group 0 to 4.5164e-07.
Epoch: 80 	Ltrain: 0.006891 	Lval: 0.007382
Epoch: 85 	Ltrain: 0.007297 	Lval: 0.007382
Epoch: 90 	Ltrain: 0.007286 	Lval: 0.007381
Epoch 00091: reducing learning rate of group 0 to 4.5164e-08.
Epoch: 95 	Ltrain: 0.007405 	Lval: 0.007381
EarlyStopper: stopping at epoch 98 with best_val_loss = 0.007384


	Fold 2/5
Epoch: 1 	Ltrain: 0.029249 	Lval: 0.018320
Epoch: 5 	Ltrain: 0.013193 	Lval: 0.011079
Epoch: 10 	Ltrain: 0.007515 	Lval: 0.007513
Epoch: 15 	Ltrain: 0.007182 	Lval: 0.007427
Epoch 00020: reducing learning rate of group 0 to 4.5164e-04.
Epoch: 20 	Ltrain: 0.007760 	Lval: 0.007071
Epoch: 25 	Ltrain: 0.006803 	Lval: 0.006650
Epoch: 30 	Ltrain: 0.006348 	Lval: 0.006550
Epoch: 35 	Ltrain: 0.006330 	Lval: 0.006460
Epoch: 40 	Ltrain: 0.006024 	Lval: 0.006378
Epoch: 45 	Ltrain: 0.006285 	Lval: 0.006367
Epoch: 50 	Ltrain: 0.005853 	Lval: 0.006308
Epoch: 55 	Ltrain: 0.005893 	Lval: 0.006248
Epoch: 60 	Ltrain: 0.006365 	Lval: 0.006255
Epoch 00061: reducing learning rate of group 0 to 4.5164e-05.
Epoch: 65 	Ltrain: 0.005841 	Lval: 0.006236
Epoch: 70 	Ltrain: 0.005760 	Lval: 0.006235
Epoch 00073: reducing learning rate of group 0 to 4.5164e-06.
Epoch: 75 	Ltrain: 0.005869 	Lval: 0.006200
Epoch: 80 	Ltrain: 0.005679 	Lval: 0.006198
Epoch 00085: reducing learning rate of group 0 to 4.5164e-07.
Epoch: 85 	Ltrain: 0.005855 	Lval: 0.006199
Epoch: 90 	Ltrain: 0.005950 	Lval: 0.006200
EarlyStopper: stopping at epoch 93 with best_val_loss = 0.006178


	Fold 3/5
Epoch: 1 	Ltrain: 0.026219 	Lval: 0.017603
Epoch: 5 	Ltrain: 0.008284 	Lval: 0.007885
Epoch: 10 	Ltrain: 0.006576 	Lval: 0.007269
Epoch: 15 	Ltrain: 0.006288 	Lval: 0.007005
Epoch: 20 	Ltrain: 0.005636 	Lval: 0.006480
Epoch: 25 	Ltrain: 0.005992 	Lval: 0.006046
Epoch: 30 	Ltrain: 0.005532 	Lval: 0.005768
Epoch 00033: reducing learning rate of group 0 to 4.5164e-04.
Epoch: 35 	Ltrain: 0.005039 	Lval: 0.005368
Epoch: 40 	Ltrain: 0.004802 	Lval: 0.005266
Epoch: 45 	Ltrain: 0.004715 	Lval: 0.005219
Epoch: 50 	Ltrain: 0.004810 	Lval: 0.005156
Epoch: 55 	Ltrain: 0.004742 	Lval: 0.005092
Epoch: 60 	Ltrain: 0.004588 	Lval: 0.005042
Epoch: 65 	Ltrain: 0.004560 	Lval: 0.004822
Epoch: 70 	Ltrain: 0.004398 	Lval: 0.004758
Epoch: 75 	Ltrain: 0.004358 	Lval: 0.004595
Epoch: 80 	Ltrain: 0.004322 	Lval: 0.004543
Epoch: 85 	Ltrain: 0.004222 	Lval: 0.004473
Epoch: 90 	Ltrain: 0.004266 	Lval: 0.004364
Epoch: 95 	Ltrain: 0.004015 	Lval: 0.004263
Epoch 00097: reducing learning rate of group 0 to 4.5164e-05.
Epoch: 100 	Ltrain: 0.003918 	Lval: 0.004087
Epoch: 105 	Ltrain: 0.003764 	Lval: 0.004057
Epoch: 110 	Ltrain: 0.003854 	Lval: 0.004057
Epoch: 115 	Ltrain: 0.003921 	Lval: 0.004060
Epoch: 120 	Ltrain: 0.003763 	Lval: 0.004022
Epoch 00124: reducing learning rate of group 0 to 4.5164e-06.
Epoch: 125 	Ltrain: 0.003757 	Lval: 0.004022
Epoch: 130 	Ltrain: 0.003752 	Lval: 0.004021
Epoch: 135 	Ltrain: 0.003689 	Lval: 0.004023
Epoch 00137: reducing learning rate of group 0 to 4.5164e-07.
Epoch: 140 	Ltrain: 0.003765 	Lval: 0.004020
Epoch: 145 	Ltrain: 0.003721 	Lval: 0.004019
EarlyStopper: stopping at epoch 145 with best_val_loss = 0.004022


	Fold 4/5
Epoch: 1 	Ltrain: 0.021523 	Lval: 0.014536
Epoch: 5 	Ltrain: 0.007298 	Lval: 0.007190
Epoch: 10 	Ltrain: 0.005547 	Lval: 0.006538
Epoch 00014: reducing learning rate of group 0 to 4.5164e-04.
Epoch: 15 	Ltrain: 0.005344 	Lval: 0.006379
Epoch: 20 	Ltrain: 0.004960 	Lval: 0.006156
Epoch: 25 	Ltrain: 0.005054 	Lval: 0.006312
Epoch 00028: reducing learning rate of group 0 to 4.5164e-05.
Epoch: 30 	Ltrain: 0.004937 	Lval: 0.006071
Epoch: 35 	Ltrain: 0.004832 	Lval: 0.006057
Epoch: 40 	Ltrain: 0.004842 	Lval: 0.006052
Epoch 00042: reducing learning rate of group 0 to 4.5164e-06.
Epoch: 45 	Ltrain: 0.004983 	Lval: 0.006046
Epoch: 50 	Ltrain: 0.004991 	Lval: 0.006044
Epoch: 55 	Ltrain: 0.004850 	Lval: 0.006042
Epoch: 60 	Ltrain: 0.004835 	Lval: 0.006039
Epoch 00064: reducing learning rate of group 0 to 4.5164e-07.
Epoch: 65 	Ltrain: 0.004976 	Lval: 0.006041
Epoch: 70 	Ltrain: 0.004882 	Lval: 0.006041
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.006046


	Fold 5/5
Epoch: 1 	Ltrain: 0.021222 	Lval: 0.018708
Epoch: 5 	Ltrain: 0.006949 	Lval: 0.007861
Epoch: 10 	Ltrain: 0.005849 	Lval: 0.007270
Epoch: 15 	Ltrain: 0.005479 	Lval: 0.007118
Epoch: 20 	Ltrain: 0.005394 	Lval: 0.006334
Epoch: 25 	Ltrain: 0.004969 	Lval: 0.006017
Epoch 00030: reducing learning rate of group 0 to 4.5164e-04.
Epoch: 30 	Ltrain: 0.004706 	Lval: 0.006072
Epoch: 35 	Ltrain: 0.004279 	Lval: 0.005279
Epoch: 40 	Ltrain: 0.004199 	Lval: 0.005352
Epoch: 45 	Ltrain: 0.004142 	Lval: 0.005226
Epoch: 50 	Ltrain: 0.004088 	Lval: 0.004855
Epoch: 55 	Ltrain: 0.003997 	Lval: 0.004743
Epoch: 60 	Ltrain: 0.003845 	Lval: 0.004619
Epoch: 65 	Ltrain: 0.003758 	Lval: 0.004403
Epoch: 70 	Ltrain: 0.003615 	Lval: 0.004389
Epoch: 75 	Ltrain: 0.003568 	Lval: 0.004179
Epoch: 80 	Ltrain: 0.003418 	Lval: 0.003939
Epoch 00084: reducing learning rate of group 0 to 4.5164e-05.
Epoch: 85 	Ltrain: 0.003275 	Lval: 0.003838
Epoch: 90 	Ltrain: 0.003218 	Lval: 0.003801
Epoch: 95 	Ltrain: 0.003189 	Lval: 0.003774
Epoch: 100 	Ltrain: 0.003194 	Lval: 0.003764
Epoch: 105 	Ltrain: 0.003210 	Lval: 0.003737
Epoch: 110 	Ltrain: 0.003187 	Lval: 0.003761
Epoch: 115 	Ltrain: 0.003169 	Lval: 0.003715
Epoch: 120 	Ltrain: 0.003099 	Lval: 0.003700
Epoch: 125 	Ltrain: 0.003130 	Lval: 0.003676
Epoch: 130 	Ltrain: 0.003091 	Lval: 0.003677
Epoch: 135 	Ltrain: 0.003084 	Lval: 0.003643
Epoch: 140 	Ltrain: 0.003131 	Lval: 0.003641
Epoch: 145 	Ltrain: 0.003100 	Lval: 0.003623
Epoch: 150 	Ltrain: 0.003059 	Lval: 0.003596
Epoch: 155 	Ltrain: 0.003024 	Lval: 0.003592
Epoch: 160 	Ltrain: 0.003080 	Lval: 0.003567
Epoch: 165 	Ltrain: 0.002994 	Lval: 0.003547
Epoch: 170 	Ltrain: 0.003005 	Lval: 0.003539
Epoch: 175 	Ltrain: 0.002987 	Lval: 0.003516
Epoch: 180 	Ltrain: 0.002977 	Lval: 0.003503
Epoch 00182: reducing learning rate of group 0 to 4.5164e-06.
Epoch: 185 	Ltrain: 0.002994 	Lval: 0.003499
Epoch: 190 	Ltrain: 0.002961 	Lval: 0.003494
Epoch: 195 	Ltrain: 0.002973 	Lval: 0.003492
Epoch 00196: reducing learning rate of group 0 to 4.5164e-07.
Epoch: 200 	Ltrain: 0.002968 	Lval: 0.003489
EarlyStopper: stopping at epoch 203 with best_val_loss = 0.003494

Best hyperparameters: {'n_hours_u': 72, 'n_hours_y': 24, 'model_class': <class 'src.modelling.GRU.GRU'>, 'input_units': 8, 'hidden_layers': 6, 'hidden_units': 128, 'output_units': 2, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'lr_shared': 0.006549397293513188, 'scheduler': <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>, 'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}, 'w_decay': 2.459722776272002e-08, 'loss_fn': MSELoss(), 'epochs': 5000, 'early_stopper': <class 'src.modelling.EarlyStopper.EarlyStopper'>, 'patience': 27, 'batch_sz': 16, 'k_folds': 5}
Best validation loss: 0.00024312142300510052
