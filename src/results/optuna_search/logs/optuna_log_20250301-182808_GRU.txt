CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007194094541441001
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.731723105083079e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.371021 	Lval: 0.336327
Epoch: 5 	Ltrain: 0.019222 	Lval: 0.019051
Epoch 00007: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 10 	Ltrain: 0.013503 	Lval: 0.013296
Epoch: 15 	Ltrain: 0.013346 	Lval: 0.013010
Epoch: 20 	Ltrain: 0.013793 	Lval: 0.012891
Epoch: 25 	Ltrain: 0.013675 	Lval: 0.012203
Epoch: 30 	Ltrain: 0.012609 	Lval: 0.012152
Epoch: 35 	Ltrain: 0.011537 	Lval: 0.011433
Epoch: 40 	Ltrain: 0.011627 	Lval: 0.011345
Epoch: 45 	Ltrain: 0.011070 	Lval: 0.010617
Epoch: 50 	Ltrain: 0.009116 	Lval: 0.010432
Epoch: 55 	Ltrain: 0.010143 	Lval: 0.009574
Epoch: 60 	Ltrain: 0.008762 	Lval: 0.009749
Epoch: 65 	Ltrain: 0.008237 	Lval: 0.009188
Epoch 00067: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 70 	Ltrain: 0.009664 	Lval: 0.008722
Epoch: 75 	Ltrain: 0.009233 	Lval: 0.008343
Epoch: 80 	Ltrain: 0.009064 	Lval: 0.008285
Epoch: 85 	Ltrain: 0.007902 	Lval: 0.008262
Epoch 00087: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 90 	Ltrain: 0.009634 	Lval: 0.008298
Epoch: 95 	Ltrain: 0.007761 	Lval: 0.008253
Epoch: 100 	Ltrain: 0.008838 	Lval: 0.008214
Epoch: 105 	Ltrain: 0.009996 	Lval: 0.008217
Epoch 00108: reducing learning rate of group 0 to 7.1941e-07.
Epoch: 110 	Ltrain: 0.008090 	Lval: 0.008252
Epoch: 115 	Ltrain: 0.007721 	Lval: 0.008250
Epoch 00120: reducing learning rate of group 0 to 7.1941e-08.
Epoch: 120 	Ltrain: 0.008051 	Lval: 0.008249
EarlyStopper: stopping at epoch 120 with best_val_loss = 0.008218


	Fold 2/5
Epoch: 1 	Ltrain: 0.138680 	Lval: 0.016999
Epoch: 5 	Ltrain: 0.009209 	Lval: 0.007309
Epoch: 10 	Ltrain: 0.006418 	Lval: 0.005612
Epoch: 15 	Ltrain: 0.005786 	Lval: 0.004888
Epoch 00019: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 20 	Ltrain: 0.005759 	Lval: 0.005515
Epoch: 25 	Ltrain: 0.004847 	Lval: 0.004479
Epoch: 30 	Ltrain: 0.004728 	Lval: 0.004484
Epoch 00031: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 35 	Ltrain: 0.004735 	Lval: 0.004478
Epoch: 40 	Ltrain: 0.004822 	Lval: 0.004493
Epoch 00045: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 45 	Ltrain: 0.004598 	Lval: 0.004497
Epoch: 50 	Ltrain: 0.004576 	Lval: 0.004494
Epoch: 55 	Ltrain: 0.004745 	Lval: 0.004503
Epoch 00057: reducing learning rate of group 0 to 7.1941e-07.
Epoch: 60 	Ltrain: 0.004654 	Lval: 0.004497
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.004468


	Fold 3/5
Epoch: 1 	Ltrain: 0.100009 	Lval: 0.014274
Epoch: 5 	Ltrain: 0.007112 	Lval: 0.005584
Epoch: 10 	Ltrain: 0.004999 	Lval: 0.004553
Epoch: 15 	Ltrain: 0.004950 	Lval: 0.006622
Epoch: 20 	Ltrain: 0.004645 	Lval: 0.004009
Epoch: 25 	Ltrain: 0.004320 	Lval: 0.003971
Epoch: 30 	Ltrain: 0.004396 	Lval: 0.003680
Epoch 00034: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 35 	Ltrain: 0.003860 	Lval: 0.003764
Epoch: 40 	Ltrain: 0.003578 	Lval: 0.003603
Epoch: 45 	Ltrain: 0.003484 	Lval: 0.003673
Epoch 00050: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 50 	Ltrain: 0.003482 	Lval: 0.003535
Epoch: 55 	Ltrain: 0.003468 	Lval: 0.003535
Epoch: 60 	Ltrain: 0.003522 	Lval: 0.003533
Epoch 00062: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 65 	Ltrain: 0.003404 	Lval: 0.003552
EarlyStopper: stopping at epoch 67 with best_val_loss = 0.003516


	Fold 4/5
Epoch: 1 	Ltrain: 0.129937 	Lval: 0.032842
Epoch: 5 	Ltrain: 0.008744 	Lval: 0.008160
Epoch: 10 	Ltrain: 0.004982 	Lval: 0.005218
Epoch: 15 	Ltrain: 0.004266 	Lval: 0.004368
Epoch 00020: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 20 	Ltrain: 0.004156 	Lval: 0.004162
Epoch: 25 	Ltrain: 0.003846 	Lval: 0.003986
Epoch: 30 	Ltrain: 0.003874 	Lval: 0.003963
Epoch 00034: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 35 	Ltrain: 0.003848 	Lval: 0.003960
Epoch: 40 	Ltrain: 0.003825 	Lval: 0.003963
Epoch: 45 	Ltrain: 0.003761 	Lval: 0.003960
EarlyStopper: stopping at epoch 45 with best_val_loss = 0.003967


	Fold 5/5
Epoch: 1 	Ltrain: 0.126208 	Lval: 0.016597
Epoch: 5 	Ltrain: 0.006313 	Lval: 0.005969
Epoch: 10 	Ltrain: 0.005202 	Lval: 0.006831
Epoch: 15 	Ltrain: 0.004631 	Lval: 0.004970
Epoch: 20 	Ltrain: 0.004279 	Lval: 0.004672
Epoch: 25 	Ltrain: 0.004036 	Lval: 0.005286
Epoch 00026: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 30 	Ltrain: 0.003544 	Lval: 0.003902
Epoch: 35 	Ltrain: 0.003508 	Lval: 0.003950
Epoch: 40 	Ltrain: 0.003512 	Lval: 0.003917
Epoch: 45 	Ltrain: 0.003533 	Lval: 0.003833
Epoch 00050: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 50 	Ltrain: 0.003454 	Lval: 0.003835
Epoch: 55 	Ltrain: 0.003393 	Lval: 0.003810
Epoch: 60 	Ltrain: 0.003375 	Lval: 0.003800
Epoch 00064: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 65 	Ltrain: 0.003439 	Lval: 0.003805
Epoch: 70 	Ltrain: 0.003416 	Lval: 0.003799
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.003803

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00012585356404335105
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.7667185122397169e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.031054 	Lval: 0.026210
Epoch: 5 	Ltrain: 0.017221 	Lval: 0.014294
Epoch: 10 	Ltrain: 0.013995 	Lval: 0.013284
Epoch: 15 	Ltrain: 0.013492 	Lval: 0.012398
Epoch: 20 	Ltrain: 0.011933 	Lval: 0.011549
Epoch: 25 	Ltrain: 0.012132 	Lval: 0.010574
Epoch: 30 	Ltrain: 0.009834 	Lval: 0.009712
Epoch: 35 	Ltrain: 0.008914 	Lval: 0.008975
Epoch: 40 	Ltrain: 0.008540 	Lval: 0.008232
Epoch: 45 	Ltrain: 0.009469 	Lval: 0.007768
Epoch: 50 	Ltrain: 0.008177 	Lval: 0.007452
Epoch: 55 	Ltrain: 0.007507 	Lval: 0.007276
Epoch: 60 	Ltrain: 0.007023 	Lval: 0.007153
Epoch: 65 	Ltrain: 0.006901 	Lval: 0.007125
Epoch: 70 	Ltrain: 0.007232 	Lval: 0.006878
Epoch: 75 	Ltrain: 0.007420 	Lval: 0.006768
Epoch: 80 	Ltrain: 0.007417 	Lval: 0.006682
Epoch: 85 	Ltrain: 0.006694 	Lval: 0.006541
Epoch: 90 	Ltrain: 0.006329 	Lval: 0.006409
Epoch: 95 	Ltrain: 0.006402 	Lval: 0.006327
Epoch: 100 	Ltrain: 0.005940 	Lval: 0.006207
Epoch: 105 	Ltrain: 0.007502 	Lval: 0.006088
Epoch: 110 	Ltrain: 0.006526 	Lval: 0.005998
Epoch: 115 	Ltrain: 0.006121 	Lval: 0.005910
Epoch: 120 	Ltrain: 0.006006 	Lval: 0.005812
Epoch: 125 	Ltrain: 0.005518 	Lval: 0.005731
Epoch: 130 	Ltrain: 0.006031 	Lval: 0.005663
Epoch: 135 	Ltrain: 0.006235 	Lval: 0.005606
Epoch: 140 	Ltrain: 0.005307 	Lval: 0.005519
Epoch: 145 	Ltrain: 0.005573 	Lval: 0.005488
Epoch: 150 	Ltrain: 0.005513 	Lval: 0.005379
Epoch: 155 	Ltrain: 0.005629 	Lval: 0.005273
Epoch: 160 	Ltrain: 0.005517 	Lval: 0.005255
Epoch: 165 	Ltrain: 0.005262 	Lval: 0.005185
Epoch: 170 	Ltrain: 0.005032 	Lval: 0.005110
Epoch: 175 	Ltrain: 0.005698 	Lval: 0.005089
Epoch: 180 	Ltrain: 0.005489 	Lval: 0.005048
Epoch: 185 	Ltrain: 0.005206 	Lval: 0.004966
Epoch: 190 	Ltrain: 0.005218 	Lval: 0.004902
Epoch: 195 	Ltrain: 0.004831 	Lval: 0.004850
Epoch: 200 	Ltrain: 0.005941 	Lval: 0.004897
Epoch: 205 	Ltrain: 0.004791 	Lval: 0.004935
Epoch: 210 	Ltrain: 0.004722 	Lval: 0.004717
Epoch: 215 	Ltrain: 0.004582 	Lval: 0.004796
Epoch: 220 	Ltrain: 0.005067 	Lval: 0.004651
Epoch: 225 	Ltrain: 0.004872 	Lval: 0.004628
Epoch: 230 	Ltrain: 0.004605 	Lval: 0.004607
Epoch: 235 	Ltrain: 0.005244 	Lval: 0.004641
Epoch 00240: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 240 	Ltrain: 0.005529 	Lval: 0.004624
Epoch: 245 	Ltrain: 0.004415 	Lval: 0.004622
Epoch: 250 	Ltrain: 0.004528 	Lval: 0.004574
Epoch: 255 	Ltrain: 0.004444 	Lval: 0.004574
Epoch 00257: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 260 	Ltrain: 0.004651 	Lval: 0.004585
EarlyStopper: stopping at epoch 261 with best_val_loss = 0.004573


	Fold 2/5
Epoch: 1 	Ltrain: 0.080248 	Lval: 0.058807
Epoch: 5 	Ltrain: 0.015972 	Lval: 0.013394
Epoch: 10 	Ltrain: 0.013275 	Lval: 0.012473
Epoch: 15 	Ltrain: 0.012867 	Lval: 0.011588
Epoch: 20 	Ltrain: 0.011288 	Lval: 0.010703
Epoch: 25 	Ltrain: 0.010225 	Lval: 0.009513
Epoch: 30 	Ltrain: 0.008818 	Lval: 0.008795
Epoch: 35 	Ltrain: 0.008168 	Lval: 0.007989
Epoch: 40 	Ltrain: 0.007740 	Lval: 0.007606
Epoch: 45 	Ltrain: 0.007471 	Lval: 0.007160
Epoch: 50 	Ltrain: 0.007196 	Lval: 0.006929
Epoch: 55 	Ltrain: 0.006931 	Lval: 0.006646
Epoch: 60 	Ltrain: 0.006870 	Lval: 0.006499
Epoch: 65 	Ltrain: 0.006995 	Lval: 0.006320
Epoch: 70 	Ltrain: 0.006353 	Lval: 0.006165
Epoch: 75 	Ltrain: 0.006380 	Lval: 0.005987
Epoch: 80 	Ltrain: 0.006553 	Lval: 0.005877
Epoch: 85 	Ltrain: 0.006238 	Lval: 0.005735
Epoch: 90 	Ltrain: 0.005885 	Lval: 0.005634
Epoch: 95 	Ltrain: 0.005866 	Lval: 0.005591
Epoch: 100 	Ltrain: 0.005719 	Lval: 0.005521
Epoch 00101: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 105 	Ltrain: 0.006080 	Lval: 0.005448
Epoch: 110 	Ltrain: 0.005788 	Lval: 0.005460
Epoch: 115 	Ltrain: 0.005636 	Lval: 0.005441
Epoch 00118: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 120 	Ltrain: 0.005581 	Lval: 0.005428
Epoch: 125 	Ltrain: 0.005691 	Lval: 0.005426
Epoch 00130: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 130 	Ltrain: 0.005670 	Lval: 0.005429
Epoch: 135 	Ltrain: 0.005671 	Lval: 0.005430
Epoch: 140 	Ltrain: 0.005844 	Lval: 0.005431
Epoch 00142: reducing learning rate of group 0 to 1.2585e-08.
Epoch: 145 	Ltrain: 0.005907 	Lval: 0.005432
EarlyStopper: stopping at epoch 146 with best_val_loss = 0.005425


	Fold 3/5
Epoch: 1 	Ltrain: 0.072443 	Lval: 0.048018
Epoch: 5 	Ltrain: 0.014784 	Lval: 0.012935
Epoch: 10 	Ltrain: 0.012472 	Lval: 0.011405
Epoch: 15 	Ltrain: 0.010843 	Lval: 0.009943
Epoch: 20 	Ltrain: 0.009366 	Lval: 0.008711
Epoch: 25 	Ltrain: 0.008303 	Lval: 0.007970
Epoch: 30 	Ltrain: 0.007882 	Lval: 0.007455
Epoch: 35 	Ltrain: 0.007442 	Lval: 0.007025
Epoch: 40 	Ltrain: 0.007137 	Lval: 0.006770
Epoch: 45 	Ltrain: 0.006774 	Lval: 0.006550
Epoch: 50 	Ltrain: 0.006609 	Lval: 0.006234
Epoch: 55 	Ltrain: 0.006330 	Lval: 0.006050
Epoch: 60 	Ltrain: 0.006188 	Lval: 0.005796
Epoch: 65 	Ltrain: 0.006185 	Lval: 0.005636
Epoch: 70 	Ltrain: 0.005900 	Lval: 0.005704
Epoch: 75 	Ltrain: 0.005675 	Lval: 0.005448
Epoch: 80 	Ltrain: 0.005522 	Lval: 0.005182
Epoch: 85 	Ltrain: 0.005374 	Lval: 0.005044
Epoch: 90 	Ltrain: 0.005320 	Lval: 0.005096
Epoch: 95 	Ltrain: 0.005178 	Lval: 0.004888
Epoch: 100 	Ltrain: 0.005049 	Lval: 0.004819
Epoch: 105 	Ltrain: 0.005135 	Lval: 0.004841
Epoch: 110 	Ltrain: 0.004948 	Lval: 0.004718
Epoch: 115 	Ltrain: 0.004888 	Lval: 0.004555
Epoch: 120 	Ltrain: 0.004847 	Lval: 0.004516
Epoch: 125 	Ltrain: 0.004788 	Lval: 0.004489
Epoch: 130 	Ltrain: 0.004746 	Lval: 0.004432
Epoch 00132: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 135 	Ltrain: 0.004701 	Lval: 0.004436
Epoch: 140 	Ltrain: 0.004592 	Lval: 0.004436
Epoch 00144: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 145 	Ltrain: 0.004711 	Lval: 0.004448
Epoch: 150 	Ltrain: 0.004640 	Lval: 0.004439
EarlyStopper: stopping at epoch 153 with best_val_loss = 0.004427


	Fold 4/5
Epoch: 1 	Ltrain: 0.066762 	Lval: 0.040489
Epoch: 5 	Ltrain: 0.014334 	Lval: 0.013546
Epoch: 10 	Ltrain: 0.011924 	Lval: 0.011737
Epoch: 15 	Ltrain: 0.010040 	Lval: 0.010022
Epoch: 20 	Ltrain: 0.008451 	Lval: 0.008511
Epoch: 25 	Ltrain: 0.007549 	Lval: 0.007569
Epoch: 30 	Ltrain: 0.006858 	Lval: 0.006870
Epoch: 35 	Ltrain: 0.006576 	Lval: 0.006452
Epoch: 40 	Ltrain: 0.006113 	Lval: 0.006095
Epoch: 45 	Ltrain: 0.005929 	Lval: 0.005824
Epoch: 50 	Ltrain: 0.005771 	Lval: 0.005575
Epoch: 55 	Ltrain: 0.005484 	Lval: 0.005373
Epoch: 60 	Ltrain: 0.005273 	Lval: 0.005178
Epoch: 65 	Ltrain: 0.005144 	Lval: 0.005049
Epoch: 70 	Ltrain: 0.004966 	Lval: 0.004933
Epoch: 75 	Ltrain: 0.004977 	Lval: 0.004741
Epoch: 80 	Ltrain: 0.004748 	Lval: 0.004676
Epoch: 85 	Ltrain: 0.004700 	Lval: 0.004611
Epoch: 90 	Ltrain: 0.004663 	Lval: 0.004706
Epoch: 95 	Ltrain: 0.004492 	Lval: 0.004511
Epoch: 100 	Ltrain: 0.004474 	Lval: 0.004482
Epoch: 105 	Ltrain: 0.004432 	Lval: 0.004453
Epoch 00107: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 110 	Ltrain: 0.004378 	Lval: 0.004401
Epoch: 115 	Ltrain: 0.004432 	Lval: 0.004397
Epoch: 120 	Ltrain: 0.004390 	Lval: 0.004403
Epoch: 125 	Ltrain: 0.004411 	Lval: 0.004387
Epoch: 130 	Ltrain: 0.004384 	Lval: 0.004383
Epoch 00131: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 135 	Ltrain: 0.004409 	Lval: 0.004389
Epoch: 140 	Ltrain: 0.004425 	Lval: 0.004385
Epoch 00143: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 145 	Ltrain: 0.004366 	Lval: 0.004386
Epoch: 150 	Ltrain: 0.004358 	Lval: 0.004386
EarlyStopper: stopping at epoch 152 with best_val_loss = 0.004379


	Fold 5/5
Epoch: 1 	Ltrain: 0.091027 	Lval: 0.063645
Epoch: 5 	Ltrain: 0.013594 	Lval: 0.013762
Epoch: 10 	Ltrain: 0.010803 	Lval: 0.011559
Epoch: 15 	Ltrain: 0.008737 	Lval: 0.009446
Epoch: 20 	Ltrain: 0.007690 	Lval: 0.008363
Epoch: 25 	Ltrain: 0.007060 	Lval: 0.007755
Epoch: 30 	Ltrain: 0.006736 	Lval: 0.007293
Epoch: 35 	Ltrain: 0.006386 	Lval: 0.006931
Epoch: 40 	Ltrain: 0.006128 	Lval: 0.006676
Epoch: 45 	Ltrain: 0.005850 	Lval: 0.006409
Epoch: 50 	Ltrain: 0.005687 	Lval: 0.006127
Epoch: 55 	Ltrain: 0.005521 	Lval: 0.005916
Epoch: 60 	Ltrain: 0.005418 	Lval: 0.005692
Epoch: 65 	Ltrain: 0.005233 	Lval: 0.005477
Epoch: 70 	Ltrain: 0.005154 	Lval: 0.005344
Epoch: 75 	Ltrain: 0.005004 	Lval: 0.005127
Epoch: 80 	Ltrain: 0.004812 	Lval: 0.005024
Epoch: 85 	Ltrain: 0.004804 	Lval: 0.004936
Epoch: 90 	Ltrain: 0.004639 	Lval: 0.004911
Epoch: 95 	Ltrain: 0.004590 	Lval: 0.004772
Epoch: 100 	Ltrain: 0.004615 	Lval: 0.004684
Epoch: 105 	Ltrain: 0.004509 	Lval: 0.004620
Epoch: 110 	Ltrain: 0.004483 	Lval: 0.004594
Epoch: 115 	Ltrain: 0.004393 	Lval: 0.004555
Epoch: 120 	Ltrain: 0.004436 	Lval: 0.004551
Epoch: 125 	Ltrain: 0.004415 	Lval: 0.004535
Epoch 00126: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 130 	Ltrain: 0.004317 	Lval: 0.004494
Epoch: 135 	Ltrain: 0.004336 	Lval: 0.004495
Epoch 00138: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 140 	Ltrain: 0.004291 	Lval: 0.004495
Epoch: 145 	Ltrain: 0.004346 	Lval: 0.004493
Epoch 00150: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 150 	Ltrain: 0.004356 	Lval: 0.004494
EarlyStopper: stopping at epoch 152 with best_val_loss = 0.004494

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.002622655022886898
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.390321341492203e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.045863 	Lval: 0.015489
Epoch: 5 	Ltrain: 0.013951 	Lval: 0.012673
Epoch: 10 	Ltrain: 0.008845 	Lval: 0.008721
Epoch: 15 	Ltrain: 0.007807 	Lval: 0.006494
Epoch: 20 	Ltrain: 0.005634 	Lval: 0.006351
Epoch: 25 	Ltrain: 0.005165 	Lval: 0.004834
Epoch: 30 	Ltrain: 0.005706 	Lval: 0.004635
Epoch: 35 	Ltrain: 0.005170 	Lval: 0.004973
Epoch 00037: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 40 	Ltrain: 0.004446 	Lval: 0.004425
Epoch: 45 	Ltrain: 0.004467 	Lval: 0.004358
Epoch: 50 	Ltrain: 0.004389 	Lval: 0.004332
Epoch 00054: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 55 	Ltrain: 0.004539 	Lval: 0.004347
Epoch: 60 	Ltrain: 0.004432 	Lval: 0.004322
Epoch: 65 	Ltrain: 0.004299 	Lval: 0.004319
Epoch 00068: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 70 	Ltrain: 0.004455 	Lval: 0.004318
Epoch: 75 	Ltrain: 0.004974 	Lval: 0.004318
Epoch 00080: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 80 	Ltrain: 0.004251 	Lval: 0.004319
Epoch: 85 	Ltrain: 0.004595 	Lval: 0.004319
EarlyStopper: stopping at epoch 86 with best_val_loss = 0.004322


	Fold 2/5
Epoch: 1 	Ltrain: 0.022368 	Lval: 0.013528
Epoch: 5 	Ltrain: 0.009220 	Lval: 0.008430
Epoch: 10 	Ltrain: 0.005851 	Lval: 0.005012
Epoch: 15 	Ltrain: 0.004772 	Lval: 0.004499
Epoch: 20 	Ltrain: 0.004707 	Lval: 0.004692
Epoch 00022: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 25 	Ltrain: 0.004303 	Lval: 0.004145
Epoch: 30 	Ltrain: 0.004232 	Lval: 0.004073
Epoch 00034: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 35 	Ltrain: 0.004133 	Lval: 0.004087
Epoch: 40 	Ltrain: 0.004188 	Lval: 0.004065
Epoch: 45 	Ltrain: 0.004136 	Lval: 0.004068
Epoch 00046: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 50 	Ltrain: 0.004080 	Lval: 0.004052
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.004018


	Fold 3/5
Epoch: 1 	Ltrain: 0.029620 	Lval: 0.014511
Epoch: 5 	Ltrain: 0.008087 	Lval: 0.006262
Epoch: 10 	Ltrain: 0.005494 	Lval: 0.004668
Epoch: 15 	Ltrain: 0.005214 	Lval: 0.004535
Epoch: 20 	Ltrain: 0.004429 	Lval: 0.004411
Epoch 00023: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 25 	Ltrain: 0.004095 	Lval: 0.003930
Epoch: 30 	Ltrain: 0.003963 	Lval: 0.004017
Epoch: 35 	Ltrain: 0.004020 	Lval: 0.003920
Epoch 00039: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 40 	Ltrain: 0.003973 	Lval: 0.003972
Epoch: 45 	Ltrain: 0.003986 	Lval: 0.003954
Epoch: 50 	Ltrain: 0.004039 	Lval: 0.003948
Epoch 00051: reducing learning rate of group 0 to 2.6227e-06.
EarlyStopper: stopping at epoch 51 with best_val_loss = 0.003930


	Fold 4/5
Epoch: 1 	Ltrain: 0.019736 	Lval: 0.013580
Epoch: 5 	Ltrain: 0.005925 	Lval: 0.005018
Epoch: 10 	Ltrain: 0.004837 	Lval: 0.004412
Epoch: 15 	Ltrain: 0.004482 	Lval: 0.004185
Epoch: 20 	Ltrain: 0.003950 	Lval: 0.004141
Epoch: 25 	Ltrain: 0.003863 	Lval: 0.003733
Epoch: 30 	Ltrain: 0.003530 	Lval: 0.003712
Epoch: 35 	Ltrain: 0.003296 	Lval: 0.003480
Epoch: 40 	Ltrain: 0.003149 	Lval: 0.003202
Epoch 00044: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 45 	Ltrain: 0.003062 	Lval: 0.003062
Epoch: 50 	Ltrain: 0.002751 	Lval: 0.003002
Epoch: 55 	Ltrain: 0.002712 	Lval: 0.002959
Epoch: 60 	Ltrain: 0.002693 	Lval: 0.002938
Epoch: 65 	Ltrain: 0.002659 	Lval: 0.002908
Epoch: 70 	Ltrain: 0.002660 	Lval: 0.002889
Epoch: 75 	Ltrain: 0.002646 	Lval: 0.002845
Epoch: 80 	Ltrain: 0.002567 	Lval: 0.002825
Epoch 00083: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 85 	Ltrain: 0.002528 	Lval: 0.002810
Epoch: 90 	Ltrain: 0.002481 	Lval: 0.002805
Epoch: 95 	Ltrain: 0.002514 	Lval: 0.002807
Epoch 00096: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 100 	Ltrain: 0.002500 	Lval: 0.002804
Epoch: 105 	Ltrain: 0.002507 	Lval: 0.002805
Epoch 00108: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 110 	Ltrain: 0.002512 	Lval: 0.002804
EarlyStopper: stopping at epoch 111 with best_val_loss = 0.002810


	Fold 5/5
Epoch: 1 	Ltrain: 0.026137 	Lval: 0.015023
Epoch: 5 	Ltrain: 0.005970 	Lval: 0.006602
Epoch: 10 	Ltrain: 0.004979 	Lval: 0.005044
Epoch: 15 	Ltrain: 0.004272 	Lval: 0.004459
Epoch: 20 	Ltrain: 0.003951 	Lval: 0.004328
Epoch: 25 	Ltrain: 0.003951 	Lval: 0.004392
Epoch 00026: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 30 	Ltrain: 0.003490 	Lval: 0.003919
Epoch: 35 	Ltrain: 0.003480 	Lval: 0.003920
Epoch: 40 	Ltrain: 0.003427 	Lval: 0.003862
Epoch: 45 	Ltrain: 0.003392 	Lval: 0.003854
Epoch: 50 	Ltrain: 0.003430 	Lval: 0.003834
Epoch: 55 	Ltrain: 0.003336 	Lval: 0.003836
Epoch: 60 	Ltrain: 0.003305 	Lval: 0.003785
Epoch: 65 	Ltrain: 0.003271 	Lval: 0.003748
Epoch: 70 	Ltrain: 0.003316 	Lval: 0.003727
Epoch 00075: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 75 	Ltrain: 0.003278 	Lval: 0.003712
Epoch: 80 	Ltrain: 0.003151 	Lval: 0.003658
Epoch: 85 	Ltrain: 0.003189 	Lval: 0.003655
Epoch: 90 	Ltrain: 0.003165 	Lval: 0.003655
Epoch: 95 	Ltrain: 0.003199 	Lval: 0.003646
Epoch 00097: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 100 	Ltrain: 0.003166 	Lval: 0.003645
Epoch: 105 	Ltrain: 0.003179 	Lval: 0.003645
Epoch 00109: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 110 	Ltrain: 0.003141 	Lval: 0.003642
Epoch: 115 	Ltrain: 0.003178 	Lval: 0.003642
Epoch: 120 	Ltrain: 0.003128 	Lval: 0.003642
EarlyStopper: stopping at epoch 119 with best_val_loss = 0.003642

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0036708403685720837
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.036796529397564e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.043594 	Lval: 0.023086
Epoch: 5 	Ltrain: 0.014700 	Lval: 0.014320
Epoch: 10 	Ltrain: 0.024327 	Lval: 0.010631
Epoch: 15 	Ltrain: 0.009577 	Lval: 0.008344
Epoch: 20 	Ltrain: 0.007919 	Lval: 0.006608
Epoch: 25 	Ltrain: 0.007346 	Lval: 0.006554
Epoch: 30 	Ltrain: 0.005717 	Lval: 0.005459
Epoch: 35 	Ltrain: 0.005454 	Lval: 0.005020
Epoch: 40 	Ltrain: 0.005456 	Lval: 0.004835
Epoch: 45 	Ltrain: 0.004983 	Lval: 0.004677
Epoch: 50 	Ltrain: 0.005352 	Lval: 0.004645
Epoch: 55 	Ltrain: 0.004540 	Lval: 0.004599
Epoch: 60 	Ltrain: 0.004767 	Lval: 0.004380
Epoch: 65 	Ltrain: 0.005744 	Lval: 0.004441
Epoch 00066: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 70 	Ltrain: 0.004789 	Lval: 0.004233
Epoch: 75 	Ltrain: 0.005380 	Lval: 0.004276
Epoch: 80 	Ltrain: 0.004438 	Lval: 0.004154
Epoch 00085: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 85 	Ltrain: 0.005071 	Lval: 0.004161
Epoch: 90 	Ltrain: 0.005503 	Lval: 0.004167
Epoch: 95 	Ltrain: 0.004312 	Lval: 0.004172
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.004159


	Fold 2/5
Epoch: 1 	Ltrain: 0.027899 	Lval: 0.018741
Epoch: 5 	Ltrain: 0.010842 	Lval: 0.009248
Epoch: 10 	Ltrain: 0.007640 	Lval: 0.006247
Epoch: 15 	Ltrain: 0.005755 	Lval: 0.005294
Epoch: 20 	Ltrain: 0.005444 	Lval: 0.004957
Epoch: 25 	Ltrain: 0.005210 	Lval: 0.005600
Epoch 00030: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 30 	Ltrain: 0.005360 	Lval: 0.004689
Epoch: 35 	Ltrain: 0.004383 	Lval: 0.004359
Epoch: 40 	Ltrain: 0.004806 	Lval: 0.004255
Epoch 00044: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 45 	Ltrain: 0.004953 	Lval: 0.004308
Epoch: 50 	Ltrain: 0.004372 	Lval: 0.004296
Epoch: 55 	Ltrain: 0.004824 	Lval: 0.004312
Epoch 00056: reducing learning rate of group 0 to 3.6708e-06.
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.004255


	Fold 3/5
Epoch: 1 	Ltrain: 0.017012 	Lval: 0.012143
Epoch: 5 	Ltrain: 0.007535 	Lval: 0.005882
Epoch: 10 	Ltrain: 0.005805 	Lval: 0.005827
Epoch: 15 	Ltrain: 0.005064 	Lval: 0.004456
Epoch: 20 	Ltrain: 0.004785 	Lval: 0.004249
Epoch: 25 	Ltrain: 0.004644 	Lval: 0.004084
Epoch: 30 	Ltrain: 0.004402 	Lval: 0.004117
Epoch 00033: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 35 	Ltrain: 0.004577 	Lval: 0.003907
Epoch: 40 	Ltrain: 0.004417 	Lval: 0.003968
Epoch 00045: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 45 	Ltrain: 0.004559 	Lval: 0.003954
Epoch: 50 	Ltrain: 0.004233 	Lval: 0.003949
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.003907


	Fold 4/5
Epoch: 1 	Ltrain: 0.019040 	Lval: 0.010973
Epoch: 5 	Ltrain: 0.006733 	Lval: 0.006137
Epoch: 10 	Ltrain: 0.005294 	Lval: 0.004981
Epoch: 15 	Ltrain: 0.004707 	Lval: 0.004263
Epoch: 20 	Ltrain: 0.004474 	Lval: 0.004259
Epoch 00023: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 25 	Ltrain: 0.004322 	Lval: 0.004031
Epoch: 30 	Ltrain: 0.004079 	Lval: 0.004025
Epoch: 35 	Ltrain: 0.004188 	Lval: 0.004006
Epoch: 40 	Ltrain: 0.004135 	Lval: 0.004016
EarlyStopper: stopping at epoch 39 with best_val_loss = 0.004007


	Fold 5/5
Epoch: 1 	Ltrain: 0.051411 	Lval: 0.015911
Epoch: 5 	Ltrain: 0.010427 	Lval: 0.009317
Epoch: 10 	Ltrain: 0.006379 	Lval: 0.006955
Epoch: 15 	Ltrain: 0.005629 	Lval: 0.005820
Epoch: 20 	Ltrain: 0.005055 	Lval: 0.004945
Epoch: 25 	Ltrain: 0.004517 	Lval: 0.004664
Epoch 00030: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 30 	Ltrain: 0.004458 	Lval: 0.004736
Epoch: 35 	Ltrain: 0.004387 	Lval: 0.004477
Epoch: 40 	Ltrain: 0.004344 	Lval: 0.004460
Epoch: 45 	Ltrain: 0.004407 	Lval: 0.004430
Epoch: 50 	Ltrain: 0.004268 	Lval: 0.004427
Epoch: 55 	Ltrain: 0.004349 	Lval: 0.004412
Epoch: 60 	Ltrain: 0.004217 	Lval: 0.004408
Epoch: 65 	Ltrain: 0.004295 	Lval: 0.004399
Epoch: 70 	Ltrain: 0.004203 	Lval: 0.004395
Epoch 00071: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 75 	Ltrain: 0.004138 	Lval: 0.004394
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.004399

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0028563448143027245
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.8400362376385822e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 10
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.027980 	Lval: 0.016301
Epoch: 5 	Ltrain: 0.012790 	Lval: 0.011418
Epoch: 10 	Ltrain: 0.008667 	Lval: 0.007611
Epoch: 15 	Ltrain: 0.006098 	Lval: 0.006020
Epoch: 20 	Ltrain: 0.005602 	Lval: 0.005240
Epoch 00024: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 25 	Ltrain: 0.005558 	Lval: 0.005096
Epoch: 30 	Ltrain: 0.005029 	Lval: 0.004973
Epoch: 35 	Ltrain: 0.005068 	Lval: 0.004950
Epoch: 40 	Ltrain: 0.004880 	Lval: 0.004952
Epoch: 45 	Ltrain: 0.005037 	Lval: 0.004906
Epoch: 50 	Ltrain: 0.004809 	Lval: 0.004890
Epoch: 55 	Ltrain: 0.005584 	Lval: 0.004862
Epoch: 60 	Ltrain: 0.005356 	Lval: 0.004828
Epoch: 65 	Ltrain: 0.004860 	Lval: 0.004810
Epoch: 70 	Ltrain: 0.005136 	Lval: 0.004792
Epoch: 75 	Ltrain: 0.005072 	Lval: 0.004754
Epoch: 80 	Ltrain: 0.005080 	Lval: 0.004754
Epoch: 85 	Ltrain: 0.004791 	Lval: 0.004714
Epoch: 90 	Ltrain: 0.004967 	Lval: 0.004693
Epoch 00093: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 95 	Ltrain: 0.004688 	Lval: 0.004697
Epoch: 100 	Ltrain: 0.004964 	Lval: 0.004689
Epoch: 105 	Ltrain: 0.004800 	Lval: 0.004683
EarlyStopper: stopping at epoch 105 with best_val_loss = 0.004686


	Fold 2/5
Epoch: 1 	Ltrain: 0.032199 	Lval: 0.014316
Epoch: 5 	Ltrain: 0.007875 	Lval: 0.006940
Epoch: 10 	Ltrain: 0.005907 	Lval: 0.005244
Epoch: 15 	Ltrain: 0.004921 	Lval: 0.005701
Epoch: 20 	Ltrain: 0.004827 	Lval: 0.004490
Epoch: 25 	Ltrain: 0.004757 	Lval: 0.004530
Epoch 00028: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 30 	Ltrain: 0.004479 	Lval: 0.004492
Epoch: 35 	Ltrain: 0.004532 	Lval: 0.004393
EarlyStopper: stopping at epoch 38 with best_val_loss = 0.004343


	Fold 3/5
Epoch: 1 	Ltrain: 0.016228 	Lval: 0.011676
Epoch: 5 	Ltrain: 0.006660 	Lval: 0.005601
Epoch: 10 	Ltrain: 0.005281 	Lval: 0.004666
Epoch: 15 	Ltrain: 0.004866 	Lval: 0.005070
Epoch 00016: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 20 	Ltrain: 0.004483 	Lval: 0.004319
Epoch: 25 	Ltrain: 0.004445 	Lval: 0.004368
Epoch 00028: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 30 	Ltrain: 0.004439 	Lval: 0.004327
EarlyStopper: stopping at epoch 29 with best_val_loss = 0.004319


	Fold 4/5
Epoch: 1 	Ltrain: 0.021718 	Lval: 0.012888
Epoch: 5 	Ltrain: 0.006125 	Lval: 0.005956
Epoch: 10 	Ltrain: 0.004856 	Lval: 0.004892
Epoch 00013: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 15 	Ltrain: 0.004390 	Lval: 0.004507
Epoch: 20 	Ltrain: 0.004377 	Lval: 0.004513
Epoch: 25 	Ltrain: 0.004357 	Lval: 0.004444
Epoch 00029: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 30 	Ltrain: 0.004343 	Lval: 0.004448
EarlyStopper: stopping at epoch 33 with best_val_loss = 0.004454


	Fold 5/5
Epoch: 1 	Ltrain: 0.023185 	Lval: 0.012468
Epoch: 5 	Ltrain: 0.006053 	Lval: 0.005965
Epoch: 10 	Ltrain: 0.004942 	Lval: 0.004881
Epoch: 15 	Ltrain: 0.004808 	Lval: 0.004685
Epoch 00019: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 20 	Ltrain: 0.004407 	Lval: 0.004554
Epoch: 25 	Ltrain: 0.004313 	Lval: 0.004598
Epoch: 30 	Ltrain: 0.004290 	Lval: 0.004536
Epoch: 35 	Ltrain: 0.004258 	Lval: 0.004535
Epoch 00038: reducing learning rate of group 0 to 2.8563e-05.
EarlyStopper: stopping at epoch 37 with best_val_loss = 0.004519

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00044590431323154054
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.414035159965753e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.037473 	Lval: 0.025852
Epoch: 5 	Ltrain: 0.021160 	Lval: 0.014029
Epoch 00010: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.014559 	Lval: 0.014094
Epoch: 15 	Ltrain: 0.015052 	Lval: 0.013781
Epoch: 20 	Ltrain: 0.015197 	Lval: 0.013423
Epoch: 25 	Ltrain: 0.017956 	Lval: 0.013246
Epoch: 30 	Ltrain: 0.016106 	Lval: 0.013089
Epoch: 35 	Ltrain: 0.016311 	Lval: 0.012958
Epoch: 40 	Ltrain: 0.014541 	Lval: 0.012921
Epoch: 45 	Ltrain: 0.013444 	Lval: 0.012890
Epoch: 50 	Ltrain: 0.017836 	Lval: 0.012883
Epoch: 55 	Ltrain: 0.018564 	Lval: 0.012818
Epoch: 60 	Ltrain: 0.012274 	Lval: 0.012748
Epoch: 65 	Ltrain: 0.013961 	Lval: 0.012689
Epoch: 70 	Ltrain: 0.012536 	Lval: 0.012617
Epoch: 75 	Ltrain: 0.013456 	Lval: 0.012587
Epoch: 80 	Ltrain: 0.013768 	Lval: 0.012540
Epoch: 85 	Ltrain: 0.015922 	Lval: 0.012487
Epoch: 90 	Ltrain: 0.014093 	Lval: 0.012446
Epoch: 95 	Ltrain: 0.013327 	Lval: 0.012383
Epoch: 100 	Ltrain: 0.014053 	Lval: 0.012327
Epoch: 105 	Ltrain: 0.017201 	Lval: 0.012259
Epoch: 110 	Ltrain: 0.014101 	Lval: 0.012214
Epoch: 115 	Ltrain: 0.013922 	Lval: 0.012186
Epoch: 120 	Ltrain: 0.012473 	Lval: 0.012129
Epoch: 125 	Ltrain: 0.016387 	Lval: 0.012088
Epoch: 130 	Ltrain: 0.014122 	Lval: 0.012073
Epoch: 135 	Ltrain: 0.012195 	Lval: 0.012006
Epoch: 140 	Ltrain: 0.014128 	Lval: 0.011900
Epoch: 145 	Ltrain: 0.011858 	Lval: 0.011846
Epoch: 150 	Ltrain: 0.012769 	Lval: 0.011819
Epoch: 155 	Ltrain: 0.014979 	Lval: 0.011743
Epoch: 160 	Ltrain: 0.013344 	Lval: 0.011671
Epoch: 165 	Ltrain: 0.014940 	Lval: 0.011612
Epoch: 170 	Ltrain: 0.020519 	Lval: 0.011591
Epoch 00173: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 175 	Ltrain: 0.012870 	Lval: 0.011603
Epoch: 180 	Ltrain: 0.013332 	Lval: 0.011585
Epoch: 185 	Ltrain: 0.017928 	Lval: 0.011574
Epoch: 190 	Ltrain: 0.011589 	Lval: 0.011565
Epoch: 195 	Ltrain: 0.012593 	Lval: 0.011557
Epoch: 200 	Ltrain: 0.012573 	Lval: 0.011548
Epoch: 205 	Ltrain: 0.013441 	Lval: 0.011539
Epoch: 210 	Ltrain: 0.014022 	Lval: 0.011525
Epoch: 215 	Ltrain: 0.013900 	Lval: 0.011520
Epoch: 220 	Ltrain: 0.011600 	Lval: 0.011514
Epoch: 225 	Ltrain: 0.012988 	Lval: 0.011501
Epoch: 230 	Ltrain: 0.014400 	Lval: 0.011487
Epoch: 235 	Ltrain: 0.013520 	Lval: 0.011474
Epoch: 240 	Ltrain: 0.014522 	Lval: 0.011465
Epoch: 245 	Ltrain: 0.017814 	Lval: 0.011453
Epoch: 250 	Ltrain: 0.015366 	Lval: 0.011446
Epoch: 255 	Ltrain: 0.013295 	Lval: 0.011437
Epoch: 260 	Ltrain: 0.012040 	Lval: 0.011429
Epoch: 265 	Ltrain: 0.012769 	Lval: 0.011418
Epoch: 270 	Ltrain: 0.016041 	Lval: 0.011412
Epoch 00271: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 275 	Ltrain: 0.012966 	Lval: 0.011412
Epoch: 280 	Ltrain: 0.014064 	Lval: 0.011411
Epoch: 285 	Ltrain: 0.014320 	Lval: 0.011409
Epoch 00287: reducing learning rate of group 0 to 4.4590e-08.
Epoch: 290 	Ltrain: 0.013493 	Lval: 0.011409
Epoch: 295 	Ltrain: 0.011452 	Lval: 0.011409
Epoch 00299: reducing learning rate of group 0 to 4.4590e-09.
Epoch: 300 	Ltrain: 0.012158 	Lval: 0.011409
Epoch: 305 	Ltrain: 0.012911 	Lval: 0.011409
EarlyStopper: stopping at epoch 306 with best_val_loss = 0.011409


	Fold 2/5
Epoch: 1 	Ltrain: 0.054482 	Lval: 0.033234
Epoch: 5 	Ltrain: 0.017807 	Lval: 0.014263
Epoch 00007: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.014597 	Lval: 0.013222
Epoch: 15 	Ltrain: 0.014032 	Lval: 0.013034
Epoch: 20 	Ltrain: 0.013760 	Lval: 0.012844
Epoch: 25 	Ltrain: 0.014134 	Lval: 0.012722
Epoch: 30 	Ltrain: 0.014087 	Lval: 0.012604
Epoch: 35 	Ltrain: 0.013473 	Lval: 0.012588
Epoch 00039: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 40 	Ltrain: 0.014168 	Lval: 0.012618
Epoch: 45 	Ltrain: 0.013407 	Lval: 0.012582
Epoch: 50 	Ltrain: 0.013221 	Lval: 0.012545
Epoch: 55 	Ltrain: 0.015149 	Lval: 0.012521
Epoch: 60 	Ltrain: 0.013779 	Lval: 0.012488
Epoch: 65 	Ltrain: 0.014926 	Lval: 0.012469
Epoch: 70 	Ltrain: 0.012887 	Lval: 0.012460
Epoch 00075: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 75 	Ltrain: 0.013021 	Lval: 0.012456
Epoch: 80 	Ltrain: 0.013919 	Lval: 0.012455
Epoch: 85 	Ltrain: 0.013979 	Lval: 0.012457
Epoch 00087: reducing learning rate of group 0 to 4.4590e-08.
Epoch: 90 	Ltrain: 0.013091 	Lval: 0.012457
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.012457


	Fold 3/5
Epoch: 1 	Ltrain: 0.106656 	Lval: 0.067450
Epoch: 5 	Ltrain: 0.019372 	Lval: 0.014684
Epoch: 10 	Ltrain: 0.015101 	Lval: 0.012324
Epoch: 15 	Ltrain: 0.014188 	Lval: 0.011086
Epoch: 20 	Ltrain: 0.011179 	Lval: 0.009601
Epoch: 25 	Ltrain: 0.010108 	Lval: 0.008688
Epoch: 30 	Ltrain: 0.008895 	Lval: 0.007983
Epoch: 35 	Ltrain: 0.008418 	Lval: 0.007592
Epoch: 40 	Ltrain: 0.007834 	Lval: 0.007196
Epoch: 45 	Ltrain: 0.007817 	Lval: 0.007053
Epoch: 50 	Ltrain: 0.007551 	Lval: 0.006731
Epoch: 55 	Ltrain: 0.006997 	Lval: 0.006338
Epoch: 60 	Ltrain: 0.006801 	Lval: 0.006315
Epoch: 65 	Ltrain: 0.006850 	Lval: 0.006063
Epoch: 70 	Ltrain: 0.006736 	Lval: 0.005911
Epoch: 75 	Ltrain: 0.006287 	Lval: 0.005672
Epoch: 80 	Ltrain: 0.006390 	Lval: 0.005470
Epoch: 85 	Ltrain: 0.006395 	Lval: 0.005359
Epoch: 90 	Ltrain: 0.005737 	Lval: 0.005211
Epoch: 95 	Ltrain: 0.006220 	Lval: 0.005137
Epoch: 100 	Ltrain: 0.005684 	Lval: 0.005010
Epoch: 105 	Ltrain: 0.005620 	Lval: 0.005008
Epoch: 110 	Ltrain: 0.005537 	Lval: 0.004843
Epoch: 115 	Ltrain: 0.005564 	Lval: 0.004748
Epoch: 120 	Ltrain: 0.005451 	Lval: 0.004758
Epoch 00123: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 125 	Ltrain: 0.005259 	Lval: 0.004827
Epoch: 130 	Ltrain: 0.005566 	Lval: 0.004766
Epoch 00135: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 135 	Ltrain: 0.005510 	Lval: 0.004752
Epoch: 140 	Ltrain: 0.005175 	Lval: 0.004747
EarlyStopper: stopping at epoch 140 with best_val_loss = 0.004705


	Fold 4/5
Epoch: 1 	Ltrain: 0.057396 	Lval: 0.030843
Epoch: 5 	Ltrain: 0.013916 	Lval: 0.011787
Epoch: 10 	Ltrain: 0.011295 	Lval: 0.009205
Epoch: 15 	Ltrain: 0.008550 	Lval: 0.008199
Epoch: 20 	Ltrain: 0.007909 	Lval: 0.007732
Epoch: 25 	Ltrain: 0.007264 	Lval: 0.007238
Epoch: 30 	Ltrain: 0.006979 	Lval: 0.007013
Epoch: 35 	Ltrain: 0.007248 	Lval: 0.006706
Epoch: 40 	Ltrain: 0.006858 	Lval: 0.006441
Epoch: 45 	Ltrain: 0.006392 	Lval: 0.006266
Epoch: 50 	Ltrain: 0.006169 	Lval: 0.006162
Epoch: 55 	Ltrain: 0.006259 	Lval: 0.005658
Epoch: 60 	Ltrain: 0.005793 	Lval: 0.005483
Epoch: 65 	Ltrain: 0.005679 	Lval: 0.005225
Epoch: 70 	Ltrain: 0.005595 	Lval: 0.005091
Epoch: 75 	Ltrain: 0.005278 	Lval: 0.004984
Epoch: 80 	Ltrain: 0.005162 	Lval: 0.004817
Epoch: 85 	Ltrain: 0.005158 	Lval: 0.004685
Epoch 00090: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 90 	Ltrain: 0.005029 	Lval: 0.004814
Epoch: 95 	Ltrain: 0.004958 	Lval: 0.004596
Epoch: 100 	Ltrain: 0.004930 	Lval: 0.004585
Epoch: 105 	Ltrain: 0.005006 	Lval: 0.004603
Epoch 00106: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 110 	Ltrain: 0.004852 	Lval: 0.004574
Epoch: 115 	Ltrain: 0.004972 	Lval: 0.004575
Epoch 00118: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 120 	Ltrain: 0.004740 	Lval: 0.004581
EarlyStopper: stopping at epoch 123 with best_val_loss = 0.004579


	Fold 5/5
Epoch: 1 	Ltrain: 0.080504 	Lval: 0.053556
Epoch: 5 	Ltrain: 0.014093 	Lval: 0.014897
Epoch: 10 	Ltrain: 0.011838 	Lval: 0.012901
Epoch: 15 	Ltrain: 0.009604 	Lval: 0.010940
Epoch: 20 	Ltrain: 0.008007 	Lval: 0.009209
Epoch: 25 	Ltrain: 0.007525 	Lval: 0.008419
Epoch: 30 	Ltrain: 0.008047 	Lval: 0.007876
Epoch: 35 	Ltrain: 0.006848 	Lval: 0.007587
Epoch: 40 	Ltrain: 0.006754 	Lval: 0.007307
Epoch: 45 	Ltrain: 0.006418 	Lval: 0.007038
Epoch: 50 	Ltrain: 0.006264 	Lval: 0.006751
Epoch: 55 	Ltrain: 0.006145 	Lval: 0.006413
Epoch: 60 	Ltrain: 0.005784 	Lval: 0.006210
Epoch: 65 	Ltrain: 0.005690 	Lval: 0.006006
Epoch: 70 	Ltrain: 0.005875 	Lval: 0.005734
Epoch: 75 	Ltrain: 0.005572 	Lval: 0.005582
Epoch: 80 	Ltrain: 0.005428 	Lval: 0.005442
Epoch: 85 	Ltrain: 0.005227 	Lval: 0.005391
Epoch: 90 	Ltrain: 0.005078 	Lval: 0.005203
Epoch: 95 	Ltrain: 0.005160 	Lval: 0.005116
Epoch 00100: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 100 	Ltrain: 0.005098 	Lval: 0.005248
Epoch: 105 	Ltrain: 0.004954 	Lval: 0.005068
Epoch: 110 	Ltrain: 0.004860 	Lval: 0.005057
Epoch 00112: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 115 	Ltrain: 0.005001 	Lval: 0.005057
Epoch: 120 	Ltrain: 0.004980 	Lval: 0.005054
Epoch 00124: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 125 	Ltrain: 0.005049 	Lval: 0.005051
EarlyStopper: stopping at epoch 124 with best_val_loss = 0.005055

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0002488305190859649
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.13621701509778e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.025682 	Lval: 0.019192
Epoch: 5 	Ltrain: 0.014155 	Lval: 0.013954
Epoch: 10 	Ltrain: 0.013145 	Lval: 0.013039
Epoch: 15 	Ltrain: 0.013568 	Lval: 0.012336
Epoch: 20 	Ltrain: 0.011361 	Lval: 0.011439
Epoch: 25 	Ltrain: 0.011740 	Lval: 0.010476
Epoch: 30 	Ltrain: 0.009499 	Lval: 0.009588
Epoch: 35 	Ltrain: 0.008308 	Lval: 0.008748
Epoch: 40 	Ltrain: 0.007601 	Lval: 0.008278
Epoch: 45 	Ltrain: 0.008537 	Lval: 0.007818
Epoch: 50 	Ltrain: 0.008012 	Lval: 0.007435
Epoch: 55 	Ltrain: 0.006858 	Lval: 0.007107
Epoch: 60 	Ltrain: 0.007331 	Lval: 0.006858
Epoch: 65 	Ltrain: 0.006709 	Lval: 0.006669
Epoch: 70 	Ltrain: 0.006407 	Lval: 0.006499
Epoch: 75 	Ltrain: 0.005812 	Lval: 0.006352
Epoch: 80 	Ltrain: 0.006189 	Lval: 0.006208
Epoch: 85 	Ltrain: 0.005585 	Lval: 0.006061
Epoch: 90 	Ltrain: 0.005737 	Lval: 0.005882
Epoch: 95 	Ltrain: 0.005496 	Lval: 0.005754
Epoch: 100 	Ltrain: 0.006127 	Lval: 0.005684
Epoch: 105 	Ltrain: 0.006444 	Lval: 0.005556
Epoch: 110 	Ltrain: 0.005365 	Lval: 0.005417
Epoch: 115 	Ltrain: 0.006457 	Lval: 0.005319
Epoch: 120 	Ltrain: 0.004959 	Lval: 0.005248
Epoch: 125 	Ltrain: 0.005739 	Lval: 0.005160
Epoch: 130 	Ltrain: 0.005993 	Lval: 0.005080
Epoch: 135 	Ltrain: 0.005133 	Lval: 0.005036
Epoch: 140 	Ltrain: 0.005272 	Lval: 0.004983
Epoch: 145 	Ltrain: 0.005059 	Lval: 0.004891
Epoch: 150 	Ltrain: 0.004939 	Lval: 0.004852
Epoch: 155 	Ltrain: 0.004673 	Lval: 0.004836
Epoch: 160 	Ltrain: 0.004599 	Lval: 0.004737
Epoch: 165 	Ltrain: 0.004948 	Lval: 0.004686
Epoch: 170 	Ltrain: 0.004828 	Lval: 0.004697
Epoch: 175 	Ltrain: 0.004798 	Lval: 0.004663
Epoch: 180 	Ltrain: 0.004741 	Lval: 0.004586
Epoch: 185 	Ltrain: 0.004738 	Lval: 0.004564
Epoch: 190 	Ltrain: 0.004623 	Lval: 0.004614
Epoch: 195 	Ltrain: 0.004685 	Lval: 0.004484
Epoch: 200 	Ltrain: 0.005285 	Lval: 0.004556
Epoch 00202: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 205 	Ltrain: 0.005160 	Lval: 0.004473
Epoch: 210 	Ltrain: 0.004507 	Lval: 0.004459
Epoch 00214: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 215 	Ltrain: 0.004473 	Lval: 0.004466
Epoch: 220 	Ltrain: 0.004395 	Lval: 0.004466
Epoch: 225 	Ltrain: 0.004789 	Lval: 0.004466
Epoch 00226: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 230 	Ltrain: 0.004831 	Lval: 0.004467
EarlyStopper: stopping at epoch 229 with best_val_loss = 0.004457


	Fold 2/5
Epoch: 1 	Ltrain: 0.111979 	Lval: 0.085492
Epoch: 5 	Ltrain: 0.018835 	Lval: 0.015546
Epoch: 10 	Ltrain: 0.013753 	Lval: 0.013185
Epoch: 15 	Ltrain: 0.013397 	Lval: 0.012684
Epoch: 20 	Ltrain: 0.012387 	Lval: 0.012120
Epoch: 25 	Ltrain: 0.011778 	Lval: 0.011236
Epoch: 30 	Ltrain: 0.010221 	Lval: 0.010170
Epoch: 35 	Ltrain: 0.009684 	Lval: 0.008984
Epoch: 40 	Ltrain: 0.008243 	Lval: 0.007891
Epoch: 45 	Ltrain: 0.007543 	Lval: 0.007202
Epoch: 50 	Ltrain: 0.007805 	Lval: 0.006808
Epoch: 55 	Ltrain: 0.007073 	Lval: 0.006602
Epoch: 60 	Ltrain: 0.006792 	Lval: 0.006428
Epoch: 65 	Ltrain: 0.006375 	Lval: 0.006185
Epoch: 70 	Ltrain: 0.006568 	Lval: 0.005995
Epoch: 75 	Ltrain: 0.006613 	Lval: 0.005870
Epoch: 80 	Ltrain: 0.005961 	Lval: 0.005792
Epoch: 85 	Ltrain: 0.006010 	Lval: 0.005697
Epoch: 90 	Ltrain: 0.006061 	Lval: 0.005635
Epoch: 95 	Ltrain: 0.005746 	Lval: 0.005504
Epoch: 100 	Ltrain: 0.005947 	Lval: 0.005475
Epoch 00105: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 105 	Ltrain: 0.005688 	Lval: 0.005411
Epoch: 110 	Ltrain: 0.005711 	Lval: 0.005367
Epoch: 115 	Ltrain: 0.006077 	Lval: 0.005385
Epoch 00117: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 120 	Ltrain: 0.005654 	Lval: 0.005400
Epoch: 125 	Ltrain: 0.005566 	Lval: 0.005391
Epoch 00129: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 130 	Ltrain: 0.005665 	Lval: 0.005386
EarlyStopper: stopping at epoch 129 with best_val_loss = 0.005368


	Fold 3/5
Epoch: 1 	Ltrain: 0.057297 	Lval: 0.037395
Epoch: 5 	Ltrain: 0.014519 	Lval: 0.012569
Epoch: 10 	Ltrain: 0.011608 	Lval: 0.010372
Epoch: 15 	Ltrain: 0.009225 	Lval: 0.008536
Epoch: 20 	Ltrain: 0.007871 	Lval: 0.007405
Epoch: 25 	Ltrain: 0.007340 	Lval: 0.006980
Epoch: 30 	Ltrain: 0.006925 	Lval: 0.006578
Epoch: 35 	Ltrain: 0.006527 	Lval: 0.006355
Epoch: 40 	Ltrain: 0.006348 	Lval: 0.006029
Epoch: 45 	Ltrain: 0.006200 	Lval: 0.005875
Epoch: 50 	Ltrain: 0.005885 	Lval: 0.005545
Epoch: 55 	Ltrain: 0.005774 	Lval: 0.005353
Epoch: 60 	Ltrain: 0.005695 	Lval: 0.005413
Epoch: 65 	Ltrain: 0.005705 	Lval: 0.005191
Epoch 00068: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 70 	Ltrain: 0.005306 	Lval: 0.004947
Epoch: 75 	Ltrain: 0.005939 	Lval: 0.004955
Epoch: 80 	Ltrain: 0.005312 	Lval: 0.004930
Epoch 00084: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 85 	Ltrain: 0.005359 	Lval: 0.004939
Epoch: 90 	Ltrain: 0.005325 	Lval: 0.004942
Epoch: 95 	Ltrain: 0.005212 	Lval: 0.004945
Epoch 00096: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 100 	Ltrain: 0.005321 	Lval: 0.004947
EarlyStopper: stopping at epoch 102 with best_val_loss = 0.004930


	Fold 4/5
Epoch: 1 	Ltrain: 0.092967 	Lval: 0.064838
Epoch: 5 	Ltrain: 0.015319 	Lval: 0.013662
Epoch: 10 	Ltrain: 0.013027 	Lval: 0.012016
Epoch: 15 	Ltrain: 0.010745 	Lval: 0.010178
Epoch: 20 	Ltrain: 0.008385 	Lval: 0.008329
Epoch: 25 	Ltrain: 0.007357 	Lval: 0.007279
Epoch: 30 	Ltrain: 0.006805 	Lval: 0.006732
Epoch: 35 	Ltrain: 0.006366 	Lval: 0.006411
Epoch: 40 	Ltrain: 0.006185 	Lval: 0.006032
Epoch: 45 	Ltrain: 0.006219 	Lval: 0.005888
Epoch: 50 	Ltrain: 0.005721 	Lval: 0.005730
Epoch: 55 	Ltrain: 0.005526 	Lval: 0.005445
Epoch: 60 	Ltrain: 0.005413 	Lval: 0.005329
Epoch: 65 	Ltrain: 0.005225 	Lval: 0.005134
Epoch: 70 	Ltrain: 0.005063 	Lval: 0.004977
Epoch: 75 	Ltrain: 0.005122 	Lval: 0.004931
Epoch: 80 	Ltrain: 0.004857 	Lval: 0.004862
Epoch: 85 	Ltrain: 0.004776 	Lval: 0.004750
Epoch: 90 	Ltrain: 0.004756 	Lval: 0.004650
Epoch: 95 	Ltrain: 0.004665 	Lval: 0.004593
Epoch: 100 	Ltrain: 0.004599 	Lval: 0.004688
Epoch: 105 	Ltrain: 0.004562 	Lval: 0.004496
Epoch 00109: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 110 	Ltrain: 0.004460 	Lval: 0.004487
Epoch: 115 	Ltrain: 0.004471 	Lval: 0.004478
Epoch: 120 	Ltrain: 0.004545 	Lval: 0.004479
Epoch 00121: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 125 	Ltrain: 0.004412 	Lval: 0.004476
Epoch: 130 	Ltrain: 0.004471 	Lval: 0.004473
Epoch 00133: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 135 	Ltrain: 0.004409 	Lval: 0.004473
Epoch: 140 	Ltrain: 0.004546 	Lval: 0.004473
EarlyStopper: stopping at epoch 139 with best_val_loss = 0.004469


	Fold 5/5
Epoch: 1 	Ltrain: 0.099077 	Lval: 0.068518
Epoch: 5 	Ltrain: 0.014642 	Lval: 0.014553
Epoch: 10 	Ltrain: 0.012549 	Lval: 0.012995
Epoch: 15 	Ltrain: 0.010541 	Lval: 0.011109
Epoch: 20 	Ltrain: 0.008796 	Lval: 0.009085
Epoch: 25 	Ltrain: 0.007478 	Lval: 0.007786
Epoch: 30 	Ltrain: 0.006680 	Lval: 0.007037
Epoch: 35 	Ltrain: 0.006198 	Lval: 0.006464
Epoch: 40 	Ltrain: 0.005777 	Lval: 0.006064
Epoch: 45 	Ltrain: 0.005492 	Lval: 0.005760
Epoch: 50 	Ltrain: 0.005368 	Lval: 0.005469
Epoch: 55 	Ltrain: 0.005096 	Lval: 0.005266
Epoch: 60 	Ltrain: 0.004888 	Lval: 0.005079
Epoch: 65 	Ltrain: 0.004811 	Lval: 0.004993
Epoch: 70 	Ltrain: 0.004722 	Lval: 0.004850
Epoch: 75 	Ltrain: 0.004612 	Lval: 0.004748
Epoch: 80 	Ltrain: 0.004637 	Lval: 0.004665
Epoch: 85 	Ltrain: 0.004453 	Lval: 0.004604
Epoch: 90 	Ltrain: 0.004411 	Lval: 0.004556
Epoch 00094: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 95 	Ltrain: 0.004364 	Lval: 0.004532
Epoch: 100 	Ltrain: 0.004404 	Lval: 0.004528
Epoch: 105 	Ltrain: 0.004306 	Lval: 0.004530
Epoch: 110 	Ltrain: 0.004324 	Lval: 0.004526
Epoch: 115 	Ltrain: 0.004324 	Lval: 0.004519
Epoch 00119: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 120 	Ltrain: 0.004343 	Lval: 0.004519
Epoch: 125 	Ltrain: 0.004336 	Lval: 0.004517
Epoch: 130 	Ltrain: 0.004362 	Lval: 0.004517
Epoch 00135: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 135 	Ltrain: 0.004328 	Lval: 0.004516
EarlyStopper: stopping at epoch 134 with best_val_loss = 0.004521

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0012757488347627612
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.30569702558947e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.035230 	Lval: 0.017634
Epoch: 5 	Ltrain: 0.011162 	Lval: 0.010084
Epoch: 10 	Ltrain: 0.007505 	Lval: 0.008375
Epoch: 15 	Ltrain: 0.005900 	Lval: 0.005702
Epoch: 20 	Ltrain: 0.006519 	Lval: 0.005093
Epoch: 25 	Ltrain: 0.005457 	Lval: 0.004752
Epoch: 30 	Ltrain: 0.005124 	Lval: 0.004610
Epoch: 35 	Ltrain: 0.004619 	Lval: 0.004617
Epoch: 40 	Ltrain: 0.004218 	Lval: 0.004324
Epoch 00041: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 45 	Ltrain: 0.004301 	Lval: 0.004234
Epoch: 50 	Ltrain: 0.004320 	Lval: 0.004243
Epoch: 55 	Ltrain: 0.004293 	Lval: 0.004237
Epoch: 60 	Ltrain: 0.004334 	Lval: 0.004221
Epoch: 65 	Ltrain: 0.004268 	Lval: 0.004204
Epoch: 70 	Ltrain: 0.004119 	Lval: 0.004216
Epoch 00071: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 75 	Ltrain: 0.004294 	Lval: 0.004193
Epoch: 80 	Ltrain: 0.004177 	Lval: 0.004196
Epoch 00083: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 85 	Ltrain: 0.004226 	Lval: 0.004193
Epoch: 90 	Ltrain: 0.004237 	Lval: 0.004193
Epoch 00095: reducing learning rate of group 0 to 1.2757e-07.
Epoch: 95 	Ltrain: 0.004156 	Lval: 0.004193
Epoch: 100 	Ltrain: 0.004115 	Lval: 0.004193
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.004194


	Fold 2/5
Epoch: 1 	Ltrain: 0.024852 	Lval: 0.014151
Epoch: 5 	Ltrain: 0.007381 	Lval: 0.006717
Epoch: 10 	Ltrain: 0.005409 	Lval: 0.005543
Epoch: 15 	Ltrain: 0.004781 	Lval: 0.004690
Epoch: 20 	Ltrain: 0.004698 	Lval: 0.004335
Epoch: 25 	Ltrain: 0.004509 	Lval: 0.004630
Epoch 00028: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 30 	Ltrain: 0.004266 	Lval: 0.004395
Epoch: 35 	Ltrain: 0.004263 	Lval: 0.004272
Epoch 00040: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 40 	Ltrain: 0.004252 	Lval: 0.004286
Epoch: 45 	Ltrain: 0.004390 	Lval: 0.004241
Epoch: 50 	Ltrain: 0.004248 	Lval: 0.004241
Epoch 00052: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 55 	Ltrain: 0.004287 	Lval: 0.004252
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004190


	Fold 3/5
Epoch: 1 	Ltrain: 0.020505 	Lval: 0.012483
Epoch: 5 	Ltrain: 0.006458 	Lval: 0.006672
Epoch: 10 	Ltrain: 0.004775 	Lval: 0.004497
Epoch 00015: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 15 	Ltrain: 0.004989 	Lval: 0.005038
Epoch: 20 	Ltrain: 0.004281 	Lval: 0.004240
Epoch: 25 	Ltrain: 0.004297 	Lval: 0.004428
Epoch 00027: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 30 	Ltrain: 0.004251 	Lval: 0.004239
Epoch: 35 	Ltrain: 0.004238 	Lval: 0.004246
Epoch: 40 	Ltrain: 0.004245 	Lval: 0.004245
Epoch: 45 	Ltrain: 0.004241 	Lval: 0.004228
Epoch: 50 	Ltrain: 0.004268 	Lval: 0.004231
Epoch 00052: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 55 	Ltrain: 0.004244 	Lval: 0.004231
Epoch: 60 	Ltrain: 0.004250 	Lval: 0.004233
Epoch 00064: reducing learning rate of group 0 to 1.2757e-07.
Epoch: 65 	Ltrain: 0.004268 	Lval: 0.004233
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.004228


	Fold 4/5
Epoch: 1 	Ltrain: 0.025259 	Lval: 0.012154
Epoch: 5 	Ltrain: 0.005840 	Lval: 0.005659
Epoch: 10 	Ltrain: 0.004666 	Lval: 0.004416
Epoch: 15 	Ltrain: 0.004221 	Lval: 0.004413
Epoch 00020: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 20 	Ltrain: 0.004266 	Lval: 0.004673
Epoch: 25 	Ltrain: 0.003970 	Lval: 0.004203
Epoch: 30 	Ltrain: 0.003939 	Lval: 0.004224
Epoch: 35 	Ltrain: 0.003939 	Lval: 0.004174
Epoch 00038: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 40 	Ltrain: 0.003902 	Lval: 0.004155
Epoch: 45 	Ltrain: 0.003896 	Lval: 0.004168
Epoch 00050: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 50 	Ltrain: 0.003895 	Lval: 0.004159
Epoch: 55 	Ltrain: 0.003895 	Lval: 0.004155
Epoch: 60 	Ltrain: 0.003889 	Lval: 0.004154
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.004144


	Fold 5/5
Epoch: 1 	Ltrain: 0.022747 	Lval: 0.011540
Epoch: 5 	Ltrain: 0.005597 	Lval: 0.005277
Epoch: 10 	Ltrain: 0.004344 	Lval: 0.004577
Epoch: 15 	Ltrain: 0.004240 	Lval: 0.004664
Epoch: 20 	Ltrain: 0.004088 	Lval: 0.004345
Epoch: 25 	Ltrain: 0.003941 	Lval: 0.004220
Epoch: 30 	Ltrain: 0.003995 	Lval: 0.004306
Epoch 00031: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 35 	Ltrain: 0.003631 	Lval: 0.004034
Epoch: 40 	Ltrain: 0.003634 	Lval: 0.004000
Epoch: 45 	Ltrain: 0.003605 	Lval: 0.004008
Epoch 00047: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 50 	Ltrain: 0.003563 	Lval: 0.003974
Epoch: 55 	Ltrain: 0.003572 	Lval: 0.003973
Epoch: 60 	Ltrain: 0.003571 	Lval: 0.003967
Epoch 00064: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 65 	Ltrain: 0.003564 	Lval: 0.003968
Epoch: 70 	Ltrain: 0.003547 	Lval: 0.003967
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.003970

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00030969089415100936
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.4813380065359772e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.132863 	Lval: 0.094040
Epoch: 5 	Ltrain: 0.016504 	Lval: 0.015488
Epoch: 10 	Ltrain: 0.014359 	Lval: 0.013924
Epoch: 15 	Ltrain: 0.014518 	Lval: 0.013423
Epoch: 20 	Ltrain: 0.012355 	Lval: 0.011992
Epoch: 25 	Ltrain: 0.009860 	Lval: 0.009306
Epoch: 30 	Ltrain: 0.009480 	Lval: 0.008394
Epoch: 35 	Ltrain: 0.008139 	Lval: 0.007852
Epoch: 40 	Ltrain: 0.007499 	Lval: 0.007389
Epoch: 45 	Ltrain: 0.007231 	Lval: 0.007209
Epoch: 50 	Ltrain: 0.007193 	Lval: 0.006647
Epoch: 55 	Ltrain: 0.006675 	Lval: 0.006628
Epoch: 60 	Ltrain: 0.006285 	Lval: 0.006081
Epoch: 65 	Ltrain: 0.006607 	Lval: 0.005823
Epoch: 70 	Ltrain: 0.006011 	Lval: 0.005640
Epoch: 75 	Ltrain: 0.005505 	Lval: 0.005470
Epoch: 80 	Ltrain: 0.005703 	Lval: 0.005368
Epoch: 85 	Ltrain: 0.005167 	Lval: 0.005234
Epoch: 90 	Ltrain: 0.005231 	Lval: 0.005248
Epoch: 95 	Ltrain: 0.005179 	Lval: 0.005045
Epoch: 100 	Ltrain: 0.005623 	Lval: 0.005053
Epoch: 105 	Ltrain: 0.005100 	Lval: 0.004952
Epoch 00109: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 110 	Ltrain: 0.005296 	Lval: 0.004942
Epoch: 115 	Ltrain: 0.004965 	Lval: 0.004913
Epoch: 120 	Ltrain: 0.004948 	Lval: 0.004906
Epoch: 125 	Ltrain: 0.004997 	Lval: 0.004904
Epoch: 130 	Ltrain: 0.004945 	Lval: 0.004922
Epoch: 135 	Ltrain: 0.005101 	Lval: 0.004890
Epoch 00138: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 140 	Ltrain: 0.004995 	Lval: 0.004889
Epoch: 145 	Ltrain: 0.004964 	Lval: 0.004891
EarlyStopper: stopping at epoch 144 with best_val_loss = 0.004898


	Fold 2/5
Epoch: 1 	Ltrain: 0.057197 	Lval: 0.020925
Epoch: 5 	Ltrain: 0.014927 	Lval: 0.013880
Epoch: 10 	Ltrain: 0.012817 	Lval: 0.011844
Epoch: 15 	Ltrain: 0.009566 	Lval: 0.008626
Epoch: 20 	Ltrain: 0.008127 	Lval: 0.007636
Epoch: 25 	Ltrain: 0.007369 	Lval: 0.007023
Epoch: 30 	Ltrain: 0.006855 	Lval: 0.006383
Epoch: 35 	Ltrain: 0.006380 	Lval: 0.005971
Epoch: 40 	Ltrain: 0.006126 	Lval: 0.005903
Epoch: 45 	Ltrain: 0.005763 	Lval: 0.005716
Epoch: 50 	Ltrain: 0.005775 	Lval: 0.005421
Epoch: 55 	Ltrain: 0.005379 	Lval: 0.004981
Epoch: 60 	Ltrain: 0.005407 	Lval: 0.005070
Epoch: 65 	Ltrain: 0.005078 	Lval: 0.004712
Epoch: 70 	Ltrain: 0.004835 	Lval: 0.004760
Epoch 00072: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 75 	Ltrain: 0.004728 	Lval: 0.004603
Epoch: 80 	Ltrain: 0.004729 	Lval: 0.004587
Epoch: 85 	Ltrain: 0.004724 	Lval: 0.004568
Epoch 00088: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 90 	Ltrain: 0.004651 	Lval: 0.004555
Epoch: 95 	Ltrain: 0.004629 	Lval: 0.004563
EarlyStopper: stopping at epoch 97 with best_val_loss = 0.004536


	Fold 3/5
Epoch: 1 	Ltrain: 0.089842 	Lval: 0.029979
Epoch: 5 	Ltrain: 0.015822 	Lval: 0.014061
Epoch: 10 	Ltrain: 0.010713 	Lval: 0.009442
Epoch: 15 	Ltrain: 0.008008 	Lval: 0.007580
Epoch: 20 	Ltrain: 0.007060 	Lval: 0.006711
Epoch: 25 	Ltrain: 0.006413 	Lval: 0.005927
Epoch: 30 	Ltrain: 0.005971 	Lval: 0.005507
Epoch: 35 	Ltrain: 0.005609 	Lval: 0.005281
Epoch: 40 	Ltrain: 0.005412 	Lval: 0.004960
Epoch: 45 	Ltrain: 0.005201 	Lval: 0.005227
Epoch 00046: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 50 	Ltrain: 0.004975 	Lval: 0.004732
Epoch: 55 	Ltrain: 0.005063 	Lval: 0.004745
Epoch 00058: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 60 	Ltrain: 0.004955 	Lval: 0.004721
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.004715


	Fold 4/5
Epoch: 1 	Ltrain: 0.057675 	Lval: 0.014680
Epoch: 5 	Ltrain: 0.014065 	Lval: 0.013041
Epoch: 10 	Ltrain: 0.008446 	Lval: 0.008374
Epoch: 15 	Ltrain: 0.006927 	Lval: 0.007324
Epoch: 20 	Ltrain: 0.006080 	Lval: 0.006172
Epoch: 25 	Ltrain: 0.005380 	Lval: 0.005249
Epoch: 30 	Ltrain: 0.004897 	Lval: 0.004797
Epoch: 35 	Ltrain: 0.004507 	Lval: 0.004534
Epoch: 40 	Ltrain: 0.004445 	Lval: 0.004442
Epoch: 45 	Ltrain: 0.004293 	Lval: 0.004391
Epoch: 50 	Ltrain: 0.004310 	Lval: 0.004361
Epoch: 55 	Ltrain: 0.004273 	Lval: 0.004323
Epoch: 60 	Ltrain: 0.004182 	Lval: 0.004407
Epoch 00062: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 65 	Ltrain: 0.004068 	Lval: 0.004245
Epoch: 70 	Ltrain: 0.004059 	Lval: 0.004260
Epoch 00074: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 75 	Ltrain: 0.004049 	Lval: 0.004250
Epoch: 80 	Ltrain: 0.004051 	Lval: 0.004252
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.004235


	Fold 5/5
Epoch: 1 	Ltrain: 0.047213 	Lval: 0.015389
Epoch: 5 	Ltrain: 0.011329 	Lval: 0.010409
Epoch: 10 	Ltrain: 0.007686 	Lval: 0.008085
Epoch: 15 	Ltrain: 0.006383 	Lval: 0.006655
Epoch: 20 	Ltrain: 0.005706 	Lval: 0.005754
Epoch: 25 	Ltrain: 0.005158 	Lval: 0.005143
Epoch: 30 	Ltrain: 0.004824 	Lval: 0.004816
Epoch: 35 	Ltrain: 0.004598 	Lval: 0.004908
Epoch: 40 	Ltrain: 0.004567 	Lval: 0.004641
Epoch 00042: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 45 	Ltrain: 0.004331 	Lval: 0.004555
Epoch: 50 	Ltrain: 0.004342 	Lval: 0.004544
Epoch 00054: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 55 	Ltrain: 0.004330 	Lval: 0.004549
Epoch: 60 	Ltrain: 0.004309 	Lval: 0.004543
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.004537

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005745115971827479
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.22616303415697e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.083480 	Lval: 0.016641
Epoch: 5 	Ltrain: 0.007680 	Lval: 0.007757
Epoch: 10 	Ltrain: 0.004959 	Lval: 0.005347
Epoch: 15 	Ltrain: 0.005182 	Lval: 0.006389
Epoch 00016: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 20 	Ltrain: 0.004743 	Lval: 0.004485
Epoch: 25 	Ltrain: 0.004623 	Lval: 0.004419
Epoch: 30 	Ltrain: 0.004466 	Lval: 0.004387
Epoch: 35 	Ltrain: 0.004586 	Lval: 0.004362
Epoch: 40 	Ltrain: 0.004442 	Lval: 0.004354
Epoch 00043: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 45 	Ltrain: 0.004366 	Lval: 0.004308
Epoch: 50 	Ltrain: 0.004669 	Lval: 0.004301
Epoch: 55 	Ltrain: 0.004265 	Lval: 0.004296
Epoch: 60 	Ltrain: 0.004244 	Lval: 0.004294
Epoch: 65 	Ltrain: 0.004686 	Lval: 0.004285
Epoch 00068: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 70 	Ltrain: 0.004228 	Lval: 0.004286
Epoch: 75 	Ltrain: 0.004495 	Lval: 0.004282
Epoch: 80 	Ltrain: 0.004508 	Lval: 0.004283
Epoch 00082: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 85 	Ltrain: 0.004240 	Lval: 0.004285
EarlyStopper: stopping at epoch 86 with best_val_loss = 0.004286


	Fold 2/5
Epoch: 1 	Ltrain: 0.036656 	Lval: 0.017373
Epoch: 5 	Ltrain: 0.005431 	Lval: 0.004786
Epoch 00009: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 10 	Ltrain: 0.004762 	Lval: 0.004353
Epoch: 15 	Ltrain: 0.004446 	Lval: 0.004283
Epoch: 20 	Ltrain: 0.004379 	Lval: 0.004326
Epoch 00023: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 25 	Ltrain: 0.004342 	Lval: 0.004219
Epoch: 30 	Ltrain: 0.004256 	Lval: 0.004236
Epoch 00035: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 35 	Ltrain: 0.004246 	Lval: 0.004224
Epoch: 40 	Ltrain: 0.004280 	Lval: 0.004226
Epoch: 45 	Ltrain: 0.004307 	Lval: 0.004233
Epoch 00047: reducing learning rate of group 0 to 5.7451e-07.
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.004219


	Fold 3/5
Epoch: 1 	Ltrain: 0.028940 	Lval: 0.014778
Epoch: 5 	Ltrain: 0.006538 	Lval: 0.005377
Epoch: 10 	Ltrain: 0.004784 	Lval: 0.004838
Epoch 00015: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 15 	Ltrain: 0.004630 	Lval: 0.004333
Epoch: 20 	Ltrain: 0.004015 	Lval: 0.003982
Epoch: 25 	Ltrain: 0.003910 	Lval: 0.003853
Epoch: 30 	Ltrain: 0.003834 	Lval: 0.003828
Epoch: 35 	Ltrain: 0.003751 	Lval: 0.003794
Epoch 00036: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 40 	Ltrain: 0.003621 	Lval: 0.003745
Epoch: 45 	Ltrain: 0.003629 	Lval: 0.003747
Epoch: 50 	Ltrain: 0.003601 	Lval: 0.003735
Epoch 00051: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 55 	Ltrain: 0.003597 	Lval: 0.003727
Epoch: 60 	Ltrain: 0.003613 	Lval: 0.003724
Epoch: 65 	Ltrain: 0.003590 	Lval: 0.003721
Epoch 00067: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 70 	Ltrain: 0.003603 	Lval: 0.003720
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.003725


	Fold 4/5
Epoch: 1 	Ltrain: 0.020898 	Lval: 0.007250
Epoch: 5 	Ltrain: 0.004974 	Lval: 0.005344
Epoch 00010: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 10 	Ltrain: 0.004644 	Lval: 0.004784
Epoch: 15 	Ltrain: 0.003804 	Lval: 0.003978
Epoch: 20 	Ltrain: 0.003740 	Lval: 0.003870
Epoch: 25 	Ltrain: 0.003582 	Lval: 0.003916
Epoch: 30 	Ltrain: 0.003542 	Lval: 0.003911
Epoch: 35 	Ltrain: 0.003440 	Lval: 0.003650
Epoch: 40 	Ltrain: 0.003351 	Lval: 0.003475
Epoch: 45 	Ltrain: 0.003300 	Lval: 0.003351
Epoch: 50 	Ltrain: 0.003207 	Lval: 0.003257
Epoch: 55 	Ltrain: 0.003084 	Lval: 0.003247
Epoch: 60 	Ltrain: 0.002962 	Lval: 0.003065
Epoch 00064: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 65 	Ltrain: 0.002825 	Lval: 0.003021
Epoch: 70 	Ltrain: 0.002741 	Lval: 0.002957
Epoch: 75 	Ltrain: 0.002733 	Lval: 0.002925
Epoch 00079: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 80 	Ltrain: 0.002706 	Lval: 0.002929
Epoch: 85 	Ltrain: 0.002698 	Lval: 0.002926
Epoch: 90 	Ltrain: 0.002700 	Lval: 0.002923
Epoch: 95 	Ltrain: 0.002703 	Lval: 0.002920
EarlyStopper: stopping at epoch 97 with best_val_loss = 0.002925


	Fold 5/5
Epoch: 1 	Ltrain: 0.023933 	Lval: 0.007462
Epoch: 5 	Ltrain: 0.005031 	Lval: 0.005046
Epoch: 10 	Ltrain: 0.004249 	Lval: 0.004245
Epoch: 15 	Ltrain: 0.004119 	Lval: 0.004117
Epoch: 20 	Ltrain: 0.004585 	Lval: 0.004527
Epoch 00022: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 25 	Ltrain: 0.003291 	Lval: 0.003681
Epoch: 30 	Ltrain: 0.003190 	Lval: 0.003541
Epoch: 35 	Ltrain: 0.003119 	Lval: 0.003489
Epoch: 40 	Ltrain: 0.003050 	Lval: 0.003414
Epoch: 45 	Ltrain: 0.003040 	Lval: 0.003275
Epoch: 50 	Ltrain: 0.002923 	Lval: 0.003282
Epoch 00051: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 55 	Ltrain: 0.002801 	Lval: 0.003153
Epoch: 60 	Ltrain: 0.002794 	Lval: 0.003145
Epoch: 65 	Ltrain: 0.002780 	Lval: 0.003134
Epoch: 70 	Ltrain: 0.002780 	Lval: 0.003124
Epoch: 75 	Ltrain: 0.002773 	Lval: 0.003114
Epoch: 80 	Ltrain: 0.002750 	Lval: 0.003111
Epoch: 85 	Ltrain: 0.002757 	Lval: 0.003104
Epoch 00087: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 90 	Ltrain: 0.002727 	Lval: 0.003089
Epoch: 95 	Ltrain: 0.002722 	Lval: 0.003089
Epoch: 100 	Ltrain: 0.002725 	Lval: 0.003088
Epoch 00101: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 105 	Ltrain: 0.002713 	Lval: 0.003087
Epoch: 110 	Ltrain: 0.002717 	Lval: 0.003087
EarlyStopper: stopping at epoch 110 with best_val_loss = 0.003091

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007072780301199806
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.023976847523278e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.084080 	Lval: 0.036180
Epoch: 5 	Ltrain: 0.009431 	Lval: 0.006942
Epoch: 10 	Ltrain: 0.004813 	Lval: 0.004725
Epoch: 15 	Ltrain: 0.005312 	Lval: 0.004908
Epoch: 20 	Ltrain: 0.004387 	Lval: 0.004306
Epoch 00022: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 25 	Ltrain: 0.003811 	Lval: 0.003639
Epoch: 30 	Ltrain: 0.003490 	Lval: 0.003574
Epoch: 35 	Ltrain: 0.003640 	Lval: 0.003520
Epoch: 40 	Ltrain: 0.003487 	Lval: 0.003475
Epoch: 45 	Ltrain: 0.003573 	Lval: 0.003487
Epoch: 50 	Ltrain: 0.003462 	Lval: 0.003401
Epoch: 55 	Ltrain: 0.003531 	Lval: 0.003321
Epoch: 60 	Ltrain: 0.003401 	Lval: 0.003326
Epoch: 65 	Ltrain: 0.003354 	Lval: 0.003393
Epoch: 70 	Ltrain: 0.003273 	Lval: 0.003173
Epoch: 75 	Ltrain: 0.003183 	Lval: 0.003149
Epoch: 80 	Ltrain: 0.003289 	Lval: 0.003023
Epoch: 85 	Ltrain: 0.002969 	Lval: 0.002964
Epoch: 90 	Ltrain: 0.002956 	Lval: 0.002929
Epoch: 95 	Ltrain: 0.003013 	Lval: 0.002886
Epoch: 100 	Ltrain: 0.003028 	Lval: 0.002807
Epoch: 105 	Ltrain: 0.002622 	Lval: 0.002733
Epoch: 110 	Ltrain: 0.002619 	Lval: 0.002672
Epoch: 115 	Ltrain: 0.002622 	Lval: 0.002622
Epoch 00116: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 120 	Ltrain: 0.002475 	Lval: 0.002528
Epoch: 125 	Ltrain: 0.002472 	Lval: 0.002506
Epoch: 130 	Ltrain: 0.002389 	Lval: 0.002490
Epoch: 135 	Ltrain: 0.002582 	Lval: 0.002475
Epoch: 140 	Ltrain: 0.002507 	Lval: 0.002465
Epoch: 145 	Ltrain: 0.002420 	Lval: 0.002463
Epoch: 150 	Ltrain: 0.002482 	Lval: 0.002451
Epoch: 155 	Ltrain: 0.002429 	Lval: 0.002444
Epoch: 160 	Ltrain: 0.002471 	Lval: 0.002434
Epoch: 165 	Ltrain: 0.002348 	Lval: 0.002423
Epoch: 170 	Ltrain: 0.002397 	Lval: 0.002430
Epoch: 175 	Ltrain: 0.002447 	Lval: 0.002410
Epoch: 180 	Ltrain: 0.002372 	Lval: 0.002410
Epoch: 185 	Ltrain: 0.002359 	Lval: 0.002404
Epoch: 190 	Ltrain: 0.002379 	Lval: 0.002392
Epoch: 195 	Ltrain: 0.002406 	Lval: 0.002372
Epoch: 200 	Ltrain: 0.002398 	Lval: 0.002366
Epoch: 205 	Ltrain: 0.002390 	Lval: 0.002356
Epoch: 210 	Ltrain: 0.002395 	Lval: 0.002362
Epoch: 215 	Ltrain: 0.002339 	Lval: 0.002359
Epoch: 220 	Ltrain: 0.002256 	Lval: 0.002328
Epoch: 225 	Ltrain: 0.002229 	Lval: 0.002323
Epoch: 230 	Ltrain: 0.002254 	Lval: 0.002320
Epoch: 235 	Ltrain: 0.002294 	Lval: 0.002300
Epoch: 240 	Ltrain: 0.002236 	Lval: 0.002297
Epoch: 245 	Ltrain: 0.002309 	Lval: 0.002285
Epoch: 250 	Ltrain: 0.002295 	Lval: 0.002274
Epoch: 255 	Ltrain: 0.002266 	Lval: 0.002258
Epoch: 260 	Ltrain: 0.002208 	Lval: 0.002242
Epoch: 265 	Ltrain: 0.002236 	Lval: 0.002227
Epoch: 270 	Ltrain: 0.002223 	Lval: 0.002232
Epoch: 275 	Ltrain: 0.002257 	Lval: 0.002220
Epoch: 280 	Ltrain: 0.002183 	Lval: 0.002207
Epoch: 285 	Ltrain: 0.002161 	Lval: 0.002197
Epoch: 290 	Ltrain: 0.002203 	Lval: 0.002184
Epoch: 295 	Ltrain: 0.002227 	Lval: 0.002171
Epoch 00300: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 300 	Ltrain: 0.002135 	Lval: 0.002165
Epoch: 305 	Ltrain: 0.002069 	Lval: 0.002158
Epoch: 310 	Ltrain: 0.002179 	Lval: 0.002153
Epoch 00312: reducing learning rate of group 0 to 7.0728e-07.
Epoch: 315 	Ltrain: 0.002091 	Lval: 0.002153
Epoch: 320 	Ltrain: 0.002094 	Lval: 0.002153
EarlyStopper: stopping at epoch 319 with best_val_loss = 0.002157


	Fold 2/5
Epoch: 1 	Ltrain: 0.077033 	Lval: 0.019808
Epoch: 5 	Ltrain: 0.006273 	Lval: 0.006015
Epoch: 10 	Ltrain: 0.004889 	Lval: 0.005334
Epoch: 15 	Ltrain: 0.004905 	Lval: 0.004347
Epoch: 20 	Ltrain: 0.004292 	Lval: 0.004114
Epoch: 25 	Ltrain: 0.003952 	Lval: 0.003855
Epoch 00027: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 30 	Ltrain: 0.003489 	Lval: 0.003491
Epoch: 35 	Ltrain: 0.003366 	Lval: 0.003512
Epoch: 40 	Ltrain: 0.003340 	Lval: 0.003380
Epoch: 45 	Ltrain: 0.003238 	Lval: 0.003289
Epoch 00046: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 50 	Ltrain: 0.003259 	Lval: 0.003299
Epoch: 55 	Ltrain: 0.003233 	Lval: 0.003281
Epoch 00058: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 60 	Ltrain: 0.003162 	Lval: 0.003284
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.003275


	Fold 3/5
Epoch: 1 	Ltrain: 0.045419 	Lval: 0.015117
Epoch: 5 	Ltrain: 0.005256 	Lval: 0.004775
Epoch 00009: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 10 	Ltrain: 0.004668 	Lval: 0.004396
Epoch: 15 	Ltrain: 0.004313 	Lval: 0.004287
Epoch: 20 	Ltrain: 0.004240 	Lval: 0.004091
Epoch: 25 	Ltrain: 0.004098 	Lval: 0.003969
Epoch: 30 	Ltrain: 0.003883 	Lval: 0.003833
Epoch: 35 	Ltrain: 0.003802 	Lval: 0.003779
Epoch: 40 	Ltrain: 0.003773 	Lval: 0.003881
Epoch 00042: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 45 	Ltrain: 0.003530 	Lval: 0.003711
Epoch: 50 	Ltrain: 0.003542 	Lval: 0.003691
Epoch: 55 	Ltrain: 0.003514 	Lval: 0.003672
Epoch 00058: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 60 	Ltrain: 0.003504 	Lval: 0.003676
Epoch: 65 	Ltrain: 0.003476 	Lval: 0.003673
Epoch 00070: reducing learning rate of group 0 to 7.0728e-07.
Epoch: 70 	Ltrain: 0.003497 	Lval: 0.003669
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.003639


	Fold 4/5
Epoch: 1 	Ltrain: 0.051750 	Lval: 0.008710
Epoch: 5 	Ltrain: 0.004787 	Lval: 0.004753
Epoch: 10 	Ltrain: 0.004426 	Lval: 0.005317
Epoch: 15 	Ltrain: 0.003957 	Lval: 0.004390
Epoch: 20 	Ltrain: 0.003455 	Lval: 0.003653
Epoch: 25 	Ltrain: 0.003261 	Lval: 0.003195
Epoch 00030: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 30 	Ltrain: 0.002982 	Lval: 0.003229
Epoch: 35 	Ltrain: 0.002444 	Lval: 0.002638
Epoch: 40 	Ltrain: 0.002376 	Lval: 0.002555
Epoch: 45 	Ltrain: 0.002318 	Lval: 0.002536
Epoch: 50 	Ltrain: 0.002239 	Lval: 0.002455
Epoch: 55 	Ltrain: 0.002191 	Lval: 0.002487
Epoch: 60 	Ltrain: 0.002113 	Lval: 0.002352
Epoch: 65 	Ltrain: 0.002023 	Lval: 0.002268
Epoch: 70 	Ltrain: 0.001980 	Lval: 0.002236
Epoch: 75 	Ltrain: 0.001892 	Lval: 0.002153
Epoch 00077: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 80 	Ltrain: 0.001744 	Lval: 0.002033
Epoch: 85 	Ltrain: 0.001720 	Lval: 0.002013
Epoch: 90 	Ltrain: 0.001706 	Lval: 0.001989
Epoch: 95 	Ltrain: 0.001692 	Lval: 0.001984
Epoch: 100 	Ltrain: 0.001676 	Lval: 0.001963
Epoch: 105 	Ltrain: 0.001662 	Lval: 0.001945
Epoch: 110 	Ltrain: 0.001648 	Lval: 0.001933
Epoch: 115 	Ltrain: 0.001630 	Lval: 0.001915
Epoch: 120 	Ltrain: 0.001629 	Lval: 0.001896
Epoch: 125 	Ltrain: 0.001611 	Lval: 0.001899
Epoch: 130 	Ltrain: 0.001597 	Lval: 0.001857
Epoch: 135 	Ltrain: 0.001574 	Lval: 0.001839
Epoch: 140 	Ltrain: 0.001562 	Lval: 0.001824
Epoch: 145 	Ltrain: 0.001542 	Lval: 0.001800
Epoch: 150 	Ltrain: 0.001533 	Lval: 0.001774
Epoch: 155 	Ltrain: 0.001509 	Lval: 0.001745
Epoch: 160 	Ltrain: 0.001492 	Lval: 0.001742
Epoch: 165 	Ltrain: 0.001476 	Lval: 0.001700
Epoch: 170 	Ltrain: 0.001460 	Lval: 0.001687
Epoch: 175 	Ltrain: 0.001437 	Lval: 0.001647
Epoch: 180 	Ltrain: 0.001419 	Lval: 0.001622
Epoch: 185 	Ltrain: 0.001410 	Lval: 0.001620
Epoch: 190 	Ltrain: 0.001382 	Lval: 0.001572
Epoch: 195 	Ltrain: 0.001365 	Lval: 0.001571
Epoch 00196: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 200 	Ltrain: 0.001325 	Lval: 0.001530
Epoch: 205 	Ltrain: 0.001326 	Lval: 0.001527
Epoch: 210 	Ltrain: 0.001323 	Lval: 0.001524
Epoch: 215 	Ltrain: 0.001323 	Lval: 0.001521
Epoch: 220 	Ltrain: 0.001318 	Lval: 0.001518
Epoch: 225 	Ltrain: 0.001313 	Lval: 0.001514
Epoch: 230 	Ltrain: 0.001314 	Lval: 0.001512
Epoch: 235 	Ltrain: 0.001310 	Lval: 0.001509
Epoch: 240 	Ltrain: 0.001310 	Lval: 0.001507
Epoch: 245 	Ltrain: 0.001304 	Lval: 0.001503
Epoch: 250 	Ltrain: 0.001303 	Lval: 0.001498
Epoch: 255 	Ltrain: 0.001301 	Lval: 0.001496
Epoch: 260 	Ltrain: 0.001298 	Lval: 0.001493
Epoch: 265 	Ltrain: 0.001300 	Lval: 0.001491
Epoch: 270 	Ltrain: 0.001297 	Lval: 0.001487
Epoch: 275 	Ltrain: 0.001292 	Lval: 0.001483
Epoch: 280 	Ltrain: 0.001289 	Lval: 0.001481
Epoch: 285 	Ltrain: 0.001287 	Lval: 0.001477
Epoch: 290 	Ltrain: 0.001287 	Lval: 0.001475
Epoch: 295 	Ltrain: 0.001283 	Lval: 0.001471
Epoch: 300 	Ltrain: 0.001281 	Lval: 0.001469
Epoch: 305 	Ltrain: 0.001278 	Lval: 0.001467
Epoch: 310 	Ltrain: 0.001276 	Lval: 0.001463
Epoch: 315 	Ltrain: 0.001272 	Lval: 0.001458
Epoch: 320 	Ltrain: 0.001272 	Lval: 0.001456
Epoch: 325 	Ltrain: 0.001268 	Lval: 0.001454
Epoch: 330 	Ltrain: 0.001266 	Lval: 0.001451
Epoch: 335 	Ltrain: 0.001264 	Lval: 0.001446
Epoch: 340 	Ltrain: 0.001260 	Lval: 0.001444
Epoch: 345 	Ltrain: 0.001260 	Lval: 0.001441
Epoch: 350 	Ltrain: 0.001256 	Lval: 0.001439
Epoch: 355 	Ltrain: 0.001252 	Lval: 0.001435
Epoch: 360 	Ltrain: 0.001254 	Lval: 0.001432
EarlyStopper: stopping at epoch 363 with best_val_loss = 0.001439


	Fold 5/5
Epoch: 1 	Ltrain: 0.044067 	Lval: 0.008556
Epoch: 5 	Ltrain: 0.004886 	Lval: 0.004754
Epoch: 10 	Ltrain: 0.004345 	Lval: 0.004173
Epoch: 15 	Ltrain: 0.003897 	Lval: 0.004356
Epoch 00018: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 20 	Ltrain: 0.003170 	Lval: 0.003569
Epoch: 25 	Ltrain: 0.003068 	Lval: 0.003509
Epoch: 30 	Ltrain: 0.003026 	Lval: 0.003485
Epoch: 35 	Ltrain: 0.002944 	Lval: 0.003340
Epoch: 40 	Ltrain: 0.002886 	Lval: 0.003270
Epoch: 45 	Ltrain: 0.002846 	Lval: 0.003271
Epoch: 50 	Ltrain: 0.002751 	Lval: 0.003129
Epoch 00054: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 55 	Ltrain: 0.002670 	Lval: 0.003034
Epoch: 60 	Ltrain: 0.002605 	Lval: 0.003018
Epoch: 65 	Ltrain: 0.002576 	Lval: 0.003000
Epoch: 70 	Ltrain: 0.002567 	Lval: 0.002981
Epoch: 75 	Ltrain: 0.002559 	Lval: 0.002969
Epoch: 80 	Ltrain: 0.002543 	Lval: 0.002964
Epoch 00083: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 85 	Ltrain: 0.002522 	Lval: 0.002944
Epoch: 90 	Ltrain: 0.002515 	Lval: 0.002940
Epoch: 95 	Ltrain: 0.002508 	Lval: 0.002938
Epoch: 100 	Ltrain: 0.002521 	Lval: 0.002934
Epoch 00105: reducing learning rate of group 0 to 7.0728e-07.
Epoch: 105 	Ltrain: 0.002513 	Lval: 0.002935
Epoch: 110 	Ltrain: 0.002508 	Lval: 0.002935
EarlyStopper: stopping at epoch 113 with best_val_loss = 0.002936

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009975264079239824
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.9689216151938953e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.273665 	Lval: 0.039285
Epoch: 5 	Ltrain: 0.011817 	Lval: 0.009847
Epoch: 10 	Ltrain: 0.006980 	Lval: 0.005366
Epoch: 15 	Ltrain: 0.005083 	Lval: 0.004812
Epoch: 20 	Ltrain: 0.005413 	Lval: 0.004518
Epoch: 25 	Ltrain: 0.004281 	Lval: 0.004126
Epoch: 30 	Ltrain: 0.004205 	Lval: 0.003920
Epoch: 35 	Ltrain: 0.004155 	Lval: 0.003968
Epoch: 40 	Ltrain: 0.004203 	Lval: 0.003679
Epoch: 45 	Ltrain: 0.003396 	Lval: 0.003400
Epoch 00049: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 50 	Ltrain: 0.004219 	Lval: 0.003422
Epoch: 55 	Ltrain: 0.003167 	Lval: 0.003125
Epoch: 60 	Ltrain: 0.003194 	Lval: 0.003031
Epoch: 65 	Ltrain: 0.003095 	Lval: 0.002945
Epoch: 70 	Ltrain: 0.002859 	Lval: 0.002878
Epoch: 75 	Ltrain: 0.002740 	Lval: 0.002815
Epoch: 80 	Ltrain: 0.002820 	Lval: 0.002749
Epoch: 85 	Ltrain: 0.002672 	Lval: 0.002685
Epoch: 90 	Ltrain: 0.002651 	Lval: 0.002636
Epoch: 95 	Ltrain: 0.002663 	Lval: 0.002586
Epoch: 100 	Ltrain: 0.002641 	Lval: 0.002540
Epoch: 105 	Ltrain: 0.002554 	Lval: 0.002462
Epoch: 110 	Ltrain: 0.002450 	Lval: 0.002421
Epoch: 115 	Ltrain: 0.002311 	Lval: 0.002343
Epoch: 120 	Ltrain: 0.002302 	Lval: 0.002271
Epoch: 125 	Ltrain: 0.002231 	Lval: 0.002208
Epoch: 130 	Ltrain: 0.002274 	Lval: 0.002164
Epoch: 135 	Ltrain: 0.002086 	Lval: 0.002123
Epoch: 140 	Ltrain: 0.002069 	Lval: 0.002026
Epoch: 145 	Ltrain: 0.002014 	Lval: 0.001942
Epoch: 150 	Ltrain: 0.001864 	Lval: 0.001885
Epoch: 155 	Ltrain: 0.001834 	Lval: 0.001925
Epoch 00158: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 160 	Ltrain: 0.001731 	Lval: 0.001756
Epoch: 165 	Ltrain: 0.001783 	Lval: 0.001732
Epoch: 170 	Ltrain: 0.001638 	Lval: 0.001722
Epoch: 175 	Ltrain: 0.001735 	Lval: 0.001712
Epoch: 180 	Ltrain: 0.001645 	Lval: 0.001705
Epoch: 185 	Ltrain: 0.001709 	Lval: 0.001693
Epoch: 190 	Ltrain: 0.001632 	Lval: 0.001685
Epoch: 195 	Ltrain: 0.001674 	Lval: 0.001677
Epoch: 200 	Ltrain: 0.001656 	Lval: 0.001669
Epoch: 205 	Ltrain: 0.001613 	Lval: 0.001660
Epoch: 210 	Ltrain: 0.001583 	Lval: 0.001651
Epoch: 215 	Ltrain: 0.001644 	Lval: 0.001642
Epoch: 220 	Ltrain: 0.001592 	Lval: 0.001632
Epoch: 225 	Ltrain: 0.001597 	Lval: 0.001629
Epoch: 230 	Ltrain: 0.001628 	Lval: 0.001618
Epoch: 235 	Ltrain: 0.001602 	Lval: 0.001606
Epoch: 240 	Ltrain: 0.001578 	Lval: 0.001597
Epoch: 245 	Ltrain: 0.001539 	Lval: 0.001589
Epoch: 250 	Ltrain: 0.001554 	Lval: 0.001576
Epoch: 255 	Ltrain: 0.001528 	Lval: 0.001569
Epoch: 260 	Ltrain: 0.001511 	Lval: 0.001559
Epoch: 265 	Ltrain: 0.001509 	Lval: 0.001548
Epoch: 270 	Ltrain: 0.001480 	Lval: 0.001540
Epoch: 275 	Ltrain: 0.001491 	Lval: 0.001532
Epoch: 280 	Ltrain: 0.001511 	Lval: 0.001520
Epoch: 285 	Ltrain: 0.001550 	Lval: 0.001510
Epoch: 290 	Ltrain: 0.001535 	Lval: 0.001501
Epoch: 295 	Ltrain: 0.001461 	Lval: 0.001493
Epoch: 300 	Ltrain: 0.001506 	Lval: 0.001481
Epoch: 305 	Ltrain: 0.001442 	Lval: 0.001471
Epoch: 310 	Ltrain: 0.001483 	Lval: 0.001461
Epoch: 315 	Ltrain: 0.001418 	Lval: 0.001450
Epoch: 320 	Ltrain: 0.001452 	Lval: 0.001441
Epoch: 325 	Ltrain: 0.001424 	Lval: 0.001432
Epoch: 330 	Ltrain: 0.001487 	Lval: 0.001420
Epoch: 335 	Ltrain: 0.001411 	Lval: 0.001410
Epoch: 340 	Ltrain: 0.001406 	Lval: 0.001399
Epoch: 345 	Ltrain: 0.001443 	Lval: 0.001391
Epoch: 350 	Ltrain: 0.001376 	Lval: 0.001379
Epoch: 355 	Ltrain: 0.001359 	Lval: 0.001370
Epoch: 360 	Ltrain: 0.001351 	Lval: 0.001357
Epoch: 365 	Ltrain: 0.001343 	Lval: 0.001347
Epoch: 370 	Ltrain: 0.001349 	Lval: 0.001337
Epoch: 375 	Ltrain: 0.001357 	Lval: 0.001325
Epoch: 380 	Ltrain: 0.001246 	Lval: 0.001313
Epoch: 385 	Ltrain: 0.001323 	Lval: 0.001304
Epoch: 390 	Ltrain: 0.001254 	Lval: 0.001293
Epoch: 395 	Ltrain: 0.001282 	Lval: 0.001281
Epoch: 400 	Ltrain: 0.001257 	Lval: 0.001270
Epoch: 405 	Ltrain: 0.001262 	Lval: 0.001258
Epoch: 410 	Ltrain: 0.001233 	Lval: 0.001249
Epoch: 415 	Ltrain: 0.001227 	Lval: 0.001237
Epoch: 420 	Ltrain: 0.001196 	Lval: 0.001229
Epoch: 425 	Ltrain: 0.001223 	Lval: 0.001217
Epoch: 430 	Ltrain: 0.001199 	Lval: 0.001206
Epoch: 435 	Ltrain: 0.001243 	Lval: 0.001192
Epoch: 440 	Ltrain: 0.001164 	Lval: 0.001185
Epoch: 445 	Ltrain: 0.001170 	Lval: 0.001170
Epoch: 450 	Ltrain: 0.001131 	Lval: 0.001161
Epoch: 455 	Ltrain: 0.001166 	Lval: 0.001147
Epoch: 460 	Ltrain: 0.001106 	Lval: 0.001140
Epoch: 465 	Ltrain: 0.001106 	Lval: 0.001132
Epoch: 470 	Ltrain: 0.001119 	Lval: 0.001117
Epoch: 475 	Ltrain: 0.001152 	Lval: 0.001106
Epoch: 480 	Ltrain: 0.001028 	Lval: 0.001092
Epoch: 485 	Ltrain: 0.001074 	Lval: 0.001087
Epoch: 490 	Ltrain: 0.001100 	Lval: 0.001071
Epoch: 495 	Ltrain: 0.001089 	Lval: 0.001059
Epoch: 500 	Ltrain: 0.000993 	Lval: 0.001046
Epoch: 505 	Ltrain: 0.001017 	Lval: 0.001038
Epoch: 510 	Ltrain: 0.001012 	Lval: 0.001025
Epoch: 515 	Ltrain: 0.000997 	Lval: 0.001015
Epoch: 520 	Ltrain: 0.000978 	Lval: 0.001003
Epoch: 525 	Ltrain: 0.000984 	Lval: 0.000994
Epoch: 530 	Ltrain: 0.000950 	Lval: 0.000980
Epoch: 535 	Ltrain: 0.000977 	Lval: 0.000968
Epoch: 540 	Ltrain: 0.000974 	Lval: 0.000958
Epoch: 545 	Ltrain: 0.000961 	Lval: 0.000949
Epoch: 550 	Ltrain: 0.000906 	Lval: 0.000934
Epoch: 555 	Ltrain: 0.000969 	Lval: 0.000925
Epoch: 560 	Ltrain: 0.000888 	Lval: 0.000913
Epoch: 565 	Ltrain: 0.000870 	Lval: 0.000902
Epoch: 570 	Ltrain: 0.000899 	Lval: 0.000892
Epoch: 575 	Ltrain: 0.000872 	Lval: 0.000881
Epoch: 580 	Ltrain: 0.000885 	Lval: 0.000881
Epoch: 585 	Ltrain: 0.000812 	Lval: 0.000856
Epoch: 590 	Ltrain: 0.000860 	Lval: 0.000848
Epoch: 595 	Ltrain: 0.000875 	Lval: 0.000832
Epoch: 600 	Ltrain: 0.000833 	Lval: 0.000822
Epoch: 605 	Ltrain: 0.000801 	Lval: 0.000814
Epoch: 610 	Ltrain: 0.000792 	Lval: 0.000815
Epoch: 615 	Ltrain: 0.000798 	Lval: 0.000804
Epoch: 620 	Ltrain: 0.000782 	Lval: 0.000787
Epoch: 625 	Ltrain: 0.000730 	Lval: 0.000768
Epoch: 630 	Ltrain: 0.000759 	Lval: 0.000759
Epoch: 635 	Ltrain: 0.000702 	Lval: 0.000747
Epoch: 640 	Ltrain: 0.000754 	Lval: 0.000747
Epoch: 645 	Ltrain: 0.000707 	Lval: 0.000742
Epoch: 650 	Ltrain: 0.000705 	Lval: 0.000717
Epoch: 655 	Ltrain: 0.000675 	Lval: 0.000703
Epoch: 660 	Ltrain: 0.000657 	Lval: 0.000694
Epoch 00664: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 665 	Ltrain: 0.000682 	Lval: 0.000687
Epoch: 670 	Ltrain: 0.000653 	Lval: 0.000679
Epoch: 675 	Ltrain: 0.000636 	Lval: 0.000678
Epoch: 680 	Ltrain: 0.000663 	Lval: 0.000676
Epoch: 685 	Ltrain: 0.000694 	Lval: 0.000675
Epoch: 690 	Ltrain: 0.000651 	Lval: 0.000674
Epoch: 695 	Ltrain: 0.000654 	Lval: 0.000673
EarlyStopper: stopping at epoch 696 with best_val_loss = 0.000677


	Fold 2/5
Epoch: 1 	Ltrain: 0.183316 	Lval: 0.022266
Epoch: 5 	Ltrain: 0.007439 	Lval: 0.006204
Epoch: 10 	Ltrain: 0.005068 	Lval: 0.005111
Epoch: 15 	Ltrain: 0.004730 	Lval: 0.004847
Epoch 00018: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 20 	Ltrain: 0.004246 	Lval: 0.004302
Epoch: 25 	Ltrain: 0.004109 	Lval: 0.004040
Epoch: 30 	Ltrain: 0.004050 	Lval: 0.004076
Epoch 00033: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 35 	Ltrain: 0.004175 	Lval: 0.003994
Epoch: 40 	Ltrain: 0.004129 	Lval: 0.003967
EarlyStopper: stopping at epoch 41 with best_val_loss = 0.003968


	Fold 3/5
Epoch: 1 	Ltrain: 0.065691 	Lval: 0.015039
Epoch: 5 	Ltrain: 0.005041 	Lval: 0.005515
Epoch 00010: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 10 	Ltrain: 0.004927 	Lval: 0.005324
Epoch: 15 	Ltrain: 0.003869 	Lval: 0.003797
Epoch: 20 	Ltrain: 0.003797 	Lval: 0.003802
Epoch: 25 	Ltrain: 0.003741 	Lval: 0.003822
Epoch: 30 	Ltrain: 0.003729 	Lval: 0.003619
Epoch: 35 	Ltrain: 0.003522 	Lval: 0.003683
Epoch: 40 	Ltrain: 0.003416 	Lval: 0.003643
Epoch: 45 	Ltrain: 0.003358 	Lval: 0.003540
Epoch: 50 	Ltrain: 0.003211 	Lval: 0.003390
Epoch 00051: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 55 	Ltrain: 0.003034 	Lval: 0.003227
Epoch: 60 	Ltrain: 0.003019 	Lval: 0.003222
Epoch: 65 	Ltrain: 0.002990 	Lval: 0.003201
Epoch: 70 	Ltrain: 0.002985 	Lval: 0.003199
Epoch 00075: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 75 	Ltrain: 0.002967 	Lval: 0.003227
Epoch: 80 	Ltrain: 0.002953 	Lval: 0.003180
Epoch: 85 	Ltrain: 0.002938 	Lval: 0.003183
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.003188


	Fold 4/5
Epoch: 1 	Ltrain: 0.056957 	Lval: 0.011692
Epoch: 5 	Ltrain: 0.004800 	Lval: 0.005212
Epoch: 10 	Ltrain: 0.004211 	Lval: 0.004244
Epoch: 15 	Ltrain: 0.003959 	Lval: 0.003766
Epoch: 20 	Ltrain: 0.003656 	Lval: 0.003703
Epoch: 25 	Ltrain: 0.003276 	Lval: 0.003413
Epoch: 30 	Ltrain: 0.003290 	Lval: 0.003978
Epoch: 35 	Ltrain: 0.003033 	Lval: 0.003028
Epoch 00037: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 40 	Ltrain: 0.002202 	Lval: 0.002450
Epoch: 45 	Ltrain: 0.002071 	Lval: 0.002362
Epoch: 50 	Ltrain: 0.001992 	Lval: 0.002298
Epoch: 55 	Ltrain: 0.001922 	Lval: 0.002248
Epoch: 60 	Ltrain: 0.001822 	Lval: 0.002021
Epoch: 65 	Ltrain: 0.001718 	Lval: 0.001946
Epoch: 70 	Ltrain: 0.001637 	Lval: 0.001779
Epoch: 75 	Ltrain: 0.001548 	Lval: 0.001711
Epoch: 80 	Ltrain: 0.001442 	Lval: 0.001624
Epoch: 85 	Ltrain: 0.001387 	Lval: 0.001469
Epoch: 90 	Ltrain: 0.001260 	Lval: 0.001316
Epoch: 95 	Ltrain: 0.001220 	Lval: 0.001259
Epoch: 100 	Ltrain: 0.001131 	Lval: 0.001292
Epoch 00103: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 105 	Ltrain: 0.000944 	Lval: 0.001024
Epoch: 110 	Ltrain: 0.000912 	Lval: 0.000990
Epoch: 115 	Ltrain: 0.000890 	Lval: 0.000975
Epoch: 120 	Ltrain: 0.000876 	Lval: 0.000953
Epoch: 125 	Ltrain: 0.000859 	Lval: 0.000938
Epoch: 130 	Ltrain: 0.000849 	Lval: 0.000920
Epoch: 135 	Ltrain: 0.000836 	Lval: 0.000907
Epoch: 140 	Ltrain: 0.000820 	Lval: 0.000890
Epoch: 145 	Ltrain: 0.000810 	Lval: 0.000874
Epoch: 150 	Ltrain: 0.000795 	Lval: 0.000856
Epoch: 155 	Ltrain: 0.000782 	Lval: 0.000851
Epoch: 160 	Ltrain: 0.000767 	Lval: 0.000825
Epoch: 165 	Ltrain: 0.000753 	Lval: 0.000812
Epoch: 170 	Ltrain: 0.000737 	Lval: 0.000789
Epoch: 175 	Ltrain: 0.000723 	Lval: 0.000773
Epoch: 180 	Ltrain: 0.000706 	Lval: 0.000758
Epoch: 185 	Ltrain: 0.000696 	Lval: 0.000740
Epoch: 190 	Ltrain: 0.000680 	Lval: 0.000731
Epoch: 195 	Ltrain: 0.000663 	Lval: 0.000706
Epoch: 200 	Ltrain: 0.000650 	Lval: 0.000688
Epoch: 205 	Ltrain: 0.000632 	Lval: 0.000666
Epoch: 210 	Ltrain: 0.000617 	Lval: 0.000652
Epoch: 215 	Ltrain: 0.000605 	Lval: 0.000634
Epoch: 220 	Ltrain: 0.000591 	Lval: 0.000618
Epoch: 225 	Ltrain: 0.000577 	Lval: 0.000613
Epoch: 230 	Ltrain: 0.000562 	Lval: 0.000591
Epoch: 235 	Ltrain: 0.000543 	Lval: 0.000571
Epoch: 240 	Ltrain: 0.000536 	Lval: 0.000561
Epoch: 245 	Ltrain: 0.000520 	Lval: 0.000546
Epoch: 250 	Ltrain: 0.000508 	Lval: 0.000532
Epoch: 255 	Ltrain: 0.000494 	Lval: 0.000513
Epoch: 260 	Ltrain: 0.000483 	Lval: 0.000509
Epoch: 265 	Ltrain: 0.000467 	Lval: 0.000494
Epoch: 270 	Ltrain: 0.000459 	Lval: 0.000484
Epoch: 275 	Ltrain: 0.000446 	Lval: 0.000465
Epoch: 280 	Ltrain: 0.000442 	Lval: 0.000462
Epoch 00281: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 285 	Ltrain: 0.000411 	Lval: 0.000438
Epoch: 290 	Ltrain: 0.000409 	Lval: 0.000435
Epoch: 295 	Ltrain: 0.000407 	Lval: 0.000434
Epoch: 300 	Ltrain: 0.000405 	Lval: 0.000432
Epoch: 305 	Ltrain: 0.000404 	Lval: 0.000430
Epoch: 310 	Ltrain: 0.000403 	Lval: 0.000429
Epoch: 315 	Ltrain: 0.000402 	Lval: 0.000428
EarlyStopper: stopping at epoch 316 with best_val_loss = 0.000432


	Fold 5/5
Epoch: 1 	Ltrain: 0.050659 	Lval: 0.008665
Epoch: 5 	Ltrain: 0.004659 	Lval: 0.004988
Epoch: 10 	Ltrain: 0.004130 	Lval: 0.004783
Epoch 00012: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 15 	Ltrain: 0.003501 	Lval: 0.003897
Epoch: 20 	Ltrain: 0.003421 	Lval: 0.003798
Epoch: 25 	Ltrain: 0.003355 	Lval: 0.003679
Epoch: 30 	Ltrain: 0.003259 	Lval: 0.003625
Epoch: 35 	Ltrain: 0.003205 	Lval: 0.003603
Epoch: 40 	Ltrain: 0.003176 	Lval: 0.003491
Epoch: 45 	Ltrain: 0.003021 	Lval: 0.003363
Epoch: 50 	Ltrain: 0.002907 	Lval: 0.003286
Epoch 00055: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 55 	Ltrain: 0.002908 	Lval: 0.003265
Epoch: 60 	Ltrain: 0.002657 	Lval: 0.003112
Epoch: 65 	Ltrain: 0.002631 	Lval: 0.003098
Epoch: 70 	Ltrain: 0.002617 	Lval: 0.003077
Epoch: 75 	Ltrain: 0.002611 	Lval: 0.003065
Epoch 00076: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 80 	Ltrain: 0.002575 	Lval: 0.003048
Epoch: 85 	Ltrain: 0.002574 	Lval: 0.003047
Epoch: 90 	Ltrain: 0.002583 	Lval: 0.003047
Epoch: 95 	Ltrain: 0.002571 	Lval: 0.003045
EarlyStopper: stopping at epoch 95 with best_val_loss = 0.003049

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009681872024514883
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.0585572347001232e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 17
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.213242 	Lval: 0.025799
Epoch: 5 	Ltrain: 0.012009 	Lval: 0.009598
Epoch: 10 	Ltrain: 0.006339 	Lval: 0.005056
Epoch: 15 	Ltrain: 0.004911 	Lval: 0.004305
Epoch: 20 	Ltrain: 0.004600 	Lval: 0.005255
Epoch 00022: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 25 	Ltrain: 0.003840 	Lval: 0.003703
Epoch: 30 	Ltrain: 0.003885 	Lval: 0.003569
Epoch: 35 	Ltrain: 0.003584 	Lval: 0.003499
Epoch: 40 	Ltrain: 0.003686 	Lval: 0.003459
Epoch: 45 	Ltrain: 0.003554 	Lval: 0.003442
Epoch: 50 	Ltrain: 0.003445 	Lval: 0.003320
Epoch: 55 	Ltrain: 0.003270 	Lval: 0.003300
Epoch: 60 	Ltrain: 0.003344 	Lval: 0.003244
Epoch: 65 	Ltrain: 0.003221 	Lval: 0.003188
Epoch: 70 	Ltrain: 0.003412 	Lval: 0.003124
Epoch: 75 	Ltrain: 0.003116 	Lval: 0.003080
Epoch: 80 	Ltrain: 0.002997 	Lval: 0.003025
Epoch: 85 	Ltrain: 0.003365 	Lval: 0.003105
Epoch 00088: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 90 	Ltrain: 0.003046 	Lval: 0.002887
Epoch: 95 	Ltrain: 0.002994 	Lval: 0.002863
Epoch: 100 	Ltrain: 0.002980 	Lval: 0.002859
Epoch: 105 	Ltrain: 0.002886 	Lval: 0.002848
Epoch: 110 	Ltrain: 0.002796 	Lval: 0.002836
Epoch: 115 	Ltrain: 0.002829 	Lval: 0.002830
Epoch: 120 	Ltrain: 0.002835 	Lval: 0.002828
Epoch: 125 	Ltrain: 0.002958 	Lval: 0.002813
Epoch: 130 	Ltrain: 0.002850 	Lval: 0.002805
Epoch: 135 	Ltrain: 0.002869 	Lval: 0.002795
Epoch: 140 	Ltrain: 0.002716 	Lval: 0.002788
Epoch: 145 	Ltrain: 0.002775 	Lval: 0.002775
Epoch: 150 	Ltrain: 0.002674 	Lval: 0.002777
Epoch: 155 	Ltrain: 0.002810 	Lval: 0.002761
Epoch: 160 	Ltrain: 0.002714 	Lval: 0.002754
Epoch: 165 	Ltrain: 0.002880 	Lval: 0.002742
Epoch: 170 	Ltrain: 0.002870 	Lval: 0.002741
Epoch: 175 	Ltrain: 0.002791 	Lval: 0.002718
Epoch: 180 	Ltrain: 0.002772 	Lval: 0.002707
Epoch: 185 	Ltrain: 0.002629 	Lval: 0.002702
Epoch: 190 	Ltrain: 0.002851 	Lval: 0.002687
Epoch: 195 	Ltrain: 0.002711 	Lval: 0.002678
Epoch: 200 	Ltrain: 0.002746 	Lval: 0.002674
Epoch 00202: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 205 	Ltrain: 0.002822 	Lval: 0.002670
Epoch: 210 	Ltrain: 0.002718 	Lval: 0.002666
Epoch: 215 	Ltrain: 0.002598 	Lval: 0.002665
EarlyStopper: stopping at epoch 214 with best_val_loss = 0.002672


	Fold 2/5
Epoch: 1 	Ltrain: 0.250648 	Lval: 0.030527
Epoch: 5 	Ltrain: 0.014240 	Lval: 0.012931
Epoch: 10 	Ltrain: 0.007805 	Lval: 0.006645
Epoch: 15 	Ltrain: 0.005267 	Lval: 0.004935
Epoch: 20 	Ltrain: 0.004533 	Lval: 0.004320
Epoch: 25 	Ltrain: 0.004609 	Lval: 0.005239
Epoch: 30 	Ltrain: 0.004380 	Lval: 0.003993
Epoch: 35 	Ltrain: 0.004182 	Lval: 0.004379
Epoch 00036: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 40 	Ltrain: 0.003781 	Lval: 0.003827
Epoch: 45 	Ltrain: 0.003768 	Lval: 0.003679
Epoch: 50 	Ltrain: 0.003772 	Lval: 0.003733
Epoch 00052: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 55 	Ltrain: 0.003743 	Lval: 0.003693
Epoch: 60 	Ltrain: 0.003674 	Lval: 0.003696
EarlyStopper: stopping at epoch 61 with best_val_loss = 0.003679


	Fold 3/5
Epoch: 1 	Ltrain: 0.193160 	Lval: 0.020918
Epoch: 5 	Ltrain: 0.011580 	Lval: 0.008097
Epoch: 10 	Ltrain: 0.005967 	Lval: 0.005063
Epoch: 15 	Ltrain: 0.004990 	Lval: 0.004739
Epoch: 20 	Ltrain: 0.004497 	Lval: 0.004174
Epoch: 25 	Ltrain: 0.004291 	Lval: 0.003975
Epoch 00028: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 30 	Ltrain: 0.003614 	Lval: 0.003652
Epoch: 35 	Ltrain: 0.003567 	Lval: 0.003606
Epoch: 40 	Ltrain: 0.003495 	Lval: 0.003557
Epoch 00044: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 45 	Ltrain: 0.003462 	Lval: 0.003584
Epoch: 50 	Ltrain: 0.003451 	Lval: 0.003575
Epoch: 55 	Ltrain: 0.003412 	Lval: 0.003576
EarlyStopper: stopping at epoch 56 with best_val_loss = 0.003557


	Fold 4/5
Epoch: 1 	Ltrain: 0.089163 	Lval: 0.014576
Epoch: 5 	Ltrain: 0.004729 	Lval: 0.004556
Epoch: 10 	Ltrain: 0.004023 	Lval: 0.004686
Epoch: 15 	Ltrain: 0.003621 	Lval: 0.004043
Epoch: 20 	Ltrain: 0.003888 	Lval: 0.003585
Epoch: 25 	Ltrain: 0.003189 	Lval: 0.003150
Epoch 00029: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 30 	Ltrain: 0.002968 	Lval: 0.002926
Epoch: 35 	Ltrain: 0.002448 	Lval: 0.002747
Epoch: 40 	Ltrain: 0.002359 	Lval: 0.002660
Epoch: 45 	Ltrain: 0.002304 	Lval: 0.002658
Epoch: 50 	Ltrain: 0.002253 	Lval: 0.002509
Epoch: 55 	Ltrain: 0.002179 	Lval: 0.002463
Epoch: 60 	Ltrain: 0.002111 	Lval: 0.002434
Epoch: 65 	Ltrain: 0.002024 	Lval: 0.002270
Epoch: 70 	Ltrain: 0.001929 	Lval: 0.002237
Epoch: 75 	Ltrain: 0.001831 	Lval: 0.002038
Epoch: 80 	Ltrain: 0.001766 	Lval: 0.001999
Epoch: 85 	Ltrain: 0.001687 	Lval: 0.001865
Epoch: 90 	Ltrain: 0.001604 	Lval: 0.001762
Epoch: 95 	Ltrain: 0.001554 	Lval: 0.001680
Epoch: 100 	Ltrain: 0.001398 	Lval: 0.001532
Epoch: 105 	Ltrain: 0.001303 	Lval: 0.001418
Epoch: 110 	Ltrain: 0.001255 	Lval: 0.001313
Epoch: 115 	Ltrain: 0.001183 	Lval: 0.001284
Epoch: 120 	Ltrain: 0.001128 	Lval: 0.001138
Epoch 00124: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 125 	Ltrain: 0.000987 	Lval: 0.001019
Epoch: 130 	Ltrain: 0.000858 	Lval: 0.000958
Epoch: 135 	Ltrain: 0.000839 	Lval: 0.000939
Epoch: 140 	Ltrain: 0.000824 	Lval: 0.000919
Epoch: 145 	Ltrain: 0.000812 	Lval: 0.000907
Epoch: 150 	Ltrain: 0.000798 	Lval: 0.000892
Epoch: 155 	Ltrain: 0.000786 	Lval: 0.000874
Epoch: 160 	Ltrain: 0.000776 	Lval: 0.000861
Epoch: 165 	Ltrain: 0.000762 	Lval: 0.000853
Epoch: 170 	Ltrain: 0.000750 	Lval: 0.000833
Epoch: 175 	Ltrain: 0.000739 	Lval: 0.000818
Epoch: 180 	Ltrain: 0.000722 	Lval: 0.000803
Epoch: 185 	Ltrain: 0.000711 	Lval: 0.000786
Epoch: 190 	Ltrain: 0.000701 	Lval: 0.000775
Epoch: 195 	Ltrain: 0.000681 	Lval: 0.000753
Epoch: 200 	Ltrain: 0.000671 	Lval: 0.000738
Epoch: 205 	Ltrain: 0.000658 	Lval: 0.000723
Epoch: 210 	Ltrain: 0.000645 	Lval: 0.000705
Epoch: 215 	Ltrain: 0.000631 	Lval: 0.000687
Epoch: 220 	Ltrain: 0.000615 	Lval: 0.000677
Epoch: 225 	Ltrain: 0.000602 	Lval: 0.000661
Epoch: 230 	Ltrain: 0.000591 	Lval: 0.000638
Epoch: 235 	Ltrain: 0.000573 	Lval: 0.000623
Epoch: 240 	Ltrain: 0.000561 	Lval: 0.000609
Epoch: 245 	Ltrain: 0.000548 	Lval: 0.000593
Epoch: 250 	Ltrain: 0.000532 	Lval: 0.000582
Epoch: 255 	Ltrain: 0.000522 	Lval: 0.000564
Epoch: 260 	Ltrain: 0.000509 	Lval: 0.000549
Epoch: 265 	Ltrain: 0.000503 	Lval: 0.000528
Epoch: 270 	Ltrain: 0.000485 	Lval: 0.000521
Epoch: 275 	Ltrain: 0.000476 	Lval: 0.000507
Epoch: 280 	Ltrain: 0.000470 	Lval: 0.000495
Epoch: 285 	Ltrain: 0.000448 	Lval: 0.000476
Epoch: 290 	Ltrain: 0.000442 	Lval: 0.000468
Epoch: 295 	Ltrain: 0.000440 	Lval: 0.000464
Epoch: 300 	Ltrain: 0.000425 	Lval: 0.000443
Epoch: 305 	Ltrain: 0.000412 	Lval: 0.000435
Epoch: 310 	Ltrain: 0.000395 	Lval: 0.000419
Epoch: 315 	Ltrain: 0.000398 	Lval: 0.000421
Epoch: 320 	Ltrain: 0.000388 	Lval: 0.000412
Epoch: 325 	Ltrain: 0.000376 	Lval: 0.000390
Epoch 00329: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 330 	Ltrain: 0.000364 	Lval: 0.000372
Epoch: 335 	Ltrain: 0.000340 	Lval: 0.000363
Epoch: 340 	Ltrain: 0.000338 	Lval: 0.000360
Epoch: 345 	Ltrain: 0.000336 	Lval: 0.000359
Epoch: 350 	Ltrain: 0.000335 	Lval: 0.000357
EarlyStopper: stopping at epoch 353 with best_val_loss = 0.000362


	Fold 5/5
Epoch: 1 	Ltrain: 0.136684 	Lval: 0.015111
Epoch: 5 	Ltrain: 0.005342 	Lval: 0.005154
Epoch: 10 	Ltrain: 0.004623 	Lval: 0.004815
Epoch: 15 	Ltrain: 0.004110 	Lval: 0.004587
Epoch 00020: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 20 	Ltrain: 0.003764 	Lval: 0.004193
Epoch: 25 	Ltrain: 0.003429 	Lval: 0.003821
Epoch: 30 	Ltrain: 0.003416 	Lval: 0.003878
Epoch: 35 	Ltrain: 0.003323 	Lval: 0.003765
Epoch 00037: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 40 	Ltrain: 0.003260 	Lval: 0.003701
Epoch: 45 	Ltrain: 0.003254 	Lval: 0.003710
Epoch 00049: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 50 	Ltrain: 0.003243 	Lval: 0.003687
Epoch: 55 	Ltrain: 0.003242 	Lval: 0.003687
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.003686

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0012363506550085658
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.2473652296614074e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.023571 	Lval: 0.015583
Epoch: 5 	Ltrain: 0.009319 	Lval: 0.008393
Epoch: 10 	Ltrain: 0.006121 	Lval: 0.008121
Epoch: 15 	Ltrain: 0.005183 	Lval: 0.005127
Epoch: 20 	Ltrain: 0.005639 	Lval: 0.005019
Epoch 00021: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 25 	Ltrain: 0.004578 	Lval: 0.004468
Epoch: 30 	Ltrain: 0.004517 	Lval: 0.004412
Epoch: 35 	Ltrain: 0.004485 	Lval: 0.004375
Epoch 00039: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 40 	Ltrain: 0.004407 	Lval: 0.004363
Epoch: 45 	Ltrain: 0.004262 	Lval: 0.004357
Epoch: 50 	Ltrain: 0.004831 	Lval: 0.004350
Epoch: 55 	Ltrain: 0.004391 	Lval: 0.004347
Epoch: 60 	Ltrain: 0.004308 	Lval: 0.004342
Epoch: 65 	Ltrain: 0.004406 	Lval: 0.004335
Epoch: 70 	Ltrain: 0.004389 	Lval: 0.004333
Epoch: 75 	Ltrain: 0.004725 	Lval: 0.004324
Epoch 00079: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 80 	Ltrain: 0.004295 	Lval: 0.004323
Epoch: 85 	Ltrain: 0.004415 	Lval: 0.004323
EarlyStopper: stopping at epoch 88 with best_val_loss = 0.004329


	Fold 2/5
Epoch: 1 	Ltrain: 0.023259 	Lval: 0.013090
Epoch: 5 	Ltrain: 0.007343 	Lval: 0.007260
Epoch: 10 	Ltrain: 0.005422 	Lval: 0.005157
Epoch: 15 	Ltrain: 0.004723 	Lval: 0.004576
Epoch: 20 	Ltrain: 0.004419 	Lval: 0.004340
Epoch: 25 	Ltrain: 0.004516 	Lval: 0.004018
Epoch: 30 	Ltrain: 0.004264 	Lval: 0.004055
Epoch: 35 	Ltrain: 0.004035 	Lval: 0.003815
Epoch: 40 	Ltrain: 0.004038 	Lval: 0.003636
Epoch: 45 	Ltrain: 0.003925 	Lval: 0.003499
Epoch: 50 	Ltrain: 0.003578 	Lval: 0.003817
Epoch 00051: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 55 	Ltrain: 0.003270 	Lval: 0.003203
Epoch: 60 	Ltrain: 0.003114 	Lval: 0.003117
Epoch 00064: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 65 	Ltrain: 0.003080 	Lval: 0.003119
Epoch: 70 	Ltrain: 0.003040 	Lval: 0.003123
Epoch: 75 	Ltrain: 0.003100 	Lval: 0.003137
Epoch 00076: reducing learning rate of group 0 to 1.2364e-06.
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.003117


	Fold 3/5
Epoch: 1 	Ltrain: 0.018626 	Lval: 0.013471
Epoch: 5 	Ltrain: 0.006207 	Lval: 0.006037
Epoch: 10 	Ltrain: 0.004581 	Lval: 0.004571
Epoch: 15 	Ltrain: 0.004477 	Lval: 0.004404
Epoch: 20 	Ltrain: 0.004103 	Lval: 0.004021
Epoch: 25 	Ltrain: 0.004014 	Lval: 0.003731
Epoch: 30 	Ltrain: 0.003788 	Lval: 0.003849
Epoch: 35 	Ltrain: 0.003490 	Lval: 0.003468
Epoch: 40 	Ltrain: 0.003400 	Lval: 0.003278
Epoch: 45 	Ltrain: 0.003291 	Lval: 0.003151
Epoch 00046: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 50 	Ltrain: 0.002788 	Lval: 0.002885
Epoch: 55 	Ltrain: 0.002756 	Lval: 0.002859
Epoch 00058: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 60 	Ltrain: 0.002697 	Lval: 0.002840
Epoch: 65 	Ltrain: 0.002691 	Lval: 0.002842
Epoch: 70 	Ltrain: 0.002677 	Lval: 0.002839
Epoch: 75 	Ltrain: 0.002668 	Lval: 0.002830
Epoch: 80 	Ltrain: 0.002675 	Lval: 0.002832
Epoch 00082: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 85 	Ltrain: 0.002683 	Lval: 0.002833
Epoch: 90 	Ltrain: 0.002670 	Lval: 0.002832
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.002830


	Fold 4/5
Epoch: 1 	Ltrain: 0.013295 	Lval: 0.008413
Epoch: 5 	Ltrain: 0.005262 	Lval: 0.005422
Epoch: 10 	Ltrain: 0.004468 	Lval: 0.004555
Epoch: 15 	Ltrain: 0.003997 	Lval: 0.004162
Epoch: 20 	Ltrain: 0.003618 	Lval: 0.003765
Epoch: 25 	Ltrain: 0.003284 	Lval: 0.003587
Epoch: 30 	Ltrain: 0.003288 	Lval: 0.003307
Epoch: 35 	Ltrain: 0.002998 	Lval: 0.003170
Epoch 00037: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 40 	Ltrain: 0.002635 	Lval: 0.002858
Epoch: 45 	Ltrain: 0.002591 	Lval: 0.002828
Epoch 00050: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 50 	Ltrain: 0.002554 	Lval: 0.002827
Epoch: 55 	Ltrain: 0.002535 	Lval: 0.002792
Epoch: 60 	Ltrain: 0.002519 	Lval: 0.002792
Epoch 00065: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 65 	Ltrain: 0.002533 	Lval: 0.002789
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.002795


	Fold 5/5
Epoch: 1 	Ltrain: 0.017675 	Lval: 0.010192
Epoch: 5 	Ltrain: 0.005049 	Lval: 0.004796
Epoch: 10 	Ltrain: 0.004317 	Lval: 0.004804
Epoch: 15 	Ltrain: 0.004038 	Lval: 0.004417
Epoch 00020: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 20 	Ltrain: 0.004054 	Lval: 0.004268
Epoch: 25 	Ltrain: 0.003636 	Lval: 0.003953
Epoch: 30 	Ltrain: 0.003578 	Lval: 0.003944
Epoch: 35 	Ltrain: 0.003546 	Lval: 0.003898
Epoch: 40 	Ltrain: 0.003515 	Lval: 0.003906
Epoch: 45 	Ltrain: 0.003496 	Lval: 0.003835
Epoch: 50 	Ltrain: 0.003457 	Lval: 0.003882
Epoch: 55 	Ltrain: 0.003381 	Lval: 0.003794
Epoch: 60 	Ltrain: 0.003391 	Lval: 0.003742
Epoch 00061: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 65 	Ltrain: 0.003271 	Lval: 0.003693
Epoch: 70 	Ltrain: 0.003269 	Lval: 0.003693
Epoch: 75 	Ltrain: 0.003267 	Lval: 0.003678
Epoch 00079: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 80 	Ltrain: 0.003255 	Lval: 0.003682
EarlyStopper: stopping at epoch 83 with best_val_loss = 0.003687

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004668373855365965
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.915314468484723e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 12
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.038872 	Lval: 0.012227
Epoch: 5 	Ltrain: 0.007420 	Lval: 0.006893
Epoch: 10 	Ltrain: 0.005131 	Lval: 0.004890
Epoch 00014: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 15 	Ltrain: 0.004881 	Lval: 0.004627
Epoch: 20 	Ltrain: 0.004451 	Lval: 0.004352
Epoch: 25 	Ltrain: 0.004388 	Lval: 0.004302
Epoch: 30 	Ltrain: 0.004462 	Lval: 0.004245
Epoch: 35 	Ltrain: 0.004386 	Lval: 0.004175
Epoch: 40 	Ltrain: 0.004231 	Lval: 0.004102
Epoch: 45 	Ltrain: 0.004445 	Lval: 0.004006
Epoch: 50 	Ltrain: 0.004032 	Lval: 0.003932
Epoch: 55 	Ltrain: 0.004055 	Lval: 0.003935
Epoch: 60 	Ltrain: 0.004272 	Lval: 0.003821
Epoch: 65 	Ltrain: 0.004085 	Lval: 0.003802
Epoch: 70 	Ltrain: 0.003877 	Lval: 0.003856
Epoch: 75 	Ltrain: 0.003709 	Lval: 0.003637
Epoch: 80 	Ltrain: 0.003630 	Lval: 0.003589
Epoch: 85 	Ltrain: 0.003597 	Lval: 0.003606
Epoch: 90 	Ltrain: 0.003714 	Lval: 0.003852
Epoch 00091: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 95 	Ltrain: 0.003768 	Lval: 0.003510
Epoch: 100 	Ltrain: 0.003631 	Lval: 0.003487
Epoch: 105 	Ltrain: 0.003533 	Lval: 0.003477
Epoch: 110 	Ltrain: 0.003361 	Lval: 0.003464
Epoch 00114: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 115 	Ltrain: 0.003390 	Lval: 0.003463
Epoch: 120 	Ltrain: 0.003414 	Lval: 0.003460
EarlyStopper: stopping at epoch 121 with best_val_loss = 0.003464


	Fold 2/5
Epoch: 1 	Ltrain: 0.030096 	Lval: 0.011371
Epoch: 5 	Ltrain: 0.005430 	Lval: 0.005510
Epoch: 10 	Ltrain: 0.004889 	Lval: 0.004757
Epoch: 15 	Ltrain: 0.004362 	Lval: 0.004788
Epoch: 20 	Ltrain: 0.004293 	Lval: 0.004143
Epoch: 25 	Ltrain: 0.004286 	Lval: 0.003447
Epoch: 30 	Ltrain: 0.003688 	Lval: 0.003433
Epoch 00033: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 35 	Ltrain: 0.003135 	Lval: 0.002996
Epoch: 40 	Ltrain: 0.002907 	Lval: 0.002914
Epoch: 45 	Ltrain: 0.002862 	Lval: 0.002887
Epoch 00048: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 50 	Ltrain: 0.002757 	Lval: 0.002815
Epoch: 55 	Ltrain: 0.002752 	Lval: 0.002812
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.002801


	Fold 3/5
Epoch: 1 	Ltrain: 0.025021 	Lval: 0.008424
Epoch: 5 	Ltrain: 0.005440 	Lval: 0.004851
Epoch: 10 	Ltrain: 0.004456 	Lval: 0.004207
Epoch 00013: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 15 	Ltrain: 0.003952 	Lval: 0.003926
Epoch: 20 	Ltrain: 0.003826 	Lval: 0.003887
Epoch: 25 	Ltrain: 0.003687 	Lval: 0.003703
Epoch: 30 	Ltrain: 0.003594 	Lval: 0.003709
Epoch 00031: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 35 	Ltrain: 0.003484 	Lval: 0.003594
Epoch: 40 	Ltrain: 0.003501 	Lval: 0.003610
Epoch: 45 	Ltrain: 0.003504 	Lval: 0.003598
Epoch 00047: reducing learning rate of group 0 to 4.6684e-06.
EarlyStopper: stopping at epoch 46 with best_val_loss = 0.003594


	Fold 4/5
Epoch: 1 	Ltrain: 0.021515 	Lval: 0.009685
Epoch: 5 	Ltrain: 0.005039 	Lval: 0.004369
Epoch: 10 	Ltrain: 0.004078 	Lval: 0.004025
Epoch 00014: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 15 	Ltrain: 0.003510 	Lval: 0.003664
Epoch: 20 	Ltrain: 0.003269 	Lval: 0.003512
Epoch: 25 	Ltrain: 0.003198 	Lval: 0.003424
Epoch 00030: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 30 	Ltrain: 0.003139 	Lval: 0.003406
Epoch: 35 	Ltrain: 0.003030 	Lval: 0.003306
Epoch: 40 	Ltrain: 0.003030 	Lval: 0.003304
Epoch 00042: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 45 	Ltrain: 0.003006 	Lval: 0.003293
Epoch: 50 	Ltrain: 0.003012 	Lval: 0.003290
EarlyStopper: stopping at epoch 49 with best_val_loss = 0.003288


	Fold 5/5
Epoch: 1 	Ltrain: 0.022216 	Lval: 0.008315
Epoch: 5 	Ltrain: 0.004494 	Lval: 0.004616
Epoch: 10 	Ltrain: 0.004318 	Lval: 0.004518
Epoch 00011: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 15 	Ltrain: 0.003544 	Lval: 0.003861
Epoch: 20 	Ltrain: 0.003443 	Lval: 0.003761
Epoch: 25 	Ltrain: 0.003366 	Lval: 0.003731
Epoch: 30 	Ltrain: 0.003217 	Lval: 0.003593
Epoch: 35 	Ltrain: 0.003128 	Lval: 0.003531
Epoch: 40 	Ltrain: 0.003112 	Lval: 0.003423
Epoch: 45 	Ltrain: 0.002985 	Lval: 0.003288
Epoch: 50 	Ltrain: 0.003002 	Lval: 0.003450
Epoch 00053: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 55 	Ltrain: 0.002778 	Lval: 0.003128
Epoch: 60 	Ltrain: 0.002766 	Lval: 0.003134
Epoch: 65 	Ltrain: 0.002750 	Lval: 0.003108
Epoch: 70 	Ltrain: 0.002738 	Lval: 0.003104
Epoch: 75 	Ltrain: 0.002723 	Lval: 0.003094
Epoch: 80 	Ltrain: 0.002713 	Lval: 0.003084
Epoch 00082: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 85 	Ltrain: 0.002698 	Lval: 0.003077
Epoch: 90 	Ltrain: 0.002693 	Lval: 0.003076
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.003078

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0019052688486910151
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.920667063304991e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 30
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.030467 	Lval: 0.013733
Epoch: 5 	Ltrain: 0.014916 	Lval: 0.011498
Epoch: 10 	Ltrain: 0.009118 	Lval: 0.009515
Epoch: 15 	Ltrain: 0.009233 	Lval: 0.008004
Epoch: 20 	Ltrain: 0.006157 	Lval: 0.006208
Epoch 00025: reducing learning rate of group 0 to 1.9053e-04.
Epoch: 25 	Ltrain: 0.006204 	Lval: 0.007387
Epoch: 30 	Ltrain: 0.005572 	Lval: 0.005850
Epoch: 35 	Ltrain: 0.005706 	Lval: 0.004992
Epoch: 40 	Ltrain: 0.005492 	Lval: 0.004760
Epoch: 45 	Ltrain: 0.004950 	Lval: 0.004741
Epoch: 50 	Ltrain: 0.004568 	Lval: 0.004782
Epoch 00051: reducing learning rate of group 0 to 1.9053e-05.
Epoch: 55 	Ltrain: 0.005115 	Lval: 0.004640
Epoch: 60 	Ltrain: 0.005169 	Lval: 0.004603
Epoch: 65 	Ltrain: 0.005602 	Lval: 0.004599
Epoch 00067: reducing learning rate of group 0 to 1.9053e-06.
Epoch: 70 	Ltrain: 0.004951 	Lval: 0.004624
Epoch: 75 	Ltrain: 0.004637 	Lval: 0.004628
Epoch 00079: reducing learning rate of group 0 to 1.9053e-07.
Epoch: 80 	Ltrain: 0.005189 	Lval: 0.004623
Epoch: 85 	Ltrain: 0.004477 	Lval: 0.004622
Epoch: 90 	Ltrain: 0.005211 	Lval: 0.004622
Epoch 00091: reducing learning rate of group 0 to 1.9053e-08.
EarlyStopper: stopping at epoch 90 with best_val_loss = 0.004598


	Fold 2/5
Epoch: 1 	Ltrain: 0.043368 	Lval: 0.023793
Epoch: 5 	Ltrain: 0.012462 	Lval: 0.010476
Epoch: 10 	Ltrain: 0.007811 	Lval: 0.006873
Epoch: 15 	Ltrain: 0.006514 	Lval: 0.005807
Epoch: 20 	Ltrain: 0.005644 	Lval: 0.004639
Epoch 00023: reducing learning rate of group 0 to 1.9053e-04.
Epoch: 25 	Ltrain: 0.005234 	Lval: 0.004814
Epoch: 30 	Ltrain: 0.005148 	Lval: 0.004488
Epoch: 35 	Ltrain: 0.004757 	Lval: 0.004326
Epoch 00036: reducing learning rate of group 0 to 1.9053e-05.
Epoch: 40 	Ltrain: 0.004808 	Lval: 0.004360
Epoch: 45 	Ltrain: 0.004997 	Lval: 0.004351
Epoch 00048: reducing learning rate of group 0 to 1.9053e-06.
Epoch: 50 	Ltrain: 0.004766 	Lval: 0.004358
Epoch: 55 	Ltrain: 0.004732 	Lval: 0.004360
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.004262


	Fold 3/5
Epoch: 1 	Ltrain: 0.026575 	Lval: 0.012480
Epoch: 5 	Ltrain: 0.009321 	Lval: 0.008844
Epoch: 10 	Ltrain: 0.006062 	Lval: 0.004991
Epoch: 15 	Ltrain: 0.004862 	Lval: 0.004632
Epoch 00020: reducing learning rate of group 0 to 1.9053e-04.
Epoch: 20 	Ltrain: 0.004438 	Lval: 0.004765
Epoch: 25 	Ltrain: 0.004662 	Lval: 0.003977
Epoch: 30 	Ltrain: 0.004209 	Lval: 0.003925
Epoch: 35 	Ltrain: 0.004223 	Lval: 0.003982
Epoch 00036: reducing learning rate of group 0 to 1.9053e-05.
Epoch: 40 	Ltrain: 0.004327 	Lval: 0.003954
Epoch: 45 	Ltrain: 0.004961 	Lval: 0.003985
Epoch 00048: reducing learning rate of group 0 to 1.9053e-06.
Epoch: 50 	Ltrain: 0.004271 	Lval: 0.003957
Epoch: 55 	Ltrain: 0.004656 	Lval: 0.003948
Epoch 00060: reducing learning rate of group 0 to 1.9053e-07.
Epoch: 60 	Ltrain: 0.004632 	Lval: 0.003952
Epoch: 65 	Ltrain: 0.004291 	Lval: 0.003951
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.003913


	Fold 4/5
Epoch: 1 	Ltrain: 0.023602 	Lval: 0.013263
Epoch: 5 	Ltrain: 0.007888 	Lval: 0.006707
Epoch: 10 	Ltrain: 0.004945 	Lval: 0.004442
Epoch: 15 	Ltrain: 0.004498 	Lval: 0.004351
Epoch: 20 	Ltrain: 0.004235 	Lval: 0.003976
Epoch: 25 	Ltrain: 0.003933 	Lval: 0.004091
Epoch 00027: reducing learning rate of group 0 to 1.9053e-04.
Epoch: 30 	Ltrain: 0.003795 	Lval: 0.003774
Epoch: 35 	Ltrain: 0.003809 	Lval: 0.003788
Epoch 00039: reducing learning rate of group 0 to 1.9053e-05.
Epoch: 40 	Ltrain: 0.003683 	Lval: 0.003785
Epoch: 45 	Ltrain: 0.003617 	Lval: 0.003765
Epoch: 50 	Ltrain: 0.003655 	Lval: 0.003761
Epoch 00051: reducing learning rate of group 0 to 1.9053e-06.
Epoch: 55 	Ltrain: 0.003753 	Lval: 0.003761
Epoch: 60 	Ltrain: 0.003801 	Lval: 0.003761
Epoch 00063: reducing learning rate of group 0 to 1.9053e-07.
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.003760


	Fold 5/5
Epoch: 1 	Ltrain: 0.025850 	Lval: 0.014430
Epoch: 5 	Ltrain: 0.007846 	Lval: 0.007747
Epoch: 10 	Ltrain: 0.005161 	Lval: 0.005478
Epoch: 15 	Ltrain: 0.004944 	Lval: 0.005162
Epoch: 20 	Ltrain: 0.004248 	Lval: 0.004445
Epoch: 25 	Ltrain: 0.004247 	Lval: 0.004223
Epoch: 30 	Ltrain: 0.003948 	Lval: 0.004665
Epoch 00033: reducing learning rate of group 0 to 1.9053e-04.
Epoch: 35 	Ltrain: 0.003581 	Lval: 0.003964
Epoch: 40 	Ltrain: 0.003555 	Lval: 0.003966
Epoch: 45 	Ltrain: 0.003609 	Lval: 0.003952
Epoch 00048: reducing learning rate of group 0 to 1.9053e-05.
Epoch: 50 	Ltrain: 0.003532 	Lval: 0.003958
Epoch: 55 	Ltrain: 0.003571 	Lval: 0.003941
Epoch: 60 	Ltrain: 0.003689 	Lval: 0.003943
Epoch 00061: reducing learning rate of group 0 to 1.9053e-06.
Epoch: 65 	Ltrain: 0.003441 	Lval: 0.003942
Epoch: 70 	Ltrain: 0.003603 	Lval: 0.003941
Epoch 00073: reducing learning rate of group 0 to 1.9053e-07.
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.003950

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009584509599652445
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.3983961523105986e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.365928 	Lval: 0.045375
Epoch: 5 	Ltrain: 0.013878 	Lval: 0.011429
Epoch: 10 	Ltrain: 0.006129 	Lval: 0.005510
Epoch: 15 	Ltrain: 0.005874 	Lval: 0.006725
Epoch: 20 	Ltrain: 0.005425 	Lval: 0.006943
Epoch 00022: reducing learning rate of group 0 to 9.5845e-04.
Epoch: 25 	Ltrain: 0.004508 	Lval: 0.004373
Epoch: 30 	Ltrain: 0.004653 	Lval: 0.004262
Epoch: 35 	Ltrain: 0.004290 	Lval: 0.004222
Epoch: 40 	Ltrain: 0.004152 	Lval: 0.004192
Epoch: 45 	Ltrain: 0.004388 	Lval: 0.004155
Epoch: 50 	Ltrain: 0.004403 	Lval: 0.004104
Epoch: 55 	Ltrain: 0.004234 	Lval: 0.004078
Epoch: 60 	Ltrain: 0.004151 	Lval: 0.004038
Epoch: 65 	Ltrain: 0.004000 	Lval: 0.003973
Epoch: 70 	Ltrain: 0.004041 	Lval: 0.003924
Epoch: 75 	Ltrain: 0.003957 	Lval: 0.003889
Epoch: 80 	Ltrain: 0.004061 	Lval: 0.003837
Epoch: 85 	Ltrain: 0.003957 	Lval: 0.003770
Epoch: 90 	Ltrain: 0.003989 	Lval: 0.003747
Epoch: 95 	Ltrain: 0.003877 	Lval: 0.003655
Epoch: 100 	Ltrain: 0.003746 	Lval: 0.003607
Epoch: 105 	Ltrain: 0.003731 	Lval: 0.003550
Epoch: 110 	Ltrain: 0.003630 	Lval: 0.003527
Epoch: 115 	Ltrain: 0.003414 	Lval: 0.003434
Epoch: 120 	Ltrain: 0.003503 	Lval: 0.003507
Epoch: 125 	Ltrain: 0.003376 	Lval: 0.003297
Epoch: 130 	Ltrain: 0.003284 	Lval: 0.003203
Epoch: 135 	Ltrain: 0.003377 	Lval: 0.003153
Epoch: 140 	Ltrain: 0.003067 	Lval: 0.003091
Epoch: 145 	Ltrain: 0.003001 	Lval: 0.002970
Epoch: 150 	Ltrain: 0.002893 	Lval: 0.002986
Epoch 00152: reducing learning rate of group 0 to 9.5845e-05.
Epoch: 155 	Ltrain: 0.002802 	Lval: 0.002841
Epoch: 160 	Ltrain: 0.002797 	Lval: 0.002826
Epoch: 165 	Ltrain: 0.002774 	Lval: 0.002815
Epoch: 170 	Ltrain: 0.002771 	Lval: 0.002808
Epoch: 175 	Ltrain: 0.002787 	Lval: 0.002797
Epoch: 180 	Ltrain: 0.002814 	Lval: 0.002791
Epoch: 185 	Ltrain: 0.002698 	Lval: 0.002781
Epoch: 190 	Ltrain: 0.002680 	Lval: 0.002770
Epoch: 195 	Ltrain: 0.002694 	Lval: 0.002759
Epoch: 200 	Ltrain: 0.002738 	Lval: 0.002749
Epoch: 205 	Ltrain: 0.002743 	Lval: 0.002735
Epoch: 210 	Ltrain: 0.002719 	Lval: 0.002738
Epoch: 215 	Ltrain: 0.002619 	Lval: 0.002724
Epoch: 220 	Ltrain: 0.002715 	Lval: 0.002704
Epoch: 225 	Ltrain: 0.002749 	Lval: 0.002696
Epoch: 230 	Ltrain: 0.002616 	Lval: 0.002693
Epoch: 235 	Ltrain: 0.002594 	Lval: 0.002673
Epoch: 240 	Ltrain: 0.002779 	Lval: 0.002662
Epoch: 245 	Ltrain: 0.002597 	Lval: 0.002650
Epoch: 250 	Ltrain: 0.002644 	Lval: 0.002636
Epoch: 255 	Ltrain: 0.002584 	Lval: 0.002626
Epoch: 260 	Ltrain: 0.002628 	Lval: 0.002616
Epoch: 265 	Ltrain: 0.002608 	Lval: 0.002623
Epoch: 270 	Ltrain: 0.002502 	Lval: 0.002592
Epoch: 275 	Ltrain: 0.002665 	Lval: 0.002576
Epoch: 280 	Ltrain: 0.002540 	Lval: 0.002560
Epoch: 285 	Ltrain: 0.002564 	Lval: 0.002558
Epoch: 290 	Ltrain: 0.002543 	Lval: 0.002544
Epoch 00292: reducing learning rate of group 0 to 9.5845e-06.
Epoch: 295 	Ltrain: 0.002520 	Lval: 0.002535
Epoch: 300 	Ltrain: 0.002476 	Lval: 0.002531
Epoch 00304: reducing learning rate of group 0 to 9.5845e-07.
Epoch: 305 	Ltrain: 0.002566 	Lval: 0.002530
Epoch: 310 	Ltrain: 0.002555 	Lval: 0.002529
EarlyStopper: stopping at epoch 313 with best_val_loss = 0.002535


	Fold 2/5
Epoch: 1 	Ltrain: 0.121765 	Lval: 0.013996
Epoch: 5 	Ltrain: 0.007142 	Lval: 0.005746
Epoch: 10 	Ltrain: 0.005103 	Lval: 0.005414
Epoch: 15 	Ltrain: 0.004723 	Lval: 0.004968
Epoch: 20 	Ltrain: 0.004154 	Lval: 0.004065
Epoch: 25 	Ltrain: 0.003813 	Lval: 0.003724
Epoch: 30 	Ltrain: 0.003826 	Lval: 0.003547
Epoch 00034: reducing learning rate of group 0 to 9.5845e-04.
Epoch: 35 	Ltrain: 0.003338 	Lval: 0.003211
Epoch: 40 	Ltrain: 0.003037 	Lval: 0.003163
Epoch: 45 	Ltrain: 0.003019 	Lval: 0.003111
Epoch: 50 	Ltrain: 0.002956 	Lval: 0.003066
Epoch: 55 	Ltrain: 0.002868 	Lval: 0.002910
Epoch: 60 	Ltrain: 0.002786 	Lval: 0.002823
Epoch: 65 	Ltrain: 0.002710 	Lval: 0.002790
Epoch: 70 	Ltrain: 0.002660 	Lval: 0.002630
Epoch: 75 	Ltrain: 0.002508 	Lval: 0.002528
Epoch: 80 	Ltrain: 0.002426 	Lval: 0.002479
Epoch: 85 	Ltrain: 0.002265 	Lval: 0.002343
Epoch: 90 	Ltrain: 0.002263 	Lval: 0.002274
Epoch: 95 	Ltrain: 0.002109 	Lval: 0.002134
Epoch: 100 	Ltrain: 0.002075 	Lval: 0.002197
Epoch: 105 	Ltrain: 0.001913 	Lval: 0.001979
Epoch: 110 	Ltrain: 0.001775 	Lval: 0.001822
Epoch: 115 	Ltrain: 0.001687 	Lval: 0.001710
Epoch: 120 	Ltrain: 0.001538 	Lval: 0.001581
Epoch: 125 	Ltrain: 0.001427 	Lval: 0.001481
Epoch: 130 	Ltrain: 0.001267 	Lval: 0.001327
Epoch: 135 	Ltrain: 0.001235 	Lval: 0.001290
Epoch: 140 	Ltrain: 0.001072 	Lval: 0.001080
Epoch: 145 	Ltrain: 0.000999 	Lval: 0.001015
Epoch: 150 	Ltrain: 0.000912 	Lval: 0.000906
Epoch: 155 	Ltrain: 0.000792 	Lval: 0.000813
Epoch: 160 	Ltrain: 0.000784 	Lval: 0.000838
Epoch 00161: reducing learning rate of group 0 to 9.5845e-05.
Epoch: 165 	Ltrain: 0.000613 	Lval: 0.000669
Epoch: 170 	Ltrain: 0.000589 	Lval: 0.000651
Epoch: 175 	Ltrain: 0.000580 	Lval: 0.000640
Epoch: 180 	Ltrain: 0.000566 	Lval: 0.000628
Epoch: 185 	Ltrain: 0.000549 	Lval: 0.000618
Epoch: 190 	Ltrain: 0.000541 	Lval: 0.000609
Epoch: 195 	Ltrain: 0.000542 	Lval: 0.000600
Epoch: 200 	Ltrain: 0.000528 	Lval: 0.000590
Epoch: 205 	Ltrain: 0.000514 	Lval: 0.000580
Epoch: 210 	Ltrain: 0.000513 	Lval: 0.000570
Epoch: 215 	Ltrain: 0.000506 	Lval: 0.000559
Epoch: 220 	Ltrain: 0.000500 	Lval: 0.000550
Epoch: 225 	Ltrain: 0.000485 	Lval: 0.000541
Epoch: 230 	Ltrain: 0.000472 	Lval: 0.000533
Epoch: 235 	Ltrain: 0.000464 	Lval: 0.000521
Epoch: 240 	Ltrain: 0.000456 	Lval: 0.000511
Epoch: 245 	Ltrain: 0.000445 	Lval: 0.000500
Epoch: 250 	Ltrain: 0.000435 	Lval: 0.000491
Epoch: 255 	Ltrain: 0.000428 	Lval: 0.000481
Epoch: 260 	Ltrain: 0.000426 	Lval: 0.000470
Epoch: 265 	Ltrain: 0.000412 	Lval: 0.000461
Epoch: 270 	Ltrain: 0.000404 	Lval: 0.000450
Epoch: 275 	Ltrain: 0.000401 	Lval: 0.000442
Epoch: 280 	Ltrain: 0.000386 	Lval: 0.000431
Epoch: 285 	Ltrain: 0.000376 	Lval: 0.000422
Epoch: 290 	Ltrain: 0.000369 	Lval: 0.000412
Epoch: 295 	Ltrain: 0.000364 	Lval: 0.000404
Epoch: 300 	Ltrain: 0.000352 	Lval: 0.000395
Epoch: 305 	Ltrain: 0.000346 	Lval: 0.000383
Epoch: 310 	Ltrain: 0.000338 	Lval: 0.000375
Epoch: 315 	Ltrain: 0.000329 	Lval: 0.000367
Epoch: 320 	Ltrain: 0.000319 	Lval: 0.000358
Epoch: 325 	Ltrain: 0.000313 	Lval: 0.000351
Epoch: 330 	Ltrain: 0.000303 	Lval: 0.000341
Epoch: 335 	Ltrain: 0.000300 	Lval: 0.000332
Epoch: 340 	Ltrain: 0.000288 	Lval: 0.000323
Epoch: 345 	Ltrain: 0.000288 	Lval: 0.000316
Epoch: 350 	Ltrain: 0.000276 	Lval: 0.000308
Epoch: 355 	Ltrain: 0.000266 	Lval: 0.000301
Epoch: 360 	Ltrain: 0.000263 	Lval: 0.000292
Epoch: 365 	Ltrain: 0.000256 	Lval: 0.000286
Epoch: 370 	Ltrain: 0.000247 	Lval: 0.000279
Epoch: 375 	Ltrain: 0.000242 	Lval: 0.000272
Epoch: 380 	Ltrain: 0.000239 	Lval: 0.000266
Epoch: 385 	Ltrain: 0.000232 	Lval: 0.000259
Epoch: 390 	Ltrain: 0.000229 	Lval: 0.000252
Epoch: 395 	Ltrain: 0.000222 	Lval: 0.000248
Epoch: 400 	Ltrain: 0.000219 	Lval: 0.000241
Epoch: 405 	Ltrain: 0.000210 	Lval: 0.000232
Epoch: 410 	Ltrain: 0.000205 	Lval: 0.000228
Epoch: 415 	Ltrain: 0.000207 	Lval: 0.000231
Epoch: 420 	Ltrain: 0.000200 	Lval: 0.000220
Epoch: 425 	Ltrain: 0.000193 	Lval: 0.000214
Epoch: 430 	Ltrain: 0.000187 	Lval: 0.000204
Epoch: 435 	Ltrain: 0.000182 	Lval: 0.000200
Epoch 00439: reducing learning rate of group 0 to 9.5845e-06.
Epoch: 440 	Ltrain: 0.000188 	Lval: 0.000196
Epoch: 445 	Ltrain: 0.000170 	Lval: 0.000189
Epoch: 450 	Ltrain: 0.000172 	Lval: 0.000189
Epoch: 455 	Ltrain: 0.000170 	Lval: 0.000188
Epoch: 460 	Ltrain: 0.000169 	Lval: 0.000187
EarlyStopper: stopping at epoch 461 with best_val_loss = 0.000190


	Fold 3/5
Epoch: 1 	Ltrain: 0.216302 	Lval: 0.015553
Epoch: 5 	Ltrain: 0.016097 	Lval: 0.012149
Epoch: 10 	Ltrain: 0.007217 	Lval: 0.005649
Epoch: 15 	Ltrain: 0.005835 	Lval: 0.004889
Epoch: 20 	Ltrain: 0.004831 	Lval: 0.004902
Epoch: 25 	Ltrain: 0.004935 	Lval: 0.004236
Epoch: 30 	Ltrain: 0.004728 	Lval: 0.004345
Epoch: 35 	Ltrain: 0.004034 	Lval: 0.004914
Epoch 00036: reducing learning rate of group 0 to 9.5845e-04.
Epoch: 40 	Ltrain: 0.003080 	Lval: 0.003185
Epoch: 45 	Ltrain: 0.002980 	Lval: 0.003108
Epoch: 50 	Ltrain: 0.002943 	Lval: 0.003323
Epoch 00053: reducing learning rate of group 0 to 9.5845e-05.
Epoch: 55 	Ltrain: 0.002814 	Lval: 0.003035
Epoch: 60 	Ltrain: 0.002796 	Lval: 0.003004
Epoch 00065: reducing learning rate of group 0 to 9.5845e-06.
Epoch: 65 	Ltrain: 0.002807 	Lval: 0.003006
Epoch: 70 	Ltrain: 0.002774 	Lval: 0.002992
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.002992


	Fold 4/5
Epoch: 1 	Ltrain: 0.072387 	Lval: 0.013930
Epoch: 5 	Ltrain: 0.005397 	Lval: 0.005428
Epoch: 10 	Ltrain: 0.004272 	Lval: 0.004119
Epoch: 15 	Ltrain: 0.004075 	Lval: 0.003967
Epoch: 20 	Ltrain: 0.003744 	Lval: 0.003721
Epoch 00021: reducing learning rate of group 0 to 9.5845e-04.
Epoch: 25 	Ltrain: 0.002872 	Lval: 0.003221
Epoch: 30 	Ltrain: 0.002805 	Lval: 0.003101
Epoch: 35 	Ltrain: 0.002744 	Lval: 0.003059
Epoch: 40 	Ltrain: 0.002671 	Lval: 0.002968
Epoch: 45 	Ltrain: 0.002627 	Lval: 0.002913
Epoch: 50 	Ltrain: 0.002556 	Lval: 0.002814
Epoch: 55 	Ltrain: 0.002481 	Lval: 0.002707
Epoch: 60 	Ltrain: 0.002384 	Lval: 0.002635
Epoch: 65 	Ltrain: 0.002309 	Lval: 0.002579
Epoch: 70 	Ltrain: 0.002231 	Lval: 0.002453
Epoch: 75 	Ltrain: 0.002258 	Lval: 0.002386
Epoch: 80 	Ltrain: 0.002010 	Lval: 0.002198
Epoch: 85 	Ltrain: 0.001883 	Lval: 0.002085
Epoch: 90 	Ltrain: 0.001883 	Lval: 0.002031
Epoch: 95 	Ltrain: 0.001759 	Lval: 0.001842
Epoch: 100 	Ltrain: 0.001557 	Lval: 0.001706
Epoch: 105 	Ltrain: 0.001536 	Lval: 0.001705
Epoch: 110 	Ltrain: 0.001473 	Lval: 0.001595
Epoch 00115: reducing learning rate of group 0 to 9.5845e-05.
Epoch: 115 	Ltrain: 0.001367 	Lval: 0.001475
Epoch: 120 	Ltrain: 0.001093 	Lval: 0.001217
Epoch: 125 	Ltrain: 0.001068 	Lval: 0.001195
Epoch: 130 	Ltrain: 0.001049 	Lval: 0.001172
Epoch: 135 	Ltrain: 0.001032 	Lval: 0.001152
Epoch: 140 	Ltrain: 0.001023 	Lval: 0.001128
Epoch: 145 	Ltrain: 0.001000 	Lval: 0.001099
Epoch: 150 	Ltrain: 0.000982 	Lval: 0.001089
Epoch: 155 	Ltrain: 0.000963 	Lval: 0.001064
Epoch: 160 	Ltrain: 0.000946 	Lval: 0.001041
Epoch: 165 	Ltrain: 0.000929 	Lval: 0.001017
Epoch: 170 	Ltrain: 0.000911 	Lval: 0.000991
Epoch: 175 	Ltrain: 0.000891 	Lval: 0.000977
Epoch: 180 	Ltrain: 0.000871 	Lval: 0.000949
Epoch: 185 	Ltrain: 0.000853 	Lval: 0.000919
Epoch: 190 	Ltrain: 0.000827 	Lval: 0.000895
Epoch: 195 	Ltrain: 0.000808 	Lval: 0.000871
Epoch: 200 	Ltrain: 0.000789 	Lval: 0.000859
Epoch: 205 	Ltrain: 0.000767 	Lval: 0.000827
Epoch: 210 	Ltrain: 0.000747 	Lval: 0.000800
Epoch: 215 	Ltrain: 0.000726 	Lval: 0.000782
Epoch: 220 	Ltrain: 0.000711 	Lval: 0.000763
Epoch: 225 	Ltrain: 0.000686 	Lval: 0.000747
Epoch: 230 	Ltrain: 0.000664 	Lval: 0.000710
Epoch: 235 	Ltrain: 0.000648 	Lval: 0.000698
Epoch: 240 	Ltrain: 0.000627 	Lval: 0.000668
Epoch: 245 	Ltrain: 0.000609 	Lval: 0.000657
Epoch: 250 	Ltrain: 0.000594 	Lval: 0.000643
Epoch: 255 	Ltrain: 0.000573 	Lval: 0.000612
Epoch: 260 	Ltrain: 0.000551 	Lval: 0.000590
Epoch: 265 	Ltrain: 0.000542 	Lval: 0.000582
Epoch: 270 	Ltrain: 0.000520 	Lval: 0.000562
Epoch: 275 	Ltrain: 0.000514 	Lval: 0.000568
Epoch: 280 	Ltrain: 0.000489 	Lval: 0.000524
Epoch: 285 	Ltrain: 0.000469 	Lval: 0.000508
Epoch: 290 	Ltrain: 0.000456 	Lval: 0.000491
Epoch: 295 	Ltrain: 0.000448 	Lval: 0.000484
Epoch: 300 	Ltrain: 0.000454 	Lval: 0.000485
Epoch: 305 	Ltrain: 0.000422 	Lval: 0.000456
Epoch: 310 	Ltrain: 0.000408 	Lval: 0.000441
Epoch: 315 	Ltrain: 0.000390 	Lval: 0.000422
Epoch: 320 	Ltrain: 0.000384 	Lval: 0.000421
Epoch: 325 	Ltrain: 0.000378 	Lval: 0.000405
Epoch: 330 	Ltrain: 0.000356 	Lval: 0.000386
Epoch 00335: reducing learning rate of group 0 to 9.5845e-06.
Epoch: 335 	Ltrain: 0.000359 	Lval: 0.000385
Epoch: 340 	Ltrain: 0.000324 	Lval: 0.000357
Epoch: 345 	Ltrain: 0.000322 	Lval: 0.000355
Epoch: 350 	Ltrain: 0.000320 	Lval: 0.000353
Epoch: 355 	Ltrain: 0.000319 	Lval: 0.000352
Epoch: 360 	Ltrain: 0.000318 	Lval: 0.000350
Epoch: 365 	Ltrain: 0.000317 	Lval: 0.000349
EarlyStopper: stopping at epoch 364 with best_val_loss = 0.000355


	Fold 5/5
Epoch: 1 	Ltrain: 0.088448 	Lval: 0.013127
Epoch: 5 	Ltrain: 0.005392 	Lval: 0.006202
Epoch: 10 	Ltrain: 0.004228 	Lval: 0.004345
Epoch: 15 	Ltrain: 0.003731 	Lval: 0.003917
Epoch: 20 	Ltrain: 0.003383 	Lval: 0.003539
Epoch 00024: reducing learning rate of group 0 to 9.5845e-04.
Epoch: 25 	Ltrain: 0.002963 	Lval: 0.003208
Epoch: 30 	Ltrain: 0.002688 	Lval: 0.003082
Epoch: 35 	Ltrain: 0.002604 	Lval: 0.002968
Epoch: 40 	Ltrain: 0.002505 	Lval: 0.002841
Epoch: 45 	Ltrain: 0.002425 	Lval: 0.002759
Epoch: 50 	Ltrain: 0.002363 	Lval: 0.002670
Epoch: 55 	Ltrain: 0.002258 	Lval: 0.002545
Epoch: 60 	Ltrain: 0.002177 	Lval: 0.002418
Epoch: 65 	Ltrain: 0.002085 	Lval: 0.002339
Epoch: 70 	Ltrain: 0.001933 	Lval: 0.002192
Epoch: 75 	Ltrain: 0.001820 	Lval: 0.002083
Epoch: 80 	Ltrain: 0.001733 	Lval: 0.001960
Epoch: 85 	Ltrain: 0.001624 	Lval: 0.001773
Epoch: 90 	Ltrain: 0.001421 	Lval: 0.001559
Epoch: 95 	Ltrain: 0.001374 	Lval: 0.001576
Epoch: 100 	Ltrain: 0.001331 	Lval: 0.001398
Epoch: 105 	Ltrain: 0.001216 	Lval: 0.001367
Epoch: 110 	Ltrain: 0.001078 	Lval: 0.001171
Epoch: 115 	Ltrain: 0.000995 	Lval: 0.001160
Epoch: 120 	Ltrain: 0.000885 	Lval: 0.000944
Epoch 00125: reducing learning rate of group 0 to 9.5845e-05.
Epoch: 125 	Ltrain: 0.000923 	Lval: 0.001072
Epoch: 130 	Ltrain: 0.000669 	Lval: 0.000712
Epoch: 135 	Ltrain: 0.000648 	Lval: 0.000690
Epoch: 140 	Ltrain: 0.000634 	Lval: 0.000674
Epoch: 145 	Ltrain: 0.000623 	Lval: 0.000663
Epoch: 150 	Ltrain: 0.000612 	Lval: 0.000650
Epoch: 155 	Ltrain: 0.000602 	Lval: 0.000636
Epoch: 160 	Ltrain: 0.000589 	Lval: 0.000622
Epoch: 165 	Ltrain: 0.000579 	Lval: 0.000609
Epoch: 170 	Ltrain: 0.000568 	Lval: 0.000595
Epoch: 175 	Ltrain: 0.000558 	Lval: 0.000581
Epoch: 180 	Ltrain: 0.000544 	Lval: 0.000568
Epoch: 185 	Ltrain: 0.000533 	Lval: 0.000554
Epoch: 190 	Ltrain: 0.000519 	Lval: 0.000544
Epoch: 195 	Ltrain: 0.000507 	Lval: 0.000524
Epoch: 200 	Ltrain: 0.000496 	Lval: 0.000513
Epoch: 205 	Ltrain: 0.000481 	Lval: 0.000502
Epoch: 210 	Ltrain: 0.000467 	Lval: 0.000482
Epoch: 215 	Ltrain: 0.000455 	Lval: 0.000470
Epoch: 220 	Ltrain: 0.000441 	Lval: 0.000456
Epoch: 225 	Ltrain: 0.000428 	Lval: 0.000441
Epoch: 230 	Ltrain: 0.000417 	Lval: 0.000427
Epoch: 235 	Ltrain: 0.000402 	Lval: 0.000420
Epoch: 240 	Ltrain: 0.000390 	Lval: 0.000406
Epoch: 245 	Ltrain: 0.000380 	Lval: 0.000396
Epoch: 250 	Ltrain: 0.000366 	Lval: 0.000380
Epoch: 255 	Ltrain: 0.000354 	Lval: 0.000366
Epoch: 260 	Ltrain: 0.000343 	Lval: 0.000359
Epoch 00265: reducing learning rate of group 0 to 9.5845e-06.
Epoch: 265 	Ltrain: 0.000342 	Lval: 0.000356
Epoch: 270 	Ltrain: 0.000318 	Lval: 0.000335
Epoch: 275 	Ltrain: 0.000316 	Lval: 0.000333
Epoch: 280 	Ltrain: 0.000314 	Lval: 0.000331
Epoch: 285 	Ltrain: 0.000313 	Lval: 0.000330
Epoch: 290 	Ltrain: 0.000311 	Lval: 0.000328
Epoch: 295 	Ltrain: 0.000310 	Lval: 0.000327
Epoch: 300 	Ltrain: 0.000308 	Lval: 0.000326
EarlyStopper: stopping at epoch 302 with best_val_loss = 0.000330

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0008070606406819337
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.3695608058473556e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.032482 	Lval: 0.018188
Epoch: 5 	Ltrain: 0.011629 	Lval: 0.010549
Epoch: 10 	Ltrain: 0.009235 	Lval: 0.008435
Epoch: 15 	Ltrain: 0.006488 	Lval: 0.006052
Epoch: 20 	Ltrain: 0.005171 	Lval: 0.005011
Epoch: 25 	Ltrain: 0.004989 	Lval: 0.005155
Epoch 00028: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 30 	Ltrain: 0.004735 	Lval: 0.004614
Epoch: 35 	Ltrain: 0.004831 	Lval: 0.004558
Epoch: 40 	Ltrain: 0.004638 	Lval: 0.004519
Epoch: 45 	Ltrain: 0.004450 	Lval: 0.004506
Epoch: 50 	Ltrain: 0.004444 	Lval: 0.004471
Epoch: 55 	Ltrain: 0.004435 	Lval: 0.004427
Epoch: 60 	Ltrain: 0.004982 	Lval: 0.004406
Epoch: 65 	Ltrain: 0.004552 	Lval: 0.004374
Epoch: 70 	Ltrain: 0.004252 	Lval: 0.004332
Epoch: 75 	Ltrain: 0.004340 	Lval: 0.004305
Epoch: 80 	Ltrain: 0.004308 	Lval: 0.004308
Epoch: 85 	Ltrain: 0.004463 	Lval: 0.004238
Epoch: 90 	Ltrain: 0.004334 	Lval: 0.004207
Epoch: 95 	Ltrain: 0.004383 	Lval: 0.004184
Epoch: 100 	Ltrain: 0.004208 	Lval: 0.004144
Epoch: 105 	Ltrain: 0.004276 	Lval: 0.004128
Epoch: 110 	Ltrain: 0.004120 	Lval: 0.004075
Epoch: 115 	Ltrain: 0.004412 	Lval: 0.004078
Epoch: 120 	Ltrain: 0.004283 	Lval: 0.004044
Epoch: 125 	Ltrain: 0.004099 	Lval: 0.004015
Epoch: 130 	Ltrain: 0.003888 	Lval: 0.003966
Epoch: 135 	Ltrain: 0.004066 	Lval: 0.003950
Epoch: 140 	Ltrain: 0.003959 	Lval: 0.003926
Epoch: 145 	Ltrain: 0.003774 	Lval: 0.003905
Epoch: 150 	Ltrain: 0.003789 	Lval: 0.003884
Epoch: 155 	Ltrain: 0.003827 	Lval: 0.003846
Epoch: 160 	Ltrain: 0.003909 	Lval: 0.003869
Epoch: 165 	Ltrain: 0.003956 	Lval: 0.003807
Epoch: 170 	Ltrain: 0.003689 	Lval: 0.003797
Epoch: 175 	Ltrain: 0.003820 	Lval: 0.003779
Epoch 00178: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 180 	Ltrain: 0.003920 	Lval: 0.003774
Epoch: 185 	Ltrain: 0.003791 	Lval: 0.003774
Epoch: 190 	Ltrain: 0.003734 	Lval: 0.003759
Epoch 00191: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 195 	Ltrain: 0.003780 	Lval: 0.003757
Epoch: 200 	Ltrain: 0.003780 	Lval: 0.003754
EarlyStopper: stopping at epoch 201 with best_val_loss = 0.003760


	Fold 2/5
Epoch: 1 	Ltrain: 0.021869 	Lval: 0.014607
Epoch: 5 	Ltrain: 0.008643 	Lval: 0.007452
Epoch: 10 	Ltrain: 0.005396 	Lval: 0.004918
Epoch: 15 	Ltrain: 0.004829 	Lval: 0.005511
Epoch: 20 	Ltrain: 0.004579 	Lval: 0.004339
Epoch 00024: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 25 	Ltrain: 0.004313 	Lval: 0.004172
Epoch: 30 	Ltrain: 0.004179 	Lval: 0.004085
Epoch: 35 	Ltrain: 0.004127 	Lval: 0.004101
Epoch 00040: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 40 	Ltrain: 0.004150 	Lval: 0.004039
Epoch: 45 	Ltrain: 0.004073 	Lval: 0.004060
Epoch: 50 	Ltrain: 0.004034 	Lval: 0.004053
Epoch 00052: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 55 	Ltrain: 0.004087 	Lval: 0.004042
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.004034


	Fold 3/5
Epoch: 1 	Ltrain: 0.018774 	Lval: 0.011701
Epoch: 5 	Ltrain: 0.007091 	Lval: 0.007372
Epoch: 10 	Ltrain: 0.005003 	Lval: 0.004825
Epoch: 15 	Ltrain: 0.004758 	Lval: 0.004816
Epoch: 20 	Ltrain: 0.004339 	Lval: 0.004063
Epoch: 25 	Ltrain: 0.004131 	Lval: 0.004318
Epoch 00028: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 30 	Ltrain: 0.003702 	Lval: 0.003727
Epoch: 35 	Ltrain: 0.003662 	Lval: 0.003849
Epoch: 40 	Ltrain: 0.003579 	Lval: 0.003700
Epoch: 45 	Ltrain: 0.003568 	Lval: 0.003685
Epoch 00048: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 50 	Ltrain: 0.003496 	Lval: 0.003649
Epoch: 55 	Ltrain: 0.003485 	Lval: 0.003639
Epoch 00060: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 60 	Ltrain: 0.003490 	Lval: 0.003636
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.003634


	Fold 4/5
Epoch: 1 	Ltrain: 0.017483 	Lval: 0.012189
Epoch: 5 	Ltrain: 0.006362 	Lval: 0.005470
Epoch: 10 	Ltrain: 0.004607 	Lval: 0.004368
Epoch: 15 	Ltrain: 0.004145 	Lval: 0.004350
Epoch 00020: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 20 	Ltrain: 0.003883 	Lval: 0.004167
Epoch: 25 	Ltrain: 0.003599 	Lval: 0.003835
Epoch: 30 	Ltrain: 0.003553 	Lval: 0.003833
Epoch: 35 	Ltrain: 0.003534 	Lval: 0.003808
Epoch: 40 	Ltrain: 0.003483 	Lval: 0.003760
Epoch: 45 	Ltrain: 0.003460 	Lval: 0.003726
Epoch 00047: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 50 	Ltrain: 0.003410 	Lval: 0.003703
Epoch: 55 	Ltrain: 0.003398 	Lval: 0.003702
Epoch 00060: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 60 	Ltrain: 0.003402 	Lval: 0.003695
Epoch: 65 	Ltrain: 0.003390 	Lval: 0.003697
EarlyStopper: stopping at epoch 67 with best_val_loss = 0.003696


	Fold 5/5
Epoch: 1 	Ltrain: 0.017409 	Lval: 0.011450
Epoch: 5 	Ltrain: 0.005975 	Lval: 0.006487
Epoch: 10 	Ltrain: 0.004632 	Lval: 0.004861
Epoch: 15 	Ltrain: 0.004185 	Lval: 0.004422
Epoch: 20 	Ltrain: 0.003908 	Lval: 0.004301
Epoch 00025: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 25 	Ltrain: 0.003669 	Lval: 0.004055
Epoch: 30 	Ltrain: 0.003418 	Lval: 0.003792
Epoch: 35 	Ltrain: 0.003387 	Lval: 0.003753
Epoch: 40 	Ltrain: 0.003340 	Lval: 0.003723
Epoch: 45 	Ltrain: 0.003320 	Lval: 0.003706
Epoch: 50 	Ltrain: 0.003264 	Lval: 0.003667
Epoch: 55 	Ltrain: 0.003238 	Lval: 0.003650
Epoch: 60 	Ltrain: 0.003198 	Lval: 0.003590
Epoch: 65 	Ltrain: 0.003153 	Lval: 0.003549
Epoch: 70 	Ltrain: 0.003121 	Lval: 0.003532
Epoch 00071: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 75 	Ltrain: 0.003073 	Lval: 0.003488
Epoch: 80 	Ltrain: 0.003059 	Lval: 0.003485
Epoch: 85 	Ltrain: 0.003063 	Lval: 0.003472
Epoch 00089: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 90 	Ltrain: 0.003048 	Lval: 0.003475
Epoch: 95 	Ltrain: 0.003047 	Lval: 0.003474
Epoch: 100 	Ltrain: 0.003045 	Lval: 0.003473
Epoch 00101: reducing learning rate of group 0 to 8.0706e-08.
Epoch: 105 	Ltrain: 0.003042 	Lval: 0.003474
EarlyStopper: stopping at epoch 104 with best_val_loss = 0.003472

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009370754018612745
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.3158764824896704e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.268797 	Lval: 0.084096
Epoch: 5 	Ltrain: 0.014309 	Lval: 0.012519
Epoch: 10 	Ltrain: 0.007085 	Lval: 0.005841
Epoch: 15 	Ltrain: 0.005398 	Lval: 0.005262
Epoch: 20 	Ltrain: 0.005667 	Lval: 0.005338
Epoch: 25 	Ltrain: 0.005129 	Lval: 0.004544
Epoch: 30 	Ltrain: 0.004340 	Lval: 0.004048
Epoch: 35 	Ltrain: 0.004671 	Lval: 0.004256
Epoch: 40 	Ltrain: 0.004005 	Lval: 0.003748
Epoch: 45 	Ltrain: 0.003627 	Lval: 0.003361
Epoch: 50 	Ltrain: 0.003540 	Lval: 0.003198
Epoch: 55 	Ltrain: 0.003365 	Lval: 0.003228
Epoch 00057: reducing learning rate of group 0 to 9.3708e-04.
Epoch: 60 	Ltrain: 0.002767 	Lval: 0.002680
Epoch: 65 	Ltrain: 0.002427 	Lval: 0.002521
Epoch: 70 	Ltrain: 0.002537 	Lval: 0.002478
Epoch: 75 	Ltrain: 0.002392 	Lval: 0.002413
Epoch: 80 	Ltrain: 0.002283 	Lval: 0.002356
Epoch: 85 	Ltrain: 0.002323 	Lval: 0.002317
Epoch: 90 	Ltrain: 0.002265 	Lval: 0.002268
Epoch: 95 	Ltrain: 0.002168 	Lval: 0.002219
Epoch: 100 	Ltrain: 0.002172 	Lval: 0.002185
Epoch: 105 	Ltrain: 0.002140 	Lval: 0.002118
Epoch: 110 	Ltrain: 0.002010 	Lval: 0.002047
Epoch: 115 	Ltrain: 0.002050 	Lval: 0.002032
Epoch 00118: reducing learning rate of group 0 to 9.3708e-05.
Epoch: 120 	Ltrain: 0.001849 	Lval: 0.001964
Epoch: 125 	Ltrain: 0.001868 	Lval: 0.001951
Epoch: 130 	Ltrain: 0.001958 	Lval: 0.001942
Epoch: 135 	Ltrain: 0.001904 	Lval: 0.001935
Epoch: 140 	Ltrain: 0.001836 	Lval: 0.001929
Epoch: 145 	Ltrain: 0.001880 	Lval: 0.001923
Epoch: 150 	Ltrain: 0.001880 	Lval: 0.001918
Epoch: 155 	Ltrain: 0.001856 	Lval: 0.001910
Epoch: 160 	Ltrain: 0.001808 	Lval: 0.001904
Epoch: 165 	Ltrain: 0.001836 	Lval: 0.001900
Epoch: 170 	Ltrain: 0.001859 	Lval: 0.001891
Epoch: 175 	Ltrain: 0.001857 	Lval: 0.001884
Epoch: 180 	Ltrain: 0.001862 	Lval: 0.001878
Epoch: 185 	Ltrain: 0.001777 	Lval: 0.001871
Epoch: 190 	Ltrain: 0.001806 	Lval: 0.001865
Epoch: 195 	Ltrain: 0.001768 	Lval: 0.001856
Epoch: 200 	Ltrain: 0.001709 	Lval: 0.001850
Epoch: 205 	Ltrain: 0.001783 	Lval: 0.001842
Epoch: 210 	Ltrain: 0.001724 	Lval: 0.001833
Epoch: 215 	Ltrain: 0.001792 	Lval: 0.001826
Epoch: 220 	Ltrain: 0.001815 	Lval: 0.001820
Epoch: 225 	Ltrain: 0.001775 	Lval: 0.001811
Epoch: 230 	Ltrain: 0.001730 	Lval: 0.001803
Epoch: 235 	Ltrain: 0.001804 	Lval: 0.001797
Epoch: 240 	Ltrain: 0.001714 	Lval: 0.001786
Epoch: 245 	Ltrain: 0.001675 	Lval: 0.001780
Epoch: 250 	Ltrain: 0.001667 	Lval: 0.001771
Epoch: 255 	Ltrain: 0.001733 	Lval: 0.001766
Epoch: 260 	Ltrain: 0.001656 	Lval: 0.001757
Epoch: 265 	Ltrain: 0.001709 	Lval: 0.001749
Epoch: 270 	Ltrain: 0.001744 	Lval: 0.001741
Epoch: 275 	Ltrain: 0.001615 	Lval: 0.001735
Epoch: 280 	Ltrain: 0.001712 	Lval: 0.001722
Epoch: 285 	Ltrain: 0.001587 	Lval: 0.001713
Epoch: 290 	Ltrain: 0.001618 	Lval: 0.001704
Epoch: 295 	Ltrain: 0.001560 	Lval: 0.001693
Epoch: 300 	Ltrain: 0.001669 	Lval: 0.001685
Epoch: 305 	Ltrain: 0.001600 	Lval: 0.001676
Epoch: 310 	Ltrain: 0.001583 	Lval: 0.001670
Epoch: 315 	Ltrain: 0.001580 	Lval: 0.001660
Epoch: 320 	Ltrain: 0.001617 	Lval: 0.001649
Epoch: 325 	Ltrain: 0.001603 	Lval: 0.001637
Epoch: 330 	Ltrain: 0.001596 	Lval: 0.001631
Epoch: 335 	Ltrain: 0.001583 	Lval: 0.001618
Epoch: 340 	Ltrain: 0.001558 	Lval: 0.001609
Epoch: 345 	Ltrain: 0.001530 	Lval: 0.001602
Epoch: 350 	Ltrain: 0.001483 	Lval: 0.001593
Epoch: 355 	Ltrain: 0.001496 	Lval: 0.001578
Epoch: 360 	Ltrain: 0.001435 	Lval: 0.001572
Epoch: 365 	Ltrain: 0.001541 	Lval: 0.001560
Epoch: 370 	Ltrain: 0.001443 	Lval: 0.001547
Epoch: 375 	Ltrain: 0.001467 	Lval: 0.001531
Epoch: 380 	Ltrain: 0.001471 	Lval: 0.001527
Epoch: 385 	Ltrain: 0.001500 	Lval: 0.001517
Epoch: 390 	Ltrain: 0.001528 	Lval: 0.001504
Epoch: 395 	Ltrain: 0.001401 	Lval: 0.001493
Epoch: 400 	Ltrain: 0.001455 	Lval: 0.001482
Epoch: 405 	Ltrain: 0.001458 	Lval: 0.001474
Epoch: 410 	Ltrain: 0.001421 	Lval: 0.001463
Epoch: 415 	Ltrain: 0.001367 	Lval: 0.001454
Epoch: 420 	Ltrain: 0.001394 	Lval: 0.001439
Epoch: 425 	Ltrain: 0.001383 	Lval: 0.001425
Epoch: 430 	Ltrain: 0.001366 	Lval: 0.001418
Epoch: 435 	Ltrain: 0.001446 	Lval: 0.001406
Epoch: 440 	Ltrain: 0.001369 	Lval: 0.001392
Epoch: 445 	Ltrain: 0.001335 	Lval: 0.001382
Epoch: 450 	Ltrain: 0.001291 	Lval: 0.001373
Epoch: 455 	Ltrain: 0.001335 	Lval: 0.001361
Epoch: 460 	Ltrain: 0.001311 	Lval: 0.001351
Epoch: 465 	Ltrain: 0.001317 	Lval: 0.001334
Epoch: 470 	Ltrain: 0.001269 	Lval: 0.001327
Epoch: 475 	Ltrain: 0.001291 	Lval: 0.001312
Epoch: 480 	Ltrain: 0.001254 	Lval: 0.001296
Epoch: 485 	Ltrain: 0.001272 	Lval: 0.001283
Epoch: 490 	Ltrain: 0.001214 	Lval: 0.001271
Epoch: 495 	Ltrain: 0.001175 	Lval: 0.001261
Epoch: 500 	Ltrain: 0.001203 	Lval: 0.001250
Epoch: 505 	Ltrain: 0.001138 	Lval: 0.001240
Epoch: 510 	Ltrain: 0.001206 	Lval: 0.001224
Epoch: 515 	Ltrain: 0.001117 	Lval: 0.001213
Epoch: 520 	Ltrain: 0.001151 	Lval: 0.001198
Epoch: 525 	Ltrain: 0.001110 	Lval: 0.001192
Epoch: 530 	Ltrain: 0.001092 	Lval: 0.001174
Epoch: 535 	Ltrain: 0.001111 	Lval: 0.001165
Epoch: 540 	Ltrain: 0.001136 	Lval: 0.001160
Epoch: 545 	Ltrain: 0.001054 	Lval: 0.001136
Epoch: 550 	Ltrain: 0.001073 	Lval: 0.001120
Epoch 00555: reducing learning rate of group 0 to 9.3708e-06.
Epoch: 555 	Ltrain: 0.001049 	Lval: 0.001121
Epoch: 560 	Ltrain: 0.001073 	Lval: 0.001102
Epoch: 565 	Ltrain: 0.001016 	Lval: 0.001101
Epoch: 570 	Ltrain: 0.001016 	Lval: 0.001099
EarlyStopper: stopping at epoch 570 with best_val_loss = 0.001104


	Fold 2/5
Epoch: 1 	Ltrain: 0.133464 	Lval: 0.015323
Epoch: 5 	Ltrain: 0.010084 	Lval: 0.010149
Epoch: 10 	Ltrain: 0.005097 	Lval: 0.005064
Epoch: 15 	Ltrain: 0.004874 	Lval: 0.004508
Epoch: 20 	Ltrain: 0.004335 	Lval: 0.004274
Epoch: 25 	Ltrain: 0.004070 	Lval: 0.003885
Epoch: 30 	Ltrain: 0.004024 	Lval: 0.004502
Epoch 00033: reducing learning rate of group 0 to 9.3708e-04.
Epoch: 35 	Ltrain: 0.003498 	Lval: 0.003304
Epoch: 40 	Ltrain: 0.003246 	Lval: 0.003220
Epoch: 45 	Ltrain: 0.003210 	Lval: 0.003173
Epoch: 50 	Ltrain: 0.003103 	Lval: 0.003134
Epoch: 55 	Ltrain: 0.003083 	Lval: 0.003081
Epoch 00057: reducing learning rate of group 0 to 9.3708e-05.
Epoch: 60 	Ltrain: 0.002948 	Lval: 0.003005
Epoch: 65 	Ltrain: 0.002885 	Lval: 0.002991
Epoch: 70 	Ltrain: 0.002887 	Lval: 0.003001
Epoch 00071: reducing learning rate of group 0 to 9.3708e-06.
Epoch: 75 	Ltrain: 0.002923 	Lval: 0.002992
Epoch: 80 	Ltrain: 0.002855 	Lval: 0.002990
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.002985


	Fold 3/5
Epoch: 1 	Ltrain: 0.164647 	Lval: 0.016242
Epoch: 5 	Ltrain: 0.019619 	Lval: 0.015529
Epoch 00006: reducing learning rate of group 0 to 9.3708e-04.
Epoch: 10 	Ltrain: 0.009793 	Lval: 0.007990
Epoch: 15 	Ltrain: 0.007219 	Lval: 0.006502
Epoch: 20 	Ltrain: 0.006445 	Lval: 0.005622
Epoch: 25 	Ltrain: 0.006362 	Lval: 0.005491
Epoch 00026: reducing learning rate of group 0 to 9.3708e-05.
Epoch: 30 	Ltrain: 0.005640 	Lval: 0.005318
Epoch: 35 	Ltrain: 0.005598 	Lval: 0.005397
Epoch 00038: reducing learning rate of group 0 to 9.3708e-06.
Epoch: 40 	Ltrain: 0.005548 	Lval: 0.005279
Epoch: 45 	Ltrain: 0.005565 	Lval: 0.005291
EarlyStopper: stopping at epoch 46 with best_val_loss = 0.005275


	Fold 4/5
Epoch: 1 	Ltrain: 0.203050 	Lval: 0.022810
Epoch: 5 	Ltrain: 0.009744 	Lval: 0.005933
Epoch: 10 	Ltrain: 0.005028 	Lval: 0.004693
Epoch: 15 	Ltrain: 0.004694 	Lval: 0.004556
Epoch 00020: reducing learning rate of group 0 to 9.3708e-04.
Epoch: 20 	Ltrain: 0.004257 	Lval: 0.004633
Epoch: 25 	Ltrain: 0.003707 	Lval: 0.003895
Epoch: 30 	Ltrain: 0.003596 	Lval: 0.003836
Epoch: 35 	Ltrain: 0.003511 	Lval: 0.003796
Epoch: 40 	Ltrain: 0.003408 	Lval: 0.003601
Epoch 00044: reducing learning rate of group 0 to 9.3708e-05.
Epoch: 45 	Ltrain: 0.003296 	Lval: 0.003574
Epoch: 50 	Ltrain: 0.003226 	Lval: 0.003528
Epoch: 55 	Ltrain: 0.003214 	Lval: 0.003558
Epoch: 60 	Ltrain: 0.003194 	Lval: 0.003509
Epoch 00062: reducing learning rate of group 0 to 9.3708e-06.
Epoch: 65 	Ltrain: 0.003180 	Lval: 0.003491
Epoch: 70 	Ltrain: 0.003174 	Lval: 0.003489
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.003479


	Fold 5/5
Epoch: 1 	Ltrain: 0.164857 	Lval: 0.015310
Epoch: 5 	Ltrain: 0.021214 	Lval: 0.019161
Epoch 00006: reducing learning rate of group 0 to 9.3708e-04.
Epoch: 10 	Ltrain: 0.015831 	Lval: 0.015486
Epoch: 15 	Ltrain: 0.015748 	Lval: 0.016139
Epoch: 20 	Ltrain: 0.014320 	Lval: 0.015080
Epoch: 25 	Ltrain: 0.008326 	Lval: 0.007731
Epoch: 30 	Ltrain: 0.006610 	Lval: 0.006515
Epoch: 35 	Ltrain: 0.005982 	Lval: 0.006287
Epoch 00038: reducing learning rate of group 0 to 9.3708e-05.
Epoch: 40 	Ltrain: 0.005454 	Lval: 0.005660
Epoch: 45 	Ltrain: 0.005420 	Lval: 0.005690
Epoch: 50 	Ltrain: 0.005362 	Lval: 0.005520
Epoch: 55 	Ltrain: 0.005353 	Lval: 0.005481
Epoch: 60 	Ltrain: 0.005372 	Lval: 0.005557
Epoch 00062: reducing learning rate of group 0 to 9.3708e-06.
Epoch: 65 	Ltrain: 0.005169 	Lval: 0.005427
Epoch: 70 	Ltrain: 0.005184 	Lval: 0.005403
Epoch 00074: reducing learning rate of group 0 to 9.3708e-07.
Epoch: 75 	Ltrain: 0.005166 	Lval: 0.005403
Epoch: 80 	Ltrain: 0.005159 	Lval: 0.005398
EarlyStopper: stopping at epoch 79 with best_val_loss = 0.005385

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0018960689475363206
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.233498407023453e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.042526 	Lval: 0.018069
Epoch: 5 	Ltrain: 0.013267 	Lval: 0.013655
Epoch: 10 	Ltrain: 0.011176 	Lval: 0.010287
Epoch: 15 	Ltrain: 0.012856 	Lval: 0.009663
Epoch: 20 	Ltrain: 0.008906 	Lval: 0.007644
Epoch: 25 	Ltrain: 0.007653 	Lval: 0.007051
Epoch: 30 	Ltrain: 0.005483 	Lval: 0.005803
Epoch: 35 	Ltrain: 0.004901 	Lval: 0.005079
Epoch: 40 	Ltrain: 0.005353 	Lval: 0.005092
Epoch 00043: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 45 	Ltrain: 0.005953 	Lval: 0.004417
Epoch: 50 	Ltrain: 0.005008 	Lval: 0.004329
Epoch: 55 	Ltrain: 0.005215 	Lval: 0.004360
Epoch 00056: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 60 	Ltrain: 0.005071 	Lval: 0.004273
Epoch: 65 	Ltrain: 0.004561 	Lval: 0.004259
Epoch: 70 	Ltrain: 0.004310 	Lval: 0.004257
Epoch 00075: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 75 	Ltrain: 0.004698 	Lval: 0.004262
Epoch: 80 	Ltrain: 0.004480 	Lval: 0.004263
Epoch: 85 	Ltrain: 0.004628 	Lval: 0.004263
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.004259


	Fold 2/5
Epoch: 1 	Ltrain: 0.032290 	Lval: 0.015972
Epoch: 5 	Ltrain: 0.011433 	Lval: 0.010992
Epoch: 10 	Ltrain: 0.008330 	Lval: 0.007219
Epoch: 15 	Ltrain: 0.007049 	Lval: 0.005088
Epoch: 20 	Ltrain: 0.005466 	Lval: 0.004427
Epoch: 25 	Ltrain: 0.005108 	Lval: 0.004644
Epoch 00026: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 30 	Ltrain: 0.005129 	Lval: 0.004466
Epoch: 35 	Ltrain: 0.004795 	Lval: 0.004194
Epoch: 40 	Ltrain: 0.004758 	Lval: 0.004128
Epoch: 45 	Ltrain: 0.004790 	Lval: 0.004214
Epoch 00048: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 50 	Ltrain: 0.004434 	Lval: 0.004166
Epoch: 55 	Ltrain: 0.005036 	Lval: 0.004218
Epoch 00060: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 60 	Ltrain: 0.004631 	Lval: 0.004209
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.004100


	Fold 3/5
Epoch: 1 	Ltrain: 0.021207 	Lval: 0.013231
Epoch: 5 	Ltrain: 0.008783 	Lval: 0.008828
Epoch: 10 	Ltrain: 0.006044 	Lval: 0.005294
Epoch: 15 	Ltrain: 0.005124 	Lval: 0.004294
Epoch: 20 	Ltrain: 0.004885 	Lval: 0.004690
Epoch 00023: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 25 	Ltrain: 0.004457 	Lval: 0.003906
Epoch: 30 	Ltrain: 0.004607 	Lval: 0.003849
Epoch 00035: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 35 	Ltrain: 0.004604 	Lval: 0.003932
Epoch: 40 	Ltrain: 0.004188 	Lval: 0.003878
Epoch: 45 	Ltrain: 0.004357 	Lval: 0.003899
Epoch 00047: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 50 	Ltrain: 0.004326 	Lval: 0.003889
EarlyStopper: stopping at epoch 49 with best_val_loss = 0.003849


	Fold 4/5
Epoch: 1 	Ltrain: 0.021995 	Lval: 0.012896
Epoch: 5 	Ltrain: 0.007723 	Lval: 0.006787
Epoch: 10 	Ltrain: 0.005061 	Lval: 0.004545
Epoch: 15 	Ltrain: 0.004682 	Lval: 0.004114
Epoch: 20 	Ltrain: 0.004125 	Lval: 0.003961
Epoch: 25 	Ltrain: 0.003955 	Lval: 0.003841
Epoch: 30 	Ltrain: 0.004200 	Lval: 0.003780
Epoch 00032: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 35 	Ltrain: 0.003662 	Lval: 0.003656
Epoch: 40 	Ltrain: 0.003604 	Lval: 0.003626
Epoch: 45 	Ltrain: 0.003598 	Lval: 0.003599
Epoch 00049: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 50 	Ltrain: 0.003409 	Lval: 0.003630
Epoch: 55 	Ltrain: 0.003406 	Lval: 0.003602
Epoch: 60 	Ltrain: 0.003432 	Lval: 0.003619
Epoch 00061: reducing learning rate of group 0 to 1.8961e-06.
EarlyStopper: stopping at epoch 61 with best_val_loss = 0.003607


	Fold 5/5
Epoch: 1 	Ltrain: 0.021019 	Lval: 0.015487
Epoch: 5 	Ltrain: 0.008339 	Lval: 0.007953
Epoch: 10 	Ltrain: 0.004943 	Lval: 0.004737
Epoch 00014: reducing learning rate of group 0 to 1.8961e-04.
Epoch: 15 	Ltrain: 0.004462 	Lval: 0.004655
Epoch: 20 	Ltrain: 0.004341 	Lval: 0.004415
Epoch: 25 	Ltrain: 0.004275 	Lval: 0.004465
Epoch 00027: reducing learning rate of group 0 to 1.8961e-05.
Epoch: 30 	Ltrain: 0.004278 	Lval: 0.004388
Epoch: 35 	Ltrain: 0.004248 	Lval: 0.004385
Epoch: 40 	Ltrain: 0.004170 	Lval: 0.004378
Epoch: 45 	Ltrain: 0.004121 	Lval: 0.004384
Epoch 00046: reducing learning rate of group 0 to 1.8961e-06.
Epoch: 50 	Ltrain: 0.004260 	Lval: 0.004379
Epoch: 55 	Ltrain: 0.004138 	Lval: 0.004375
Epoch 00060: reducing learning rate of group 0 to 1.8961e-07.
Epoch: 60 	Ltrain: 0.004330 	Lval: 0.004376
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.004378

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0006969127036536701
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.051022326320595e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 15
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.024517 	Lval: 0.015830
Epoch: 5 	Ltrain: 0.012118 	Lval: 0.011735
Epoch: 10 	Ltrain: 0.009846 	Lval: 0.008663
Epoch: 15 	Ltrain: 0.007344 	Lval: 0.007144
Epoch: 20 	Ltrain: 0.006126 	Lval: 0.005716
Epoch: 25 	Ltrain: 0.005996 	Lval: 0.005670
Epoch: 30 	Ltrain: 0.005351 	Lval: 0.005433
Epoch: 35 	Ltrain: 0.005152 	Lval: 0.004741
Epoch 00039: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 40 	Ltrain: 0.004856 	Lval: 0.004623
Epoch: 45 	Ltrain: 0.004570 	Lval: 0.004586
Epoch: 50 	Ltrain: 0.004709 	Lval: 0.004613
Epoch: 55 	Ltrain: 0.004607 	Lval: 0.004573
Epoch: 60 	Ltrain: 0.004515 	Lval: 0.004543
Epoch: 65 	Ltrain: 0.005022 	Lval: 0.004549
Epoch: 70 	Ltrain: 0.004609 	Lval: 0.004520
Epoch: 75 	Ltrain: 0.004524 	Lval: 0.004504
Epoch: 80 	Ltrain: 0.004468 	Lval: 0.004498
Epoch: 85 	Ltrain: 0.004570 	Lval: 0.004466
Epoch: 90 	Ltrain: 0.004474 	Lval: 0.004455
Epoch: 95 	Ltrain: 0.004537 	Lval: 0.004454
Epoch: 100 	Ltrain: 0.004520 	Lval: 0.004436
Epoch: 105 	Ltrain: 0.004614 	Lval: 0.004414
Epoch: 110 	Ltrain: 0.004529 	Lval: 0.004388
Epoch: 115 	Ltrain: 0.004715 	Lval: 0.004371
Epoch: 120 	Ltrain: 0.004257 	Lval: 0.004387
Epoch: 125 	Ltrain: 0.004296 	Lval: 0.004336
Epoch: 130 	Ltrain: 0.004540 	Lval: 0.004318
Epoch: 135 	Ltrain: 0.004384 	Lval: 0.004313
Epoch 00136: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 140 	Ltrain: 0.004575 	Lval: 0.004288
Epoch: 145 	Ltrain: 0.004270 	Lval: 0.004284
Epoch: 150 	Ltrain: 0.004254 	Lval: 0.004282
Epoch 00154: reducing learning rate of group 0 to 6.9691e-07.
Epoch: 155 	Ltrain: 0.004280 	Lval: 0.004283
EarlyStopper: stopping at epoch 154 with best_val_loss = 0.004288


	Fold 2/5
Epoch: 1 	Ltrain: 0.028403 	Lval: 0.016043
Epoch: 5 	Ltrain: 0.010558 	Lval: 0.010571
Epoch: 10 	Ltrain: 0.006995 	Lval: 0.007256
Epoch: 15 	Ltrain: 0.005498 	Lval: 0.005116
Epoch: 20 	Ltrain: 0.005291 	Lval: 0.004636
Epoch: 25 	Ltrain: 0.004740 	Lval: 0.004483
Epoch: 30 	Ltrain: 0.004596 	Lval: 0.004348
Epoch: 35 	Ltrain: 0.004587 	Lval: 0.004268
Epoch 00039: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 40 	Ltrain: 0.004536 	Lval: 0.004543
Epoch: 45 	Ltrain: 0.004299 	Lval: 0.004259
Epoch: 50 	Ltrain: 0.004249 	Lval: 0.004226
Epoch: 55 	Ltrain: 0.004359 	Lval: 0.004291
Epoch 00058: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 60 	Ltrain: 0.004323 	Lval: 0.004240
Epoch: 65 	Ltrain: 0.004268 	Lval: 0.004241
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.004199


	Fold 3/5
Epoch: 1 	Ltrain: 0.030547 	Lval: 0.014420
Epoch: 5 	Ltrain: 0.009188 	Lval: 0.008023
Epoch: 10 	Ltrain: 0.006233 	Lval: 0.005652
Epoch: 15 	Ltrain: 0.005082 	Lval: 0.004982
Epoch: 20 	Ltrain: 0.004706 	Lval: 0.004463
Epoch: 25 	Ltrain: 0.004395 	Lval: 0.004296
Epoch: 30 	Ltrain: 0.004361 	Lval: 0.004216
Epoch: 35 	Ltrain: 0.004290 	Lval: 0.004100
Epoch: 40 	Ltrain: 0.004085 	Lval: 0.003965
Epoch: 45 	Ltrain: 0.003996 	Lval: 0.003887
Epoch: 50 	Ltrain: 0.003818 	Lval: 0.003937
Epoch 00052: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 55 	Ltrain: 0.003705 	Lval: 0.003708
Epoch: 60 	Ltrain: 0.003660 	Lval: 0.003704
Epoch: 65 	Ltrain: 0.003631 	Lval: 0.003808
Epoch: 70 	Ltrain: 0.003619 	Lval: 0.003705
Epoch 00072: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 75 	Ltrain: 0.003592 	Lval: 0.003700
Epoch: 80 	Ltrain: 0.003599 	Lval: 0.003703
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.003672


	Fold 4/5
Epoch: 1 	Ltrain: 0.015776 	Lval: 0.011356
Epoch: 5 	Ltrain: 0.006629 	Lval: 0.006268
Epoch: 10 	Ltrain: 0.004762 	Lval: 0.004736
Epoch: 15 	Ltrain: 0.004396 	Lval: 0.005317
Epoch: 20 	Ltrain: 0.004200 	Lval: 0.004296
Epoch: 25 	Ltrain: 0.004061 	Lval: 0.004097
Epoch: 30 	Ltrain: 0.003944 	Lval: 0.005115
Epoch: 35 	Ltrain: 0.003797 	Lval: 0.004034
Epoch: 40 	Ltrain: 0.003714 	Lval: 0.003841
Epoch: 45 	Ltrain: 0.003661 	Lval: 0.003800
Epoch: 50 	Ltrain: 0.003457 	Lval: 0.003750
Epoch: 55 	Ltrain: 0.003420 	Lval: 0.003656
Epoch: 60 	Ltrain: 0.003273 	Lval: 0.003544
Epoch: 65 	Ltrain: 0.003151 	Lval: 0.003306
Epoch: 70 	Ltrain: 0.003016 	Lval: 0.003240
Epoch: 75 	Ltrain: 0.003074 	Lval: 0.003195
Epoch 00077: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 80 	Ltrain: 0.002731 	Lval: 0.002922
Epoch: 85 	Ltrain: 0.002715 	Lval: 0.002910
Epoch: 90 	Ltrain: 0.002698 	Lval: 0.002883
Epoch: 95 	Ltrain: 0.002681 	Lval: 0.002863
Epoch: 100 	Ltrain: 0.002675 	Lval: 0.002856
Epoch: 105 	Ltrain: 0.002656 	Lval: 0.002842
Epoch: 110 	Ltrain: 0.002651 	Lval: 0.002842
Epoch 00112: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 115 	Ltrain: 0.002613 	Lval: 0.002822
Epoch: 120 	Ltrain: 0.002611 	Lval: 0.002823
EarlyStopper: stopping at epoch 121 with best_val_loss = 0.002829


	Fold 5/5
Epoch: 1 	Ltrain: 0.016790 	Lval: 0.013535
Epoch: 5 	Ltrain: 0.006835 	Lval: 0.006658
Epoch: 10 	Ltrain: 0.004854 	Lval: 0.005187
Epoch: 15 	Ltrain: 0.004306 	Lval: 0.004680
Epoch: 20 	Ltrain: 0.004128 	Lval: 0.004321
Epoch: 25 	Ltrain: 0.003987 	Lval: 0.004164
Epoch: 30 	Ltrain: 0.003847 	Lval: 0.004292
Epoch: 35 	Ltrain: 0.003708 	Lval: 0.004106
Epoch: 40 	Ltrain: 0.003542 	Lval: 0.003885
Epoch 00045: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 45 	Ltrain: 0.003474 	Lval: 0.003890
Epoch: 50 	Ltrain: 0.003250 	Lval: 0.003654
Epoch: 55 	Ltrain: 0.003240 	Lval: 0.003629
Epoch 00060: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 60 	Ltrain: 0.003216 	Lval: 0.003652
Epoch: 65 	Ltrain: 0.003199 	Lval: 0.003614
Epoch: 70 	Ltrain: 0.003207 	Lval: 0.003613
Epoch 00073: reducing learning rate of group 0 to 6.9691e-07.
Epoch: 75 	Ltrain: 0.003210 	Lval: 0.003613
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.003617

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005552727450107967
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.0836872516287e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.043629 	Lval: 0.018269
Epoch: 5 	Ltrain: 0.009290 	Lval: 0.006372
Epoch: 10 	Ltrain: 0.006063 	Lval: 0.004808
Epoch: 15 	Ltrain: 0.004797 	Lval: 0.004573
Epoch: 20 	Ltrain: 0.004698 	Lval: 0.005385
Epoch: 25 	Ltrain: 0.004621 	Lval: 0.004650
Epoch 00027: reducing learning rate of group 0 to 5.5527e-04.
Epoch: 30 	Ltrain: 0.003780 	Lval: 0.003610
Epoch: 35 	Ltrain: 0.003626 	Lval: 0.003485
Epoch: 40 	Ltrain: 0.003611 	Lval: 0.003432
Epoch: 45 	Ltrain: 0.003443 	Lval: 0.003371
Epoch: 50 	Ltrain: 0.003409 	Lval: 0.003306
Epoch: 55 	Ltrain: 0.003594 	Lval: 0.003231
Epoch: 60 	Ltrain: 0.003255 	Lval: 0.003160
Epoch: 65 	Ltrain: 0.003089 	Lval: 0.003055
Epoch: 70 	Ltrain: 0.003143 	Lval: 0.002979
Epoch: 75 	Ltrain: 0.002816 	Lval: 0.002897
Epoch: 80 	Ltrain: 0.002984 	Lval: 0.002888
Epoch 00082: reducing learning rate of group 0 to 5.5527e-05.
Epoch: 85 	Ltrain: 0.002714 	Lval: 0.002725
Epoch: 90 	Ltrain: 0.002626 	Lval: 0.002710
Epoch: 95 	Ltrain: 0.002754 	Lval: 0.002696
Epoch: 100 	Ltrain: 0.002665 	Lval: 0.002684
Epoch: 105 	Ltrain: 0.002777 	Lval: 0.002681
Epoch: 110 	Ltrain: 0.002632 	Lval: 0.002663
Epoch: 115 	Ltrain: 0.002661 	Lval: 0.002647
Epoch: 120 	Ltrain: 0.002665 	Lval: 0.002633
Epoch: 125 	Ltrain: 0.002578 	Lval: 0.002623
Epoch: 130 	Ltrain: 0.002737 	Lval: 0.002611
Epoch: 135 	Ltrain: 0.002553 	Lval: 0.002600
Epoch: 140 	Ltrain: 0.002516 	Lval: 0.002598
Epoch: 145 	Ltrain: 0.002577 	Lval: 0.002569
Epoch: 150 	Ltrain: 0.002477 	Lval: 0.002549
Epoch: 155 	Ltrain: 0.002573 	Lval: 0.002548
Epoch: 160 	Ltrain: 0.002459 	Lval: 0.002528
Epoch: 165 	Ltrain: 0.002535 	Lval: 0.002516
Epoch: 170 	Ltrain: 0.002512 	Lval: 0.002509
Epoch: 175 	Ltrain: 0.002485 	Lval: 0.002495
Epoch: 180 	Ltrain: 0.002493 	Lval: 0.002484
Epoch: 185 	Ltrain: 0.002475 	Lval: 0.002464
Epoch: 190 	Ltrain: 0.002519 	Lval: 0.002452
Epoch: 195 	Ltrain: 0.002447 	Lval: 0.002438
Epoch: 200 	Ltrain: 0.002482 	Lval: 0.002427
Epoch: 205 	Ltrain: 0.002415 	Lval: 0.002420
Epoch: 210 	Ltrain: 0.002591 	Lval: 0.002418
Epoch: 215 	Ltrain: 0.002443 	Lval: 0.002404
Epoch: 220 	Ltrain: 0.002443 	Lval: 0.002383
Epoch: 225 	Ltrain: 0.002376 	Lval: 0.002382
Epoch 00230: reducing learning rate of group 0 to 5.5527e-06.
Epoch: 230 	Ltrain: 0.002332 	Lval: 0.002388
Epoch: 235 	Ltrain: 0.002366 	Lval: 0.002364
Epoch: 240 	Ltrain: 0.002570 	Lval: 0.002361
Epoch: 245 	Ltrain: 0.002325 	Lval: 0.002360
Epoch: 250 	Ltrain: 0.002393 	Lval: 0.002356
EarlyStopper: stopping at epoch 252 with best_val_loss = 0.002364


	Fold 2/5
Epoch: 1 	Ltrain: 0.045948 	Lval: 0.013269
Epoch: 5 	Ltrain: 0.005681 	Lval: 0.005249
Epoch: 10 	Ltrain: 0.005234 	Lval: 0.004714
Epoch: 15 	Ltrain: 0.004586 	Lval: 0.004124
Epoch: 20 	Ltrain: 0.004153 	Lval: 0.004138
Epoch: 25 	Ltrain: 0.003855 	Lval: 0.004228
Epoch: 30 	Ltrain: 0.003449 	Lval: 0.003492
Epoch: 35 	Ltrain: 0.003580 	Lval: 0.003331
Epoch: 40 	Ltrain: 0.003002 	Lval: 0.002642
Epoch 00044: reducing learning rate of group 0 to 5.5527e-04.
Epoch: 45 	Ltrain: 0.002560 	Lval: 0.002441
Epoch: 50 	Ltrain: 0.002145 	Lval: 0.002171
Epoch: 55 	Ltrain: 0.002075 	Lval: 0.002083
Epoch: 60 	Ltrain: 0.001981 	Lval: 0.002033
Epoch: 65 	Ltrain: 0.001850 	Lval: 0.001990
Epoch: 70 	Ltrain: 0.001801 	Lval: 0.001884
Epoch: 75 	Ltrain: 0.001743 	Lval: 0.001786
Epoch: 80 	Ltrain: 0.001696 	Lval: 0.001707
Epoch: 85 	Ltrain: 0.001587 	Lval: 0.001659
Epoch: 90 	Ltrain: 0.001513 	Lval: 0.001579
Epoch: 95 	Ltrain: 0.001413 	Lval: 0.001498
Epoch: 100 	Ltrain: 0.001361 	Lval: 0.001418
Epoch: 105 	Ltrain: 0.001321 	Lval: 0.001382
Epoch: 110 	Ltrain: 0.001246 	Lval: 0.001269
Epoch: 115 	Ltrain: 0.001164 	Lval: 0.001196
Epoch: 120 	Ltrain: 0.001117 	Lval: 0.001164
Epoch: 125 	Ltrain: 0.001007 	Lval: 0.001067
Epoch: 130 	Ltrain: 0.000967 	Lval: 0.001024
Epoch: 135 	Ltrain: 0.000930 	Lval: 0.000958
Epoch: 140 	Ltrain: 0.000852 	Lval: 0.000877
Epoch: 145 	Ltrain: 0.000805 	Lval: 0.000868
Epoch: 150 	Ltrain: 0.000843 	Lval: 0.000914
Epoch 00151: reducing learning rate of group 0 to 5.5527e-05.
Epoch: 155 	Ltrain: 0.000673 	Lval: 0.000737
Epoch: 160 	Ltrain: 0.000662 	Lval: 0.000724
Epoch: 165 	Ltrain: 0.000655 	Lval: 0.000715
Epoch: 170 	Ltrain: 0.000649 	Lval: 0.000708
Epoch: 175 	Ltrain: 0.000653 	Lval: 0.000701
Epoch: 180 	Ltrain: 0.000632 	Lval: 0.000695
Epoch: 185 	Ltrain: 0.000635 	Lval: 0.000689
Epoch: 190 	Ltrain: 0.000632 	Lval: 0.000683
Epoch: 195 	Ltrain: 0.000620 	Lval: 0.000676
Epoch: 200 	Ltrain: 0.000614 	Lval: 0.000670
Epoch: 205 	Ltrain: 0.000613 	Lval: 0.000663
Epoch: 210 	Ltrain: 0.000598 	Lval: 0.000657
Epoch: 215 	Ltrain: 0.000608 	Lval: 0.000650
Epoch: 220 	Ltrain: 0.000591 	Lval: 0.000644
Epoch: 225 	Ltrain: 0.000584 	Lval: 0.000636
Epoch: 230 	Ltrain: 0.000577 	Lval: 0.000630
Epoch: 235 	Ltrain: 0.000571 	Lval: 0.000624
Epoch: 240 	Ltrain: 0.000564 	Lval: 0.000617
Epoch: 245 	Ltrain: 0.000554 	Lval: 0.000611
Epoch: 250 	Ltrain: 0.000560 	Lval: 0.000604
Epoch: 255 	Ltrain: 0.000549 	Lval: 0.000596
Epoch: 260 	Ltrain: 0.000540 	Lval: 0.000588
Epoch: 265 	Ltrain: 0.000528 	Lval: 0.000582
Epoch: 270 	Ltrain: 0.000533 	Lval: 0.000574
Epoch: 275 	Ltrain: 0.000515 	Lval: 0.000567
Epoch: 280 	Ltrain: 0.000508 	Lval: 0.000560
Epoch: 285 	Ltrain: 0.000501 	Lval: 0.000554
Epoch: 290 	Ltrain: 0.000496 	Lval: 0.000546
Epoch: 295 	Ltrain: 0.000491 	Lval: 0.000539
Epoch: 300 	Ltrain: 0.000485 	Lval: 0.000530
Epoch: 305 	Ltrain: 0.000473 	Lval: 0.000524
Epoch: 310 	Ltrain: 0.000468 	Lval: 0.000517
Epoch: 315 	Ltrain: 0.000460 	Lval: 0.000510
Epoch: 320 	Ltrain: 0.000455 	Lval: 0.000502
Epoch: 325 	Ltrain: 0.000451 	Lval: 0.000493
Epoch: 330 	Ltrain: 0.000443 	Lval: 0.000488
Epoch: 335 	Ltrain: 0.000444 	Lval: 0.000481
Epoch: 340 	Ltrain: 0.000435 	Lval: 0.000473
Epoch: 345 	Ltrain: 0.000426 	Lval: 0.000465
Epoch: 350 	Ltrain: 0.000415 	Lval: 0.000459
Epoch: 355 	Ltrain: 0.000406 	Lval: 0.000452
Epoch: 360 	Ltrain: 0.000405 	Lval: 0.000446
Epoch: 365 	Ltrain: 0.000400 	Lval: 0.000438
Epoch: 370 	Ltrain: 0.000389 	Lval: 0.000431
Epoch: 375 	Ltrain: 0.000392 	Lval: 0.000425
Epoch: 380 	Ltrain: 0.000374 	Lval: 0.000417
Epoch: 385 	Ltrain: 0.000368 	Lval: 0.000412
Epoch: 390 	Ltrain: 0.000367 	Lval: 0.000405
Epoch: 395 	Ltrain: 0.000358 	Lval: 0.000396
Epoch: 400 	Ltrain: 0.000348 	Lval: 0.000388
Epoch: 405 	Ltrain: 0.000353 	Lval: 0.000384
Epoch: 410 	Ltrain: 0.000336 	Lval: 0.000375
Epoch: 415 	Ltrain: 0.000337 	Lval: 0.000373
Epoch: 420 	Ltrain: 0.000323 	Lval: 0.000363
Epoch: 425 	Ltrain: 0.000324 	Lval: 0.000358
Epoch: 430 	Ltrain: 0.000322 	Lval: 0.000352
Epoch: 435 	Ltrain: 0.000311 	Lval: 0.000345
Epoch: 440 	Ltrain: 0.000309 	Lval: 0.000339
Epoch: 445 	Ltrain: 0.000299 	Lval: 0.000334
Epoch: 450 	Ltrain: 0.000293 	Lval: 0.000327
Epoch: 455 	Ltrain: 0.000285 	Lval: 0.000322
Epoch: 460 	Ltrain: 0.000287 	Lval: 0.000317
Epoch: 465 	Ltrain: 0.000278 	Lval: 0.000311
Epoch: 470 	Ltrain: 0.000269 	Lval: 0.000303
Epoch: 475 	Ltrain: 0.000266 	Lval: 0.000300
Epoch: 480 	Ltrain: 0.000263 	Lval: 0.000296
Epoch: 485 	Ltrain: 0.000264 	Lval: 0.000293
Epoch 00486: reducing learning rate of group 0 to 5.5527e-06.
Epoch: 490 	Ltrain: 0.000251 	Lval: 0.000284
Epoch: 495 	Ltrain: 0.000249 	Lval: 0.000283
Epoch: 500 	Ltrain: 0.000251 	Lval: 0.000283
Epoch: 505 	Ltrain: 0.000250 	Lval: 0.000282
EarlyStopper: stopping at epoch 505 with best_val_loss = 0.000287


	Fold 3/5
Epoch: 1 	Ltrain: 0.035850 	Lval: 0.010105
Epoch: 5 	Ltrain: 0.006264 	Lval: 0.006837
Epoch: 10 	Ltrain: 0.004735 	Lval: 0.004253
Epoch: 15 	Ltrain: 0.004137 	Lval: 0.004009
Epoch: 20 	Ltrain: 0.003922 	Lval: 0.003802
Epoch: 25 	Ltrain: 0.003303 	Lval: 0.003193
Epoch 00030: reducing learning rate of group 0 to 5.5527e-04.
Epoch: 30 	Ltrain: 0.003219 	Lval: 0.003215
Epoch: 35 	Ltrain: 0.002450 	Lval: 0.002575
Epoch: 40 	Ltrain: 0.002332 	Lval: 0.002505
Epoch: 45 	Ltrain: 0.002232 	Lval: 0.002408
Epoch: 50 	Ltrain: 0.002146 	Lval: 0.002312
Epoch: 55 	Ltrain: 0.002052 	Lval: 0.002226
Epoch: 60 	Ltrain: 0.001970 	Lval: 0.002173
Epoch: 65 	Ltrain: 0.001908 	Lval: 0.002053
Epoch: 70 	Ltrain: 0.001818 	Lval: 0.001978
Epoch: 75 	Ltrain: 0.001769 	Lval: 0.001964
Epoch: 80 	Ltrain: 0.001634 	Lval: 0.001892
Epoch: 85 	Ltrain: 0.001537 	Lval: 0.001765
Epoch: 90 	Ltrain: 0.001458 	Lval: 0.001676
Epoch: 95 	Ltrain: 0.001354 	Lval: 0.001488
Epoch: 100 	Ltrain: 0.001314 	Lval: 0.001420
Epoch: 105 	Ltrain: 0.001251 	Lval: 0.001436
Epoch: 110 	Ltrain: 0.001145 	Lval: 0.001327
Epoch 00115: reducing learning rate of group 0 to 5.5527e-05.
Epoch: 115 	Ltrain: 0.001177 	Lval: 0.001335
Epoch: 120 	Ltrain: 0.000961 	Lval: 0.001129
Epoch: 125 	Ltrain: 0.000952 	Lval: 0.001118
Epoch: 130 	Ltrain: 0.000938 	Lval: 0.001104
Epoch: 135 	Ltrain: 0.000928 	Lval: 0.001094
Epoch: 140 	Ltrain: 0.000920 	Lval: 0.001083
Epoch: 145 	Ltrain: 0.000914 	Lval: 0.001074
Epoch: 150 	Ltrain: 0.000902 	Lval: 0.001060
Epoch: 155 	Ltrain: 0.000891 	Lval: 0.001050
Epoch: 160 	Ltrain: 0.000884 	Lval: 0.001036
Epoch: 165 	Ltrain: 0.000879 	Lval: 0.001028
Epoch: 170 	Ltrain: 0.000868 	Lval: 0.001016
Epoch: 175 	Ltrain: 0.000859 	Lval: 0.001006
Epoch: 180 	Ltrain: 0.000848 	Lval: 0.000992
Epoch: 185 	Ltrain: 0.000840 	Lval: 0.000982
Epoch: 190 	Ltrain: 0.000832 	Lval: 0.000971
Epoch: 195 	Ltrain: 0.000820 	Lval: 0.000959
Epoch: 200 	Ltrain: 0.000811 	Lval: 0.000949
Epoch: 205 	Ltrain: 0.000801 	Lval: 0.000934
Epoch: 210 	Ltrain: 0.000795 	Lval: 0.000924
Epoch: 215 	Ltrain: 0.000782 	Lval: 0.000913
Epoch: 220 	Ltrain: 0.000770 	Lval: 0.000898
Epoch: 225 	Ltrain: 0.000759 	Lval: 0.000886
Epoch: 230 	Ltrain: 0.000745 	Lval: 0.000875
Epoch: 235 	Ltrain: 0.000735 	Lval: 0.000858
Epoch: 240 	Ltrain: 0.000730 	Lval: 0.000850
Epoch: 245 	Ltrain: 0.000714 	Lval: 0.000832
Epoch: 250 	Ltrain: 0.000704 	Lval: 0.000820
Epoch: 255 	Ltrain: 0.000696 	Lval: 0.000811
Epoch: 260 	Ltrain: 0.000683 	Lval: 0.000796
Epoch: 265 	Ltrain: 0.000678 	Lval: 0.000785
Epoch: 270 	Ltrain: 0.000664 	Lval: 0.000772
Epoch: 275 	Ltrain: 0.000651 	Lval: 0.000759
Epoch: 280 	Ltrain: 0.000647 	Lval: 0.000749
Epoch: 285 	Ltrain: 0.000632 	Lval: 0.000735
Epoch: 290 	Ltrain: 0.000625 	Lval: 0.000722
Epoch: 295 	Ltrain: 0.000611 	Lval: 0.000710
Epoch: 300 	Ltrain: 0.000604 	Lval: 0.000699
Epoch: 305 	Ltrain: 0.000591 	Lval: 0.000686
Epoch: 310 	Ltrain: 0.000582 	Lval: 0.000674
Epoch: 315 	Ltrain: 0.000575 	Lval: 0.000663
Epoch: 320 	Ltrain: 0.000565 	Lval: 0.000655
Epoch: 325 	Ltrain: 0.000554 	Lval: 0.000642
Epoch: 330 	Ltrain: 0.000543 	Lval: 0.000631
Epoch: 335 	Ltrain: 0.000535 	Lval: 0.000620
Epoch: 340 	Ltrain: 0.000525 	Lval: 0.000608
Epoch: 345 	Ltrain: 0.000516 	Lval: 0.000597
Epoch: 350 	Ltrain: 0.000508 	Lval: 0.000589
Epoch: 355 	Ltrain: 0.000500 	Lval: 0.000581
Epoch: 360 	Ltrain: 0.000493 	Lval: 0.000568
Epoch: 365 	Ltrain: 0.000485 	Lval: 0.000563
Epoch: 370 	Ltrain: 0.000473 	Lval: 0.000546
Epoch: 375 	Ltrain: 0.000466 	Lval: 0.000539
Epoch: 380 	Ltrain: 0.000455 	Lval: 0.000528
Epoch: 385 	Ltrain: 0.000448 	Lval: 0.000524
Epoch: 390 	Ltrain: 0.000443 	Lval: 0.000511
Epoch: 395 	Ltrain: 0.000432 	Lval: 0.000504
Epoch: 400 	Ltrain: 0.000424 	Lval: 0.000491
Epoch: 405 	Ltrain: 0.000416 	Lval: 0.000485
Epoch: 410 	Ltrain: 0.000416 	Lval: 0.000486
Epoch: 415 	Ltrain: 0.000403 	Lval: 0.000469
Epoch: 420 	Ltrain: 0.000400 	Lval: 0.000461
Epoch: 425 	Ltrain: 0.000390 	Lval: 0.000454
Epoch: 430 	Ltrain: 0.000389 	Lval: 0.000446
Epoch: 435 	Ltrain: 0.000376 	Lval: 0.000437
Epoch: 440 	Ltrain: 0.000371 	Lval: 0.000431
Epoch: 445 	Ltrain: 0.000363 	Lval: 0.000421
Epoch: 450 	Ltrain: 0.000358 	Lval: 0.000414
Epoch: 455 	Ltrain: 0.000353 	Lval: 0.000415
Epoch 00457: reducing learning rate of group 0 to 5.5527e-06.
Epoch: 460 	Ltrain: 0.000344 	Lval: 0.000399
Epoch: 465 	Ltrain: 0.000337 	Lval: 0.000398
Epoch: 470 	Ltrain: 0.000337 	Lval: 0.000397
Epoch: 475 	Ltrain: 0.000336 	Lval: 0.000396
EarlyStopper: stopping at epoch 477 with best_val_loss = 0.000400


	Fold 4/5
Epoch: 1 	Ltrain: 0.023609 	Lval: 0.008371
Epoch: 5 	Ltrain: 0.004790 	Lval: 0.004674
Epoch: 10 	Ltrain: 0.004402 	Lval: 0.004136
Epoch: 15 	Ltrain: 0.003798 	Lval: 0.003692
Epoch: 20 	Ltrain: 0.003370 	Lval: 0.003357
Epoch: 25 	Ltrain: 0.002978 	Lval: 0.003646
Epoch 00028: reducing learning rate of group 0 to 5.5527e-04.
Epoch: 30 	Ltrain: 0.002406 	Lval: 0.002589
Epoch: 35 	Ltrain: 0.002296 	Lval: 0.002529
Epoch: 40 	Ltrain: 0.002217 	Lval: 0.002429
Epoch: 45 	Ltrain: 0.002157 	Lval: 0.002382
Epoch: 50 	Ltrain: 0.002044 	Lval: 0.002324
Epoch: 55 	Ltrain: 0.001972 	Lval: 0.002163
Epoch: 60 	Ltrain: 0.001868 	Lval: 0.002144
Epoch: 65 	Ltrain: 0.001785 	Lval: 0.001932
Epoch: 70 	Ltrain: 0.001656 	Lval: 0.001839
Epoch: 75 	Ltrain: 0.001577 	Lval: 0.001721
Epoch: 80 	Ltrain: 0.001423 	Lval: 0.001646
Epoch: 85 	Ltrain: 0.001368 	Lval: 0.001514
Epoch: 90 	Ltrain: 0.001312 	Lval: 0.001470
Epoch: 95 	Ltrain: 0.001141 	Lval: 0.001253
Epoch: 100 	Ltrain: 0.001127 	Lval: 0.001320
Epoch: 105 	Ltrain: 0.001032 	Lval: 0.001172
Epoch: 110 	Ltrain: 0.000908 	Lval: 0.001039
Epoch: 115 	Ltrain: 0.000886 	Lval: 0.000949
Epoch: 120 	Ltrain: 0.000905 	Lval: 0.000976
Epoch 00121: reducing learning rate of group 0 to 5.5527e-05.
Epoch: 125 	Ltrain: 0.000652 	Lval: 0.000754
Epoch: 130 	Ltrain: 0.000634 	Lval: 0.000736
Epoch: 135 	Ltrain: 0.000622 	Lval: 0.000722
Epoch: 140 	Ltrain: 0.000615 	Lval: 0.000709
Epoch: 145 	Ltrain: 0.000601 	Lval: 0.000699
Epoch: 150 	Ltrain: 0.000592 	Lval: 0.000687
Epoch: 155 	Ltrain: 0.000582 	Lval: 0.000674
Epoch: 160 	Ltrain: 0.000573 	Lval: 0.000663
Epoch: 165 	Ltrain: 0.000563 	Lval: 0.000651
Epoch: 170 	Ltrain: 0.000551 	Lval: 0.000639
Epoch: 175 	Ltrain: 0.000542 	Lval: 0.000628
Epoch: 180 	Ltrain: 0.000532 	Lval: 0.000615
Epoch: 185 	Ltrain: 0.000523 	Lval: 0.000603
Epoch: 190 	Ltrain: 0.000512 	Lval: 0.000595
Epoch: 195 	Ltrain: 0.000501 	Lval: 0.000577
Epoch: 200 	Ltrain: 0.000488 	Lval: 0.000566
Epoch: 205 	Ltrain: 0.000479 	Lval: 0.000554
Epoch: 210 	Ltrain: 0.000468 	Lval: 0.000542
Epoch: 215 	Ltrain: 0.000458 	Lval: 0.000530
Epoch: 220 	Ltrain: 0.000447 	Lval: 0.000515
Epoch: 225 	Ltrain: 0.000437 	Lval: 0.000508
Epoch: 230 	Ltrain: 0.000426 	Lval: 0.000495
Epoch: 235 	Ltrain: 0.000415 	Lval: 0.000479
Epoch: 240 	Ltrain: 0.000406 	Lval: 0.000469
Epoch: 245 	Ltrain: 0.000396 	Lval: 0.000458
Epoch: 250 	Ltrain: 0.000386 	Lval: 0.000448
Epoch: 255 	Ltrain: 0.000376 	Lval: 0.000433
Epoch: 260 	Ltrain: 0.000366 	Lval: 0.000424
Epoch: 265 	Ltrain: 0.000357 	Lval: 0.000412
Epoch: 270 	Ltrain: 0.000349 	Lval: 0.000404
Epoch: 275 	Ltrain: 0.000340 	Lval: 0.000391
Epoch: 280 	Ltrain: 0.000332 	Lval: 0.000379
Epoch: 285 	Ltrain: 0.000323 	Lval: 0.000369
Epoch: 290 	Ltrain: 0.000315 	Lval: 0.000361
Epoch: 295 	Ltrain: 0.000307 	Lval: 0.000353
Epoch: 300 	Ltrain: 0.000299 	Lval: 0.000342
Epoch: 305 	Ltrain: 0.000292 	Lval: 0.000333
Epoch: 310 	Ltrain: 0.000285 	Lval: 0.000326
Epoch: 315 	Ltrain: 0.000279 	Lval: 0.000316
Epoch: 320 	Ltrain: 0.000270 	Lval: 0.000308
Epoch: 325 	Ltrain: 0.000262 	Lval: 0.000298
Epoch: 330 	Ltrain: 0.000258 	Lval: 0.000290
Epoch: 335 	Ltrain: 0.000251 	Lval: 0.000285
Epoch: 340 	Ltrain: 0.000245 	Lval: 0.000279
Epoch: 345 	Ltrain: 0.000238 	Lval: 0.000269
Epoch: 350 	Ltrain: 0.000233 	Lval: 0.000264
Epoch: 355 	Ltrain: 0.000227 	Lval: 0.000256
Epoch: 360 	Ltrain: 0.000222 	Lval: 0.000251
Epoch: 365 	Ltrain: 0.000216 	Lval: 0.000244
Epoch: 370 	Ltrain: 0.000212 	Lval: 0.000238
Epoch: 375 	Ltrain: 0.000207 	Lval: 0.000232
Epoch: 380 	Ltrain: 0.000203 	Lval: 0.000228
Epoch: 385 	Ltrain: 0.000197 	Lval: 0.000222
Epoch: 390 	Ltrain: 0.000193 	Lval: 0.000216
Epoch: 395 	Ltrain: 0.000189 	Lval: 0.000210
Epoch: 400 	Ltrain: 0.000185 	Lval: 0.000207
Epoch: 405 	Ltrain: 0.000184 	Lval: 0.000205
Epoch: 410 	Ltrain: 0.000178 	Lval: 0.000198
Epoch: 415 	Ltrain: 0.000172 	Lval: 0.000192
Epoch: 420 	Ltrain: 0.000169 	Lval: 0.000189
Epoch: 425 	Ltrain: 0.000171 	Lval: 0.000190
Epoch: 430 	Ltrain: 0.000165 	Lval: 0.000183
Epoch: 435 	Ltrain: 0.000160 	Lval: 0.000178
Epoch: 440 	Ltrain: 0.000158 	Lval: 0.000176
Epoch 00443: reducing learning rate of group 0 to 5.5527e-06.
Epoch: 445 	Ltrain: 0.000149 	Lval: 0.000168
Epoch: 450 	Ltrain: 0.000147 	Lval: 0.000167
Epoch: 455 	Ltrain: 0.000146 	Lval: 0.000166
Epoch: 460 	Ltrain: 0.000146 	Lval: 0.000166
EarlyStopper: stopping at epoch 462 with best_val_loss = 0.000169


	Fold 5/5
Epoch: 1 	Ltrain: 0.023028 	Lval: 0.011959
Epoch: 5 	Ltrain: 0.004750 	Lval: 0.004786
Epoch: 10 	Ltrain: 0.003900 	Lval: 0.003942
Epoch: 15 	Ltrain: 0.003535 	Lval: 0.004154
Epoch 00018: reducing learning rate of group 0 to 5.5527e-04.
Epoch: 20 	Ltrain: 0.002814 	Lval: 0.003186
Epoch: 25 	Ltrain: 0.002696 	Lval: 0.003046
Epoch: 30 	Ltrain: 0.002598 	Lval: 0.003026
Epoch: 35 	Ltrain: 0.002527 	Lval: 0.002872
Epoch: 40 	Ltrain: 0.002444 	Lval: 0.002794
Epoch: 45 	Ltrain: 0.002342 	Lval: 0.002689
Epoch: 50 	Ltrain: 0.002286 	Lval: 0.002626
Epoch: 55 	Ltrain: 0.002252 	Lval: 0.002523
Epoch: 60 	Ltrain: 0.002158 	Lval: 0.002460
Epoch: 65 	Ltrain: 0.002034 	Lval: 0.002338
Epoch 00066: reducing learning rate of group 0 to 5.5527e-05.
Epoch: 70 	Ltrain: 0.001846 	Lval: 0.002184
Epoch: 75 	Ltrain: 0.001826 	Lval: 0.002178
Epoch: 80 	Ltrain: 0.001813 	Lval: 0.002148
Epoch 00083: reducing learning rate of group 0 to 5.5527e-06.
Epoch: 85 	Ltrain: 0.001795 	Lval: 0.002135
Epoch: 90 	Ltrain: 0.001794 	Lval: 0.002133
Epoch: 95 	Ltrain: 0.001791 	Lval: 0.002131
Epoch: 100 	Ltrain: 0.001789 	Lval: 0.002129
Epoch: 105 	Ltrain: 0.001787 	Lval: 0.002126
Epoch: 110 	Ltrain: 0.001786 	Lval: 0.002125
EarlyStopper: stopping at epoch 110 with best_val_loss = 0.002132

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0045010052706624904
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.411599348730193e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.031742 	Lval: 0.014613
Epoch: 5 	Ltrain: 0.007350 	Lval: 0.006708
Epoch: 10 	Ltrain: 0.004973 	Lval: 0.005638
Epoch: 15 	Ltrain: 0.004614 	Lval: 0.004893
Epoch 00017: reducing learning rate of group 0 to 4.5010e-04.
Epoch: 20 	Ltrain: 0.004313 	Lval: 0.004065
Epoch: 25 	Ltrain: 0.004223 	Lval: 0.004010
Epoch: 30 	Ltrain: 0.004114 	Lval: 0.003930
Epoch: 35 	Ltrain: 0.003977 	Lval: 0.003894
Epoch: 40 	Ltrain: 0.003975 	Lval: 0.003827
Epoch: 45 	Ltrain: 0.003874 	Lval: 0.003804
Epoch: 50 	Ltrain: 0.003732 	Lval: 0.003757
Epoch 00052: reducing learning rate of group 0 to 4.5010e-05.
Epoch: 55 	Ltrain: 0.003689 	Lval: 0.003641
Epoch: 60 	Ltrain: 0.003674 	Lval: 0.003625
Epoch 00065: reducing learning rate of group 0 to 4.5010e-06.
Epoch: 65 	Ltrain: 0.003551 	Lval: 0.003644
Epoch: 70 	Ltrain: 0.003600 	Lval: 0.003627
Epoch: 75 	Ltrain: 0.003564 	Lval: 0.003620
Epoch: 80 	Ltrain: 0.003746 	Lval: 0.003619
Epoch: 85 	Ltrain: 0.003699 	Lval: 0.003614
Epoch 00089: reducing learning rate of group 0 to 4.5010e-07.
Epoch: 90 	Ltrain: 0.003846 	Lval: 0.003615
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.003622


	Fold 2/5
Epoch: 1 	Ltrain: 0.026057 	Lval: 0.009848
Epoch: 5 	Ltrain: 0.005413 	Lval: 0.005664
Epoch: 10 	Ltrain: 0.004833 	Lval: 0.004494
Epoch: 15 	Ltrain: 0.004503 	Lval: 0.004867
Epoch: 20 	Ltrain: 0.004128 	Lval: 0.004069
Epoch: 25 	Ltrain: 0.003649 	Lval: 0.003757
Epoch: 30 	Ltrain: 0.003752 	Lval: 0.003675
Epoch: 35 	Ltrain: 0.003195 	Lval: 0.003046
Epoch: 40 	Ltrain: 0.002993 	Lval: 0.002848
Epoch 00045: reducing learning rate of group 0 to 4.5010e-04.
Epoch: 45 	Ltrain: 0.002801 	Lval: 0.002799
Epoch: 50 	Ltrain: 0.002172 	Lval: 0.002222
Epoch: 55 	Ltrain: 0.002042 	Lval: 0.002088
Epoch: 60 	Ltrain: 0.001989 	Lval: 0.002031
Epoch: 65 	Ltrain: 0.001930 	Lval: 0.001954
Epoch: 70 	Ltrain: 0.001788 	Lval: 0.001880
Epoch: 75 	Ltrain: 0.001725 	Lval: 0.001816
Epoch: 80 	Ltrain: 0.001634 	Lval: 0.001690
Epoch: 85 	Ltrain: 0.001564 	Lval: 0.001682
Epoch: 90 	Ltrain: 0.001449 	Lval: 0.001521
Epoch: 95 	Ltrain: 0.001407 	Lval: 0.001504
Epoch: 100 	Ltrain: 0.001349 	Lval: 0.001384
Epoch: 105 	Ltrain: 0.001283 	Lval: 0.001327
Epoch: 110 	Ltrain: 0.001189 	Lval: 0.001238
Epoch: 115 	Ltrain: 0.001130 	Lval: 0.001160
Epoch: 120 	Ltrain: 0.001050 	Lval: 0.001095
Epoch: 125 	Ltrain: 0.001014 	Lval: 0.001043
Epoch: 130 	Ltrain: 0.000978 	Lval: 0.001012
Epoch 00132: reducing learning rate of group 0 to 4.5010e-05.
Epoch: 135 	Ltrain: 0.000853 	Lval: 0.000918
Epoch: 140 	Ltrain: 0.000838 	Lval: 0.000902
Epoch: 145 	Ltrain: 0.000824 	Lval: 0.000893
Epoch: 150 	Ltrain: 0.000814 	Lval: 0.000885
Epoch: 155 	Ltrain: 0.000820 	Lval: 0.000877
Epoch: 160 	Ltrain: 0.000819 	Lval: 0.000869
Epoch: 165 	Ltrain: 0.000800 	Lval: 0.000861
Epoch: 170 	Ltrain: 0.000798 	Lval: 0.000854
Epoch: 175 	Ltrain: 0.000789 	Lval: 0.000847
Epoch: 180 	Ltrain: 0.000771 	Lval: 0.000839
Epoch: 185 	Ltrain: 0.000782 	Lval: 0.000830
Epoch: 190 	Ltrain: 0.000765 	Lval: 0.000823
Epoch: 195 	Ltrain: 0.000777 	Lval: 0.000815
Epoch: 200 	Ltrain: 0.000753 	Lval: 0.000807
Epoch: 205 	Ltrain: 0.000742 	Lval: 0.000801
Epoch: 210 	Ltrain: 0.000725 	Lval: 0.000794
Epoch: 215 	Ltrain: 0.000726 	Lval: 0.000785
Epoch: 220 	Ltrain: 0.000719 	Lval: 0.000778
Epoch: 225 	Ltrain: 0.000709 	Lval: 0.000770
Epoch: 230 	Ltrain: 0.000702 	Lval: 0.000763
Epoch: 235 	Ltrain: 0.000689 	Lval: 0.000753
Epoch: 240 	Ltrain: 0.000694 	Lval: 0.000747
Epoch: 245 	Ltrain: 0.000679 	Lval: 0.000738
Epoch: 250 	Ltrain: 0.000667 	Lval: 0.000731
Epoch: 255 	Ltrain: 0.000674 	Lval: 0.000722
Epoch: 260 	Ltrain: 0.000658 	Lval: 0.000715
Epoch: 265 	Ltrain: 0.000654 	Lval: 0.000707
Epoch: 270 	Ltrain: 0.000631 	Lval: 0.000698
Epoch: 275 	Ltrain: 0.000633 	Lval: 0.000691
Epoch: 280 	Ltrain: 0.000629 	Lval: 0.000684
Epoch: 285 	Ltrain: 0.000623 	Lval: 0.000674
Epoch: 290 	Ltrain: 0.000620 	Lval: 0.000667
Epoch: 295 	Ltrain: 0.000613 	Lval: 0.000658
Epoch: 300 	Ltrain: 0.000595 	Lval: 0.000651
Epoch: 305 	Ltrain: 0.000586 	Lval: 0.000643
Epoch: 310 	Ltrain: 0.000583 	Lval: 0.000636
Epoch: 315 	Ltrain: 0.000576 	Lval: 0.000627
Epoch: 320 	Ltrain: 0.000567 	Lval: 0.000618
Epoch: 325 	Ltrain: 0.000557 	Lval: 0.000612
Epoch: 330 	Ltrain: 0.000554 	Lval: 0.000604
Epoch: 335 	Ltrain: 0.000548 	Lval: 0.000597
Epoch: 340 	Ltrain: 0.000535 	Lval: 0.000588
Epoch: 345 	Ltrain: 0.000525 	Lval: 0.000579
Epoch: 350 	Ltrain: 0.000520 	Lval: 0.000573
Epoch: 355 	Ltrain: 0.000524 	Lval: 0.000567
Epoch: 360 	Ltrain: 0.000504 	Lval: 0.000557
Epoch: 365 	Ltrain: 0.000502 	Lval: 0.000551
Epoch: 370 	Ltrain: 0.000495 	Lval: 0.000542
Epoch: 375 	Ltrain: 0.000487 	Lval: 0.000536
Epoch: 380 	Ltrain: 0.000481 	Lval: 0.000532
Epoch: 385 	Ltrain: 0.000476 	Lval: 0.000520
Epoch: 390 	Ltrain: 0.000466 	Lval: 0.000511
Epoch: 395 	Ltrain: 0.000459 	Lval: 0.000505
Epoch: 400 	Ltrain: 0.000452 	Lval: 0.000499
Epoch: 405 	Ltrain: 0.000453 	Lval: 0.000497
Epoch: 410 	Ltrain: 0.000446 	Lval: 0.000484
Epoch: 415 	Ltrain: 0.000440 	Lval: 0.000481
Epoch: 420 	Ltrain: 0.000436 	Lval: 0.000477
Epoch: 425 	Ltrain: 0.000423 	Lval: 0.000469
Epoch: 430 	Ltrain: 0.000418 	Lval: 0.000459
Epoch: 435 	Ltrain: 0.000412 	Lval: 0.000453
Epoch: 440 	Ltrain: 0.000405 	Lval: 0.000446
Epoch: 445 	Ltrain: 0.000404 	Lval: 0.000438
Epoch: 450 	Ltrain: 0.000393 	Lval: 0.000432
Epoch: 455 	Ltrain: 0.000396 	Lval: 0.000432
Epoch 00456: reducing learning rate of group 0 to 4.5010e-06.
Epoch: 460 	Ltrain: 0.000379 	Lval: 0.000422
Epoch: 465 	Ltrain: 0.000381 	Lval: 0.000421
Epoch: 470 	Ltrain: 0.000378 	Lval: 0.000420
Epoch: 475 	Ltrain: 0.000377 	Lval: 0.000419
Epoch: 480 	Ltrain: 0.000379 	Lval: 0.000419
EarlyStopper: stopping at epoch 479 with best_val_loss = 0.000422


	Fold 3/5
Epoch: 1 	Ltrain: 0.026383 	Lval: 0.010723
Epoch: 5 	Ltrain: 0.005108 	Lval: 0.004401
Epoch: 10 	Ltrain: 0.004361 	Lval: 0.004346
Epoch: 15 	Ltrain: 0.004474 	Lval: 0.005026
Epoch: 20 	Ltrain: 0.004054 	Lval: 0.004020
Epoch: 25 	Ltrain: 0.003530 	Lval: 0.003303
Epoch 00026: reducing learning rate of group 0 to 4.5010e-04.
Epoch: 30 	Ltrain: 0.002792 	Lval: 0.002880
Epoch: 35 	Ltrain: 0.002706 	Lval: 0.002804
Epoch: 40 	Ltrain: 0.002652 	Lval: 0.002757
Epoch: 45 	Ltrain: 0.002597 	Lval: 0.002685
Epoch: 50 	Ltrain: 0.002552 	Lval: 0.002683
Epoch: 55 	Ltrain: 0.002473 	Lval: 0.002547
Epoch: 60 	Ltrain: 0.002439 	Lval: 0.002506
Epoch: 65 	Ltrain: 0.002370 	Lval: 0.002451
Epoch: 70 	Ltrain: 0.002285 	Lval: 0.002405
Epoch: 75 	Ltrain: 0.002184 	Lval: 0.002332
Epoch: 80 	Ltrain: 0.002089 	Lval: 0.002199
Epoch 00084: reducing learning rate of group 0 to 4.5010e-05.
Epoch: 85 	Ltrain: 0.002012 	Lval: 0.002130
Epoch: 90 	Ltrain: 0.001918 	Lval: 0.002113
Epoch: 95 	Ltrain: 0.001911 	Lval: 0.002090
Epoch: 100 	Ltrain: 0.001897 	Lval: 0.002082
Epoch: 105 	Ltrain: 0.001881 	Lval: 0.002072
Epoch: 110 	Ltrain: 0.001862 	Lval: 0.002060
Epoch: 115 	Ltrain: 0.001850 	Lval: 0.002043
Epoch: 120 	Ltrain: 0.001849 	Lval: 0.002031
Epoch: 125 	Ltrain: 0.001844 	Lval: 0.002017
Epoch: 130 	Ltrain: 0.001842 	Lval: 0.002017
Epoch: 135 	Ltrain: 0.001820 	Lval: 0.002002
Epoch: 140 	Ltrain: 0.001802 	Lval: 0.001983
Epoch: 145 	Ltrain: 0.001787 	Lval: 0.001980
Epoch: 150 	Ltrain: 0.001771 	Lval: 0.001967
Epoch: 155 	Ltrain: 0.001773 	Lval: 0.001949
Epoch: 160 	Ltrain: 0.001765 	Lval: 0.001940
Epoch: 165 	Ltrain: 0.001746 	Lval: 0.001921
Epoch: 170 	Ltrain: 0.001747 	Lval: 0.001913
Epoch: 175 	Ltrain: 0.001720 	Lval: 0.001901
Epoch 00178: reducing learning rate of group 0 to 4.5010e-06.
Epoch: 180 	Ltrain: 0.001692 	Lval: 0.001889
Epoch: 185 	Ltrain: 0.001695 	Lval: 0.001886
Epoch: 190 	Ltrain: 0.001696 	Lval: 0.001885
EarlyStopper: stopping at epoch 193 with best_val_loss = 0.001894


	Fold 4/5
Epoch: 1 	Ltrain: 0.018878 	Lval: 0.007897
Epoch: 5 	Ltrain: 0.005016 	Lval: 0.004698
Epoch: 10 	Ltrain: 0.004122 	Lval: 0.004669
Epoch: 15 	Ltrain: 0.003631 	Lval: 0.003900
Epoch: 20 	Ltrain: 0.003648 	Lval: 0.004488
Epoch: 25 	Ltrain: 0.003502 	Lval: 0.003321
Epoch: 30 	Ltrain: 0.002824 	Lval: 0.002913
Epoch: 35 	Ltrain: 0.002585 	Lval: 0.002913
Epoch 00040: reducing learning rate of group 0 to 4.5010e-04.
Epoch: 40 	Ltrain: 0.002506 	Lval: 0.002853
Epoch: 45 	Ltrain: 0.001797 	Lval: 0.002016
Epoch: 50 	Ltrain: 0.001714 	Lval: 0.001884
Epoch: 55 	Ltrain: 0.001639 	Lval: 0.001800
Epoch: 60 	Ltrain: 0.001539 	Lval: 0.001720
Epoch: 65 	Ltrain: 0.001452 	Lval: 0.001620
Epoch: 70 	Ltrain: 0.001366 	Lval: 0.001585
Epoch: 75 	Ltrain: 0.001265 	Lval: 0.001410
Epoch: 80 	Ltrain: 0.001181 	Lval: 0.001323
Epoch: 85 	Ltrain: 0.001113 	Lval: 0.001239
Epoch: 90 	Ltrain: 0.001015 	Lval: 0.001144
Epoch: 95 	Ltrain: 0.000949 	Lval: 0.001054
Epoch: 100 	Ltrain: 0.000884 	Lval: 0.000985
Epoch: 105 	Ltrain: 0.000821 	Lval: 0.000902
Epoch: 110 	Ltrain: 0.000768 	Lval: 0.000856
Epoch: 115 	Ltrain: 0.000715 	Lval: 0.000779
Epoch: 120 	Ltrain: 0.000677 	Lval: 0.000712
Epoch: 125 	Ltrain: 0.000625 	Lval: 0.000668
Epoch: 130 	Ltrain: 0.000568 	Lval: 0.000628
Epoch 00132: reducing learning rate of group 0 to 4.5010e-05.
Epoch: 135 	Ltrain: 0.000463 	Lval: 0.000512
Epoch: 140 	Ltrain: 0.000447 	Lval: 0.000497
Epoch: 145 	Ltrain: 0.000438 	Lval: 0.000488
Epoch: 150 	Ltrain: 0.000432 	Lval: 0.000478
Epoch: 155 	Ltrain: 0.000426 	Lval: 0.000471
Epoch: 160 	Ltrain: 0.000419 	Lval: 0.000464
Epoch: 165 	Ltrain: 0.000412 	Lval: 0.000456
Epoch: 170 	Ltrain: 0.000407 	Lval: 0.000448
Epoch: 175 	Ltrain: 0.000399 	Lval: 0.000440
Epoch: 180 	Ltrain: 0.000393 	Lval: 0.000432
Epoch: 185 	Ltrain: 0.000385 	Lval: 0.000424
Epoch: 190 	Ltrain: 0.000379 	Lval: 0.000417
Epoch: 195 	Ltrain: 0.000373 	Lval: 0.000409
Epoch: 200 	Ltrain: 0.000366 	Lval: 0.000400
Epoch: 205 	Ltrain: 0.000359 	Lval: 0.000393
Epoch: 210 	Ltrain: 0.000352 	Lval: 0.000384
Epoch: 215 	Ltrain: 0.000345 	Lval: 0.000376
Epoch: 220 	Ltrain: 0.000337 	Lval: 0.000367
Epoch: 225 	Ltrain: 0.000330 	Lval: 0.000361
Epoch: 230 	Ltrain: 0.000324 	Lval: 0.000352
Epoch: 235 	Ltrain: 0.000317 	Lval: 0.000343
Epoch: 240 	Ltrain: 0.000310 	Lval: 0.000335
Epoch: 245 	Ltrain: 0.000303 	Lval: 0.000327
Epoch: 250 	Ltrain: 0.000297 	Lval: 0.000321
Epoch: 255 	Ltrain: 0.000290 	Lval: 0.000313
Epoch: 260 	Ltrain: 0.000283 	Lval: 0.000306
Epoch: 265 	Ltrain: 0.000277 	Lval: 0.000297
Epoch: 270 	Ltrain: 0.000270 	Lval: 0.000290
Epoch: 275 	Ltrain: 0.000264 	Lval: 0.000284
Epoch: 280 	Ltrain: 0.000258 	Lval: 0.000275
Epoch: 285 	Ltrain: 0.000252 	Lval: 0.000268
Epoch: 290 	Ltrain: 0.000247 	Lval: 0.000263
Epoch: 295 	Ltrain: 0.000241 	Lval: 0.000255
Epoch: 300 	Ltrain: 0.000235 	Lval: 0.000248
Epoch: 305 	Ltrain: 0.000229 	Lval: 0.000242
Epoch: 310 	Ltrain: 0.000223 	Lval: 0.000236
Epoch: 315 	Ltrain: 0.000219 	Lval: 0.000230
Epoch: 320 	Ltrain: 0.000215 	Lval: 0.000226
Epoch: 325 	Ltrain: 0.000209 	Lval: 0.000219
Epoch: 330 	Ltrain: 0.000204 	Lval: 0.000213
Epoch: 335 	Ltrain: 0.000199 	Lval: 0.000208
Epoch: 340 	Ltrain: 0.000194 	Lval: 0.000203
Epoch: 345 	Ltrain: 0.000190 	Lval: 0.000198
Epoch: 350 	Ltrain: 0.000187 	Lval: 0.000194
Epoch: 355 	Ltrain: 0.000182 	Lval: 0.000189
Epoch: 360 	Ltrain: 0.000179 	Lval: 0.000185
Epoch: 365 	Ltrain: 0.000175 	Lval: 0.000181
Epoch: 370 	Ltrain: 0.000170 	Lval: 0.000177
Epoch: 375 	Ltrain: 0.000167 	Lval: 0.000171
Epoch: 380 	Ltrain: 0.000163 	Lval: 0.000167
Epoch: 385 	Ltrain: 0.000160 	Lval: 0.000164
Epoch: 390 	Ltrain: 0.000158 	Lval: 0.000165
Epoch: 395 	Ltrain: 0.000153 	Lval: 0.000155
Epoch: 400 	Ltrain: 0.000149 	Lval: 0.000153
Epoch: 405 	Ltrain: 0.000146 	Lval: 0.000149
Epoch: 410 	Ltrain: 0.000144 	Lval: 0.000147
Epoch: 415 	Ltrain: 0.000141 	Lval: 0.000143
Epoch: 420 	Ltrain: 0.000136 	Lval: 0.000139
Epoch: 425 	Ltrain: 0.000135 	Lval: 0.000136
Epoch: 430 	Ltrain: 0.000132 	Lval: 0.000135
Epoch: 435 	Ltrain: 0.000129 	Lval: 0.000131
Epoch: 440 	Ltrain: 0.000127 	Lval: 0.000129
Epoch: 445 	Ltrain: 0.000124 	Lval: 0.000125
Epoch 00449: reducing learning rate of group 0 to 4.5010e-06.
Epoch: 450 	Ltrain: 0.000122 	Lval: 0.000122
Epoch: 455 	Ltrain: 0.000118 	Lval: 0.000119
Epoch: 460 	Ltrain: 0.000118 	Lval: 0.000119
Epoch: 465 	Ltrain: 0.000117 	Lval: 0.000118
Epoch: 470 	Ltrain: 0.000117 	Lval: 0.000118
Epoch: 475 	Ltrain: 0.000116 	Lval: 0.000118
EarlyStopper: stopping at epoch 476 with best_val_loss = 0.000119


	Fold 5/5
Epoch: 1 	Ltrain: 0.020683 	Lval: 0.009819
Epoch: 5 	Ltrain: 0.004877 	Lval: 0.004711
Epoch: 10 	Ltrain: 0.004461 	Lval: 0.004249
Epoch: 15 	Ltrain: 0.003665 	Lval: 0.003994
Epoch: 20 	Ltrain: 0.003613 	Lval: 0.004611
Epoch: 25 	Ltrain: 0.003428 	Lval: 0.003672
Epoch: 30 	Ltrain: 0.003058 	Lval: 0.003429
Epoch: 35 	Ltrain: 0.002720 	Lval: 0.003077
Epoch 00040: reducing learning rate of group 0 to 4.5010e-04.
Epoch: 40 	Ltrain: 0.002737 	Lval: 0.002803
Epoch: 45 	Ltrain: 0.001910 	Lval: 0.002175
Epoch: 50 	Ltrain: 0.001813 	Lval: 0.002090
Epoch: 55 	Ltrain: 0.001733 	Lval: 0.001969
Epoch: 60 	Ltrain: 0.001653 	Lval: 0.001870
Epoch: 65 	Ltrain: 0.001568 	Lval: 0.001791
Epoch: 70 	Ltrain: 0.001468 	Lval: 0.001680
Epoch: 75 	Ltrain: 0.001403 	Lval: 0.001579
Epoch: 80 	Ltrain: 0.001326 	Lval: 0.001511
Epoch: 85 	Ltrain: 0.001230 	Lval: 0.001357
Epoch: 90 	Ltrain: 0.001128 	Lval: 0.001252
Epoch: 95 	Ltrain: 0.001055 	Lval: 0.001163
Epoch: 100 	Ltrain: 0.000979 	Lval: 0.001127
Epoch: 105 	Ltrain: 0.000908 	Lval: 0.001004
Epoch: 110 	Ltrain: 0.000886 	Lval: 0.000979
Epoch: 115 	Ltrain: 0.000804 	Lval: 0.000869
Epoch: 120 	Ltrain: 0.000761 	Lval: 0.000812
Epoch: 125 	Ltrain: 0.000677 	Lval: 0.000776
Epoch 00127: reducing learning rate of group 0 to 4.5010e-05.
Epoch: 130 	Ltrain: 0.000587 	Lval: 0.000651
Epoch: 135 	Ltrain: 0.000570 	Lval: 0.000634
Epoch: 140 	Ltrain: 0.000560 	Lval: 0.000623
Epoch: 145 	Ltrain: 0.000553 	Lval: 0.000615
Epoch: 150 	Ltrain: 0.000546 	Lval: 0.000606
Epoch: 155 	Ltrain: 0.000539 	Lval: 0.000597
Epoch: 160 	Ltrain: 0.000532 	Lval: 0.000590
Epoch: 165 	Ltrain: 0.000525 	Lval: 0.000582
Epoch: 170 	Ltrain: 0.000518 	Lval: 0.000573
Epoch: 175 	Ltrain: 0.000512 	Lval: 0.000564
Epoch: 180 	Ltrain: 0.000504 	Lval: 0.000556
Epoch: 185 	Ltrain: 0.000496 	Lval: 0.000547
Epoch: 190 	Ltrain: 0.000489 	Lval: 0.000540
Epoch: 195 	Ltrain: 0.000480 	Lval: 0.000529
Epoch: 200 	Ltrain: 0.000473 	Lval: 0.000521
Epoch: 205 	Ltrain: 0.000465 	Lval: 0.000510
Epoch: 210 	Ltrain: 0.000457 	Lval: 0.000502
Epoch: 215 	Ltrain: 0.000450 	Lval: 0.000494
Epoch: 220 	Ltrain: 0.000442 	Lval: 0.000486
Epoch: 225 	Ltrain: 0.000434 	Lval: 0.000478
Epoch: 230 	Ltrain: 0.000425 	Lval: 0.000468
Epoch: 235 	Ltrain: 0.000419 	Lval: 0.000460
Epoch: 240 	Ltrain: 0.000410 	Lval: 0.000451
Epoch: 245 	Ltrain: 0.000402 	Lval: 0.000443
Epoch: 250 	Ltrain: 0.000395 	Lval: 0.000433
Epoch: 255 	Ltrain: 0.000388 	Lval: 0.000427
Epoch: 260 	Ltrain: 0.000380 	Lval: 0.000419
Epoch: 265 	Ltrain: 0.000372 	Lval: 0.000409
Epoch: 270 	Ltrain: 0.000366 	Lval: 0.000402
Epoch: 275 	Ltrain: 0.000358 	Lval: 0.000393
Epoch: 280 	Ltrain: 0.000353 	Lval: 0.000388
Epoch: 285 	Ltrain: 0.000344 	Lval: 0.000381
Epoch: 290 	Ltrain: 0.000338 	Lval: 0.000372
Epoch: 295 	Ltrain: 0.000331 	Lval: 0.000360
Epoch: 300 	Ltrain: 0.000325 	Lval: 0.000354
Epoch: 305 	Ltrain: 0.000319 	Lval: 0.000347
Epoch 00310: reducing learning rate of group 0 to 4.5010e-06.
Epoch: 310 	Ltrain: 0.000317 	Lval: 0.000354
Epoch: 315 	Ltrain: 0.000306 	Lval: 0.000336
Epoch: 320 	Ltrain: 0.000304 	Lval: 0.000335
Epoch: 325 	Ltrain: 0.000304 	Lval: 0.000334
Epoch: 330 	Ltrain: 0.000302 	Lval: 0.000334
EarlyStopper: stopping at epoch 330 with best_val_loss = 0.000342

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003814520592692092
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.630232420847237e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.048281 	Lval: 0.016505
Epoch: 5 	Ltrain: 0.008938 	Lval: 0.008623
Epoch: 10 	Ltrain: 0.006051 	Lval: 0.006412
Epoch: 15 	Ltrain: 0.005217 	Lval: 0.005578
Epoch: 20 	Ltrain: 0.004801 	Lval: 0.004611
Epoch 00025: reducing learning rate of group 0 to 3.8145e-04.
Epoch: 25 	Ltrain: 0.004887 	Lval: 0.006897
Epoch: 30 	Ltrain: 0.003900 	Lval: 0.003844
Epoch: 35 	Ltrain: 0.003896 	Lval: 0.003766
Epoch: 40 	Ltrain: 0.003747 	Lval: 0.003715
Epoch: 45 	Ltrain: 0.003632 	Lval: 0.003652
Epoch: 50 	Ltrain: 0.003589 	Lval: 0.003582
Epoch: 55 	Ltrain: 0.003930 	Lval: 0.003516
Epoch: 60 	Ltrain: 0.003392 	Lval: 0.003463
Epoch: 65 	Ltrain: 0.003355 	Lval: 0.003390
Epoch: 70 	Ltrain: 0.003387 	Lval: 0.003293
Epoch: 75 	Ltrain: 0.003307 	Lval: 0.003209
Epoch: 80 	Ltrain: 0.003194 	Lval: 0.003084
Epoch: 85 	Ltrain: 0.003031 	Lval: 0.003020
Epoch: 90 	Ltrain: 0.003133 	Lval: 0.002966
Epoch: 95 	Ltrain: 0.002917 	Lval: 0.002832
Epoch: 100 	Ltrain: 0.002953 	Lval: 0.002777
Epoch: 105 	Ltrain: 0.003027 	Lval: 0.002865
Epoch: 110 	Ltrain: 0.002797 	Lval: 0.002655
Epoch: 115 	Ltrain: 0.002704 	Lval: 0.002556
Epoch: 120 	Ltrain: 0.002708 	Lval: 0.002573
Epoch 00122: reducing learning rate of group 0 to 3.8145e-05.
Epoch: 125 	Ltrain: 0.002583 	Lval: 0.002446
Epoch: 130 	Ltrain: 0.002532 	Lval: 0.002430
Epoch: 135 	Ltrain: 0.002391 	Lval: 0.002422
Epoch: 140 	Ltrain: 0.002421 	Lval: 0.002413
Epoch: 145 	Ltrain: 0.002327 	Lval: 0.002407
Epoch: 150 	Ltrain: 0.002385 	Lval: 0.002405
Epoch: 155 	Ltrain: 0.002459 	Lval: 0.002393
Epoch: 160 	Ltrain: 0.002510 	Lval: 0.002380
Epoch 00164: reducing learning rate of group 0 to 3.8145e-06.
Epoch: 165 	Ltrain: 0.002268 	Lval: 0.002380
Epoch: 170 	Ltrain: 0.002369 	Lval: 0.002373
Epoch: 175 	Ltrain: 0.002397 	Lval: 0.002372
Epoch: 180 	Ltrain: 0.002237 	Lval: 0.002371
Epoch 00182: reducing learning rate of group 0 to 3.8145e-07.
Epoch: 185 	Ltrain: 0.002426 	Lval: 0.002371
EarlyStopper: stopping at epoch 186 with best_val_loss = 0.002375


	Fold 2/5
Epoch: 1 	Ltrain: 0.024525 	Lval: 0.014975
Epoch: 5 	Ltrain: 0.006395 	Lval: 0.006026
Epoch: 10 	Ltrain: 0.004766 	Lval: 0.004351
Epoch: 15 	Ltrain: 0.004569 	Lval: 0.004225
Epoch: 20 	Ltrain: 0.004322 	Lval: 0.003939
Epoch: 25 	Ltrain: 0.003751 	Lval: 0.003564
Epoch: 30 	Ltrain: 0.003525 	Lval: 0.003810
Epoch 00031: reducing learning rate of group 0 to 3.8145e-04.
Epoch: 35 	Ltrain: 0.002791 	Lval: 0.002822
Epoch: 40 	Ltrain: 0.002677 	Lval: 0.002683
Epoch: 45 	Ltrain: 0.002657 	Lval: 0.002639
Epoch: 50 	Ltrain: 0.002562 	Lval: 0.002599
Epoch: 55 	Ltrain: 0.002528 	Lval: 0.002460
Epoch 00057: reducing learning rate of group 0 to 3.8145e-05.
Epoch: 60 	Ltrain: 0.002346 	Lval: 0.002409
Epoch: 65 	Ltrain: 0.002325 	Lval: 0.002399
Epoch: 70 	Ltrain: 0.002370 	Lval: 0.002394
Epoch: 75 	Ltrain: 0.002318 	Lval: 0.002388
Epoch 00080: reducing learning rate of group 0 to 3.8145e-06.
Epoch: 80 	Ltrain: 0.002322 	Lval: 0.002380
Epoch: 85 	Ltrain: 0.002294 	Lval: 0.002378
EarlyStopper: stopping at epoch 88 with best_val_loss = 0.002387


	Fold 3/5
Epoch: 1 	Ltrain: 0.025808 	Lval: 0.014033
Epoch: 5 	Ltrain: 0.005682 	Lval: 0.005324
Epoch: 10 	Ltrain: 0.004586 	Lval: 0.004284
Epoch 00015: reducing learning rate of group 0 to 3.8145e-04.
Epoch: 15 	Ltrain: 0.004082 	Lval: 0.004835
Epoch: 20 	Ltrain: 0.003466 	Lval: 0.003685
Epoch: 25 	Ltrain: 0.003415 	Lval: 0.003565
Epoch: 30 	Ltrain: 0.003395 	Lval: 0.003423
Epoch: 35 	Ltrain: 0.003263 	Lval: 0.003354
Epoch 00037: reducing learning rate of group 0 to 3.8145e-05.
Epoch: 40 	Ltrain: 0.003079 	Lval: 0.003226
Epoch: 45 	Ltrain: 0.003067 	Lval: 0.003210
Epoch 00049: reducing learning rate of group 0 to 3.8145e-06.
Epoch: 50 	Ltrain: 0.003056 	Lval: 0.003197
Epoch: 55 	Ltrain: 0.003053 	Lval: 0.003197
Epoch: 60 	Ltrain: 0.003027 	Lval: 0.003196
Epoch 00061: reducing learning rate of group 0 to 3.8145e-07.
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.003195


	Fold 4/5
Epoch: 1 	Ltrain: 0.024094 	Lval: 0.011244
Epoch: 5 	Ltrain: 0.005271 	Lval: 0.005040
Epoch: 10 	Ltrain: 0.004620 	Lval: 0.004844
Epoch: 15 	Ltrain: 0.003765 	Lval: 0.003787
Epoch: 20 	Ltrain: 0.003523 	Lval: 0.004059
Epoch: 25 	Ltrain: 0.003170 	Lval: 0.003249
Epoch: 30 	Ltrain: 0.003014 	Lval: 0.002953
Epoch: 35 	Ltrain: 0.002633 	Lval: 0.002668
Epoch: 40 	Ltrain: 0.002420 	Lval: 0.002583
Epoch: 45 	Ltrain: 0.002287 	Lval: 0.002204
Epoch: 50 	Ltrain: 0.001925 	Lval: 0.001928
Epoch: 55 	Ltrain: 0.001888 	Lval: 0.001871
Epoch: 60 	Ltrain: 0.001504 	Lval: 0.001777
Epoch: 65 	Ltrain: 0.001286 	Lval: 0.001407
Epoch 00068: reducing learning rate of group 0 to 3.8145e-04.
Epoch: 70 	Ltrain: 0.000789 	Lval: 0.000834
Epoch: 75 	Ltrain: 0.000656 	Lval: 0.000720
Epoch: 80 	Ltrain: 0.000598 	Lval: 0.000656
Epoch: 85 	Ltrain: 0.000553 	Lval: 0.000605
Epoch: 90 	Ltrain: 0.000510 	Lval: 0.000555
Epoch: 95 	Ltrain: 0.000470 	Lval: 0.000512
Epoch: 100 	Ltrain: 0.000433 	Lval: 0.000471
Epoch: 105 	Ltrain: 0.000395 	Lval: 0.000426
Epoch: 110 	Ltrain: 0.000358 	Lval: 0.000388
Epoch: 115 	Ltrain: 0.000329 	Lval: 0.000354
Epoch: 120 	Ltrain: 0.000293 	Lval: 0.000314
Epoch: 125 	Ltrain: 0.000264 	Lval: 0.000281
Epoch: 130 	Ltrain: 0.000237 	Lval: 0.000249
Epoch: 135 	Ltrain: 0.000223 	Lval: 0.000240
Epoch 00138: reducing learning rate of group 0 to 3.8145e-05.
Epoch: 140 	Ltrain: 0.000203 	Lval: 0.000213
Epoch: 145 	Ltrain: 0.000189 	Lval: 0.000204
Epoch: 150 	Ltrain: 0.000184 	Lval: 0.000200
Epoch: 155 	Ltrain: 0.000181 	Lval: 0.000197
Epoch: 160 	Ltrain: 0.000178 	Lval: 0.000194
Epoch: 165 	Ltrain: 0.000176 	Lval: 0.000191
Epoch: 170 	Ltrain: 0.000173 	Lval: 0.000188
Epoch: 175 	Ltrain: 0.000170 	Lval: 0.000185
Epoch: 180 	Ltrain: 0.000168 	Lval: 0.000182
Epoch: 185 	Ltrain: 0.000165 	Lval: 0.000179
Epoch: 190 	Ltrain: 0.000162 	Lval: 0.000176
Epoch: 195 	Ltrain: 0.000159 	Lval: 0.000172
Epoch: 200 	Ltrain: 0.000156 	Lval: 0.000169
Epoch: 205 	Ltrain: 0.000153 	Lval: 0.000166
Epoch: 210 	Ltrain: 0.000150 	Lval: 0.000162
Epoch: 215 	Ltrain: 0.000147 	Lval: 0.000159
Epoch: 220 	Ltrain: 0.000143 	Lval: 0.000156
Epoch: 225 	Ltrain: 0.000140 	Lval: 0.000152
Epoch: 230 	Ltrain: 0.000137 	Lval: 0.000149
Epoch: 235 	Ltrain: 0.000134 	Lval: 0.000145
Epoch: 240 	Ltrain: 0.000131 	Lval: 0.000142
Epoch: 245 	Ltrain: 0.000127 	Lval: 0.000138
Epoch: 250 	Ltrain: 0.000125 	Lval: 0.000135
Epoch: 255 	Ltrain: 0.000121 	Lval: 0.000132
Epoch: 260 	Ltrain: 0.000119 	Lval: 0.000128
Epoch: 265 	Ltrain: 0.000115 	Lval: 0.000125
Epoch: 270 	Ltrain: 0.000112 	Lval: 0.000122
Epoch: 275 	Ltrain: 0.000109 	Lval: 0.000119
Epoch: 280 	Ltrain: 0.000106 	Lval: 0.000116
Epoch: 285 	Ltrain: 0.000104 	Lval: 0.000113
Epoch: 290 	Ltrain: 0.000101 	Lval: 0.000110
Epoch: 295 	Ltrain: 0.000098 	Lval: 0.000106
Epoch: 300 	Ltrain: 0.000096 	Lval: 0.000104
Epoch: 305 	Ltrain: 0.000093 	Lval: 0.000101
Epoch: 310 	Ltrain: 0.000091 	Lval: 0.000098
Epoch: 315 	Ltrain: 0.000088 	Lval: 0.000096
Epoch: 320 	Ltrain: 0.000086 	Lval: 0.000093
Epoch: 325 	Ltrain: 0.000083 	Lval: 0.000090
Epoch: 330 	Ltrain: 0.000081 	Lval: 0.000088
Epoch: 335 	Ltrain: 0.000079 	Lval: 0.000085
Epoch: 340 	Ltrain: 0.000077 	Lval: 0.000083
Epoch: 345 	Ltrain: 0.000075 	Lval: 0.000081
Epoch: 350 	Ltrain: 0.000073 	Lval: 0.000078
EarlyStopper: stopping at epoch 351 with best_val_loss = 0.000086


	Fold 5/5
Epoch: 1 	Ltrain: 0.019897 	Lval: 0.010621
Epoch: 5 	Ltrain: 0.004908 	Lval: 0.005850
Epoch: 10 	Ltrain: 0.003983 	Lval: 0.004174
Epoch: 15 	Ltrain: 0.004104 	Lval: 0.004021
Epoch: 20 	Ltrain: 0.003492 	Lval: 0.004543
Epoch: 25 	Ltrain: 0.003623 	Lval: 0.004286
Epoch: 30 	Ltrain: 0.002977 	Lval: 0.003106
Epoch: 35 	Ltrain: 0.002629 	Lval: 0.002780
Epoch 00037: reducing learning rate of group 0 to 3.8145e-04.
Epoch: 40 	Ltrain: 0.002022 	Lval: 0.002278
Epoch: 45 	Ltrain: 0.001881 	Lval: 0.002152
Epoch: 50 	Ltrain: 0.001801 	Lval: 0.002078
Epoch: 55 	Ltrain: 0.001720 	Lval: 0.001969
Epoch: 60 	Ltrain: 0.001630 	Lval: 0.001885
Epoch: 65 	Ltrain: 0.001543 	Lval: 0.001795
Epoch: 70 	Ltrain: 0.001456 	Lval: 0.001699
Epoch: 75 	Ltrain: 0.001341 	Lval: 0.001594
Epoch: 80 	Ltrain: 0.001242 	Lval: 0.001472
Epoch: 85 	Ltrain: 0.001160 	Lval: 0.001356
Epoch: 90 	Ltrain: 0.001083 	Lval: 0.001268
Epoch: 95 	Ltrain: 0.001008 	Lval: 0.001179
Epoch: 100 	Ltrain: 0.000950 	Lval: 0.001076
Epoch: 105 	Ltrain: 0.000873 	Lval: 0.001027
Epoch: 110 	Ltrain: 0.000805 	Lval: 0.000896
Epoch: 115 	Ltrain: 0.000784 	Lval: 0.000873
Epoch: 120 	Ltrain: 0.000692 	Lval: 0.000770
Epoch: 125 	Ltrain: 0.000637 	Lval: 0.000715
Epoch: 130 	Ltrain: 0.000604 	Lval: 0.000684
Epoch: 135 	Ltrain: 0.000551 	Lval: 0.000620
Epoch 00140: reducing learning rate of group 0 to 3.8145e-05.
Epoch: 140 	Ltrain: 0.000549 	Lval: 0.000604
Epoch: 145 	Ltrain: 0.000416 	Lval: 0.000487
Epoch: 150 	Ltrain: 0.000404 	Lval: 0.000474
Epoch: 155 	Ltrain: 0.000396 	Lval: 0.000466
Epoch: 160 	Ltrain: 0.000390 	Lval: 0.000460
Epoch: 165 	Ltrain: 0.000385 	Lval: 0.000453
Epoch: 170 	Ltrain: 0.000378 	Lval: 0.000445
Epoch: 175 	Ltrain: 0.000373 	Lval: 0.000439
Epoch: 180 	Ltrain: 0.000367 	Lval: 0.000432
Epoch: 185 	Ltrain: 0.000362 	Lval: 0.000424
Epoch: 190 	Ltrain: 0.000355 	Lval: 0.000416
Epoch: 195 	Ltrain: 0.000348 	Lval: 0.000409
Epoch: 200 	Ltrain: 0.000342 	Lval: 0.000401
Epoch: 205 	Ltrain: 0.000338 	Lval: 0.000394
Epoch: 210 	Ltrain: 0.000330 	Lval: 0.000387
Epoch: 215 	Ltrain: 0.000324 	Lval: 0.000378
Epoch: 220 	Ltrain: 0.000318 	Lval: 0.000371
Epoch: 225 	Ltrain: 0.000312 	Lval: 0.000363
Epoch: 230 	Ltrain: 0.000305 	Lval: 0.000355
Epoch: 235 	Ltrain: 0.000299 	Lval: 0.000347
Epoch: 240 	Ltrain: 0.000293 	Lval: 0.000340
Epoch: 245 	Ltrain: 0.000286 	Lval: 0.000331
Epoch: 250 	Ltrain: 0.000281 	Lval: 0.000324
Epoch: 255 	Ltrain: 0.000274 	Lval: 0.000318
Epoch: 260 	Ltrain: 0.000269 	Lval: 0.000309
Epoch: 265 	Ltrain: 0.000262 	Lval: 0.000303
Epoch: 270 	Ltrain: 0.000257 	Lval: 0.000295
Epoch: 275 	Ltrain: 0.000251 	Lval: 0.000288
Epoch: 280 	Ltrain: 0.000246 	Lval: 0.000281
Epoch: 285 	Ltrain: 0.000241 	Lval: 0.000275
Epoch: 290 	Ltrain: 0.000237 	Lval: 0.000267
Epoch: 295 	Ltrain: 0.000231 	Lval: 0.000261
Epoch: 300 	Ltrain: 0.000225 	Lval: 0.000255
Epoch: 305 	Ltrain: 0.000221 	Lval: 0.000249
Epoch: 310 	Ltrain: 0.000215 	Lval: 0.000242
Epoch: 315 	Ltrain: 0.000212 	Lval: 0.000238
Epoch: 320 	Ltrain: 0.000207 	Lval: 0.000231
Epoch: 325 	Ltrain: 0.000204 	Lval: 0.000226
Epoch: 330 	Ltrain: 0.000199 	Lval: 0.000221
Epoch: 335 	Ltrain: 0.000195 	Lval: 0.000216
Epoch: 340 	Ltrain: 0.000190 	Lval: 0.000210
Epoch: 345 	Ltrain: 0.000187 	Lval: 0.000206
Epoch: 350 	Ltrain: 0.000183 	Lval: 0.000202
Epoch: 355 	Ltrain: 0.000179 	Lval: 0.000197
Epoch: 360 	Ltrain: 0.000175 	Lval: 0.000192
Epoch: 365 	Ltrain: 0.000171 	Lval: 0.000188
Epoch: 370 	Ltrain: 0.000167 	Lval: 0.000183
Epoch: 375 	Ltrain: 0.000164 	Lval: 0.000178
Epoch: 380 	Ltrain: 0.000161 	Lval: 0.000176
Epoch: 385 	Ltrain: 0.000158 	Lval: 0.000172
Epoch: 390 	Ltrain: 0.000155 	Lval: 0.000167
Epoch: 395 	Ltrain: 0.000151 	Lval: 0.000163
Epoch: 400 	Ltrain: 0.000149 	Lval: 0.000161
Epoch: 405 	Ltrain: 0.000146 	Lval: 0.000157
Epoch: 410 	Ltrain: 0.000143 	Lval: 0.000153
Epoch: 415 	Ltrain: 0.000139 	Lval: 0.000150
Epoch: 420 	Ltrain: 0.000137 	Lval: 0.000146
Epoch: 425 	Ltrain: 0.000134 	Lval: 0.000143
Epoch: 430 	Ltrain: 0.000131 	Lval: 0.000140
Epoch: 435 	Ltrain: 0.000130 	Lval: 0.000137
Epoch: 440 	Ltrain: 0.000126 	Lval: 0.000133
Epoch: 445 	Ltrain: 0.000124 	Lval: 0.000131
Epoch: 450 	Ltrain: 0.000122 	Lval: 0.000129
Epoch: 455 	Ltrain: 0.000120 	Lval: 0.000126
Epoch: 460 	Ltrain: 0.000119 	Lval: 0.000124
Epoch: 465 	Ltrain: 0.000116 	Lval: 0.000122
Epoch: 470 	Ltrain: 0.000113 	Lval: 0.000117
Epoch: 475 	Ltrain: 0.000111 	Lval: 0.000116
Epoch: 480 	Ltrain: 0.000108 	Lval: 0.000113
Epoch: 485 	Ltrain: 0.000107 	Lval: 0.000110
Epoch: 490 	Ltrain: 0.000107 	Lval: 0.000112
Epoch 00491: reducing learning rate of group 0 to 3.8145e-06.
EarlyStopper: stopping at epoch 490 with best_val_loss = 0.000117

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005168429761463177
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.1616322406069123e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.058757 	Lval: 0.020063
Epoch: 5 	Ltrain: 0.007746 	Lval: 0.007853
Epoch: 10 	Ltrain: 0.005369 	Lval: 0.005100
Epoch: 15 	Ltrain: 0.004769 	Lval: 0.004707
Epoch: 20 	Ltrain: 0.004440 	Lval: 0.004751
Epoch: 25 	Ltrain: 0.004021 	Lval: 0.004220
Epoch 00027: reducing learning rate of group 0 to 5.1684e-04.
Epoch: 30 	Ltrain: 0.003651 	Lval: 0.003604
Epoch: 35 	Ltrain: 0.003566 	Lval: 0.003544
Epoch: 40 	Ltrain: 0.003567 	Lval: 0.003524
Epoch: 45 	Ltrain: 0.003624 	Lval: 0.003458
Epoch: 50 	Ltrain: 0.003446 	Lval: 0.003428
Epoch: 55 	Ltrain: 0.003587 	Lval: 0.003392
Epoch: 60 	Ltrain: 0.003334 	Lval: 0.003323
Epoch: 65 	Ltrain: 0.003481 	Lval: 0.003299
Epoch: 70 	Ltrain: 0.003211 	Lval: 0.003204
Epoch: 75 	Ltrain: 0.003348 	Lval: 0.003200
Epoch: 80 	Ltrain: 0.003105 	Lval: 0.003189
Epoch: 85 	Ltrain: 0.003143 	Lval: 0.003057
Epoch: 90 	Ltrain: 0.002917 	Lval: 0.002974
Epoch: 95 	Ltrain: 0.002917 	Lval: 0.002902
Epoch: 100 	Ltrain: 0.003020 	Lval: 0.002874
Epoch: 105 	Ltrain: 0.002846 	Lval: 0.002805
Epoch: 110 	Ltrain: 0.002762 	Lval: 0.002715
Epoch: 115 	Ltrain: 0.002867 	Lval: 0.002757
Epoch: 120 	Ltrain: 0.002694 	Lval: 0.002565
Epoch: 125 	Ltrain: 0.002575 	Lval: 0.002574
Epoch: 130 	Ltrain: 0.002588 	Lval: 0.002561
Epoch: 135 	Ltrain: 0.002465 	Lval: 0.002389
Epoch: 140 	Ltrain: 0.002582 	Lval: 0.002418
Epoch 00141: reducing learning rate of group 0 to 5.1684e-05.
Epoch: 145 	Ltrain: 0.002271 	Lval: 0.002269
Epoch: 150 	Ltrain: 0.002211 	Lval: 0.002271
Epoch: 155 	Ltrain: 0.002205 	Lval: 0.002268
Epoch: 160 	Ltrain: 0.002157 	Lval: 0.002253
Epoch: 165 	Ltrain: 0.002204 	Lval: 0.002235
Epoch: 170 	Ltrain: 0.002208 	Lval: 0.002231
Epoch 00175: reducing learning rate of group 0 to 5.1684e-06.
Epoch: 175 	Ltrain: 0.002144 	Lval: 0.002226
Epoch: 180 	Ltrain: 0.002171 	Lval: 0.002224
Epoch: 185 	Ltrain: 0.002182 	Lval: 0.002224
Epoch 00187: reducing learning rate of group 0 to 5.1684e-07.
Epoch: 190 	Ltrain: 0.002168 	Lval: 0.002224
Epoch: 195 	Ltrain: 0.002335 	Lval: 0.002224
Epoch 00199: reducing learning rate of group 0 to 5.1684e-08.
EarlyStopper: stopping at epoch 198 with best_val_loss = 0.002225


	Fold 2/5
Epoch: 1 	Ltrain: 0.035979 	Lval: 0.010328
Epoch: 5 	Ltrain: 0.005188 	Lval: 0.004942
Epoch: 10 	Ltrain: 0.004888 	Lval: 0.004289
Epoch: 15 	Ltrain: 0.004759 	Lval: 0.004338
Epoch: 20 	Ltrain: 0.003818 	Lval: 0.003869
Epoch: 25 	Ltrain: 0.003712 	Lval: 0.003316
Epoch 00029: reducing learning rate of group 0 to 5.1684e-04.
Epoch: 30 	Ltrain: 0.003151 	Lval: 0.003034
Epoch: 35 	Ltrain: 0.002833 	Lval: 0.002974
Epoch: 40 	Ltrain: 0.002760 	Lval: 0.002791
Epoch 00041: reducing learning rate of group 0 to 5.1684e-05.
Epoch: 45 	Ltrain: 0.002657 	Lval: 0.002741
Epoch: 50 	Ltrain: 0.002642 	Lval: 0.002722
Epoch: 55 	Ltrain: 0.002668 	Lval: 0.002716
Epoch 00060: reducing learning rate of group 0 to 5.1684e-06.
Epoch: 60 	Ltrain: 0.002640 	Lval: 0.002712
Epoch: 65 	Ltrain: 0.002632 	Lval: 0.002712
Epoch: 70 	Ltrain: 0.002651 	Lval: 0.002712
Epoch 00072: reducing learning rate of group 0 to 5.1684e-07.
Epoch: 75 	Ltrain: 0.002654 	Lval: 0.002711
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.002707


	Fold 3/5
Epoch: 1 	Ltrain: 0.021711 	Lval: 0.010406
Epoch: 5 	Ltrain: 0.005608 	Lval: 0.005373
Epoch: 10 	Ltrain: 0.004325 	Lval: 0.004080
Epoch: 15 	Ltrain: 0.003817 	Lval: 0.003921
Epoch: 20 	Ltrain: 0.003285 	Lval: 0.003386
Epoch: 25 	Ltrain: 0.003547 	Lval: 0.003225
Epoch: 30 	Ltrain: 0.003030 	Lval: 0.002741
Epoch: 35 	Ltrain: 0.002530 	Lval: 0.002443
Epoch: 40 	Ltrain: 0.002367 	Lval: 0.002286
Epoch: 45 	Ltrain: 0.001952 	Lval: 0.001946
Epoch 00048: reducing learning rate of group 0 to 5.1684e-04.
Epoch: 50 	Ltrain: 0.001374 	Lval: 0.001476
Epoch: 55 	Ltrain: 0.001176 	Lval: 0.001315
Epoch: 60 	Ltrain: 0.001105 	Lval: 0.001236
Epoch: 65 	Ltrain: 0.001028 	Lval: 0.001160
Epoch: 70 	Ltrain: 0.000968 	Lval: 0.001081
Epoch: 75 	Ltrain: 0.000904 	Lval: 0.001013
Epoch: 80 	Ltrain: 0.000845 	Lval: 0.000929
Epoch: 85 	Ltrain: 0.000788 	Lval: 0.000860
Epoch: 90 	Ltrain: 0.000732 	Lval: 0.000788
Epoch: 95 	Ltrain: 0.000674 	Lval: 0.000733
Epoch: 100 	Ltrain: 0.000634 	Lval: 0.000680
Epoch: 105 	Ltrain: 0.000604 	Lval: 0.000646
Epoch: 110 	Ltrain: 0.000550 	Lval: 0.000586
Epoch: 115 	Ltrain: 0.000505 	Lval: 0.000532
Epoch: 120 	Ltrain: 0.000490 	Lval: 0.000516
Epoch: 125 	Ltrain: 0.000475 	Lval: 0.000502
Epoch: 130 	Ltrain: 0.000423 	Lval: 0.000469
Epoch: 135 	Ltrain: 0.000428 	Lval: 0.000466
Epoch: 140 	Ltrain: 0.000374 	Lval: 0.000394
Epoch: 145 	Ltrain: 0.000377 	Lval: 0.000402
Epoch 00148: reducing learning rate of group 0 to 5.1684e-05.
Epoch: 150 	Ltrain: 0.000306 	Lval: 0.000329
Epoch: 155 	Ltrain: 0.000286 	Lval: 0.000314
Epoch: 160 	Ltrain: 0.000281 	Lval: 0.000309
Epoch: 165 	Ltrain: 0.000275 	Lval: 0.000305
Epoch: 170 	Ltrain: 0.000272 	Lval: 0.000302
Epoch: 175 	Ltrain: 0.000269 	Lval: 0.000298
Epoch: 180 	Ltrain: 0.000266 	Lval: 0.000294
Epoch: 185 	Ltrain: 0.000263 	Lval: 0.000291
Epoch: 190 	Ltrain: 0.000260 	Lval: 0.000287
Epoch: 195 	Ltrain: 0.000256 	Lval: 0.000283
Epoch: 200 	Ltrain: 0.000253 	Lval: 0.000280
Epoch: 205 	Ltrain: 0.000249 	Lval: 0.000276
Epoch: 210 	Ltrain: 0.000246 	Lval: 0.000272
Epoch: 215 	Ltrain: 0.000242 	Lval: 0.000267
Epoch: 220 	Ltrain: 0.000239 	Lval: 0.000263
Epoch: 225 	Ltrain: 0.000235 	Lval: 0.000259
Epoch: 230 	Ltrain: 0.000231 	Lval: 0.000256
Epoch: 235 	Ltrain: 0.000229 	Lval: 0.000251
Epoch: 240 	Ltrain: 0.000223 	Lval: 0.000246
Epoch: 245 	Ltrain: 0.000219 	Lval: 0.000241
Epoch: 250 	Ltrain: 0.000216 	Lval: 0.000238
Epoch: 255 	Ltrain: 0.000212 	Lval: 0.000233
Epoch: 260 	Ltrain: 0.000207 	Lval: 0.000228
Epoch: 265 	Ltrain: 0.000204 	Lval: 0.000224
Epoch: 270 	Ltrain: 0.000200 	Lval: 0.000219
Epoch: 275 	Ltrain: 0.000196 	Lval: 0.000215
Epoch: 280 	Ltrain: 0.000192 	Lval: 0.000210
Epoch: 285 	Ltrain: 0.000187 	Lval: 0.000206
Epoch: 290 	Ltrain: 0.000184 	Lval: 0.000201
Epoch: 295 	Ltrain: 0.000180 	Lval: 0.000196
Epoch: 300 	Ltrain: 0.000177 	Lval: 0.000192
Epoch: 305 	Ltrain: 0.000172 	Lval: 0.000187
Epoch: 310 	Ltrain: 0.000169 	Lval: 0.000183
Epoch: 315 	Ltrain: 0.000164 	Lval: 0.000179
Epoch: 320 	Ltrain: 0.000161 	Lval: 0.000175
Epoch: 325 	Ltrain: 0.000157 	Lval: 0.000170
Epoch: 330 	Ltrain: 0.000153 	Lval: 0.000167
Epoch: 335 	Ltrain: 0.000149 	Lval: 0.000163
Epoch: 340 	Ltrain: 0.000146 	Lval: 0.000158
Epoch: 345 	Ltrain: 0.000143 	Lval: 0.000155
Epoch: 350 	Ltrain: 0.000139 	Lval: 0.000152
Epoch: 355 	Ltrain: 0.000137 	Lval: 0.000148
Epoch: 360 	Ltrain: 0.000134 	Lval: 0.000145
Epoch: 365 	Ltrain: 0.000130 	Lval: 0.000141
Epoch: 370 	Ltrain: 0.000128 	Lval: 0.000139
Epoch: 375 	Ltrain: 0.000124 	Lval: 0.000135
Epoch: 380 	Ltrain: 0.000121 	Lval: 0.000131
Epoch: 385 	Ltrain: 0.000120 	Lval: 0.000129
Epoch: 390 	Ltrain: 0.000118 	Lval: 0.000126
Epoch: 395 	Ltrain: 0.000116 	Lval: 0.000123
Epoch: 400 	Ltrain: 0.000112 	Lval: 0.000121
Epoch: 405 	Ltrain: 0.000108 	Lval: 0.000117
Epoch: 410 	Ltrain: 0.000107 	Lval: 0.000114
Epoch: 415 	Ltrain: 0.000104 	Lval: 0.000112
Epoch: 420 	Ltrain: 0.000102 	Lval: 0.000109
Epoch 00423: reducing learning rate of group 0 to 5.1684e-06.
Epoch: 425 	Ltrain: 0.000098 	Lval: 0.000106
Epoch: 430 	Ltrain: 0.000098 	Lval: 0.000106
Epoch: 435 	Ltrain: 0.000097 	Lval: 0.000105
EarlyStopper: stopping at epoch 438 with best_val_loss = 0.000109


	Fold 4/5
Epoch: 1 	Ltrain: 0.024211 	Lval: 0.007608
Epoch: 5 	Ltrain: 0.004807 	Lval: 0.004684
Epoch: 10 	Ltrain: 0.003895 	Lval: 0.003920
Epoch: 15 	Ltrain: 0.003994 	Lval: 0.003572
Epoch 00019: reducing learning rate of group 0 to 5.1684e-04.
Epoch: 20 	Ltrain: 0.003033 	Lval: 0.003050
Epoch: 25 	Ltrain: 0.002694 	Lval: 0.002946
Epoch: 30 	Ltrain: 0.002603 	Lval: 0.002823
Epoch: 35 	Ltrain: 0.002557 	Lval: 0.002719
Epoch: 40 	Ltrain: 0.002451 	Lval: 0.002675
Epoch: 45 	Ltrain: 0.002375 	Lval: 0.002547
Epoch: 50 	Ltrain: 0.002288 	Lval: 0.002609
Epoch: 55 	Ltrain: 0.002193 	Lval: 0.002417
Epoch: 60 	Ltrain: 0.002112 	Lval: 0.002358
Epoch: 65 	Ltrain: 0.002091 	Lval: 0.002279
Epoch: 70 	Ltrain: 0.001982 	Lval: 0.002206
Epoch: 75 	Ltrain: 0.001892 	Lval: 0.002059
Epoch: 80 	Ltrain: 0.001840 	Lval: 0.002021
Epoch: 85 	Ltrain: 0.001827 	Lval: 0.001985
Epoch 00087: reducing learning rate of group 0 to 5.1684e-05.
Epoch: 90 	Ltrain: 0.001572 	Lval: 0.001766
Epoch: 95 	Ltrain: 0.001551 	Lval: 0.001737
Epoch: 100 	Ltrain: 0.001542 	Lval: 0.001722
Epoch: 105 	Ltrain: 0.001522 	Lval: 0.001705
Epoch: 110 	Ltrain: 0.001506 	Lval: 0.001685
Epoch: 115 	Ltrain: 0.001499 	Lval: 0.001668
Epoch: 120 	Ltrain: 0.001479 	Lval: 0.001657
Epoch: 125 	Ltrain: 0.001473 	Lval: 0.001643
Epoch: 130 	Ltrain: 0.001456 	Lval: 0.001620
Epoch: 135 	Ltrain: 0.001444 	Lval: 0.001598
Epoch: 140 	Ltrain: 0.001433 	Lval: 0.001576
Epoch: 145 	Ltrain: 0.001417 	Lval: 0.001581
Epoch: 150 	Ltrain: 0.001415 	Lval: 0.001547
Epoch: 155 	Ltrain: 0.001390 	Lval: 0.001544
Epoch: 160 	Ltrain: 0.001380 	Lval: 0.001521
Epoch: 165 	Ltrain: 0.001371 	Lval: 0.001512
Epoch: 170 	Ltrain: 0.001354 	Lval: 0.001486
Epoch: 175 	Ltrain: 0.001338 	Lval: 0.001478
Epoch: 180 	Ltrain: 0.001317 	Lval: 0.001450
Epoch: 185 	Ltrain: 0.001305 	Lval: 0.001441
Epoch: 190 	Ltrain: 0.001294 	Lval: 0.001420
Epoch: 195 	Ltrain: 0.001281 	Lval: 0.001413
Epoch: 200 	Ltrain: 0.001275 	Lval: 0.001389
Epoch: 205 	Ltrain: 0.001250 	Lval: 0.001373
Epoch: 210 	Ltrain: 0.001237 	Lval: 0.001356
Epoch 00215: reducing learning rate of group 0 to 5.1684e-06.
Epoch: 215 	Ltrain: 0.001231 	Lval: 0.001354
Epoch: 220 	Ltrain: 0.001200 	Lval: 0.001335
Epoch: 225 	Ltrain: 0.001200 	Lval: 0.001332
Epoch: 230 	Ltrain: 0.001199 	Lval: 0.001329
Epoch: 235 	Ltrain: 0.001197 	Lval: 0.001328
Epoch: 240 	Ltrain: 0.001196 	Lval: 0.001326
Epoch: 245 	Ltrain: 0.001196 	Lval: 0.001325
Epoch: 250 	Ltrain: 0.001194 	Lval: 0.001323
Epoch: 255 	Ltrain: 0.001192 	Lval: 0.001322
EarlyStopper: stopping at epoch 255 with best_val_loss = 0.001328


	Fold 5/5
Epoch: 1 	Ltrain: 0.018541 	Lval: 0.007670
Epoch: 5 	Ltrain: 0.004513 	Lval: 0.004509
Epoch: 10 	Ltrain: 0.004017 	Lval: 0.004660
Epoch 00012: reducing learning rate of group 0 to 5.1684e-04.
Epoch: 15 	Ltrain: 0.003392 	Lval: 0.003812
Epoch: 20 	Ltrain: 0.003247 	Lval: 0.003692
Epoch: 25 	Ltrain: 0.003194 	Lval: 0.003614
Epoch 00027: reducing learning rate of group 0 to 5.1684e-05.
Epoch: 30 	Ltrain: 0.003069 	Lval: 0.003491
Epoch: 35 	Ltrain: 0.003052 	Lval: 0.003481
Epoch: 40 	Ltrain: 0.003047 	Lval: 0.003471
Epoch: 45 	Ltrain: 0.003040 	Lval: 0.003456
Epoch: 50 	Ltrain: 0.003024 	Lval: 0.003430
Epoch: 55 	Ltrain: 0.003016 	Lval: 0.003439
Epoch: 60 	Ltrain: 0.002999 	Lval: 0.003404
Epoch: 65 	Ltrain: 0.002987 	Lval: 0.003381
Epoch 00069: reducing learning rate of group 0 to 5.1684e-06.
Epoch: 70 	Ltrain: 0.002963 	Lval: 0.003378
Epoch: 75 	Ltrain: 0.002959 	Lval: 0.003374
Epoch: 80 	Ltrain: 0.002963 	Lval: 0.003370
Epoch 00085: reducing learning rate of group 0 to 5.1684e-07.
Epoch: 85 	Ltrain: 0.002958 	Lval: 0.003369
Epoch: 90 	Ltrain: 0.002953 	Lval: 0.003369
Epoch: 95 	Ltrain: 0.002959 	Lval: 0.003368
EarlyStopper: stopping at epoch 95 with best_val_loss = 0.003371

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0024806175048926117
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.9284914122770262e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.021408 	Lval: 0.014149
Epoch: 5 	Ltrain: 0.008325 	Lval: 0.007085
Epoch: 10 	Ltrain: 0.005111 	Lval: 0.004903
Epoch 00014: reducing learning rate of group 0 to 2.4806e-04.
Epoch: 15 	Ltrain: 0.005424 	Lval: 0.004764
Epoch: 20 	Ltrain: 0.004593 	Lval: 0.004465
Epoch: 25 	Ltrain: 0.004427 	Lval: 0.004416
Epoch: 30 	Ltrain: 0.004558 	Lval: 0.004373
Epoch: 35 	Ltrain: 0.004645 	Lval: 0.004298
Epoch: 40 	Ltrain: 0.004399 	Lval: 0.004269
Epoch: 45 	Ltrain: 0.004489 	Lval: 0.004176
Epoch: 50 	Ltrain: 0.004085 	Lval: 0.004094
Epoch: 55 	Ltrain: 0.003975 	Lval: 0.003974
Epoch: 60 	Ltrain: 0.004241 	Lval: 0.003908
Epoch: 65 	Ltrain: 0.004286 	Lval: 0.003845
Epoch: 70 	Ltrain: 0.003893 	Lval: 0.003787
Epoch 00074: reducing learning rate of group 0 to 2.4806e-05.
Epoch: 75 	Ltrain: 0.003830 	Lval: 0.003766
Epoch: 80 	Ltrain: 0.003942 	Lval: 0.003726
Epoch: 85 	Ltrain: 0.003733 	Lval: 0.003717
Epoch 00089: reducing learning rate of group 0 to 2.4806e-06.
Epoch: 90 	Ltrain: 0.003977 	Lval: 0.003719
Epoch: 95 	Ltrain: 0.004090 	Lval: 0.003717
Epoch: 100 	Ltrain: 0.003682 	Lval: 0.003719
Epoch 00101: reducing learning rate of group 0 to 2.4806e-07.
Epoch: 105 	Ltrain: 0.003702 	Lval: 0.003719
EarlyStopper: stopping at epoch 106 with best_val_loss = 0.003718


	Fold 2/5
Epoch: 1 	Ltrain: 0.019867 	Lval: 0.015343
Epoch: 5 	Ltrain: 0.005994 	Lval: 0.006137
Epoch: 10 	Ltrain: 0.004769 	Lval: 0.004508
Epoch: 15 	Ltrain: 0.004444 	Lval: 0.004435
Epoch: 20 	Ltrain: 0.004434 	Lval: 0.003933
Epoch: 25 	Ltrain: 0.004040 	Lval: 0.003722
Epoch: 30 	Ltrain: 0.004379 	Lval: 0.004153
Epoch 00031: reducing learning rate of group 0 to 2.4806e-04.
Epoch: 35 	Ltrain: 0.003247 	Lval: 0.003316
Epoch: 40 	Ltrain: 0.003207 	Lval: 0.003162
Epoch: 45 	Ltrain: 0.003085 	Lval: 0.003113
Epoch 00050: reducing learning rate of group 0 to 2.4806e-05.
Epoch: 50 	Ltrain: 0.003073 	Lval: 0.003045
Epoch: 55 	Ltrain: 0.002952 	Lval: 0.003026
Epoch: 60 	Ltrain: 0.003011 	Lval: 0.003021
Epoch: 65 	Ltrain: 0.002961 	Lval: 0.003027
Epoch: 70 	Ltrain: 0.002895 	Lval: 0.003015
Epoch: 75 	Ltrain: 0.002927 	Lval: 0.003008
Epoch: 80 	Ltrain: 0.002964 	Lval: 0.003013
Epoch 00082: reducing learning rate of group 0 to 2.4806e-06.
Epoch: 85 	Ltrain: 0.002943 	Lval: 0.003002
Epoch: 90 	Ltrain: 0.002875 	Lval: 0.003000
Epoch: 95 	Ltrain: 0.002972 	Lval: 0.002997
Epoch: 100 	Ltrain: 0.002899 	Lval: 0.002999
Epoch 00101: reducing learning rate of group 0 to 2.4806e-07.
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.002998


	Fold 3/5
Epoch: 1 	Ltrain: 0.020503 	Lval: 0.010822
Epoch: 5 	Ltrain: 0.005400 	Lval: 0.004656
Epoch: 10 	Ltrain: 0.004533 	Lval: 0.004316
Epoch: 15 	Ltrain: 0.004195 	Lval: 0.004103
Epoch: 20 	Ltrain: 0.003852 	Lval: 0.003776
Epoch: 25 	Ltrain: 0.003783 	Lval: 0.003484
Epoch: 30 	Ltrain: 0.003641 	Lval: 0.003732
Epoch: 35 	Ltrain: 0.003354 	Lval: 0.002989
Epoch: 40 	Ltrain: 0.002737 	Lval: 0.003048
Epoch: 45 	Ltrain: 0.002873 	Lval: 0.003287
Epoch: 50 	Ltrain: 0.002354 	Lval: 0.002492
Epoch: 55 	Ltrain: 0.002189 	Lval: 0.002242
Epoch: 60 	Ltrain: 0.001970 	Lval: 0.002057
Epoch: 65 	Ltrain: 0.001707 	Lval: 0.001892
Epoch: 70 	Ltrain: 0.001560 	Lval: 0.001604
Epoch: 75 	Ltrain: 0.001318 	Lval: 0.001455
Epoch: 80 	Ltrain: 0.001091 	Lval: 0.001070
Epoch: 85 	Ltrain: 0.000951 	Lval: 0.000956
Epoch: 90 	Ltrain: 0.000847 	Lval: 0.000864
Epoch: 95 	Ltrain: 0.000734 	Lval: 0.000732
Epoch 00097: reducing learning rate of group 0 to 2.4806e-04.
Epoch: 100 	Ltrain: 0.000413 	Lval: 0.000439
Epoch: 105 	Ltrain: 0.000344 	Lval: 0.000383
Epoch: 110 	Ltrain: 0.000315 	Lval: 0.000353
Epoch: 115 	Ltrain: 0.000296 	Lval: 0.000329
Epoch: 120 	Ltrain: 0.000278 	Lval: 0.000311
Epoch: 125 	Ltrain: 0.000262 	Lval: 0.000292
Epoch: 130 	Ltrain: 0.000248 	Lval: 0.000275
Epoch: 135 	Ltrain: 0.000233 	Lval: 0.000257
Epoch: 140 	Ltrain: 0.000219 	Lval: 0.000241
Epoch: 145 	Ltrain: 0.000205 	Lval: 0.000226
Epoch: 150 	Ltrain: 0.000192 	Lval: 0.000211
Epoch: 155 	Ltrain: 0.000179 	Lval: 0.000196
Epoch: 160 	Ltrain: 0.000167 	Lval: 0.000181
Epoch: 165 	Ltrain: 0.000155 	Lval: 0.000167
Epoch: 170 	Ltrain: 0.000144 	Lval: 0.000154
Epoch: 175 	Ltrain: 0.000135 	Lval: 0.000144
Epoch: 180 	Ltrain: 0.000129 	Lval: 0.000139
Epoch: 185 	Ltrain: 0.000118 	Lval: 0.000123
Epoch: 190 	Ltrain: 0.000107 	Lval: 0.000112
Epoch: 195 	Ltrain: 0.000099 	Lval: 0.000101
Epoch: 200 	Ltrain: 0.000091 	Lval: 0.000093
Epoch: 205 	Ltrain: 0.000084 	Lval: 0.000086
Epoch: 210 	Ltrain: 0.000077 	Lval: 0.000077
Epoch: 215 	Ltrain: 0.000072 	Lval: 0.000071
Epoch: 220 	Ltrain: 0.000070 	Lval: 0.000068
Epoch: 225 	Ltrain: 0.000075 	Lval: 0.000077
Epoch 00227: reducing learning rate of group 0 to 2.4806e-05.
Epoch: 230 	Ltrain: 0.000060 	Lval: 0.000057
Epoch: 235 	Ltrain: 0.000055 	Lval: 0.000054
Epoch: 240 	Ltrain: 0.000055 	Lval: 0.000053
Epoch: 245 	Ltrain: 0.000054 	Lval: 0.000052
Epoch: 250 	Ltrain: 0.000053 	Lval: 0.000051
Epoch: 255 	Ltrain: 0.000053 	Lval: 0.000050
EarlyStopper: stopping at epoch 255 with best_val_loss = 0.000055


	Fold 4/5
Epoch: 1 	Ltrain: 0.015083 	Lval: 0.009594
Epoch: 5 	Ltrain: 0.005214 	Lval: 0.004743
Epoch: 10 	Ltrain: 0.004314 	Lval: 0.004478
Epoch: 15 	Ltrain: 0.003786 	Lval: 0.003973
Epoch: 20 	Ltrain: 0.003472 	Lval: 0.003698
Epoch: 25 	Ltrain: 0.003344 	Lval: 0.003216
Epoch: 30 	Ltrain: 0.002941 	Lval: 0.002970
Epoch: 35 	Ltrain: 0.002705 	Lval: 0.003397
Epoch: 40 	Ltrain: 0.002554 	Lval: 0.002537
Epoch 00044: reducing learning rate of group 0 to 2.4806e-04.
Epoch: 45 	Ltrain: 0.002064 	Lval: 0.002211
Epoch: 50 	Ltrain: 0.001803 	Lval: 0.002091
Epoch: 55 	Ltrain: 0.001721 	Lval: 0.001986
Epoch: 60 	Ltrain: 0.001656 	Lval: 0.001905
Epoch: 65 	Ltrain: 0.001591 	Lval: 0.001827
Epoch: 70 	Ltrain: 0.001540 	Lval: 0.001762
Epoch: 75 	Ltrain: 0.001455 	Lval: 0.001691
Epoch: 80 	Ltrain: 0.001400 	Lval: 0.001598
Epoch: 85 	Ltrain: 0.001305 	Lval: 0.001515
Epoch: 90 	Ltrain: 0.001270 	Lval: 0.001419
Epoch: 95 	Ltrain: 0.001160 	Lval: 0.001336
Epoch: 100 	Ltrain: 0.001095 	Lval: 0.001224
Epoch: 105 	Ltrain: 0.001045 	Lval: 0.001151
Epoch: 110 	Ltrain: 0.000986 	Lval: 0.001092
Epoch: 115 	Ltrain: 0.000901 	Lval: 0.001003
Epoch: 120 	Ltrain: 0.000853 	Lval: 0.000941
Epoch: 125 	Ltrain: 0.000805 	Lval: 0.000904
Epoch: 130 	Ltrain: 0.000760 	Lval: 0.000871
Epoch: 135 	Ltrain: 0.000714 	Lval: 0.000794
Epoch: 140 	Ltrain: 0.000677 	Lval: 0.000733
Epoch 00145: reducing learning rate of group 0 to 2.4806e-05.
Epoch: 145 	Ltrain: 0.000659 	Lval: 0.000724
Epoch: 150 	Ltrain: 0.000560 	Lval: 0.000636
Epoch: 155 	Ltrain: 0.000552 	Lval: 0.000626
Epoch: 160 	Ltrain: 0.000547 	Lval: 0.000620
Epoch: 165 	Ltrain: 0.000541 	Lval: 0.000614
Epoch: 170 	Ltrain: 0.000536 	Lval: 0.000607
Epoch: 175 	Ltrain: 0.000532 	Lval: 0.000601
Epoch: 180 	Ltrain: 0.000525 	Lval: 0.000597
Epoch: 185 	Ltrain: 0.000520 	Lval: 0.000591
Epoch: 190 	Ltrain: 0.000515 	Lval: 0.000583
Epoch: 195 	Ltrain: 0.000511 	Lval: 0.000577
Epoch: 200 	Ltrain: 0.000505 	Lval: 0.000571
Epoch: 205 	Ltrain: 0.000501 	Lval: 0.000564
Epoch: 210 	Ltrain: 0.000494 	Lval: 0.000558
Epoch: 215 	Ltrain: 0.000488 	Lval: 0.000552
Epoch: 220 	Ltrain: 0.000483 	Lval: 0.000546
Epoch: 225 	Ltrain: 0.000478 	Lval: 0.000540
Epoch: 230 	Ltrain: 0.000471 	Lval: 0.000533
Epoch: 235 	Ltrain: 0.000466 	Lval: 0.000527
Epoch: 240 	Ltrain: 0.000461 	Lval: 0.000520
Epoch: 245 	Ltrain: 0.000455 	Lval: 0.000514
Epoch: 250 	Ltrain: 0.000451 	Lval: 0.000508
Epoch: 255 	Ltrain: 0.000443 	Lval: 0.000501
Epoch: 260 	Ltrain: 0.000439 	Lval: 0.000496
Epoch: 265 	Ltrain: 0.000433 	Lval: 0.000489
Epoch: 270 	Ltrain: 0.000428 	Lval: 0.000483
Epoch: 275 	Ltrain: 0.000422 	Lval: 0.000477
Epoch: 280 	Ltrain: 0.000418 	Lval: 0.000471
Epoch: 285 	Ltrain: 0.000412 	Lval: 0.000465
Epoch: 290 	Ltrain: 0.000407 	Lval: 0.000459
Epoch: 295 	Ltrain: 0.000402 	Lval: 0.000455
Epoch: 300 	Ltrain: 0.000396 	Lval: 0.000446
Epoch: 305 	Ltrain: 0.000391 	Lval: 0.000440
Epoch: 310 	Ltrain: 0.000386 	Lval: 0.000435
Epoch: 315 	Ltrain: 0.000381 	Lval: 0.000431
Epoch: 320 	Ltrain: 0.000376 	Lval: 0.000423
Epoch: 325 	Ltrain: 0.000372 	Lval: 0.000419
Epoch: 330 	Ltrain: 0.000366 	Lval: 0.000413
Epoch: 335 	Ltrain: 0.000363 	Lval: 0.000407
Epoch: 340 	Ltrain: 0.000357 	Lval: 0.000403
Epoch: 345 	Ltrain: 0.000353 	Lval: 0.000396
Epoch: 350 	Ltrain: 0.000346 	Lval: 0.000390
Epoch: 355 	Ltrain: 0.000342 	Lval: 0.000385
Epoch: 360 	Ltrain: 0.000338 	Lval: 0.000381
Epoch: 365 	Ltrain: 0.000333 	Lval: 0.000375
Epoch: 370 	Ltrain: 0.000329 	Lval: 0.000371
Epoch: 375 	Ltrain: 0.000325 	Lval: 0.000364
Epoch: 380 	Ltrain: 0.000322 	Lval: 0.000361
Epoch: 385 	Ltrain: 0.000316 	Lval: 0.000355
Epoch: 390 	Ltrain: 0.000313 	Lval: 0.000350
Epoch: 395 	Ltrain: 0.000307 	Lval: 0.000345
Epoch: 400 	Ltrain: 0.000303 	Lval: 0.000339
Epoch: 405 	Ltrain: 0.000299 	Lval: 0.000335
Epoch: 410 	Ltrain: 0.000295 	Lval: 0.000330
Epoch: 415 	Ltrain: 0.000291 	Lval: 0.000325
Epoch: 420 	Ltrain: 0.000287 	Lval: 0.000321
Epoch: 425 	Ltrain: 0.000284 	Lval: 0.000316
Epoch: 430 	Ltrain: 0.000279 	Lval: 0.000311
Epoch: 435 	Ltrain: 0.000275 	Lval: 0.000307
Epoch: 440 	Ltrain: 0.000273 	Lval: 0.000303
Epoch: 445 	Ltrain: 0.000268 	Lval: 0.000297
Epoch: 450 	Ltrain: 0.000264 	Lval: 0.000294
Epoch: 455 	Ltrain: 0.000262 	Lval: 0.000291
Epoch: 460 	Ltrain: 0.000256 	Lval: 0.000286
Epoch: 465 	Ltrain: 0.000253 	Lval: 0.000281
Epoch: 470 	Ltrain: 0.000250 	Lval: 0.000277
Epoch: 475 	Ltrain: 0.000246 	Lval: 0.000273
Epoch: 480 	Ltrain: 0.000244 	Lval: 0.000270
Epoch: 485 	Ltrain: 0.000239 	Lval: 0.000265
Epoch: 490 	Ltrain: 0.000237 	Lval: 0.000262
Epoch: 495 	Ltrain: 0.000233 	Lval: 0.000258
Epoch: 500 	Ltrain: 0.000230 	Lval: 0.000255
Epoch 00502: reducing learning rate of group 0 to 2.4806e-06.
Epoch: 505 	Ltrain: 0.000225 	Lval: 0.000251
Epoch: 510 	Ltrain: 0.000225 	Lval: 0.000250
Epoch: 515 	Ltrain: 0.000224 	Lval: 0.000250
EarlyStopper: stopping at epoch 515 with best_val_loss = 0.000259


	Fold 5/5
Epoch: 1 	Ltrain: 0.015355 	Lval: 0.009893
Epoch: 5 	Ltrain: 0.005028 	Lval: 0.005295
Epoch: 10 	Ltrain: 0.004582 	Lval: 0.004267
Epoch: 15 	Ltrain: 0.003783 	Lval: 0.004014
Epoch 00020: reducing learning rate of group 0 to 2.4806e-04.
Epoch: 20 	Ltrain: 0.003690 	Lval: 0.004170
Epoch: 25 	Ltrain: 0.002931 	Lval: 0.003357
Epoch: 30 	Ltrain: 0.002879 	Lval: 0.003215
Epoch: 35 	Ltrain: 0.002803 	Lval: 0.003108
Epoch: 40 	Ltrain: 0.002778 	Lval: 0.003193
Epoch: 45 	Ltrain: 0.002696 	Lval: 0.003004
Epoch: 50 	Ltrain: 0.002613 	Lval: 0.002899
Epoch: 55 	Ltrain: 0.002607 	Lval: 0.002816
Epoch: 60 	Ltrain: 0.002501 	Lval: 0.002804
Epoch: 65 	Ltrain: 0.002481 	Lval: 0.002680
Epoch: 70 	Ltrain: 0.002377 	Lval: 0.002644
Epoch 00073: reducing learning rate of group 0 to 2.4806e-05.
Epoch: 75 	Ltrain: 0.002225 	Lval: 0.002544
Epoch: 80 	Ltrain: 0.002213 	Lval: 0.002531
Epoch: 85 	Ltrain: 0.002209 	Lval: 0.002528
Epoch: 90 	Ltrain: 0.002199 	Lval: 0.002515
Epoch: 95 	Ltrain: 0.002182 	Lval: 0.002507
Epoch: 100 	Ltrain: 0.002179 	Lval: 0.002499
Epoch: 105 	Ltrain: 0.002170 	Lval: 0.002498
Epoch: 110 	Ltrain: 0.002163 	Lval: 0.002478
Epoch: 115 	Ltrain: 0.002155 	Lval: 0.002478
Epoch: 120 	Ltrain: 0.002144 	Lval: 0.002475
Epoch: 125 	Ltrain: 0.002139 	Lval: 0.002463
Epoch 00126: reducing learning rate of group 0 to 2.4806e-06.
Epoch: 130 	Ltrain: 0.002116 	Lval: 0.002454
Epoch: 135 	Ltrain: 0.002111 	Lval: 0.002450
Epoch: 140 	Ltrain: 0.002114 	Lval: 0.002450
Epoch: 145 	Ltrain: 0.002114 	Lval: 0.002448
EarlyStopper: stopping at epoch 145 with best_val_loss = 0.002457

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006387935330538922
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 7.374944574848426e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.092829 	Lval: 0.017976
Epoch: 5 	Ltrain: 0.011209 	Lval: 0.008102
Epoch: 10 	Ltrain: 0.005680 	Lval: 0.005750
Epoch: 15 	Ltrain: 0.004820 	Lval: 0.004954
Epoch 00018: reducing learning rate of group 0 to 6.3879e-04.
Epoch: 20 	Ltrain: 0.004729 	Lval: 0.004309
Epoch: 25 	Ltrain: 0.004298 	Lval: 0.004241
Epoch: 30 	Ltrain: 0.004275 	Lval: 0.004189
Epoch: 35 	Ltrain: 0.004253 	Lval: 0.004141
Epoch: 40 	Ltrain: 0.004229 	Lval: 0.004087
Epoch: 45 	Ltrain: 0.004153 	Lval: 0.004030
Epoch 00049: reducing learning rate of group 0 to 6.3879e-05.
Epoch: 50 	Ltrain: 0.004486 	Lval: 0.004018
Epoch: 55 	Ltrain: 0.004061 	Lval: 0.003977
Epoch: 60 	Ltrain: 0.003943 	Lval: 0.003973
Epoch: 65 	Ltrain: 0.004366 	Lval: 0.003969
Epoch: 70 	Ltrain: 0.004351 	Lval: 0.003968
Epoch: 75 	Ltrain: 0.004139 	Lval: 0.003953
Epoch 00080: reducing learning rate of group 0 to 6.3879e-06.
Epoch: 80 	Ltrain: 0.003920 	Lval: 0.003962
Epoch: 85 	Ltrain: 0.003957 	Lval: 0.003953
Epoch: 90 	Ltrain: 0.003875 	Lval: 0.003950
Epoch 00093: reducing learning rate of group 0 to 6.3879e-07.
Epoch: 95 	Ltrain: 0.003920 	Lval: 0.003950
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.003958


	Fold 2/5
Epoch: 1 	Ltrain: 0.061496 	Lval: 0.016916
Epoch: 5 	Ltrain: 0.007436 	Lval: 0.005781
Epoch: 10 	Ltrain: 0.005173 	Lval: 0.004695
Epoch: 15 	Ltrain: 0.005115 	Lval: 0.004684
Epoch: 20 	Ltrain: 0.004460 	Lval: 0.003999
Epoch: 25 	Ltrain: 0.003747 	Lval: 0.004003
Epoch: 30 	Ltrain: 0.003387 	Lval: 0.003191
Epoch: 35 	Ltrain: 0.003293 	Lval: 0.003338
Epoch: 40 	Ltrain: 0.002921 	Lval: 0.003048
Epoch: 45 	Ltrain: 0.002625 	Lval: 0.003082
Epoch 00048: reducing learning rate of group 0 to 6.3879e-04.
Epoch: 50 	Ltrain: 0.002035 	Lval: 0.002032
Epoch: 55 	Ltrain: 0.001800 	Lval: 0.001860
Epoch: 60 	Ltrain: 0.001733 	Lval: 0.001804
Epoch: 65 	Ltrain: 0.001628 	Lval: 0.001692
Epoch: 70 	Ltrain: 0.001560 	Lval: 0.001625
Epoch: 75 	Ltrain: 0.001502 	Lval: 0.001584
Epoch: 80 	Ltrain: 0.001416 	Lval: 0.001501
Epoch: 85 	Ltrain: 0.001334 	Lval: 0.001393
Epoch: 90 	Ltrain: 0.001247 	Lval: 0.001326
Epoch: 95 	Ltrain: 0.001172 	Lval: 0.001239
Epoch: 100 	Ltrain: 0.001080 	Lval: 0.001159
Epoch: 105 	Ltrain: 0.001023 	Lval: 0.001106
Epoch: 110 	Ltrain: 0.000945 	Lval: 0.001014
Epoch: 115 	Ltrain: 0.000896 	Lval: 0.000961
Epoch: 120 	Ltrain: 0.000805 	Lval: 0.000877
Epoch: 125 	Ltrain: 0.000776 	Lval: 0.000816
Epoch: 130 	Ltrain: 0.000713 	Lval: 0.000760
Epoch 00134: reducing learning rate of group 0 to 6.3879e-05.
Epoch: 135 	Ltrain: 0.000683 	Lval: 0.000723
Epoch: 140 	Ltrain: 0.000605 	Lval: 0.000676
Epoch: 145 	Ltrain: 0.000593 	Lval: 0.000664
Epoch: 150 	Ltrain: 0.000582 	Lval: 0.000656
Epoch: 155 	Ltrain: 0.000580 	Lval: 0.000647
Epoch: 160 	Ltrain: 0.000565 	Lval: 0.000639
Epoch: 165 	Ltrain: 0.000560 	Lval: 0.000633
Epoch: 170 	Ltrain: 0.000566 	Lval: 0.000625
Epoch: 175 	Ltrain: 0.000555 	Lval: 0.000617
Epoch: 180 	Ltrain: 0.000542 	Lval: 0.000609
Epoch: 185 	Ltrain: 0.000538 	Lval: 0.000602
Epoch: 190 	Ltrain: 0.000531 	Lval: 0.000594
Epoch: 195 	Ltrain: 0.000516 	Lval: 0.000586
Epoch: 200 	Ltrain: 0.000511 	Lval: 0.000578
Epoch: 205 	Ltrain: 0.000505 	Lval: 0.000570
Epoch: 210 	Ltrain: 0.000494 	Lval: 0.000563
Epoch: 215 	Ltrain: 0.000488 	Lval: 0.000555
Epoch: 220 	Ltrain: 0.000479 	Lval: 0.000545
Epoch: 225 	Ltrain: 0.000478 	Lval: 0.000537
Epoch: 230 	Ltrain: 0.000466 	Lval: 0.000529
Epoch: 235 	Ltrain: 0.000464 	Lval: 0.000520
Epoch: 240 	Ltrain: 0.000456 	Lval: 0.000511
Epoch: 245 	Ltrain: 0.000442 	Lval: 0.000503
Epoch: 250 	Ltrain: 0.000444 	Lval: 0.000494
Epoch: 255 	Ltrain: 0.000434 	Lval: 0.000486
Epoch: 260 	Ltrain: 0.000425 	Lval: 0.000477
Epoch: 265 	Ltrain: 0.000414 	Lval: 0.000469
Epoch: 270 	Ltrain: 0.000406 	Lval: 0.000458
Epoch: 275 	Ltrain: 0.000397 	Lval: 0.000448
Epoch: 280 	Ltrain: 0.000393 	Lval: 0.000439
Epoch: 285 	Ltrain: 0.000383 	Lval: 0.000429
Epoch: 290 	Ltrain: 0.000376 	Lval: 0.000421
Epoch: 295 	Ltrain: 0.000366 	Lval: 0.000412
Epoch: 300 	Ltrain: 0.000363 	Lval: 0.000403
Epoch: 305 	Ltrain: 0.000353 	Lval: 0.000394
Epoch: 310 	Ltrain: 0.000344 	Lval: 0.000386
Epoch: 315 	Ltrain: 0.000338 	Lval: 0.000376
Epoch: 320 	Ltrain: 0.000330 	Lval: 0.000368
Epoch: 325 	Ltrain: 0.000323 	Lval: 0.000361
Epoch: 330 	Ltrain: 0.000316 	Lval: 0.000350
Epoch: 335 	Ltrain: 0.000305 	Lval: 0.000341
Epoch: 340 	Ltrain: 0.000299 	Lval: 0.000332
Epoch: 345 	Ltrain: 0.000297 	Lval: 0.000324
Epoch: 350 	Ltrain: 0.000283 	Lval: 0.000316
Epoch: 355 	Ltrain: 0.000281 	Lval: 0.000308
Epoch: 360 	Ltrain: 0.000270 	Lval: 0.000301
Epoch: 365 	Ltrain: 0.000269 	Lval: 0.000297
Epoch: 370 	Ltrain: 0.000268 	Lval: 0.000292
Epoch: 375 	Ltrain: 0.000255 	Lval: 0.000286
Epoch: 380 	Ltrain: 0.000252 	Lval: 0.000282
Epoch 00383: reducing learning rate of group 0 to 6.3879e-06.
Epoch: 385 	Ltrain: 0.000244 	Lval: 0.000268
Epoch: 390 	Ltrain: 0.000239 	Lval: 0.000266
Epoch: 395 	Ltrain: 0.000236 	Lval: 0.000265
Epoch: 400 	Ltrain: 0.000237 	Lval: 0.000264
Epoch: 405 	Ltrain: 0.000234 	Lval: 0.000263
EarlyStopper: stopping at epoch 404 with best_val_loss = 0.000271


	Fold 3/5
Epoch: 1 	Ltrain: 0.084767 	Lval: 0.011907
Epoch: 5 	Ltrain: 0.006018 	Lval: 0.004813
Epoch: 10 	Ltrain: 0.004770 	Lval: 0.004522
Epoch: 15 	Ltrain: 0.004321 	Lval: 0.003889
Epoch 00020: reducing learning rate of group 0 to 6.3879e-04.
Epoch: 20 	Ltrain: 0.004535 	Lval: 0.003918
Epoch: 25 	Ltrain: 0.003434 	Lval: 0.003571
Epoch: 30 	Ltrain: 0.003401 	Lval: 0.003452
Epoch: 35 	Ltrain: 0.003269 	Lval: 0.003479
Epoch: 40 	Ltrain: 0.003249 	Lval: 0.003362
Epoch: 45 	Ltrain: 0.003180 	Lval: 0.003382
Epoch: 50 	Ltrain: 0.003109 	Lval: 0.003209
Epoch: 55 	Ltrain: 0.003005 	Lval: 0.003185
Epoch: 60 	Ltrain: 0.002971 	Lval: 0.003063
Epoch: 65 	Ltrain: 0.002889 	Lval: 0.002995
Epoch: 70 	Ltrain: 0.002861 	Lval: 0.002949
Epoch: 75 	Ltrain: 0.002755 	Lval: 0.002833
Epoch 00080: reducing learning rate of group 0 to 6.3879e-05.
Epoch: 80 	Ltrain: 0.002729 	Lval: 0.002919
Epoch: 85 	Ltrain: 0.002544 	Lval: 0.002735
Epoch: 90 	Ltrain: 0.002532 	Lval: 0.002708
Epoch: 95 	Ltrain: 0.002501 	Lval: 0.002700
Epoch: 100 	Ltrain: 0.002509 	Lval: 0.002663
Epoch: 105 	Ltrain: 0.002500 	Lval: 0.002666
Epoch: 110 	Ltrain: 0.002468 	Lval: 0.002659
Epoch 00113: reducing learning rate of group 0 to 6.3879e-06.
Epoch: 115 	Ltrain: 0.002448 	Lval: 0.002641
Epoch: 120 	Ltrain: 0.002452 	Lval: 0.002636
Epoch 00125: reducing learning rate of group 0 to 6.3879e-07.
Epoch: 125 	Ltrain: 0.002446 	Lval: 0.002636
Epoch: 130 	Ltrain: 0.002447 	Lval: 0.002635
EarlyStopper: stopping at epoch 129 with best_val_loss = 0.002641


	Fold 4/5
Epoch: 1 	Ltrain: 0.039909 	Lval: 0.012890
Epoch: 5 	Ltrain: 0.004796 	Lval: 0.005754
Epoch: 10 	Ltrain: 0.004384 	Lval: 0.004061
Epoch: 15 	Ltrain: 0.003877 	Lval: 0.003997
Epoch 00017: reducing learning rate of group 0 to 6.3879e-04.
Epoch: 20 	Ltrain: 0.002993 	Lval: 0.003272
Epoch: 25 	Ltrain: 0.002902 	Lval: 0.003166
Epoch: 30 	Ltrain: 0.002831 	Lval: 0.003070
Epoch: 35 	Ltrain: 0.002789 	Lval: 0.003032
Epoch 00040: reducing learning rate of group 0 to 6.3879e-05.
Epoch: 40 	Ltrain: 0.002743 	Lval: 0.003067
Epoch: 45 	Ltrain: 0.002598 	Lval: 0.002888
Epoch: 50 	Ltrain: 0.002591 	Lval: 0.002871
Epoch 00055: reducing learning rate of group 0 to 6.3879e-06.
Epoch: 55 	Ltrain: 0.002579 	Lval: 0.002873
Epoch: 60 	Ltrain: 0.002566 	Lval: 0.002862
Epoch: 65 	Ltrain: 0.002568 	Lval: 0.002860
Epoch: 70 	Ltrain: 0.002560 	Lval: 0.002860
Epoch 00072: reducing learning rate of group 0 to 6.3879e-07.
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.002865


	Fold 5/5
Epoch: 1 	Ltrain: 0.037052 	Lval: 0.013042
Epoch: 5 	Ltrain: 0.004947 	Lval: 0.005342
Epoch: 10 	Ltrain: 0.004603 	Lval: 0.004977
Epoch: 15 	Ltrain: 0.003650 	Lval: 0.004152
Epoch: 20 	Ltrain: 0.003297 	Lval: 0.003680
Epoch: 25 	Ltrain: 0.002939 	Lval: 0.002999
Epoch 00029: reducing learning rate of group 0 to 6.3879e-04.
Epoch: 30 	Ltrain: 0.002410 	Lval: 0.002621
Epoch: 35 	Ltrain: 0.002142 	Lval: 0.002459
Epoch: 40 	Ltrain: 0.002064 	Lval: 0.002364
Epoch: 45 	Ltrain: 0.001968 	Lval: 0.002215
Epoch: 50 	Ltrain: 0.001876 	Lval: 0.002088
Epoch: 55 	Ltrain: 0.001756 	Lval: 0.001940
Epoch: 60 	Ltrain: 0.001658 	Lval: 0.001848
Epoch: 65 	Ltrain: 0.001533 	Lval: 0.001704
Epoch: 70 	Ltrain: 0.001429 	Lval: 0.001607
Epoch: 75 	Ltrain: 0.001331 	Lval: 0.001473
Epoch: 80 	Ltrain: 0.001263 	Lval: 0.001372
Epoch: 85 	Ltrain: 0.001155 	Lval: 0.001280
Epoch: 90 	Ltrain: 0.001054 	Lval: 0.001170
Epoch: 95 	Ltrain: 0.001074 	Lval: 0.001178
Epoch: 100 	Ltrain: 0.000932 	Lval: 0.000999
Epoch: 105 	Ltrain: 0.000835 	Lval: 0.000911
Epoch: 110 	Ltrain: 0.000787 	Lval: 0.000861
Epoch: 115 	Ltrain: 0.000820 	Lval: 0.000850
Epoch: 120 	Ltrain: 0.000699 	Lval: 0.000745
Epoch: 125 	Ltrain: 0.000675 	Lval: 0.000720
Epoch 00126: reducing learning rate of group 0 to 6.3879e-05.
Epoch: 130 	Ltrain: 0.000500 	Lval: 0.000547
Epoch: 135 	Ltrain: 0.000478 	Lval: 0.000525
Epoch: 140 	Ltrain: 0.000464 	Lval: 0.000510
Epoch: 145 	Ltrain: 0.000455 	Lval: 0.000500
Epoch: 150 	Ltrain: 0.000446 	Lval: 0.000490
Epoch: 155 	Ltrain: 0.000438 	Lval: 0.000478
Epoch: 160 	Ltrain: 0.000429 	Lval: 0.000469
Epoch: 165 	Ltrain: 0.000422 	Lval: 0.000459
Epoch: 170 	Ltrain: 0.000413 	Lval: 0.000448
Epoch: 175 	Ltrain: 0.000403 	Lval: 0.000438
Epoch: 180 	Ltrain: 0.000395 	Lval: 0.000429
Epoch: 185 	Ltrain: 0.000386 	Lval: 0.000419
Epoch: 190 	Ltrain: 0.000376 	Lval: 0.000408
Epoch: 195 	Ltrain: 0.000369 	Lval: 0.000397
Epoch: 200 	Ltrain: 0.000359 	Lval: 0.000389
Epoch: 205 	Ltrain: 0.000350 	Lval: 0.000378
Epoch: 210 	Ltrain: 0.000340 	Lval: 0.000368
Epoch: 215 	Ltrain: 0.000331 	Lval: 0.000357
Epoch: 220 	Ltrain: 0.000322 	Lval: 0.000347
Epoch: 225 	Ltrain: 0.000313 	Lval: 0.000336
Epoch: 230 	Ltrain: 0.000303 	Lval: 0.000327
Epoch: 235 	Ltrain: 0.000295 	Lval: 0.000316
Epoch: 240 	Ltrain: 0.000288 	Lval: 0.000307
Epoch: 245 	Ltrain: 0.000277 	Lval: 0.000296
Epoch: 250 	Ltrain: 0.000269 	Lval: 0.000291
Epoch: 255 	Ltrain: 0.000261 	Lval: 0.000282
Epoch: 260 	Ltrain: 0.000254 	Lval: 0.000272
Epoch: 265 	Ltrain: 0.000246 	Lval: 0.000263
Epoch: 270 	Ltrain: 0.000238 	Lval: 0.000254
Epoch: 275 	Ltrain: 0.000231 	Lval: 0.000247
Epoch: 280 	Ltrain: 0.000225 	Lval: 0.000245
Epoch: 285 	Ltrain: 0.000222 	Lval: 0.000236
Epoch: 290 	Ltrain: 0.000210 	Lval: 0.000226
Epoch: 295 	Ltrain: 0.000204 	Lval: 0.000220
Epoch: 300 	Ltrain: 0.000199 	Lval: 0.000213
Epoch: 305 	Ltrain: 0.000196 	Lval: 0.000212
Epoch 00307: reducing learning rate of group 0 to 6.3879e-06.
Epoch: 310 	Ltrain: 0.000184 	Lval: 0.000199
Epoch: 315 	Ltrain: 0.000182 	Lval: 0.000196
Epoch: 320 	Ltrain: 0.000181 	Lval: 0.000195
Epoch: 325 	Ltrain: 0.000180 	Lval: 0.000194
EarlyStopper: stopping at epoch 328 with best_val_loss = 0.000203

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004080840175108124
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.997975976401953e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 17
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.036148 	Lval: 0.020167
Epoch: 5 	Ltrain: 0.007024 	Lval: 0.006155
Epoch: 10 	Ltrain: 0.005393 	Lval: 0.005037
Epoch 00012: reducing learning rate of group 0 to 4.0808e-04.
Epoch: 15 	Ltrain: 0.004707 	Lval: 0.004548
Epoch: 20 	Ltrain: 0.005152 	Lval: 0.004463
Epoch: 25 	Ltrain: 0.004424 	Lval: 0.004376
Epoch: 30 	Ltrain: 0.004437 	Lval: 0.004289
Epoch: 35 	Ltrain: 0.004270 	Lval: 0.004249
Epoch: 40 	Ltrain: 0.004334 	Lval: 0.004119
Epoch: 45 	Ltrain: 0.004122 	Lval: 0.004042
Epoch: 50 	Ltrain: 0.004036 	Lval: 0.003983
Epoch: 55 	Ltrain: 0.004135 	Lval: 0.003925
Epoch 00058: reducing learning rate of group 0 to 4.0808e-05.
Epoch: 60 	Ltrain: 0.004039 	Lval: 0.003871
Epoch: 65 	Ltrain: 0.004449 	Lval: 0.003861
Epoch: 70 	Ltrain: 0.003803 	Lval: 0.003861
Epoch: 75 	Ltrain: 0.003846 	Lval: 0.003850
Epoch: 80 	Ltrain: 0.003818 	Lval: 0.003845
Epoch: 85 	Ltrain: 0.003957 	Lval: 0.003842
Epoch: 90 	Ltrain: 0.003826 	Lval: 0.003830
Epoch 00095: reducing learning rate of group 0 to 4.0808e-06.
Epoch: 95 	Ltrain: 0.003696 	Lval: 0.003830
Epoch: 100 	Ltrain: 0.003749 	Lval: 0.003830
EarlyStopper: stopping at epoch 102 with best_val_loss = 0.003836


	Fold 2/5
Epoch: 1 	Ltrain: 0.023526 	Lval: 0.010977
Epoch: 5 	Ltrain: 0.005716 	Lval: 0.004964
Epoch: 10 	Ltrain: 0.004866 	Lval: 0.004404
Epoch: 15 	Ltrain: 0.004590 	Lval: 0.004117
Epoch: 20 	Ltrain: 0.004066 	Lval: 0.003947
Epoch 00025: reducing learning rate of group 0 to 4.0808e-04.
Epoch: 25 	Ltrain: 0.003974 	Lval: 0.003708
Epoch: 30 	Ltrain: 0.003283 	Lval: 0.003263
Epoch: 35 	Ltrain: 0.003139 	Lval: 0.003168
Epoch: 40 	Ltrain: 0.003029 	Lval: 0.003118
Epoch: 45 	Ltrain: 0.003070 	Lval: 0.002938
Epoch: 50 	Ltrain: 0.002950 	Lval: 0.002926
Epoch 00051: reducing learning rate of group 0 to 4.0808e-05.
Epoch: 55 	Ltrain: 0.002809 	Lval: 0.002856
Epoch: 60 	Ltrain: 0.002787 	Lval: 0.002849
Epoch 00063: reducing learning rate of group 0 to 4.0808e-06.
Epoch: 65 	Ltrain: 0.002786 	Lval: 0.002851
Epoch: 70 	Ltrain: 0.002858 	Lval: 0.002846
Epoch 00075: reducing learning rate of group 0 to 4.0808e-07.
Epoch: 75 	Ltrain: 0.002824 	Lval: 0.002845
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.002840


	Fold 3/5
Epoch: 1 	Ltrain: 0.023296 	Lval: 0.011758
Epoch: 5 	Ltrain: 0.005399 	Lval: 0.005643
Epoch: 10 	Ltrain: 0.004565 	Lval: 0.004933
Epoch: 15 	Ltrain: 0.004187 	Lval: 0.003865
Epoch: 20 	Ltrain: 0.003755 	Lval: 0.004734
Epoch: 25 	Ltrain: 0.003775 	Lval: 0.003589
Epoch: 30 	Ltrain: 0.003032 	Lval: 0.003264
Epoch: 35 	Ltrain: 0.002960 	Lval: 0.003027
Epoch: 40 	Ltrain: 0.003141 	Lval: 0.002921
Epoch: 45 	Ltrain: 0.002566 	Lval: 0.002397
Epoch 00047: reducing learning rate of group 0 to 4.0808e-04.
Epoch: 50 	Ltrain: 0.001879 	Lval: 0.002019
Epoch: 55 	Ltrain: 0.001778 	Lval: 0.001953
Epoch: 60 	Ltrain: 0.001731 	Lval: 0.001881
Epoch: 65 	Ltrain: 0.001679 	Lval: 0.001828
Epoch: 70 	Ltrain: 0.001619 	Lval: 0.001762
Epoch: 75 	Ltrain: 0.001579 	Lval: 0.001741
Epoch: 80 	Ltrain: 0.001527 	Lval: 0.001634
Epoch: 85 	Ltrain: 0.001446 	Lval: 0.001573
Epoch: 90 	Ltrain: 0.001384 	Lval: 0.001489
Epoch: 95 	Ltrain: 0.001317 	Lval: 0.001403
Epoch: 100 	Ltrain: 0.001240 	Lval: 0.001331
Epoch 00105: reducing learning rate of group 0 to 4.0808e-05.
Epoch: 105 	Ltrain: 0.001208 	Lval: 0.001321
Epoch: 110 	Ltrain: 0.001112 	Lval: 0.001225
Epoch: 115 	Ltrain: 0.001094 	Lval: 0.001214
Epoch: 120 	Ltrain: 0.001094 	Lval: 0.001203
Epoch: 125 	Ltrain: 0.001078 	Lval: 0.001194
Epoch: 130 	Ltrain: 0.001075 	Lval: 0.001184
Epoch: 135 	Ltrain: 0.001068 	Lval: 0.001177
Epoch: 140 	Ltrain: 0.001051 	Lval: 0.001166
Epoch: 145 	Ltrain: 0.001049 	Lval: 0.001158
Epoch: 150 	Ltrain: 0.001038 	Lval: 0.001150
Epoch: 155 	Ltrain: 0.001031 	Lval: 0.001139
Epoch: 160 	Ltrain: 0.001013 	Lval: 0.001131
Epoch: 165 	Ltrain: 0.001008 	Lval: 0.001120
Epoch: 170 	Ltrain: 0.000998 	Lval: 0.001116
Epoch: 175 	Ltrain: 0.000994 	Lval: 0.001104
Epoch: 180 	Ltrain: 0.000983 	Lval: 0.001092
Epoch: 185 	Ltrain: 0.000975 	Lval: 0.001083
Epoch: 190 	Ltrain: 0.000971 	Lval: 0.001074
Epoch: 195 	Ltrain: 0.000973 	Lval: 0.001065
Epoch: 200 	Ltrain: 0.000952 	Lval: 0.001058
Epoch: 205 	Ltrain: 0.000940 	Lval: 0.001046
Epoch: 210 	Ltrain: 0.000931 	Lval: 0.001041
Epoch: 215 	Ltrain: 0.000921 	Lval: 0.001025
Epoch: 220 	Ltrain: 0.000912 	Lval: 0.001015
Epoch: 225 	Ltrain: 0.000903 	Lval: 0.001006
Epoch: 230 	Ltrain: 0.000894 	Lval: 0.000999
Epoch: 235 	Ltrain: 0.000885 	Lval: 0.000984
Epoch: 240 	Ltrain: 0.000877 	Lval: 0.000978
Epoch: 245 	Ltrain: 0.000868 	Lval: 0.000967
Epoch: 250 	Ltrain: 0.000862 	Lval: 0.000961
Epoch: 255 	Ltrain: 0.000848 	Lval: 0.000949
Epoch: 260 	Ltrain: 0.000839 	Lval: 0.000945
Epoch: 265 	Ltrain: 0.000831 	Lval: 0.000933
Epoch: 270 	Ltrain: 0.000829 	Lval: 0.000924
Epoch: 275 	Ltrain: 0.000823 	Lval: 0.000914
Epoch: 280 	Ltrain: 0.000811 	Lval: 0.000906
Epoch: 285 	Ltrain: 0.000806 	Lval: 0.000892
Epoch: 290 	Ltrain: 0.000789 	Lval: 0.000884
Epoch: 295 	Ltrain: 0.000785 	Lval: 0.000877
Epoch: 300 	Ltrain: 0.000776 	Lval: 0.000868
Epoch: 305 	Ltrain: 0.000764 	Lval: 0.000855
Epoch: 310 	Ltrain: 0.000763 	Lval: 0.000848
Epoch: 315 	Ltrain: 0.000749 	Lval: 0.000837
Epoch: 320 	Ltrain: 0.000742 	Lval: 0.000832
Epoch: 325 	Ltrain: 0.000734 	Lval: 0.000821
Epoch: 330 	Ltrain: 0.000727 	Lval: 0.000808
Epoch: 335 	Ltrain: 0.000720 	Lval: 0.000802
Epoch: 340 	Ltrain: 0.000714 	Lval: 0.000793
Epoch: 345 	Ltrain: 0.000708 	Lval: 0.000784
Epoch: 350 	Ltrain: 0.000698 	Lval: 0.000778
Epoch: 355 	Ltrain: 0.000689 	Lval: 0.000765
Epoch: 360 	Ltrain: 0.000681 	Lval: 0.000759
Epoch: 365 	Ltrain: 0.000672 	Lval: 0.000750
Epoch: 370 	Ltrain: 0.000667 	Lval: 0.000744
Epoch: 375 	Ltrain: 0.000659 	Lval: 0.000736
Epoch: 380 	Ltrain: 0.000649 	Lval: 0.000724
Epoch: 385 	Ltrain: 0.000642 	Lval: 0.000714
Epoch: 390 	Ltrain: 0.000635 	Lval: 0.000712
Epoch: 395 	Ltrain: 0.000630 	Lval: 0.000701
Epoch: 400 	Ltrain: 0.000621 	Lval: 0.000694
Epoch: 405 	Ltrain: 0.000614 	Lval: 0.000686
Epoch: 410 	Ltrain: 0.000607 	Lval: 0.000677
Epoch: 415 	Ltrain: 0.000602 	Lval: 0.000670
Epoch: 420 	Ltrain: 0.000597 	Lval: 0.000661
Epoch: 425 	Ltrain: 0.000585 	Lval: 0.000653
Epoch: 430 	Ltrain: 0.000579 	Lval: 0.000648
Epoch: 435 	Ltrain: 0.000577 	Lval: 0.000644
Epoch: 440 	Ltrain: 0.000571 	Lval: 0.000633
Epoch: 445 	Ltrain: 0.000560 	Lval: 0.000630
Epoch: 450 	Ltrain: 0.000553 	Lval: 0.000617
Epoch: 455 	Ltrain: 0.000547 	Lval: 0.000611
Epoch: 460 	Ltrain: 0.000546 	Lval: 0.000601
Epoch: 465 	Ltrain: 0.000533 	Lval: 0.000600
Epoch: 470 	Ltrain: 0.000527 	Lval: 0.000590
Epoch: 475 	Ltrain: 0.000522 	Lval: 0.000584
Epoch: 480 	Ltrain: 0.000516 	Lval: 0.000578
Epoch: 485 	Ltrain: 0.000512 	Lval: 0.000571
Epoch: 490 	Ltrain: 0.000503 	Lval: 0.000564
Epoch: 495 	Ltrain: 0.000499 	Lval: 0.000561
Epoch 00499: reducing learning rate of group 0 to 4.0808e-06.
Epoch: 500 	Ltrain: 0.000500 	Lval: 0.000558
Epoch: 505 	Ltrain: 0.000488 	Lval: 0.000551
Epoch: 510 	Ltrain: 0.000484 	Lval: 0.000549
Epoch: 515 	Ltrain: 0.000484 	Lval: 0.000548
Epoch: 520 	Ltrain: 0.000482 	Lval: 0.000547
Epoch: 525 	Ltrain: 0.000484 	Lval: 0.000546
EarlyStopper: stopping at epoch 528 with best_val_loss = 0.000548


	Fold 4/5
Epoch: 1 	Ltrain: 0.017326 	Lval: 0.008556
Epoch: 5 	Ltrain: 0.004550 	Lval: 0.004528
Epoch: 10 	Ltrain: 0.004256 	Lval: 0.004024
Epoch: 15 	Ltrain: 0.003808 	Lval: 0.003737
Epoch: 20 	Ltrain: 0.003248 	Lval: 0.003743
Epoch: 25 	Ltrain: 0.003059 	Lval: 0.003043
Epoch 00027: reducing learning rate of group 0 to 4.0808e-04.
Epoch: 30 	Ltrain: 0.002390 	Lval: 0.002597
Epoch: 35 	Ltrain: 0.002295 	Lval: 0.002544
Epoch: 40 	Ltrain: 0.002238 	Lval: 0.002489
Epoch: 45 	Ltrain: 0.002170 	Lval: 0.002363
Epoch: 50 	Ltrain: 0.002099 	Lval: 0.002280
Epoch: 55 	Ltrain: 0.002071 	Lval: 0.002187
Epoch: 60 	Ltrain: 0.001986 	Lval: 0.002182
Epoch: 65 	Ltrain: 0.001908 	Lval: 0.002120
Epoch: 70 	Ltrain: 0.001862 	Lval: 0.002042
Epoch: 75 	Ltrain: 0.001766 	Lval: 0.001916
Epoch: 80 	Ltrain: 0.001720 	Lval: 0.001918
Epoch: 85 	Ltrain: 0.001633 	Lval: 0.001851
Epoch: 90 	Ltrain: 0.001547 	Lval: 0.001702
Epoch: 95 	Ltrain: 0.001480 	Lval: 0.001629
Epoch: 100 	Ltrain: 0.001391 	Lval: 0.001556
Epoch: 105 	Ltrain: 0.001326 	Lval: 0.001549
Epoch: 110 	Ltrain: 0.001289 	Lval: 0.001391
Epoch: 115 	Ltrain: 0.001188 	Lval: 0.001308
Epoch: 120 	Ltrain: 0.001160 	Lval: 0.001210
Epoch: 125 	Ltrain: 0.001080 	Lval: 0.001152
Epoch: 130 	Ltrain: 0.001007 	Lval: 0.001069
Epoch: 135 	Ltrain: 0.000944 	Lval: 0.001034
Epoch: 140 	Ltrain: 0.000873 	Lval: 0.000921
Epoch: 145 	Ltrain: 0.000825 	Lval: 0.000870
Epoch 00149: reducing learning rate of group 0 to 4.0808e-05.
Epoch: 150 	Ltrain: 0.000744 	Lval: 0.000787
Epoch: 155 	Ltrain: 0.000675 	Lval: 0.000752
Epoch: 160 	Ltrain: 0.000664 	Lval: 0.000741
Epoch: 165 	Ltrain: 0.000656 	Lval: 0.000731
Epoch: 170 	Ltrain: 0.000648 	Lval: 0.000725
Epoch: 175 	Ltrain: 0.000641 	Lval: 0.000716
Epoch: 180 	Ltrain: 0.000634 	Lval: 0.000707
Epoch: 185 	Ltrain: 0.000628 	Lval: 0.000699
Epoch: 190 	Ltrain: 0.000619 	Lval: 0.000693
Epoch: 195 	Ltrain: 0.000613 	Lval: 0.000685
Epoch: 200 	Ltrain: 0.000606 	Lval: 0.000675
Epoch: 205 	Ltrain: 0.000597 	Lval: 0.000667
Epoch: 210 	Ltrain: 0.000591 	Lval: 0.000660
Epoch: 215 	Ltrain: 0.000581 	Lval: 0.000650
Epoch: 220 	Ltrain: 0.000574 	Lval: 0.000640
Epoch: 225 	Ltrain: 0.000566 	Lval: 0.000634
Epoch: 230 	Ltrain: 0.000560 	Lval: 0.000624
Epoch: 235 	Ltrain: 0.000550 	Lval: 0.000617
Epoch: 240 	Ltrain: 0.000545 	Lval: 0.000610
Epoch: 245 	Ltrain: 0.000536 	Lval: 0.000602
Epoch: 250 	Ltrain: 0.000529 	Lval: 0.000593
Epoch: 255 	Ltrain: 0.000519 	Lval: 0.000583
Epoch: 260 	Ltrain: 0.000514 	Lval: 0.000576
Epoch: 265 	Ltrain: 0.000506 	Lval: 0.000572
Epoch: 270 	Ltrain: 0.000498 	Lval: 0.000558
Epoch: 275 	Ltrain: 0.000491 	Lval: 0.000551
Epoch: 280 	Ltrain: 0.000484 	Lval: 0.000543
Epoch: 285 	Ltrain: 0.000478 	Lval: 0.000537
Epoch: 290 	Ltrain: 0.000471 	Lval: 0.000531
Epoch: 295 	Ltrain: 0.000465 	Lval: 0.000521
Epoch: 300 	Ltrain: 0.000457 	Lval: 0.000518
Epoch: 305 	Ltrain: 0.000449 	Lval: 0.000506
Epoch: 310 	Ltrain: 0.000442 	Lval: 0.000498
Epoch: 315 	Ltrain: 0.000436 	Lval: 0.000491
Epoch: 320 	Ltrain: 0.000429 	Lval: 0.000482
Epoch: 325 	Ltrain: 0.000422 	Lval: 0.000476
Epoch: 330 	Ltrain: 0.000417 	Lval: 0.000469
Epoch: 335 	Ltrain: 0.000410 	Lval: 0.000461
Epoch: 340 	Ltrain: 0.000404 	Lval: 0.000453
Epoch: 345 	Ltrain: 0.000397 	Lval: 0.000446
Epoch: 350 	Ltrain: 0.000391 	Lval: 0.000441
Epoch: 355 	Ltrain: 0.000384 	Lval: 0.000433
Epoch: 360 	Ltrain: 0.000379 	Lval: 0.000426
Epoch: 365 	Ltrain: 0.000374 	Lval: 0.000419
Epoch: 370 	Ltrain: 0.000368 	Lval: 0.000414
Epoch: 375 	Ltrain: 0.000362 	Lval: 0.000408
Epoch: 380 	Ltrain: 0.000356 	Lval: 0.000400
Epoch: 385 	Ltrain: 0.000350 	Lval: 0.000395
Epoch: 390 	Ltrain: 0.000344 	Lval: 0.000388
Epoch: 395 	Ltrain: 0.000339 	Lval: 0.000383
Epoch: 400 	Ltrain: 0.000333 	Lval: 0.000375
Epoch: 405 	Ltrain: 0.000328 	Lval: 0.000370
Epoch: 410 	Ltrain: 0.000323 	Lval: 0.000365
Epoch: 415 	Ltrain: 0.000319 	Lval: 0.000361
Epoch: 420 	Ltrain: 0.000314 	Lval: 0.000353
Epoch: 425 	Ltrain: 0.000309 	Lval: 0.000349
Epoch: 430 	Ltrain: 0.000304 	Lval: 0.000348
Epoch: 435 	Ltrain: 0.000298 	Lval: 0.000337
Epoch: 440 	Ltrain: 0.000293 	Lval: 0.000331
Epoch: 445 	Ltrain: 0.000290 	Lval: 0.000328
Epoch: 450 	Ltrain: 0.000285 	Lval: 0.000322
Epoch: 455 	Ltrain: 0.000280 	Lval: 0.000315
Epoch: 460 	Ltrain: 0.000275 	Lval: 0.000312
Epoch: 465 	Ltrain: 0.000272 	Lval: 0.000307
Epoch: 470 	Ltrain: 0.000268 	Lval: 0.000302
Epoch: 475 	Ltrain: 0.000263 	Lval: 0.000297
Epoch: 480 	Ltrain: 0.000259 	Lval: 0.000292
Epoch: 485 	Ltrain: 0.000255 	Lval: 0.000288
Epoch: 490 	Ltrain: 0.000252 	Lval: 0.000283
Epoch: 495 	Ltrain: 0.000248 	Lval: 0.000278
Epoch: 500 	Ltrain: 0.000243 	Lval: 0.000274
Epoch: 505 	Ltrain: 0.000239 	Lval: 0.000270
Epoch: 510 	Ltrain: 0.000236 	Lval: 0.000266
Epoch: 515 	Ltrain: 0.000233 	Lval: 0.000262
Epoch: 520 	Ltrain: 0.000229 	Lval: 0.000260
Epoch 00522: reducing learning rate of group 0 to 4.0808e-06.
Epoch: 525 	Ltrain: 0.000223 	Lval: 0.000253
Epoch: 530 	Ltrain: 0.000222 	Lval: 0.000252
Epoch: 535 	Ltrain: 0.000221 	Lval: 0.000251
Epoch: 540 	Ltrain: 0.000221 	Lval: 0.000251
EarlyStopper: stopping at epoch 540 with best_val_loss = 0.000253


	Fold 5/5
Epoch: 1 	Ltrain: 0.015381 	Lval: 0.007705
Epoch: 5 	Ltrain: 0.005257 	Lval: 0.005697
Epoch: 10 	Ltrain: 0.003912 	Lval: 0.004040
Epoch: 15 	Ltrain: 0.003590 	Lval: 0.003964
Epoch: 20 	Ltrain: 0.003133 	Lval: 0.003505
Epoch 00023: reducing learning rate of group 0 to 4.0808e-04.
Epoch: 25 	Ltrain: 0.002490 	Lval: 0.002801
Epoch: 30 	Ltrain: 0.002395 	Lval: 0.002700
Epoch: 35 	Ltrain: 0.002298 	Lval: 0.002677
Epoch: 40 	Ltrain: 0.002236 	Lval: 0.002563
Epoch: 45 	Ltrain: 0.002165 	Lval: 0.002469
Epoch 00047: reducing learning rate of group 0 to 4.0808e-05.
Epoch: 50 	Ltrain: 0.002060 	Lval: 0.002389
Epoch: 55 	Ltrain: 0.002047 	Lval: 0.002383
Epoch: 60 	Ltrain: 0.002037 	Lval: 0.002369
Epoch: 65 	Ltrain: 0.002031 	Lval: 0.002360
Epoch: 70 	Ltrain: 0.002020 	Lval: 0.002354
Epoch 00071: reducing learning rate of group 0 to 4.0808e-06.
Epoch: 75 	Ltrain: 0.002006 	Lval: 0.002349
Epoch: 80 	Ltrain: 0.002010 	Lval: 0.002348
Epoch: 85 	Ltrain: 0.002010 	Lval: 0.002346
EarlyStopper: stopping at epoch 88 with best_val_loss = 0.002353

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.001669102672453354
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.35887785025484e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.040384 	Lval: 0.018172
Epoch: 5 	Ltrain: 0.013183 	Lval: 0.010738
Epoch: 10 	Ltrain: 0.008775 	Lval: 0.008480
Epoch: 15 	Ltrain: 0.006600 	Lval: 0.005915
Epoch: 20 	Ltrain: 0.005563 	Lval: 0.005239
Epoch: 25 	Ltrain: 0.005387 	Lval: 0.005338
Epoch 00027: reducing learning rate of group 0 to 1.6691e-04.
Epoch: 30 	Ltrain: 0.004830 	Lval: 0.004645
Epoch: 35 	Ltrain: 0.004576 	Lval: 0.004514
Epoch 00040: reducing learning rate of group 0 to 1.6691e-05.
Epoch: 40 	Ltrain: 0.004970 	Lval: 0.004524
Epoch: 45 	Ltrain: 0.004643 	Lval: 0.004511
Epoch: 50 	Ltrain: 0.004506 	Lval: 0.004503
Epoch 00052: reducing learning rate of group 0 to 1.6691e-06.
Epoch: 55 	Ltrain: 0.004536 	Lval: 0.004504
Epoch: 60 	Ltrain: 0.004696 	Lval: 0.004504
Epoch 00064: reducing learning rate of group 0 to 1.6691e-07.
Epoch: 65 	Ltrain: 0.005195 	Lval: 0.004504
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.004503


	Fold 2/5
Epoch: 1 	Ltrain: 0.024352 	Lval: 0.013957
Epoch: 5 	Ltrain: 0.008832 	Lval: 0.008346
Epoch: 10 	Ltrain: 0.005912 	Lval: 0.005655
Epoch: 15 	Ltrain: 0.004942 	Lval: 0.004459
Epoch 00020: reducing learning rate of group 0 to 1.6691e-04.
Epoch: 20 	Ltrain: 0.004735 	Lval: 0.004366
Epoch: 25 	Ltrain: 0.004404 	Lval: 0.004295
Epoch: 30 	Ltrain: 0.004399 	Lval: 0.004224
Epoch 00032: reducing learning rate of group 0 to 1.6691e-05.
Epoch: 35 	Ltrain: 0.004261 	Lval: 0.004204
Epoch: 40 	Ltrain: 0.004450 	Lval: 0.004231
Epoch 00044: reducing learning rate of group 0 to 1.6691e-06.
Epoch: 45 	Ltrain: 0.004557 	Lval: 0.004193
EarlyStopper: stopping at epoch 46 with best_val_loss = 0.004162


	Fold 3/5
Epoch: 1 	Ltrain: 0.019494 	Lval: 0.017221
Epoch: 5 	Ltrain: 0.007271 	Lval: 0.006021
Epoch: 10 	Ltrain: 0.005046 	Lval: 0.004510
Epoch: 15 	Ltrain: 0.005032 	Lval: 0.004261
Epoch 00019: reducing learning rate of group 0 to 1.6691e-04.
Epoch: 20 	Ltrain: 0.004712 	Lval: 0.004271
Epoch: 25 	Ltrain: 0.004153 	Lval: 0.003984
Epoch: 30 	Ltrain: 0.004150 	Lval: 0.003974
Epoch 00034: reducing learning rate of group 0 to 1.6691e-05.
Epoch: 35 	Ltrain: 0.004179 	Lval: 0.004011
Epoch: 40 	Ltrain: 0.004171 	Lval: 0.004031
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.003984


	Fold 4/5
Epoch: 1 	Ltrain: 0.018696 	Lval: 0.012552
Epoch: 5 	Ltrain: 0.006381 	Lval: 0.005539
Epoch: 10 	Ltrain: 0.004793 	Lval: 0.004352
Epoch: 15 	Ltrain: 0.004293 	Lval: 0.004366
Epoch: 20 	Ltrain: 0.004398 	Lval: 0.004130
Epoch: 25 	Ltrain: 0.003909 	Lval: 0.004431
Epoch: 30 	Ltrain: 0.003674 	Lval: 0.003786
Epoch: 35 	Ltrain: 0.003612 	Lval: 0.003546
Epoch 00040: reducing learning rate of group 0 to 1.6691e-04.
Epoch: 40 	Ltrain: 0.003448 	Lval: 0.003780
Epoch: 45 	Ltrain: 0.002955 	Lval: 0.003230
Epoch: 50 	Ltrain: 0.002953 	Lval: 0.003298
Epoch: 55 	Ltrain: 0.002852 	Lval: 0.003188
Epoch: 60 	Ltrain: 0.002838 	Lval: 0.003179
Epoch: 65 	Ltrain: 0.002795 	Lval: 0.003149
Epoch: 70 	Ltrain: 0.002829 	Lval: 0.003097
Epoch: 75 	Ltrain: 0.002750 	Lval: 0.003146
Epoch: 80 	Ltrain: 0.002759 	Lval: 0.003101
Epoch 00081: reducing learning rate of group 0 to 1.6691e-05.
Epoch: 85 	Ltrain: 0.002698 	Lval: 0.003017
Epoch: 90 	Ltrain: 0.002687 	Lval: 0.003021
Epoch 00093: reducing learning rate of group 0 to 1.6691e-06.
Epoch: 95 	Ltrain: 0.002681 	Lval: 0.003015
Epoch: 100 	Ltrain: 0.002684 	Lval: 0.003015
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.003019


	Fold 5/5
Epoch: 1 	Ltrain: 0.027085 	Lval: 0.013438
Epoch: 5 	Ltrain: 0.006460 	Lval: 0.006317
Epoch: 10 	Ltrain: 0.004762 	Lval: 0.004794
Epoch: 15 	Ltrain: 0.004410 	Lval: 0.004345
Epoch: 20 	Ltrain: 0.004188 	Lval: 0.004442
Epoch 00023: reducing learning rate of group 0 to 1.6691e-04.
Epoch: 25 	Ltrain: 0.003812 	Lval: 0.004057
Epoch: 30 	Ltrain: 0.003729 	Lval: 0.004039
Epoch: 35 	Ltrain: 0.003798 	Lval: 0.004031
Epoch: 40 	Ltrain: 0.003757 	Lval: 0.004019
Epoch: 45 	Ltrain: 0.003699 	Lval: 0.004015
Epoch 00050: reducing learning rate of group 0 to 1.6691e-05.
Epoch: 50 	Ltrain: 0.003746 	Lval: 0.004006
Epoch: 55 	Ltrain: 0.003629 	Lval: 0.003968
Epoch: 60 	Ltrain: 0.003580 	Lval: 0.003963
Epoch: 65 	Ltrain: 0.003590 	Lval: 0.003961
Epoch 00068: reducing learning rate of group 0 to 1.6691e-06.
Epoch: 70 	Ltrain: 0.003601 	Lval: 0.003960
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.003961

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007141463621538154
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.5628548049405046e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.077495 	Lval: 0.023700
Epoch: 5 	Ltrain: 0.018915 	Lval: 0.012028
Epoch: 10 	Ltrain: 0.008774 	Lval: 0.009294
Epoch: 15 	Ltrain: 0.007229 	Lval: 0.006094
Epoch: 20 	Ltrain: 0.009618 	Lval: 0.005372
Epoch: 25 	Ltrain: 0.007805 	Lval: 0.006001
Epoch 00026: reducing learning rate of group 0 to 7.1415e-04.
Epoch: 30 	Ltrain: 0.006654 	Lval: 0.004824
Epoch: 35 	Ltrain: 0.004432 	Lval: 0.004513
Epoch: 40 	Ltrain: 0.004668 	Lval: 0.004426
Epoch 00044: reducing learning rate of group 0 to 7.1415e-05.
Epoch: 45 	Ltrain: 0.005604 	Lval: 0.004419
Epoch: 50 	Ltrain: 0.004246 	Lval: 0.004398
Epoch: 55 	Ltrain: 0.004429 	Lval: 0.004397
Epoch 00056: reducing learning rate of group 0 to 7.1415e-06.
Epoch: 60 	Ltrain: 0.004658 	Lval: 0.004401
Epoch: 65 	Ltrain: 0.005873 	Lval: 0.004401
Epoch 00068: reducing learning rate of group 0 to 7.1415e-07.
Epoch: 70 	Ltrain: 0.004751 	Lval: 0.004401
Epoch: 75 	Ltrain: 0.004523 	Lval: 0.004401
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.004396


	Fold 2/5
Epoch: 1 	Ltrain: 0.034672 	Lval: 0.012923
Epoch: 5 	Ltrain: 0.011127 	Lval: 0.009492
Epoch: 10 	Ltrain: 0.006568 	Lval: 0.004725
Epoch 00015: reducing learning rate of group 0 to 7.1415e-04.
Epoch: 15 	Ltrain: 0.005607 	Lval: 0.004583
Epoch: 20 	Ltrain: 0.004699 	Lval: 0.004277
Epoch: 25 	Ltrain: 0.005051 	Lval: 0.004385
Epoch 00028: reducing learning rate of group 0 to 7.1415e-05.
Epoch: 30 	Ltrain: 0.005106 	Lval: 0.004344
Epoch: 35 	Ltrain: 0.004763 	Lval: 0.004305
Epoch: 40 	Ltrain: 0.004759 	Lval: 0.004238
EarlyStopper: stopping at epoch 40 with best_val_loss = 0.004247


	Fold 3/5
Epoch: 1 	Ltrain: 0.034417 	Lval: 0.015296
Epoch: 5 	Ltrain: 0.010994 	Lval: 0.008033
Epoch: 10 	Ltrain: 0.005557 	Lval: 0.004380
Epoch: 15 	Ltrain: 0.004818 	Lval: 0.004609
Epoch: 20 	Ltrain: 0.004485 	Lval: 0.004845
Epoch: 25 	Ltrain: 0.004436 	Lval: 0.004128
Epoch 00027: reducing learning rate of group 0 to 7.1415e-04.
Epoch: 30 	Ltrain: 0.003878 	Lval: 0.003874
Epoch: 35 	Ltrain: 0.003784 	Lval: 0.003720
Epoch 00039: reducing learning rate of group 0 to 7.1415e-05.
Epoch: 40 	Ltrain: 0.003756 	Lval: 0.003832
Epoch: 45 	Ltrain: 0.003717 	Lval: 0.003810
Epoch: 50 	Ltrain: 0.003838 	Lval: 0.003774
Epoch 00051: reducing learning rate of group 0 to 7.1415e-06.
Epoch: 55 	Ltrain: 0.003864 	Lval: 0.003782
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.003720


	Fold 4/5
Epoch: 1 	Ltrain: 0.029566 	Lval: 0.012936
Epoch: 5 	Ltrain: 0.005869 	Lval: 0.005051
Epoch: 10 	Ltrain: 0.005030 	Lval: 0.004506
Epoch 00013: reducing learning rate of group 0 to 7.1415e-04.
Epoch: 15 	Ltrain: 0.004298 	Lval: 0.004401
Epoch: 20 	Ltrain: 0.003968 	Lval: 0.004020
Epoch: 25 	Ltrain: 0.003940 	Lval: 0.003927
Epoch: 30 	Ltrain: 0.003918 	Lval: 0.003976
Epoch 00035: reducing learning rate of group 0 to 7.1415e-05.
Epoch: 35 	Ltrain: 0.003759 	Lval: 0.003857
Epoch: 40 	Ltrain: 0.003874 	Lval: 0.003847
Epoch: 45 	Ltrain: 0.003958 	Lval: 0.003854
Epoch 00047: reducing learning rate of group 0 to 7.1415e-06.
Epoch: 50 	Ltrain: 0.003935 	Lval: 0.003846
Epoch: 55 	Ltrain: 0.003772 	Lval: 0.003842
Epoch 00059: reducing learning rate of group 0 to 7.1415e-07.
Epoch: 60 	Ltrain: 0.003790 	Lval: 0.003839
Epoch: 65 	Ltrain: 0.003764 	Lval: 0.003840
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.003833


	Fold 5/5
Epoch: 1 	Ltrain: 0.030424 	Lval: 0.016711
Epoch: 5 	Ltrain: 0.007279 	Lval: 0.006407
Epoch: 10 	Ltrain: 0.005086 	Lval: 0.004817
Epoch 00014: reducing learning rate of group 0 to 7.1415e-04.
Epoch: 15 	Ltrain: 0.004567 	Lval: 0.004612
Epoch: 20 	Ltrain: 0.004215 	Lval: 0.004395
Epoch: 25 	Ltrain: 0.004176 	Lval: 0.004326
Epoch: 30 	Ltrain: 0.003928 	Lval: 0.004302
Epoch: 35 	Ltrain: 0.003899 	Lval: 0.004251
Epoch: 40 	Ltrain: 0.004045 	Lval: 0.004230
Epoch 00041: reducing learning rate of group 0 to 7.1415e-05.
Epoch: 45 	Ltrain: 0.003893 	Lval: 0.004188
Epoch: 50 	Ltrain: 0.003719 	Lval: 0.004181
Epoch: 55 	Ltrain: 0.003824 	Lval: 0.004181
Epoch: 60 	Ltrain: 0.003729 	Lval: 0.004175
Epoch: 65 	Ltrain: 0.003839 	Lval: 0.004164
Epoch: 70 	Ltrain: 0.003789 	Lval: 0.004162
Epoch 00071: reducing learning rate of group 0 to 7.1415e-06.
Epoch: 75 	Ltrain: 0.004013 	Lval: 0.004163
Epoch: 80 	Ltrain: 0.003715 	Lval: 0.004162
Epoch 00083: reducing learning rate of group 0 to 7.1415e-07.
Epoch: 85 	Ltrain: 0.004175 	Lval: 0.004162
EarlyStopper: stopping at epoch 85 with best_val_loss = 0.004168

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003153435261116983
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.0497343454296965e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.030592 	Lval: 0.018483
Epoch: 5 	Ltrain: 0.012476 	Lval: 0.011569
Epoch: 10 	Ltrain: 0.008630 	Lval: 0.006556
Epoch: 15 	Ltrain: 0.005918 	Lval: 0.005339
Epoch: 20 	Ltrain: 0.005121 	Lval: 0.004652
Epoch 00024: reducing learning rate of group 0 to 3.1534e-04.
Epoch: 25 	Ltrain: 0.005622 	Lval: 0.004796
Epoch: 30 	Ltrain: 0.004562 	Lval: 0.004505
Epoch: 35 	Ltrain: 0.004737 	Lval: 0.004468
Epoch: 40 	Ltrain: 0.004599 	Lval: 0.004382
Epoch: 45 	Ltrain: 0.004384 	Lval: 0.004471
Epoch: 50 	Ltrain: 0.004502 	Lval: 0.004384
Epoch: 55 	Ltrain: 0.004724 	Lval: 0.004279
Epoch 00060: reducing learning rate of group 0 to 3.1534e-05.
Epoch: 60 	Ltrain: 0.004485 	Lval: 0.004261
Epoch: 65 	Ltrain: 0.004421 	Lval: 0.004261
Epoch: 70 	Ltrain: 0.003804 	Lval: 0.004225
Epoch 00075: reducing learning rate of group 0 to 3.1534e-06.
Epoch: 75 	Ltrain: 0.004205 	Lval: 0.004247
Epoch: 80 	Ltrain: 0.004343 	Lval: 0.004244
Epoch: 85 	Ltrain: 0.004223 	Lval: 0.004238
Epoch 00087: reducing learning rate of group 0 to 3.1534e-07.
Epoch: 90 	Ltrain: 0.004202 	Lval: 0.004235
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.004225


	Fold 2/5
Epoch: 1 	Ltrain: 0.031952 	Lval: 0.019338
Epoch: 5 	Ltrain: 0.008893 	Lval: 0.007721
Epoch: 10 	Ltrain: 0.005688 	Lval: 0.005140
Epoch: 15 	Ltrain: 0.005144 	Lval: 0.005661
Epoch 00017: reducing learning rate of group 0 to 3.1534e-04.
Epoch: 20 	Ltrain: 0.004561 	Lval: 0.004361
Epoch: 25 	Ltrain: 0.004282 	Lval: 0.004169
Epoch: 30 	Ltrain: 0.004481 	Lval: 0.004242
Epoch 00035: reducing learning rate of group 0 to 3.1534e-05.
Epoch: 35 	Ltrain: 0.004346 	Lval: 0.004115
Epoch: 40 	Ltrain: 0.004259 	Lval: 0.004114
Epoch: 45 	Ltrain: 0.004183 	Lval: 0.004093
Epoch 00047: reducing learning rate of group 0 to 3.1534e-06.
Epoch: 50 	Ltrain: 0.004302 	Lval: 0.004111
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.004080


	Fold 3/5
Epoch: 1 	Ltrain: 0.024482 	Lval: 0.012506
Epoch: 5 	Ltrain: 0.006339 	Lval: 0.005713
Epoch: 10 	Ltrain: 0.004905 	Lval: 0.004462
Epoch: 15 	Ltrain: 0.004779 	Lval: 0.004457
Epoch: 20 	Ltrain: 0.004520 	Lval: 0.004069
Epoch: 25 	Ltrain: 0.003872 	Lval: 0.003748
Epoch: 30 	Ltrain: 0.003711 	Lval: 0.003550
Epoch 00032: reducing learning rate of group 0 to 3.1534e-04.
Epoch: 35 	Ltrain: 0.003035 	Lval: 0.003171
Epoch: 40 	Ltrain: 0.002981 	Lval: 0.003048
Epoch: 45 	Ltrain: 0.002875 	Lval: 0.003003
Epoch: 50 	Ltrain: 0.002847 	Lval: 0.002967
Epoch: 55 	Ltrain: 0.002825 	Lval: 0.002908
Epoch 00057: reducing learning rate of group 0 to 3.1534e-05.
Epoch: 60 	Ltrain: 0.002744 	Lval: 0.002878
Epoch: 65 	Ltrain: 0.002725 	Lval: 0.002896
Epoch: 70 	Ltrain: 0.002808 	Lval: 0.002885
Epoch: 75 	Ltrain: 0.002743 	Lval: 0.002873
Epoch 00077: reducing learning rate of group 0 to 3.1534e-06.
Epoch: 80 	Ltrain: 0.002717 	Lval: 0.002874
Epoch: 85 	Ltrain: 0.002743 	Lval: 0.002872
Epoch 00089: reducing learning rate of group 0 to 3.1534e-07.
Epoch: 90 	Ltrain: 0.002733 	Lval: 0.002870
Epoch: 95 	Ltrain: 0.002693 	Lval: 0.002870
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.002868


	Fold 4/5
Epoch: 1 	Ltrain: 0.018868 	Lval: 0.012336
Epoch: 5 	Ltrain: 0.005487 	Lval: 0.004732
Epoch: 10 	Ltrain: 0.004595 	Lval: 0.004235
Epoch: 15 	Ltrain: 0.003877 	Lval: 0.004084
Epoch: 20 	Ltrain: 0.003829 	Lval: 0.003652
Epoch: 25 	Ltrain: 0.003527 	Lval: 0.003461
Epoch 00027: reducing learning rate of group 0 to 3.1534e-04.
Epoch: 30 	Ltrain: 0.002912 	Lval: 0.003059
Epoch: 35 	Ltrain: 0.002824 	Lval: 0.002952
Epoch: 40 	Ltrain: 0.002750 	Lval: 0.002918
Epoch: 45 	Ltrain: 0.002661 	Lval: 0.002860
Epoch 00048: reducing learning rate of group 0 to 3.1534e-05.
Epoch: 50 	Ltrain: 0.002588 	Lval: 0.002821
Epoch: 55 	Ltrain: 0.002581 	Lval: 0.002820
Epoch 00060: reducing learning rate of group 0 to 3.1534e-06.
Epoch: 60 	Ltrain: 0.002570 	Lval: 0.002817
Epoch: 65 	Ltrain: 0.002600 	Lval: 0.002816
Epoch: 70 	Ltrain: 0.002597 	Lval: 0.002815
Epoch 00072: reducing learning rate of group 0 to 3.1534e-07.
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.002817


	Fold 5/5
Epoch: 1 	Ltrain: 0.021687 	Lval: 0.013052
Epoch: 5 	Ltrain: 0.006035 	Lval: 0.005182
Epoch: 10 	Ltrain: 0.004367 	Lval: 0.004330
Epoch: 15 	Ltrain: 0.003957 	Lval: 0.004417
Epoch: 20 	Ltrain: 0.003907 	Lval: 0.004369
Epoch 00022: reducing learning rate of group 0 to 3.1534e-04.
Epoch: 25 	Ltrain: 0.003252 	Lval: 0.003639
Epoch: 30 	Ltrain: 0.003148 	Lval: 0.003558
Epoch: 35 	Ltrain: 0.003104 	Lval: 0.003504
Epoch 00039: reducing learning rate of group 0 to 3.1534e-05.
Epoch: 40 	Ltrain: 0.003030 	Lval: 0.003486
Epoch: 45 	Ltrain: 0.002986 	Lval: 0.003477
Epoch: 50 	Ltrain: 0.003023 	Lval: 0.003471
Epoch: 55 	Ltrain: 0.002974 	Lval: 0.003461
Epoch: 60 	Ltrain: 0.003000 	Lval: 0.003453
Epoch: 65 	Ltrain: 0.002975 	Lval: 0.003445
Epoch: 70 	Ltrain: 0.002959 	Lval: 0.003440
Epoch: 75 	Ltrain: 0.002972 	Lval: 0.003434
Epoch: 80 	Ltrain: 0.002937 	Lval: 0.003428
Epoch: 85 	Ltrain: 0.002939 	Lval: 0.003418
Epoch: 90 	Ltrain: 0.002964 	Lval: 0.003413
Epoch: 95 	Ltrain: 0.002938 	Lval: 0.003406
Epoch: 100 	Ltrain: 0.002916 	Lval: 0.003393
Epoch: 105 	Ltrain: 0.002909 	Lval: 0.003391
Epoch 00107: reducing learning rate of group 0 to 3.1534e-06.
Epoch: 110 	Ltrain: 0.002940 	Lval: 0.003382
Epoch: 115 	Ltrain: 0.002943 	Lval: 0.003379
Epoch 00119: reducing learning rate of group 0 to 3.1534e-07.
Epoch: 120 	Ltrain: 0.002872 	Lval: 0.003380
Epoch: 125 	Ltrain: 0.002867 	Lval: 0.003380
EarlyStopper: stopping at epoch 124 with best_val_loss = 0.003383

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004484638720987479
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.1578759649024179e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.040676 	Lval: 0.018741
Epoch: 5 	Ltrain: 0.010081 	Lval: 0.008503
Epoch: 10 	Ltrain: 0.006649 	Lval: 0.004950
Epoch: 15 	Ltrain: 0.005126 	Lval: 0.004622
Epoch: 20 	Ltrain: 0.004554 	Lval: 0.004631
Epoch 00023: reducing learning rate of group 0 to 4.4846e-04.
Epoch: 25 	Ltrain: 0.004279 	Lval: 0.004034
Epoch: 30 	Ltrain: 0.003880 	Lval: 0.003926
Epoch: 35 	Ltrain: 0.003877 	Lval: 0.003859
Epoch: 40 	Ltrain: 0.003843 	Lval: 0.003789
Epoch: 45 	Ltrain: 0.003760 	Lval: 0.003745
Epoch: 50 	Ltrain: 0.003904 	Lval: 0.003680
Epoch: 55 	Ltrain: 0.003872 	Lval: 0.003610
Epoch: 60 	Ltrain: 0.003700 	Lval: 0.003698
Epoch: 65 	Ltrain: 0.003770 	Lval: 0.003509
Epoch: 70 	Ltrain: 0.003526 	Lval: 0.003393
Epoch: 75 	Ltrain: 0.003248 	Lval: 0.003316
Epoch: 80 	Ltrain: 0.003485 	Lval: 0.003206
Epoch: 85 	Ltrain: 0.003254 	Lval: 0.003168
Epoch: 90 	Ltrain: 0.003062 	Lval: 0.003086
Epoch: 95 	Ltrain: 0.003161 	Lval: 0.003015
Epoch: 100 	Ltrain: 0.002895 	Lval: 0.002960
Epoch: 105 	Ltrain: 0.002851 	Lval: 0.002881
Epoch: 110 	Ltrain: 0.002938 	Lval: 0.002893
Epoch: 115 	Ltrain: 0.002749 	Lval: 0.002736
Epoch 00120: reducing learning rate of group 0 to 4.4846e-05.
Epoch: 120 	Ltrain: 0.002886 	Lval: 0.002947
Epoch: 125 	Ltrain: 0.002571 	Lval: 0.002584
Epoch: 130 	Ltrain: 0.002583 	Lval: 0.002572
Epoch: 135 	Ltrain: 0.002553 	Lval: 0.002564
Epoch: 140 	Ltrain: 0.002576 	Lval: 0.002552
Epoch: 145 	Ltrain: 0.002503 	Lval: 0.002539
Epoch: 150 	Ltrain: 0.002589 	Lval: 0.002526
Epoch: 155 	Ltrain: 0.002481 	Lval: 0.002520
Epoch: 160 	Ltrain: 0.002549 	Lval: 0.002515
Epoch: 165 	Ltrain: 0.002411 	Lval: 0.002504
Epoch 00169: reducing learning rate of group 0 to 4.4846e-06.
Epoch: 170 	Ltrain: 0.002499 	Lval: 0.002503
Epoch: 175 	Ltrain: 0.002536 	Lval: 0.002496
Epoch: 180 	Ltrain: 0.002453 	Lval: 0.002496
Epoch 00181: reducing learning rate of group 0 to 4.4846e-07.
Epoch: 185 	Ltrain: 0.002465 	Lval: 0.002496
EarlyStopper: stopping at epoch 185 with best_val_loss = 0.002504


	Fold 2/5
Epoch: 1 	Ltrain: 0.034392 	Lval: 0.013617
Epoch: 5 	Ltrain: 0.006324 	Lval: 0.005252
Epoch: 10 	Ltrain: 0.004519 	Lval: 0.004294
Epoch: 15 	Ltrain: 0.004491 	Lval: 0.003958
Epoch 00019: reducing learning rate of group 0 to 4.4846e-04.
Epoch: 20 	Ltrain: 0.003828 	Lval: 0.003548
Epoch: 25 	Ltrain: 0.003444 	Lval: 0.003431
Epoch: 30 	Ltrain: 0.003395 	Lval: 0.003369
Epoch: 35 	Ltrain: 0.003283 	Lval: 0.003292
Epoch 00040: reducing learning rate of group 0 to 4.4846e-05.
Epoch: 40 	Ltrain: 0.003236 	Lval: 0.003332
Epoch: 45 	Ltrain: 0.003177 	Lval: 0.003245
Epoch: 50 	Ltrain: 0.003161 	Lval: 0.003249
Epoch 00055: reducing learning rate of group 0 to 4.4846e-06.
Epoch: 55 	Ltrain: 0.003146 	Lval: 0.003222
Epoch: 60 	Ltrain: 0.003239 	Lval: 0.003221
Epoch: 65 	Ltrain: 0.003131 	Lval: 0.003226
Epoch 00067: reducing learning rate of group 0 to 4.4846e-07.
Epoch: 70 	Ltrain: 0.003130 	Lval: 0.003224
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.003217


	Fold 3/5
Epoch: 1 	Ltrain: 0.024738 	Lval: 0.016509
Epoch: 5 	Ltrain: 0.005490 	Lval: 0.005332
Epoch: 10 	Ltrain: 0.004737 	Lval: 0.005020
Epoch: 15 	Ltrain: 0.004428 	Lval: 0.004141
Epoch 00016: reducing learning rate of group 0 to 4.4846e-04.
Epoch: 20 	Ltrain: 0.003330 	Lval: 0.003433
Epoch: 25 	Ltrain: 0.003240 	Lval: 0.003341
Epoch: 30 	Ltrain: 0.003119 	Lval: 0.003207
Epoch: 35 	Ltrain: 0.003078 	Lval: 0.003128
Epoch: 40 	Ltrain: 0.002950 	Lval: 0.003209
Epoch: 45 	Ltrain: 0.002840 	Lval: 0.003081
Epoch: 50 	Ltrain: 0.002841 	Lval: 0.002860
Epoch 00055: reducing learning rate of group 0 to 4.4846e-05.
Epoch: 55 	Ltrain: 0.002718 	Lval: 0.003068
Epoch: 60 	Ltrain: 0.002543 	Lval: 0.002702
Epoch: 65 	Ltrain: 0.002538 	Lval: 0.002697
Epoch 00069: reducing learning rate of group 0 to 4.4846e-06.
Epoch: 70 	Ltrain: 0.002508 	Lval: 0.002700
Epoch: 75 	Ltrain: 0.002496 	Lval: 0.002693
Epoch: 80 	Ltrain: 0.002513 	Lval: 0.002691
Epoch 00085: reducing learning rate of group 0 to 4.4846e-07.
Epoch: 85 	Ltrain: 0.002504 	Lval: 0.002690
Epoch: 90 	Ltrain: 0.002500 	Lval: 0.002690
Epoch: 95 	Ltrain: 0.002497 	Lval: 0.002690
Epoch 00097: reducing learning rate of group 0 to 4.4846e-08.
EarlyStopper: stopping at epoch 98 with best_val_loss = 0.002692


	Fold 4/5
Epoch: 1 	Ltrain: 0.021447 	Lval: 0.011120
Epoch: 5 	Ltrain: 0.005097 	Lval: 0.005851
Epoch: 10 	Ltrain: 0.004488 	Lval: 0.004156
Epoch 00015: reducing learning rate of group 0 to 4.4846e-04.
Epoch: 15 	Ltrain: 0.004483 	Lval: 0.004012
Epoch: 20 	Ltrain: 0.003175 	Lval: 0.003401
Epoch: 25 	Ltrain: 0.003098 	Lval: 0.003279
Epoch 00030: reducing learning rate of group 0 to 4.4846e-05.
Epoch: 30 	Ltrain: 0.002950 	Lval: 0.003293
Epoch: 35 	Ltrain: 0.002880 	Lval: 0.003142
Epoch: 40 	Ltrain: 0.002866 	Lval: 0.003132
Epoch: 45 	Ltrain: 0.002850 	Lval: 0.003119
Epoch: 50 	Ltrain: 0.002849 	Lval: 0.003105
Epoch 00052: reducing learning rate of group 0 to 4.4846e-06.
Epoch: 55 	Ltrain: 0.002826 	Lval: 0.003102
Epoch: 60 	Ltrain: 0.002829 	Lval: 0.003101
Epoch 00064: reducing learning rate of group 0 to 4.4846e-07.
Epoch: 65 	Ltrain: 0.002823 	Lval: 0.003099
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.003100


	Fold 5/5
Epoch: 1 	Ltrain: 0.027686 	Lval: 0.009782
Epoch: 5 	Ltrain: 0.005246 	Lval: 0.004773
Epoch: 10 	Ltrain: 0.004276 	Lval: 0.004982
Epoch: 15 	Ltrain: 0.003688 	Lval: 0.004336
Epoch: 20 	Ltrain: 0.004025 	Lval: 0.003767
Epoch: 25 	Ltrain: 0.003333 	Lval: 0.003406
Epoch: 30 	Ltrain: 0.003227 	Lval: 0.003432
Epoch 00035: reducing learning rate of group 0 to 4.4846e-04.
Epoch: 35 	Ltrain: 0.002802 	Lval: 0.003364
Epoch: 40 	Ltrain: 0.002149 	Lval: 0.002451
Epoch: 45 	Ltrain: 0.002045 	Lval: 0.002335
Epoch: 50 	Ltrain: 0.001953 	Lval: 0.002247
Epoch 00053: reducing learning rate of group 0 to 4.4846e-05.
Epoch: 55 	Ltrain: 0.001827 	Lval: 0.002134
Epoch: 60 	Ltrain: 0.001814 	Lval: 0.002121
Epoch: 65 	Ltrain: 0.001805 	Lval: 0.002107
Epoch: 70 	Ltrain: 0.001790 	Lval: 0.002095
Epoch: 75 	Ltrain: 0.001777 	Lval: 0.002078
Epoch: 80 	Ltrain: 0.001770 	Lval: 0.002059
Epoch: 85 	Ltrain: 0.001754 	Lval: 0.002053
Epoch: 90 	Ltrain: 0.001742 	Lval: 0.002031
Epoch: 95 	Ltrain: 0.001724 	Lval: 0.002015
Epoch: 100 	Ltrain: 0.001716 	Lval: 0.001994
Epoch: 105 	Ltrain: 0.001694 	Lval: 0.001978
Epoch: 110 	Ltrain: 0.001686 	Lval: 0.001959
Epoch: 115 	Ltrain: 0.001665 	Lval: 0.001938
Epoch: 120 	Ltrain: 0.001651 	Lval: 0.001926
Epoch: 125 	Ltrain: 0.001637 	Lval: 0.001899
Epoch: 130 	Ltrain: 0.001615 	Lval: 0.001881
Epoch: 135 	Ltrain: 0.001599 	Lval: 0.001852
Epoch: 140 	Ltrain: 0.001583 	Lval: 0.001837
Epoch: 145 	Ltrain: 0.001571 	Lval: 0.001811
Epoch: 150 	Ltrain: 0.001548 	Lval: 0.001788
Epoch: 155 	Ltrain: 0.001528 	Lval: 0.001765
Epoch: 160 	Ltrain: 0.001510 	Lval: 0.001754
Epoch: 165 	Ltrain: 0.001491 	Lval: 0.001730
Epoch: 170 	Ltrain: 0.001476 	Lval: 0.001702
Epoch: 175 	Ltrain: 0.001462 	Lval: 0.001679
Epoch: 180 	Ltrain: 0.001438 	Lval: 0.001650
Epoch: 185 	Ltrain: 0.001421 	Lval: 0.001623
Epoch: 190 	Ltrain: 0.001404 	Lval: 0.001615
Epoch: 195 	Ltrain: 0.001381 	Lval: 0.001590
Epoch: 200 	Ltrain: 0.001361 	Lval: 0.001559
Epoch: 205 	Ltrain: 0.001344 	Lval: 0.001537
Epoch: 210 	Ltrain: 0.001327 	Lval: 0.001528
Epoch: 215 	Ltrain: 0.001308 	Lval: 0.001503
Epoch: 220 	Ltrain: 0.001292 	Lval: 0.001476
Epoch: 225 	Ltrain: 0.001268 	Lval: 0.001457
Epoch: 230 	Ltrain: 0.001249 	Lval: 0.001433
Epoch: 235 	Ltrain: 0.001231 	Lval: 0.001414
Epoch: 240 	Ltrain: 0.001220 	Lval: 0.001392
Epoch: 245 	Ltrain: 0.001195 	Lval: 0.001373
Epoch: 250 	Ltrain: 0.001178 	Lval: 0.001346
Epoch: 255 	Ltrain: 0.001164 	Lval: 0.001334
Epoch: 260 	Ltrain: 0.001142 	Lval: 0.001305
Epoch: 265 	Ltrain: 0.001119 	Lval: 0.001293
Epoch: 270 	Ltrain: 0.001106 	Lval: 0.001266
Epoch: 275 	Ltrain: 0.001090 	Lval: 0.001250
Epoch: 280 	Ltrain: 0.001071 	Lval: 0.001232
Epoch: 285 	Ltrain: 0.001057 	Lval: 0.001205
Epoch: 290 	Ltrain: 0.001042 	Lval: 0.001188
Epoch: 295 	Ltrain: 0.001020 	Lval: 0.001173
Epoch: 300 	Ltrain: 0.001004 	Lval: 0.001151
Epoch: 305 	Ltrain: 0.000989 	Lval: 0.001125
Epoch: 310 	Ltrain: 0.000976 	Lval: 0.001105
Epoch: 315 	Ltrain: 0.000953 	Lval: 0.001099
Epoch: 320 	Ltrain: 0.000941 	Lval: 0.001071
Epoch: 325 	Ltrain: 0.000929 	Lval: 0.001065
Epoch: 330 	Ltrain: 0.000912 	Lval: 0.001040
Epoch: 335 	Ltrain: 0.000899 	Lval: 0.001022
Epoch: 340 	Ltrain: 0.000888 	Lval: 0.001007
Epoch: 345 	Ltrain: 0.000871 	Lval: 0.000990
Epoch: 350 	Ltrain: 0.000855 	Lval: 0.000972
Epoch: 355 	Ltrain: 0.000845 	Lval: 0.000952
Epoch: 360 	Ltrain: 0.000832 	Lval: 0.000952
Epoch: 365 	Ltrain: 0.000814 	Lval: 0.000923
Epoch: 370 	Ltrain: 0.000805 	Lval: 0.000911
Epoch: 375 	Ltrain: 0.000796 	Lval: 0.000893
Epoch: 380 	Ltrain: 0.000782 	Lval: 0.000877
Epoch: 385 	Ltrain: 0.000770 	Lval: 0.000864
Epoch 00388: reducing learning rate of group 0 to 4.4846e-06.
Epoch: 390 	Ltrain: 0.000743 	Lval: 0.000846
Epoch: 395 	Ltrain: 0.000740 	Lval: 0.000844
Epoch: 400 	Ltrain: 0.000738 	Lval: 0.000842
Epoch: 405 	Ltrain: 0.000737 	Lval: 0.000841
Epoch: 410 	Ltrain: 0.000736 	Lval: 0.000839
EarlyStopper: stopping at epoch 409 with best_val_loss = 0.000848

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005783765266375782
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.948274971179524e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.086700 	Lval: 0.025346
Epoch: 5 	Ltrain: 0.007534 	Lval: 0.006178
Epoch: 10 	Ltrain: 0.005510 	Lval: 0.005688
Epoch: 15 	Ltrain: 0.004813 	Lval: 0.004641
Epoch: 20 	Ltrain: 0.005119 	Lval: 0.004354
Epoch 00024: reducing learning rate of group 0 to 5.7838e-04.
Epoch: 25 	Ltrain: 0.004503 	Lval: 0.004105
Epoch: 30 	Ltrain: 0.004213 	Lval: 0.003934
Epoch: 35 	Ltrain: 0.004100 	Lval: 0.003890
Epoch: 40 	Ltrain: 0.003949 	Lval: 0.003862
Epoch: 45 	Ltrain: 0.003826 	Lval: 0.003839
Epoch: 50 	Ltrain: 0.003904 	Lval: 0.003778
Epoch: 55 	Ltrain: 0.003726 	Lval: 0.003720
Epoch: 60 	Ltrain: 0.003724 	Lval: 0.003702
Epoch: 65 	Ltrain: 0.003764 	Lval: 0.003700
Epoch 00066: reducing learning rate of group 0 to 5.7838e-05.
Epoch: 70 	Ltrain: 0.003727 	Lval: 0.003618
Epoch: 75 	Ltrain: 0.003627 	Lval: 0.003606
Epoch: 80 	Ltrain: 0.003807 	Lval: 0.003596
Epoch: 85 	Ltrain: 0.003563 	Lval: 0.003593
Epoch: 90 	Ltrain: 0.003590 	Lval: 0.003584
Epoch: 95 	Ltrain: 0.003626 	Lval: 0.003579
Epoch 00098: reducing learning rate of group 0 to 5.7838e-06.
Epoch: 100 	Ltrain: 0.003583 	Lval: 0.003583
Epoch: 105 	Ltrain: 0.003622 	Lval: 0.003580
Epoch: 110 	Ltrain: 0.003599 	Lval: 0.003576
EarlyStopper: stopping at epoch 112 with best_val_loss = 0.003578


	Fold 2/5
Epoch: 1 	Ltrain: 0.048557 	Lval: 0.011604
Epoch: 5 	Ltrain: 0.006458 	Lval: 0.005058
Epoch: 10 	Ltrain: 0.005953 	Lval: 0.005584
Epoch 00013: reducing learning rate of group 0 to 5.7838e-04.
Epoch: 15 	Ltrain: 0.004477 	Lval: 0.004335
Epoch: 20 	Ltrain: 0.004444 	Lval: 0.004439
Epoch: 25 	Ltrain: 0.004312 	Lval: 0.004190
Epoch: 30 	Ltrain: 0.004253 	Lval: 0.004123
Epoch: 35 	Ltrain: 0.004143 	Lval: 0.004021
Epoch 00039: reducing learning rate of group 0 to 5.7838e-05.
Epoch: 40 	Ltrain: 0.004100 	Lval: 0.004057
Epoch: 45 	Ltrain: 0.004074 	Lval: 0.003999
Epoch: 50 	Ltrain: 0.003953 	Lval: 0.003966
Epoch 00054: reducing learning rate of group 0 to 5.7838e-06.
Epoch: 55 	Ltrain: 0.003948 	Lval: 0.003973
Epoch: 60 	Ltrain: 0.003954 	Lval: 0.003979
Epoch: 65 	Ltrain: 0.004041 	Lval: 0.003982
Epoch 00066: reducing learning rate of group 0 to 5.7838e-07.
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.003966


	Fold 3/5
Epoch: 1 	Ltrain: 0.040667 	Lval: 0.010841
Epoch: 5 	Ltrain: 0.005578 	Lval: 0.004803
Epoch: 10 	Ltrain: 0.004843 	Lval: 0.004724
Epoch: 15 	Ltrain: 0.004552 	Lval: 0.005126
Epoch 00017: reducing learning rate of group 0 to 5.7838e-04.
Epoch: 20 	Ltrain: 0.003664 	Lval: 0.003714
Epoch: 25 	Ltrain: 0.003572 	Lval: 0.003700
Epoch: 30 	Ltrain: 0.003515 	Lval: 0.003654
Epoch 00031: reducing learning rate of group 0 to 5.7838e-05.
Epoch: 35 	Ltrain: 0.003406 	Lval: 0.003548
Epoch: 40 	Ltrain: 0.003408 	Lval: 0.003536
Epoch 00044: reducing learning rate of group 0 to 5.7838e-06.
Epoch: 45 	Ltrain: 0.003423 	Lval: 0.003539
Epoch: 50 	Ltrain: 0.003382 	Lval: 0.003541
Epoch: 55 	Ltrain: 0.003400 	Lval: 0.003539
Epoch 00056: reducing learning rate of group 0 to 5.7838e-07.
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.003536


	Fold 4/5
Epoch: 1 	Ltrain: 0.035023 	Lval: 0.011443
Epoch: 5 	Ltrain: 0.005051 	Lval: 0.006245
Epoch: 10 	Ltrain: 0.004397 	Lval: 0.004528
Epoch: 15 	Ltrain: 0.003991 	Lval: 0.003845
Epoch 00019: reducing learning rate of group 0 to 5.7838e-04.
Epoch: 20 	Ltrain: 0.003500 	Lval: 0.003509
Epoch: 25 	Ltrain: 0.003209 	Lval: 0.003459
Epoch: 30 	Ltrain: 0.003150 	Lval: 0.003354
Epoch: 35 	Ltrain: 0.003148 	Lval: 0.003402
Epoch: 40 	Ltrain: 0.003052 	Lval: 0.003292
Epoch: 45 	Ltrain: 0.002993 	Lval: 0.003158
Epoch: 50 	Ltrain: 0.002963 	Lval: 0.003103
Epoch: 55 	Ltrain: 0.002924 	Lval: 0.003050
Epoch 00058: reducing learning rate of group 0 to 5.7838e-05.
Epoch: 60 	Ltrain: 0.002769 	Lval: 0.002958
Epoch: 65 	Ltrain: 0.002748 	Lval: 0.002940
Epoch: 70 	Ltrain: 0.002738 	Lval: 0.002923
Epoch: 75 	Ltrain: 0.002741 	Lval: 0.002911
Epoch: 80 	Ltrain: 0.002729 	Lval: 0.002895
Epoch: 85 	Ltrain: 0.002726 	Lval: 0.002888
Epoch: 90 	Ltrain: 0.002705 	Lval: 0.002870
Epoch: 95 	Ltrain: 0.002708 	Lval: 0.002881
Epoch 00096: reducing learning rate of group 0 to 5.7838e-06.
Epoch: 100 	Ltrain: 0.002669 	Lval: 0.002858
Epoch: 105 	Ltrain: 0.002678 	Lval: 0.002856
Epoch 00108: reducing learning rate of group 0 to 5.7838e-07.
Epoch: 110 	Ltrain: 0.002669 	Lval: 0.002858
EarlyStopper: stopping at epoch 110 with best_val_loss = 0.002855


	Fold 5/5
Epoch: 1 	Ltrain: 0.031955 	Lval: 0.008893
Epoch: 5 	Ltrain: 0.005387 	Lval: 0.005254
Epoch: 10 	Ltrain: 0.004797 	Lval: 0.004705
Epoch 00012: reducing learning rate of group 0 to 5.7838e-04.
Epoch: 15 	Ltrain: 0.003730 	Lval: 0.004144
Epoch: 20 	Ltrain: 0.003643 	Lval: 0.004056
Epoch 00025: reducing learning rate of group 0 to 5.7838e-05.
Epoch: 25 	Ltrain: 0.003568 	Lval: 0.004019
Epoch: 30 	Ltrain: 0.003483 	Lval: 0.003923
Epoch: 35 	Ltrain: 0.003473 	Lval: 0.003913
Epoch: 40 	Ltrain: 0.003463 	Lval: 0.003890
Epoch: 45 	Ltrain: 0.003450 	Lval: 0.003877
Epoch: 50 	Ltrain: 0.003434 	Lval: 0.003874
Epoch 00055: reducing learning rate of group 0 to 5.7838e-06.
Epoch: 55 	Ltrain: 0.003449 	Lval: 0.003861
Epoch: 60 	Ltrain: 0.003417 	Lval: 0.003853
Epoch: 65 	Ltrain: 0.003409 	Lval: 0.003852
Epoch: 70 	Ltrain: 0.003422 	Lval: 0.003849
Epoch: 75 	Ltrain: 0.003422 	Lval: 0.003847
Epoch: 80 	Ltrain: 0.003412 	Lval: 0.003845
Epoch 00082: reducing learning rate of group 0 to 5.7838e-07.
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.003851

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00799502862705976
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.274986886006826e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.178433 	Lval: 0.039905
Epoch: 5 	Ltrain: 0.016567 	Lval: 0.014409
Epoch: 10 	Ltrain: 0.008090 	Lval: 0.006904
Epoch: 15 	Ltrain: 0.005418 	Lval: 0.004932
Epoch: 20 	Ltrain: 0.005588 	Lval: 0.006672
Epoch 00021: reducing learning rate of group 0 to 7.9950e-04.
Epoch: 25 	Ltrain: 0.004513 	Lval: 0.004302
Epoch: 30 	Ltrain: 0.004511 	Lval: 0.004283
Epoch 00033: reducing learning rate of group 0 to 7.9950e-05.
Epoch: 35 	Ltrain: 0.004189 	Lval: 0.004172
Epoch: 40 	Ltrain: 0.004391 	Lval: 0.004174
Epoch: 45 	Ltrain: 0.004472 	Lval: 0.004167
Epoch: 50 	Ltrain: 0.004141 	Lval: 0.004154
Epoch: 55 	Ltrain: 0.004181 	Lval: 0.004151
Epoch 00059: reducing learning rate of group 0 to 7.9950e-06.
Epoch: 60 	Ltrain: 0.004084 	Lval: 0.004157
Epoch: 65 	Ltrain: 0.004191 	Lval: 0.004152
Epoch: 70 	Ltrain: 0.004323 	Lval: 0.004150
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.004152


	Fold 2/5
Epoch: 1 	Ltrain: 0.111936 	Lval: 0.028978
Epoch: 5 	Ltrain: 0.011573 	Lval: 0.010405
Epoch: 10 	Ltrain: 0.005741 	Lval: 0.004753
Epoch: 15 	Ltrain: 0.004656 	Lval: 0.004678
Epoch: 20 	Ltrain: 0.004259 	Lval: 0.003937
Epoch: 25 	Ltrain: 0.004017 	Lval: 0.003831
Epoch 00026: reducing learning rate of group 0 to 7.9950e-04.
Epoch: 30 	Ltrain: 0.003487 	Lval: 0.003393
Epoch: 35 	Ltrain: 0.003402 	Lval: 0.003342
Epoch: 40 	Ltrain: 0.003348 	Lval: 0.003277
Epoch 00045: reducing learning rate of group 0 to 7.9950e-05.
Epoch: 45 	Ltrain: 0.003349 	Lval: 0.003333
Epoch: 50 	Ltrain: 0.003274 	Lval: 0.003279
Epoch: 55 	Ltrain: 0.003199 	Lval: 0.003246
Epoch 00057: reducing learning rate of group 0 to 7.9950e-06.
Epoch: 60 	Ltrain: 0.003189 	Lval: 0.003245
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.003240


	Fold 3/5
Epoch: 1 	Ltrain: 0.146843 	Lval: 0.019066
Epoch: 5 	Ltrain: 0.010716 	Lval: 0.008660
Epoch: 10 	Ltrain: 0.005141 	Lval: 0.004574
Epoch: 15 	Ltrain: 0.004910 	Lval: 0.004638
Epoch 00017: reducing learning rate of group 0 to 7.9950e-04.
Epoch: 20 	Ltrain: 0.004083 	Lval: 0.003984
Epoch: 25 	Ltrain: 0.003965 	Lval: 0.003918
Epoch: 30 	Ltrain: 0.003897 	Lval: 0.003923
Epoch: 35 	Ltrain: 0.003923 	Lval: 0.004223
Epoch 00040: reducing learning rate of group 0 to 7.9950e-05.
Epoch: 40 	Ltrain: 0.003875 	Lval: 0.003848
Epoch: 45 	Ltrain: 0.003787 	Lval: 0.003837
Epoch: 50 	Ltrain: 0.003771 	Lval: 0.003786
Epoch: 55 	Ltrain: 0.003731 	Lval: 0.003825
Epoch 00058: reducing learning rate of group 0 to 7.9950e-06.
Epoch: 60 	Ltrain: 0.003738 	Lval: 0.003799
Epoch: 65 	Ltrain: 0.003724 	Lval: 0.003798
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.003786


	Fold 4/5
Epoch: 1 	Ltrain: 0.082342 	Lval: 0.012962
Epoch: 5 	Ltrain: 0.004982 	Lval: 0.004731
Epoch: 10 	Ltrain: 0.004691 	Lval: 0.004177
Epoch: 15 	Ltrain: 0.003632 	Lval: 0.004264
Epoch: 20 	Ltrain: 0.003183 	Lval: 0.003587
Epoch: 25 	Ltrain: 0.002863 	Lval: 0.002943
Epoch: 30 	Ltrain: 0.002553 	Lval: 0.002524
Epoch: 35 	Ltrain: 0.002414 	Lval: 0.002493
Epoch: 40 	Ltrain: 0.001980 	Lval: 0.002231
Epoch 00041: reducing learning rate of group 0 to 7.9950e-04.
Epoch: 45 	Ltrain: 0.001326 	Lval: 0.001439
Epoch: 50 	Ltrain: 0.001205 	Lval: 0.001326
Epoch: 55 	Ltrain: 0.001119 	Lval: 0.001224
Epoch: 60 	Ltrain: 0.001037 	Lval: 0.001128
Epoch: 65 	Ltrain: 0.000959 	Lval: 0.001041
Epoch: 70 	Ltrain: 0.000888 	Lval: 0.000959
Epoch: 75 	Ltrain: 0.000815 	Lval: 0.000881
Epoch: 80 	Ltrain: 0.000747 	Lval: 0.000803
Epoch: 85 	Ltrain: 0.000679 	Lval: 0.000721
Epoch: 90 	Ltrain: 0.000645 	Lval: 0.000685
Epoch: 95 	Ltrain: 0.000580 	Lval: 0.000619
Epoch: 100 	Ltrain: 0.000537 	Lval: 0.000578
Epoch: 105 	Ltrain: 0.000508 	Lval: 0.000560
Epoch: 110 	Ltrain: 0.000462 	Lval: 0.000480
Epoch: 115 	Ltrain: 0.000441 	Lval: 0.000475
Epoch: 120 	Ltrain: 0.000418 	Lval: 0.000447
Epoch 00121: reducing learning rate of group 0 to 7.9950e-05.
Epoch: 125 	Ltrain: 0.000321 	Lval: 0.000340
Epoch: 130 	Ltrain: 0.000311 	Lval: 0.000329
Epoch: 135 	Ltrain: 0.000304 	Lval: 0.000322
Epoch: 140 	Ltrain: 0.000299 	Lval: 0.000316
Epoch: 145 	Ltrain: 0.000294 	Lval: 0.000310
Epoch: 150 	Ltrain: 0.000289 	Lval: 0.000305
Epoch: 155 	Ltrain: 0.000284 	Lval: 0.000299
Epoch: 160 	Ltrain: 0.000279 	Lval: 0.000293
Epoch: 165 	Ltrain: 0.000274 	Lval: 0.000287
Epoch: 170 	Ltrain: 0.000268 	Lval: 0.000281
Epoch: 175 	Ltrain: 0.000262 	Lval: 0.000274
Epoch: 180 	Ltrain: 0.000257 	Lval: 0.000268
Epoch: 185 	Ltrain: 0.000251 	Lval: 0.000261
Epoch: 190 	Ltrain: 0.000246 	Lval: 0.000255
Epoch: 195 	Ltrain: 0.000239 	Lval: 0.000248
Epoch: 200 	Ltrain: 0.000233 	Lval: 0.000240
Epoch: 205 	Ltrain: 0.000227 	Lval: 0.000234
Epoch: 210 	Ltrain: 0.000222 	Lval: 0.000227
Epoch: 215 	Ltrain: 0.000215 	Lval: 0.000220
Epoch: 220 	Ltrain: 0.000209 	Lval: 0.000213
Epoch: 225 	Ltrain: 0.000202 	Lval: 0.000205
Epoch: 230 	Ltrain: 0.000196 	Lval: 0.000199
Epoch: 235 	Ltrain: 0.000190 	Lval: 0.000192
Epoch: 240 	Ltrain: 0.000184 	Lval: 0.000185
Epoch: 245 	Ltrain: 0.000179 	Lval: 0.000178
Epoch: 250 	Ltrain: 0.000173 	Lval: 0.000173
Epoch: 255 	Ltrain: 0.000170 	Lval: 0.000169
Epoch: 260 	Ltrain: 0.000162 	Lval: 0.000161
Epoch: 265 	Ltrain: 0.000158 	Lval: 0.000156
Epoch: 270 	Ltrain: 0.000154 	Lval: 0.000154
Epoch: 275 	Ltrain: 0.000151 	Lval: 0.000150
Epoch 00277: reducing learning rate of group 0 to 7.9950e-06.
Epoch: 280 	Ltrain: 0.000141 	Lval: 0.000139
Epoch: 285 	Ltrain: 0.000139 	Lval: 0.000137
Epoch: 290 	Ltrain: 0.000138 	Lval: 0.000136
Epoch: 295 	Ltrain: 0.000137 	Lval: 0.000136
EarlyStopper: stopping at epoch 296 with best_val_loss = 0.000142


	Fold 5/5
Epoch: 1 	Ltrain: 0.040375 	Lval: 0.014122
Epoch: 5 	Ltrain: 0.004970 	Lval: 0.005151
Epoch: 10 	Ltrain: 0.003842 	Lval: 0.004437
Epoch: 15 	Ltrain: 0.003581 	Lval: 0.004529
Epoch 00017: reducing learning rate of group 0 to 7.9950e-04.
Epoch: 20 	Ltrain: 0.002836 	Lval: 0.003263
Epoch: 25 	Ltrain: 0.002724 	Lval: 0.003124
Epoch: 30 	Ltrain: 0.002612 	Lval: 0.002983
Epoch: 35 	Ltrain: 0.002546 	Lval: 0.002845
Epoch: 40 	Ltrain: 0.002455 	Lval: 0.002850
Epoch: 45 	Ltrain: 0.002286 	Lval: 0.002663
Epoch: 50 	Ltrain: 0.002190 	Lval: 0.002576
Epoch: 55 	Ltrain: 0.002152 	Lval: 0.002429
Epoch: 60 	Ltrain: 0.001968 	Lval: 0.002256
Epoch: 65 	Ltrain: 0.001884 	Lval: 0.002166
Epoch: 70 	Ltrain: 0.001782 	Lval: 0.002048
Epoch: 75 	Ltrain: 0.001640 	Lval: 0.001919
Epoch: 80 	Ltrain: 0.001540 	Lval: 0.001732
Epoch: 85 	Ltrain: 0.001380 	Lval: 0.001656
Epoch: 90 	Ltrain: 0.001279 	Lval: 0.001434
Epoch: 95 	Ltrain: 0.001217 	Lval: 0.001336
Epoch: 100 	Ltrain: 0.001229 	Lval: 0.001336
Epoch: 105 	Ltrain: 0.001043 	Lval: 0.001111
Epoch: 110 	Ltrain: 0.000909 	Lval: 0.001069
Epoch: 115 	Ltrain: 0.000915 	Lval: 0.001053
Epoch: 120 	Ltrain: 0.000824 	Lval: 0.000790
Epoch: 125 	Ltrain: 0.000715 	Lval: 0.000750
Epoch 00129: reducing learning rate of group 0 to 7.9950e-05.
Epoch: 130 	Ltrain: 0.000681 	Lval: 0.000636
Epoch: 135 	Ltrain: 0.000508 	Lval: 0.000551
Epoch: 140 	Ltrain: 0.000485 	Lval: 0.000527
Epoch: 145 	Ltrain: 0.000472 	Lval: 0.000510
Epoch: 150 	Ltrain: 0.000461 	Lval: 0.000498
Epoch: 155 	Ltrain: 0.000451 	Lval: 0.000485
Epoch: 160 	Ltrain: 0.000442 	Lval: 0.000473
Epoch: 165 	Ltrain: 0.000431 	Lval: 0.000459
Epoch: 170 	Ltrain: 0.000420 	Lval: 0.000448
Epoch: 175 	Ltrain: 0.000410 	Lval: 0.000435
Epoch: 180 	Ltrain: 0.000401 	Lval: 0.000424
Epoch: 185 	Ltrain: 0.000389 	Lval: 0.000410
Epoch: 190 	Ltrain: 0.000379 	Lval: 0.000397
Epoch: 195 	Ltrain: 0.000369 	Lval: 0.000385
Epoch: 200 	Ltrain: 0.000357 	Lval: 0.000372
Epoch: 205 	Ltrain: 0.000346 	Lval: 0.000359
Epoch: 210 	Ltrain: 0.000335 	Lval: 0.000348
Epoch: 215 	Ltrain: 0.000326 	Lval: 0.000335
Epoch: 220 	Ltrain: 0.000313 	Lval: 0.000320
Epoch: 225 	Ltrain: 0.000304 	Lval: 0.000311
Epoch: 230 	Ltrain: 0.000293 	Lval: 0.000297
Epoch: 235 	Ltrain: 0.000282 	Lval: 0.000286
Epoch: 240 	Ltrain: 0.000272 	Lval: 0.000275
Epoch: 245 	Ltrain: 0.000263 	Lval: 0.000266
Epoch: 250 	Ltrain: 0.000253 	Lval: 0.000252
Epoch: 255 	Ltrain: 0.000246 	Lval: 0.000244
Epoch: 260 	Ltrain: 0.000238 	Lval: 0.000234
Epoch: 265 	Ltrain: 0.000228 	Lval: 0.000225
Epoch: 270 	Ltrain: 0.000218 	Lval: 0.000214
Epoch: 275 	Ltrain: 0.000211 	Lval: 0.000209
Epoch: 280 	Ltrain: 0.000204 	Lval: 0.000200
Epoch: 285 	Ltrain: 0.000203 	Lval: 0.000195
Epoch: 290 	Ltrain: 0.000191 	Lval: 0.000188
Epoch: 295 	Ltrain: 0.000183 	Lval: 0.000178
Epoch: 300 	Ltrain: 0.000179 	Lval: 0.000171
Epoch: 305 	Ltrain: 0.000173 	Lval: 0.000171
Epoch 00307: reducing learning rate of group 0 to 7.9950e-06.
Epoch: 310 	Ltrain: 0.000159 	Lval: 0.000153
Epoch: 315 	Ltrain: 0.000157 	Lval: 0.000152
Epoch: 320 	Ltrain: 0.000156 	Lval: 0.000151
Epoch: 325 	Ltrain: 0.000155 	Lval: 0.000150
EarlyStopper: stopping at epoch 326 with best_val_loss = 0.000156

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00011789199787394103
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.358337147909115e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.059585 	Lval: 0.045717
Epoch: 5 	Ltrain: 0.014795 	Lval: 0.014738
Epoch: 10 	Ltrain: 0.013960 	Lval: 0.013475
Epoch: 15 	Ltrain: 0.012890 	Lval: 0.012525
Epoch: 20 	Ltrain: 0.011527 	Lval: 0.010716
Epoch: 25 	Ltrain: 0.009643 	Lval: 0.009757
Epoch: 30 	Ltrain: 0.009461 	Lval: 0.009300
Epoch: 35 	Ltrain: 0.009109 	Lval: 0.008863
Epoch: 40 	Ltrain: 0.008533 	Lval: 0.008483
Epoch: 45 	Ltrain: 0.009201 	Lval: 0.008087
Epoch: 50 	Ltrain: 0.007748 	Lval: 0.007690
Epoch: 55 	Ltrain: 0.007412 	Lval: 0.007332
Epoch: 60 	Ltrain: 0.007280 	Lval: 0.007062
Epoch: 65 	Ltrain: 0.006766 	Lval: 0.006751
Epoch: 70 	Ltrain: 0.006546 	Lval: 0.006580
Epoch: 75 	Ltrain: 0.006333 	Lval: 0.006080
Epoch: 80 	Ltrain: 0.005985 	Lval: 0.005852
Epoch: 85 	Ltrain: 0.005760 	Lval: 0.005804
Epoch: 90 	Ltrain: 0.005750 	Lval: 0.005509
Epoch: 95 	Ltrain: 0.006008 	Lval: 0.005729
Epoch: 100 	Ltrain: 0.005645 	Lval: 0.005280
Epoch: 105 	Ltrain: 0.005287 	Lval: 0.005172
Epoch: 110 	Ltrain: 0.005533 	Lval: 0.005081
Epoch: 115 	Ltrain: 0.005225 	Lval: 0.005026
Epoch: 120 	Ltrain: 0.005205 	Lval: 0.004931
Epoch: 125 	Ltrain: 0.005183 	Lval: 0.004950
Epoch: 130 	Ltrain: 0.005398 	Lval: 0.005311
Epoch 00132: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 135 	Ltrain: 0.004848 	Lval: 0.004797
Epoch: 140 	Ltrain: 0.004810 	Lval: 0.004794
Epoch: 145 	Ltrain: 0.004760 	Lval: 0.004795
Epoch 00147: reducing learning rate of group 0 to 1.1789e-06.
EarlyStopper: stopping at epoch 148 with best_val_loss = 0.004798


	Fold 2/5
Epoch: 1 	Ltrain: 0.056359 	Lval: 0.029104
Epoch: 5 	Ltrain: 0.014727 	Lval: 0.013825
Epoch: 10 	Ltrain: 0.012658 	Lval: 0.011676
Epoch: 15 	Ltrain: 0.009806 	Lval: 0.009666
Epoch: 20 	Ltrain: 0.008765 	Lval: 0.008513
Epoch: 25 	Ltrain: 0.008270 	Lval: 0.007826
Epoch: 30 	Ltrain: 0.007494 	Lval: 0.007210
Epoch: 35 	Ltrain: 0.006907 	Lval: 0.006899
Epoch: 40 	Ltrain: 0.006811 	Lval: 0.006270
Epoch: 45 	Ltrain: 0.006151 	Lval: 0.005892
Epoch: 50 	Ltrain: 0.005960 	Lval: 0.005524
Epoch: 55 	Ltrain: 0.005752 	Lval: 0.005533
Epoch: 60 	Ltrain: 0.005494 	Lval: 0.005253
Epoch: 65 	Ltrain: 0.005366 	Lval: 0.005188
Epoch: 70 	Ltrain: 0.005197 	Lval: 0.005042
Epoch 00073: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 75 	Ltrain: 0.004990 	Lval: 0.004833
Epoch: 80 	Ltrain: 0.004942 	Lval: 0.004843
Epoch 00085: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 85 	Ltrain: 0.004961 	Lval: 0.004787
Epoch: 90 	Ltrain: 0.004959 	Lval: 0.004793
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.004771


	Fold 3/5
Epoch: 1 	Ltrain: 0.032323 	Lval: 0.014805
Epoch: 5 	Ltrain: 0.014163 	Lval: 0.012448
Epoch: 10 	Ltrain: 0.009913 	Lval: 0.009383
Epoch: 15 	Ltrain: 0.008778 	Lval: 0.008413
Epoch: 20 	Ltrain: 0.007856 	Lval: 0.007561
Epoch: 25 	Ltrain: 0.007071 	Lval: 0.006766
Epoch: 30 	Ltrain: 0.006471 	Lval: 0.006052
Epoch: 35 	Ltrain: 0.006182 	Lval: 0.005791
Epoch: 40 	Ltrain: 0.005626 	Lval: 0.005305
Epoch: 45 	Ltrain: 0.005321 	Lval: 0.005229
Epoch: 50 	Ltrain: 0.005108 	Lval: 0.004828
Epoch: 55 	Ltrain: 0.004925 	Lval: 0.004715
Epoch 00057: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 60 	Ltrain: 0.004783 	Lval: 0.004551
Epoch: 65 	Ltrain: 0.004791 	Lval: 0.004617
Epoch: 70 	Ltrain: 0.004776 	Lval: 0.004590
Epoch 00073: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 75 	Ltrain: 0.004746 	Lval: 0.004568
Epoch: 80 	Ltrain: 0.004716 	Lval: 0.004567
Epoch 00085: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 85 	Ltrain: 0.004769 	Lval: 0.004568
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.004537


	Fold 4/5
Epoch: 1 	Ltrain: 0.091439 	Lval: 0.029871
Epoch: 5 	Ltrain: 0.013933 	Lval: 0.013512
Epoch: 10 	Ltrain: 0.009644 	Lval: 0.009761
Epoch: 15 	Ltrain: 0.008022 	Lval: 0.008112
Epoch: 20 	Ltrain: 0.007018 	Lval: 0.007311
Epoch: 25 	Ltrain: 0.006391 	Lval: 0.006449
Epoch: 30 	Ltrain: 0.005852 	Lval: 0.005985
Epoch: 35 	Ltrain: 0.005465 	Lval: 0.005373
Epoch: 40 	Ltrain: 0.005207 	Lval: 0.005007
Epoch: 45 	Ltrain: 0.004994 	Lval: 0.004862
Epoch: 50 	Ltrain: 0.004806 	Lval: 0.004659
Epoch: 55 	Ltrain: 0.004604 	Lval: 0.004650
Epoch: 60 	Ltrain: 0.004551 	Lval: 0.004538
Epoch: 65 	Ltrain: 0.004558 	Lval: 0.004512
Epoch: 70 	Ltrain: 0.004383 	Lval: 0.004410
Epoch 00074: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 75 	Ltrain: 0.004260 	Lval: 0.004410
Epoch: 80 	Ltrain: 0.004301 	Lval: 0.004408
Epoch: 85 	Ltrain: 0.004264 	Lval: 0.004415
Epoch 00090: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 90 	Ltrain: 0.004278 	Lval: 0.004394
Epoch: 95 	Ltrain: 0.004240 	Lval: 0.004392
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.004387


	Fold 5/5
Epoch: 1 	Ltrain: 0.026330 	Lval: 0.015531
Epoch: 5 	Ltrain: 0.012546 	Lval: 0.012673
Epoch: 10 	Ltrain: 0.008730 	Lval: 0.009232
Epoch: 15 	Ltrain: 0.007486 	Lval: 0.008236
Epoch: 20 	Ltrain: 0.006751 	Lval: 0.007095
Epoch: 25 	Ltrain: 0.006041 	Lval: 0.006385
Epoch: 30 	Ltrain: 0.005495 	Lval: 0.005664
Epoch: 35 	Ltrain: 0.005092 	Lval: 0.005178
Epoch: 40 	Ltrain: 0.004722 	Lval: 0.004927
Epoch: 45 	Ltrain: 0.004548 	Lval: 0.004678
Epoch: 50 	Ltrain: 0.004514 	Lval: 0.004881
Epoch 00051: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 55 	Ltrain: 0.004352 	Lval: 0.004607
Epoch: 60 	Ltrain: 0.004358 	Lval: 0.004593
Epoch 00065: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 65 	Ltrain: 0.004335 	Lval: 0.004608
Epoch: 70 	Ltrain: 0.004320 	Lval: 0.004595
Epoch: 75 	Ltrain: 0.004321 	Lval: 0.004595
Epoch 00077: reducing learning rate of group 0 to 1.1789e-07.
EarlyStopper: stopping at epoch 76 with best_val_loss = 0.004585

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0035440872204644622
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.777512007924671e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.057280 	Lval: 0.016547
Epoch: 5 	Ltrain: 0.012182 	Lval: 0.010029
Epoch: 10 	Ltrain: 0.008593 	Lval: 0.008130
Epoch: 15 	Ltrain: 0.006142 	Lval: 0.005167
Epoch: 20 	Ltrain: 0.005525 	Lval: 0.005186
Epoch: 25 	Ltrain: 0.006578 	Lval: 0.006421
Epoch 00026: reducing learning rate of group 0 to 3.5441e-04.
Epoch: 30 	Ltrain: 0.004968 	Lval: 0.004648
Epoch: 35 	Ltrain: 0.004970 	Lval: 0.004433
Epoch: 40 	Ltrain: 0.004414 	Lval: 0.004420
Epoch: 45 	Ltrain: 0.004752 	Lval: 0.004350
Epoch: 50 	Ltrain: 0.004479 	Lval: 0.004415
Epoch 00051: reducing learning rate of group 0 to 3.5441e-05.
Epoch: 55 	Ltrain: 0.004102 	Lval: 0.004294
Epoch: 60 	Ltrain: 0.004297 	Lval: 0.004277
Epoch 00063: reducing learning rate of group 0 to 3.5441e-06.
Epoch: 65 	Ltrain: 0.004739 	Lval: 0.004285
Epoch: 70 	Ltrain: 0.005184 	Lval: 0.004285
Epoch 00075: reducing learning rate of group 0 to 3.5441e-07.
Epoch: 75 	Ltrain: 0.004312 	Lval: 0.004284
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.004280


	Fold 2/5
Epoch: 1 	Ltrain: 0.025225 	Lval: 0.012683
Epoch: 5 	Ltrain: 0.009040 	Lval: 0.006085
Epoch: 10 	Ltrain: 0.005231 	Lval: 0.004603
Epoch 00014: reducing learning rate of group 0 to 3.5441e-04.
Epoch: 15 	Ltrain: 0.004989 	Lval: 0.004285
Epoch: 20 	Ltrain: 0.004555 	Lval: 0.004220
Epoch: 25 	Ltrain: 0.004360 	Lval: 0.004211
Epoch: 30 	Ltrain: 0.004418 	Lval: 0.004223
Epoch 00035: reducing learning rate of group 0 to 3.5441e-05.
Epoch: 35 	Ltrain: 0.004394 	Lval: 0.004169
Epoch: 40 	Ltrain: 0.004313 	Lval: 0.004138
Epoch: 45 	Ltrain: 0.004144 	Lval: 0.004133
Epoch 00048: reducing learning rate of group 0 to 3.5441e-06.
Epoch: 50 	Ltrain: 0.004271 	Lval: 0.004149
Epoch: 55 	Ltrain: 0.004280 	Lval: 0.004129
Epoch 00060: reducing learning rate of group 0 to 3.5441e-07.
Epoch: 60 	Ltrain: 0.004391 	Lval: 0.004124
Epoch: 65 	Ltrain: 0.004308 	Lval: 0.004123
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.004116


	Fold 3/5
Epoch: 1 	Ltrain: 0.027934 	Lval: 0.013105
Epoch: 5 	Ltrain: 0.005875 	Lval: 0.004810
Epoch: 10 	Ltrain: 0.004982 	Lval: 0.004581
Epoch: 15 	Ltrain: 0.004659 	Lval: 0.004773
Epoch: 20 	Ltrain: 0.004251 	Lval: 0.004406
Epoch: 25 	Ltrain: 0.003896 	Lval: 0.003705
Epoch 00029: reducing learning rate of group 0 to 3.5441e-04.
Epoch: 30 	Ltrain: 0.003764 	Lval: 0.003425
Epoch: 35 	Ltrain: 0.003349 	Lval: 0.003407
Epoch: 40 	Ltrain: 0.003304 	Lval: 0.003402
Epoch: 45 	Ltrain: 0.003207 	Lval: 0.003327
Epoch 00048: reducing learning rate of group 0 to 3.5441e-05.
Epoch: 50 	Ltrain: 0.003195 	Lval: 0.003299
Epoch: 55 	Ltrain: 0.003113 	Lval: 0.003286
Epoch 00060: reducing learning rate of group 0 to 3.5441e-06.
Epoch: 60 	Ltrain: 0.003110 	Lval: 0.003303
Epoch: 65 	Ltrain: 0.003122 	Lval: 0.003298
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.003279


	Fold 4/5
Epoch: 1 	Ltrain: 0.019331 	Lval: 0.013525
Epoch: 5 	Ltrain: 0.005268 	Lval: 0.005146
Epoch: 10 	Ltrain: 0.004358 	Lval: 0.004058
Epoch: 15 	Ltrain: 0.004232 	Lval: 0.004265
Epoch: 20 	Ltrain: 0.003792 	Lval: 0.003905
Epoch 00021: reducing learning rate of group 0 to 3.5441e-04.
Epoch: 25 	Ltrain: 0.003263 	Lval: 0.003494
Epoch: 30 	Ltrain: 0.003224 	Lval: 0.003641
Epoch: 35 	Ltrain: 0.003140 	Lval: 0.003372
Epoch: 40 	Ltrain: 0.003083 	Lval: 0.003355
Epoch: 45 	Ltrain: 0.003077 	Lval: 0.003231
Epoch: 50 	Ltrain: 0.002930 	Lval: 0.003347
Epoch: 55 	Ltrain: 0.002880 	Lval: 0.003137
Epoch: 60 	Ltrain: 0.002822 	Lval: 0.003187
Epoch: 65 	Ltrain: 0.002779 	Lval: 0.003082
Epoch 00070: reducing learning rate of group 0 to 3.5441e-05.
Epoch: 70 	Ltrain: 0.002751 	Lval: 0.003004
Epoch: 75 	Ltrain: 0.002615 	Lval: 0.002915
Epoch: 80 	Ltrain: 0.002581 	Lval: 0.002913
Epoch 00082: reducing learning rate of group 0 to 3.5441e-06.
Epoch: 85 	Ltrain: 0.002577 	Lval: 0.002912
Epoch: 90 	Ltrain: 0.002583 	Lval: 0.002908
Epoch 00094: reducing learning rate of group 0 to 3.5441e-07.
EarlyStopper: stopping at epoch 93 with best_val_loss = 0.002907


	Fold 5/5
Epoch: 1 	Ltrain: 0.022903 	Lval: 0.011755
Epoch: 5 	Ltrain: 0.005400 	Lval: 0.005603
Epoch: 10 	Ltrain: 0.004331 	Lval: 0.005128
Epoch 00015: reducing learning rate of group 0 to 3.5441e-04.
Epoch: 15 	Ltrain: 0.004096 	Lval: 0.004661
Epoch: 20 	Ltrain: 0.003735 	Lval: 0.004044
Epoch: 25 	Ltrain: 0.003624 	Lval: 0.004064
Epoch: 30 	Ltrain: 0.003577 	Lval: 0.003987
Epoch: 35 	Ltrain: 0.003496 	Lval: 0.003899
Epoch: 40 	Ltrain: 0.003381 	Lval: 0.003808
Epoch: 45 	Ltrain: 0.003337 	Lval: 0.003749
Epoch: 50 	Ltrain: 0.003346 	Lval: 0.003752
Epoch: 55 	Ltrain: 0.003248 	Lval: 0.003614
Epoch 00057: reducing learning rate of group 0 to 3.5441e-05.
Epoch: 60 	Ltrain: 0.003117 	Lval: 0.003574
Epoch: 65 	Ltrain: 0.003141 	Lval: 0.003565
Epoch: 70 	Ltrain: 0.003128 	Lval: 0.003558
Epoch: 75 	Ltrain: 0.003111 	Lval: 0.003551
Epoch 00078: reducing learning rate of group 0 to 3.5441e-06.
Epoch: 80 	Ltrain: 0.003129 	Lval: 0.003545
Epoch: 85 	Ltrain: 0.003095 	Lval: 0.003543
Epoch 00090: reducing learning rate of group 0 to 3.5441e-07.
Epoch: 90 	Ltrain: 0.003120 	Lval: 0.003544
Epoch: 95 	Ltrain: 0.003094 	Lval: 0.003544
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.003546

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0024939335726024726
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.218964723700113e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 25
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.022799 	Lval: 0.014349
Epoch: 5 	Ltrain: 0.011813 	Lval: 0.010899
Epoch: 10 	Ltrain: 0.007073 	Lval: 0.006952
Epoch: 15 	Ltrain: 0.005985 	Lval: 0.005083
Epoch: 20 	Ltrain: 0.004826 	Lval: 0.004769
Epoch: 25 	Ltrain: 0.004712 	Lval: 0.004523
Epoch: 30 	Ltrain: 0.004738 	Lval: 0.004441
Epoch: 35 	Ltrain: 0.005332 	Lval: 0.004600
Epoch: 40 	Ltrain: 0.004157 	Lval: 0.004498
Epoch: 45 	Ltrain: 0.004325 	Lval: 0.004238
Epoch 00050: reducing learning rate of group 0 to 2.4939e-04.
Epoch: 50 	Ltrain: 0.004225 	Lval: 0.004088
Epoch: 55 	Ltrain: 0.004052 	Lval: 0.003694
Epoch: 60 	Ltrain: 0.003690 	Lval: 0.003657
Epoch: 65 	Ltrain: 0.003801 	Lval: 0.003640
Epoch 00069: reducing learning rate of group 0 to 2.4939e-05.
Epoch: 70 	Ltrain: 0.003687 	Lval: 0.003646
Epoch: 75 	Ltrain: 0.004005 	Lval: 0.003637
Epoch: 80 	Ltrain: 0.003638 	Lval: 0.003628
Epoch: 85 	Ltrain: 0.003775 	Lval: 0.003630
Epoch 00086: reducing learning rate of group 0 to 2.4939e-06.
Epoch: 90 	Ltrain: 0.003933 	Lval: 0.003627
Epoch: 95 	Ltrain: 0.003531 	Lval: 0.003626
Epoch 00098: reducing learning rate of group 0 to 2.4939e-07.
Epoch: 100 	Ltrain: 0.003591 	Lval: 0.003626
Epoch: 105 	Ltrain: 0.003662 	Lval: 0.003626
EarlyStopper: stopping at epoch 105 with best_val_loss = 0.003626


	Fold 2/5
Epoch: 1 	Ltrain: 0.016694 	Lval: 0.012900
Epoch: 5 	Ltrain: 0.007408 	Lval: 0.006477
Epoch: 10 	Ltrain: 0.005241 	Lval: 0.005035
Epoch: 15 	Ltrain: 0.005188 	Lval: 0.004852
Epoch: 20 	Ltrain: 0.004574 	Lval: 0.004799
Epoch: 25 	Ltrain: 0.004311 	Lval: 0.004038
Epoch: 30 	Ltrain: 0.004148 	Lval: 0.004077
Epoch: 35 	Ltrain: 0.004124 	Lval: 0.004389
Epoch 00036: reducing learning rate of group 0 to 2.4939e-04.
Epoch: 40 	Ltrain: 0.003710 	Lval: 0.003810
Epoch: 45 	Ltrain: 0.003744 	Lval: 0.003761
Epoch 00048: reducing learning rate of group 0 to 2.4939e-05.
Epoch: 50 	Ltrain: 0.003797 	Lval: 0.003748
Epoch: 55 	Ltrain: 0.003648 	Lval: 0.003734
Epoch 00060: reducing learning rate of group 0 to 2.4939e-06.
Epoch: 60 	Ltrain: 0.003734 	Lval: 0.003751
Epoch: 65 	Ltrain: 0.003701 	Lval: 0.003743
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.003706


	Fold 3/5
Epoch: 1 	Ltrain: 0.025517 	Lval: 0.013051
Epoch: 5 	Ltrain: 0.006530 	Lval: 0.005896
Epoch: 10 	Ltrain: 0.005385 	Lval: 0.004712
Epoch: 15 	Ltrain: 0.004750 	Lval: 0.004586
Epoch: 20 	Ltrain: 0.004269 	Lval: 0.004084
Epoch: 25 	Ltrain: 0.004181 	Lval: 0.003911
Epoch: 30 	Ltrain: 0.004001 	Lval: 0.003802
Epoch 00034: reducing learning rate of group 0 to 2.4939e-04.
Epoch: 35 	Ltrain: 0.003717 	Lval: 0.003725
Epoch: 40 	Ltrain: 0.003593 	Lval: 0.003797
Epoch: 45 	Ltrain: 0.003552 	Lval: 0.003644
Epoch: 50 	Ltrain: 0.003478 	Lval: 0.003582
Epoch: 55 	Ltrain: 0.003479 	Lval: 0.003543
Epoch: 60 	Ltrain: 0.003454 	Lval: 0.003575
Epoch 00062: reducing learning rate of group 0 to 2.4939e-05.
Epoch: 65 	Ltrain: 0.003364 	Lval: 0.003525
Epoch: 70 	Ltrain: 0.003399 	Lval: 0.003527
Epoch 00074: reducing learning rate of group 0 to 2.4939e-06.
Epoch: 75 	Ltrain: 0.003374 	Lval: 0.003516
Epoch: 80 	Ltrain: 0.003379 	Lval: 0.003520
Epoch: 85 	Ltrain: 0.003369 	Lval: 0.003519
Epoch 00086: reducing learning rate of group 0 to 2.4939e-07.
Epoch: 90 	Ltrain: 0.003377 	Lval: 0.003519
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.003510


	Fold 4/5
Epoch: 1 	Ltrain: 0.020924 	Lval: 0.012991
Epoch: 5 	Ltrain: 0.005932 	Lval: 0.005248
Epoch: 10 	Ltrain: 0.004601 	Lval: 0.005038
Epoch: 15 	Ltrain: 0.004133 	Lval: 0.004229
Epoch: 20 	Ltrain: 0.004007 	Lval: 0.004085
Epoch: 25 	Ltrain: 0.003949 	Lval: 0.003899
Epoch: 30 	Ltrain: 0.003688 	Lval: 0.003762
Epoch: 35 	Ltrain: 0.003339 	Lval: 0.003899
Epoch 00038: reducing learning rate of group 0 to 2.4939e-04.
Epoch: 40 	Ltrain: 0.003081 	Lval: 0.003345
Epoch: 45 	Ltrain: 0.003049 	Lval: 0.003379
Epoch: 50 	Ltrain: 0.003022 	Lval: 0.003329
Epoch: 55 	Ltrain: 0.003012 	Lval: 0.003312
Epoch: 60 	Ltrain: 0.002982 	Lval: 0.003266
Epoch 00063: reducing learning rate of group 0 to 2.4939e-05.
Epoch: 65 	Ltrain: 0.002939 	Lval: 0.003243
Epoch: 70 	Ltrain: 0.002937 	Lval: 0.003242
Epoch 00075: reducing learning rate of group 0 to 2.4939e-06.
Epoch: 75 	Ltrain: 0.002941 	Lval: 0.003235
Epoch: 80 	Ltrain: 0.002931 	Lval: 0.003236
Epoch: 85 	Ltrain: 0.002929 	Lval: 0.003235
Epoch 00087: reducing learning rate of group 0 to 2.4939e-07.
Epoch: 90 	Ltrain: 0.002928 	Lval: 0.003236
Epoch: 95 	Ltrain: 0.002927 	Lval: 0.003237
EarlyStopper: stopping at epoch 95 with best_val_loss = 0.003229


	Fold 5/5
Epoch: 1 	Ltrain: 0.015729 	Lval: 0.010744
Epoch: 5 	Ltrain: 0.005556 	Lval: 0.005318
Epoch: 10 	Ltrain: 0.004364 	Lval: 0.005124
Epoch 00015: reducing learning rate of group 0 to 2.4939e-04.
Epoch: 15 	Ltrain: 0.004239 	Lval: 0.004461
Epoch: 20 	Ltrain: 0.003775 	Lval: 0.004149
Epoch: 25 	Ltrain: 0.003746 	Lval: 0.004129
Epoch 00029: reducing learning rate of group 0 to 2.4939e-05.
Epoch: 30 	Ltrain: 0.003727 	Lval: 0.004113
Epoch: 35 	Ltrain: 0.003710 	Lval: 0.004104
Epoch: 40 	Ltrain: 0.003701 	Lval: 0.004107
Epoch 00041: reducing learning rate of group 0 to 2.4939e-06.
Epoch: 45 	Ltrain: 0.003685 	Lval: 0.004103
Epoch: 50 	Ltrain: 0.003685 	Lval: 0.004104
Epoch 00053: reducing learning rate of group 0 to 2.4939e-07.
Epoch: 55 	Ltrain: 0.003698 	Lval: 0.004104
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004113

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003703344675365825
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.4656424670572764e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.044911 	Lval: 0.025734
Epoch: 5 	Ltrain: 0.011009 	Lval: 0.012585
Epoch: 10 	Ltrain: 0.009829 	Lval: 0.007832
Epoch: 15 	Ltrain: 0.005923 	Lval: 0.005870
Epoch: 20 	Ltrain: 0.005084 	Lval: 0.005736
Epoch: 25 	Ltrain: 0.005875 	Lval: 0.004714
Epoch: 30 	Ltrain: 0.005537 	Lval: 0.004634
Epoch 00034: reducing learning rate of group 0 to 3.7033e-04.
Epoch: 35 	Ltrain: 0.005028 	Lval: 0.004775
Epoch: 40 	Ltrain: 0.005105 	Lval: 0.004795
Epoch: 45 	Ltrain: 0.004178 	Lval: 0.004455
Epoch 00050: reducing learning rate of group 0 to 3.7033e-05.
Epoch: 50 	Ltrain: 0.004598 	Lval: 0.004458
Epoch: 55 	Ltrain: 0.004519 	Lval: 0.004398
Epoch: 60 	Ltrain: 0.004695 	Lval: 0.004409
Epoch 00062: reducing learning rate of group 0 to 3.7033e-06.
Epoch: 65 	Ltrain: 0.004496 	Lval: 0.004450
Epoch: 70 	Ltrain: 0.004761 	Lval: 0.004440
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.004393


	Fold 2/5
Epoch: 1 	Ltrain: 0.038415 	Lval: 0.014074
Epoch: 5 	Ltrain: 0.007767 	Lval: 0.007816
Epoch: 10 	Ltrain: 0.006324 	Lval: 0.005185
Epoch: 15 	Ltrain: 0.005393 	Lval: 0.004784
Epoch 00017: reducing learning rate of group 0 to 3.7033e-04.
Epoch: 20 	Ltrain: 0.005182 	Lval: 0.004548
Epoch: 25 	Ltrain: 0.004857 	Lval: 0.004355
Epoch 00029: reducing learning rate of group 0 to 3.7033e-05.
Epoch: 30 	Ltrain: 0.004855 	Lval: 0.004472
Epoch: 35 	Ltrain: 0.004820 	Lval: 0.004409
Epoch: 40 	Ltrain: 0.004910 	Lval: 0.004413
Epoch 00041: reducing learning rate of group 0 to 3.7033e-06.
EarlyStopper: stopping at epoch 40 with best_val_loss = 0.004355


	Fold 3/5
Epoch: 1 	Ltrain: 0.031539 	Lval: 0.015501
Epoch: 5 	Ltrain: 0.008241 	Lval: 0.008138
Epoch: 10 	Ltrain: 0.005375 	Lval: 0.004644
Epoch: 15 	Ltrain: 0.005165 	Lval: 0.005303
Epoch: 20 	Ltrain: 0.004453 	Lval: 0.004189
Epoch 00023: reducing learning rate of group 0 to 3.7033e-04.
Epoch: 25 	Ltrain: 0.004681 	Lval: 0.003900
Epoch: 30 	Ltrain: 0.004198 	Lval: 0.003928
Epoch 00035: reducing learning rate of group 0 to 3.7033e-05.
Epoch: 35 	Ltrain: 0.004339 	Lval: 0.004017
Epoch: 40 	Ltrain: 0.004180 	Lval: 0.003981
EarlyStopper: stopping at epoch 40 with best_val_loss = 0.003900


	Fold 4/5
Epoch: 1 	Ltrain: 0.028428 	Lval: 0.010679
Epoch: 5 	Ltrain: 0.006357 	Lval: 0.006080
Epoch: 10 	Ltrain: 0.004845 	Lval: 0.004312
Epoch: 15 	Ltrain: 0.004399 	Lval: 0.004086
Epoch: 20 	Ltrain: 0.004413 	Lval: 0.003997
Epoch 00024: reducing learning rate of group 0 to 3.7033e-04.
Epoch: 25 	Ltrain: 0.004197 	Lval: 0.003957
Epoch: 30 	Ltrain: 0.004194 	Lval: 0.003964
Epoch: 35 	Ltrain: 0.004031 	Lval: 0.003949
Epoch 00036: reducing learning rate of group 0 to 3.7033e-05.
Epoch: 40 	Ltrain: 0.004057 	Lval: 0.003919
Epoch: 45 	Ltrain: 0.003886 	Lval: 0.003930
EarlyStopper: stopping at epoch 44 with best_val_loss = 0.003899


	Fold 5/5
Epoch: 1 	Ltrain: 0.025441 	Lval: 0.014580
Epoch: 5 	Ltrain: 0.006241 	Lval: 0.006486
Epoch: 10 	Ltrain: 0.004977 	Lval: 0.004759
Epoch 00015: reducing learning rate of group 0 to 3.7033e-04.
Epoch: 15 	Ltrain: 0.004541 	Lval: 0.004572
Epoch: 20 	Ltrain: 0.004188 	Lval: 0.004351
Epoch: 25 	Ltrain: 0.004170 	Lval: 0.004335
Epoch: 30 	Ltrain: 0.004115 	Lval: 0.004325
Epoch: 35 	Ltrain: 0.004109 	Lval: 0.004313
Epoch 00036: reducing learning rate of group 0 to 3.7033e-05.
Epoch: 40 	Ltrain: 0.004015 	Lval: 0.004306
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.004311

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007313467551011855
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.2600367870595595e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.019406 	Lval: 0.014853
Epoch: 5 	Ltrain: 0.006990 	Lval: 0.006065
Epoch: 10 	Ltrain: 0.005762 	Lval: 0.005143
Epoch: 15 	Ltrain: 0.005152 	Lval: 0.005110
Epoch: 20 	Ltrain: 0.004552 	Lval: 0.004700
Epoch 00022: reducing learning rate of group 0 to 7.3135e-04.
Epoch: 25 	Ltrain: 0.004241 	Lval: 0.004316
Epoch: 30 	Ltrain: 0.004404 	Lval: 0.004188
Epoch: 35 	Ltrain: 0.004276 	Lval: 0.004146
Epoch: 40 	Ltrain: 0.004304 	Lval: 0.004124
Epoch: 45 	Ltrain: 0.004320 	Lval: 0.004087
Epoch: 50 	Ltrain: 0.004128 	Lval: 0.004060
Epoch: 55 	Ltrain: 0.004018 	Lval: 0.004028
Epoch: 60 	Ltrain: 0.004175 	Lval: 0.003983
Epoch: 65 	Ltrain: 0.004010 	Lval: 0.003955
Epoch: 70 	Ltrain: 0.003940 	Lval: 0.003948
Epoch: 75 	Ltrain: 0.003946 	Lval: 0.003924
Epoch: 80 	Ltrain: 0.004215 	Lval: 0.003922
Epoch: 85 	Ltrain: 0.004386 	Lval: 0.003882
Epoch: 90 	Ltrain: 0.003927 	Lval: 0.003848
Epoch: 95 	Ltrain: 0.003721 	Lval: 0.003815
Epoch 00100: reducing learning rate of group 0 to 7.3135e-05.
Epoch: 100 	Ltrain: 0.004107 	Lval: 0.003912
Epoch: 105 	Ltrain: 0.003991 	Lval: 0.003762
Epoch: 110 	Ltrain: 0.003838 	Lval: 0.003766
Epoch 00113: reducing learning rate of group 0 to 7.3135e-06.
Epoch: 115 	Ltrain: 0.003958 	Lval: 0.003761
Epoch: 120 	Ltrain: 0.003826 	Lval: 0.003756
Epoch: 125 	Ltrain: 0.003773 	Lval: 0.003755
Epoch: 130 	Ltrain: 0.003748 	Lval: 0.003752
Epoch: 135 	Ltrain: 0.003783 	Lval: 0.003752
Epoch 00138: reducing learning rate of group 0 to 7.3135e-07.
Epoch: 140 	Ltrain: 0.004385 	Lval: 0.003752
Epoch: 145 	Ltrain: 0.003693 	Lval: 0.003753
EarlyStopper: stopping at epoch 146 with best_val_loss = 0.003755


	Fold 2/5
Epoch: 1 	Ltrain: 0.028609 	Lval: 0.012390
Epoch: 5 	Ltrain: 0.006043 	Lval: 0.005358
Epoch: 10 	Ltrain: 0.005095 	Lval: 0.004592
Epoch: 15 	Ltrain: 0.004896 	Lval: 0.004640
Epoch: 20 	Ltrain: 0.004799 	Lval: 0.004309
Epoch: 25 	Ltrain: 0.004502 	Lval: 0.004605
Epoch 00027: reducing learning rate of group 0 to 7.3135e-04.
Epoch: 30 	Ltrain: 0.004188 	Lval: 0.004483
Epoch: 35 	Ltrain: 0.004214 	Lval: 0.004128
Epoch: 40 	Ltrain: 0.004142 	Lval: 0.004118
Epoch 00041: reducing learning rate of group 0 to 7.3135e-05.
Epoch: 45 	Ltrain: 0.004071 	Lval: 0.004129
Epoch: 50 	Ltrain: 0.004095 	Lval: 0.004124
Epoch 00053: reducing learning rate of group 0 to 7.3135e-06.
Epoch: 55 	Ltrain: 0.004144 	Lval: 0.004120
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.004055


	Fold 3/5
Epoch: 1 	Ltrain: 0.024349 	Lval: 0.014055
Epoch: 5 	Ltrain: 0.005459 	Lval: 0.006464
Epoch: 10 	Ltrain: 0.004797 	Lval: 0.004351
Epoch: 15 	Ltrain: 0.004487 	Lval: 0.005401
Epoch: 20 	Ltrain: 0.004412 	Lval: 0.004280
Epoch: 25 	Ltrain: 0.004445 	Lval: 0.004076
Epoch: 30 	Ltrain: 0.004352 	Lval: 0.004417
Epoch 00031: reducing learning rate of group 0 to 7.3135e-04.
Epoch: 35 	Ltrain: 0.003807 	Lval: 0.003761
Epoch: 40 	Ltrain: 0.003746 	Lval: 0.003753
Epoch: 45 	Ltrain: 0.003692 	Lval: 0.003719
Epoch 00050: reducing learning rate of group 0 to 7.3135e-05.
Epoch: 50 	Ltrain: 0.003657 	Lval: 0.003735
Epoch: 55 	Ltrain: 0.003590 	Lval: 0.003663
Epoch: 60 	Ltrain: 0.003609 	Lval: 0.003672
Epoch: 65 	Ltrain: 0.003565 	Lval: 0.003648
Epoch 00066: reducing learning rate of group 0 to 7.3135e-06.
Epoch: 70 	Ltrain: 0.003569 	Lval: 0.003643
Epoch: 75 	Ltrain: 0.003564 	Lval: 0.003646
Epoch 00078: reducing learning rate of group 0 to 7.3135e-07.
Epoch: 80 	Ltrain: 0.003575 	Lval: 0.003644
EarlyStopper: stopping at epoch 83 with best_val_loss = 0.003640


	Fold 4/5
Epoch: 1 	Ltrain: 0.014366 	Lval: 0.010182
Epoch: 5 	Ltrain: 0.004953 	Lval: 0.004577
Epoch: 10 	Ltrain: 0.004406 	Lval: 0.004653
Epoch 00015: reducing learning rate of group 0 to 7.3135e-04.
Epoch: 15 	Ltrain: 0.004582 	Lval: 0.004348
Epoch: 20 	Ltrain: 0.003944 	Lval: 0.004223
Epoch: 25 	Ltrain: 0.003934 	Lval: 0.004129
Epoch 00028: reducing learning rate of group 0 to 7.3135e-05.
Epoch: 30 	Ltrain: 0.003852 	Lval: 0.004074
Epoch: 35 	Ltrain: 0.003857 	Lval: 0.004065
Epoch: 40 	Ltrain: 0.003850 	Lval: 0.004060
Epoch 00042: reducing learning rate of group 0 to 7.3135e-06.
Epoch: 45 	Ltrain: 0.003844 	Lval: 0.004056
Epoch: 50 	Ltrain: 0.003828 	Lval: 0.004054
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.004061


	Fold 5/5
Epoch: 1 	Ltrain: 0.017157 	Lval: 0.010221
Epoch: 5 	Ltrain: 0.004749 	Lval: 0.005641
Epoch: 10 	Ltrain: 0.004361 	Lval: 0.004451
Epoch 00014: reducing learning rate of group 0 to 7.3135e-04.
Epoch: 15 	Ltrain: 0.004284 	Lval: 0.004388
Epoch: 20 	Ltrain: 0.004016 	Lval: 0.004307
Epoch: 25 	Ltrain: 0.003995 	Lval: 0.004269
Epoch 00029: reducing learning rate of group 0 to 7.3135e-05.
Epoch: 30 	Ltrain: 0.003933 	Lval: 0.004264
Epoch: 35 	Ltrain: 0.003930 	Lval: 0.004255
Epoch: 40 	Ltrain: 0.003917 	Lval: 0.004263
Epoch 00043: reducing learning rate of group 0 to 7.3135e-06.
Epoch: 45 	Ltrain: 0.003923 	Lval: 0.004255
Epoch: 50 	Ltrain: 0.003911 	Lval: 0.004255
Epoch 00055: reducing learning rate of group 0 to 7.3135e-07.
Epoch: 55 	Ltrain: 0.003918 	Lval: 0.004255
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004259

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0029430226207277192
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.0512128611476095e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.043142 	Lval: 0.020936
Epoch: 5 	Ltrain: 0.016953 	Lval: 0.015108
Epoch: 10 	Ltrain: 0.009961 	Lval: 0.011392
Epoch: 15 	Ltrain: 0.007392 	Lval: 0.006921
Epoch: 20 	Ltrain: 0.005851 	Lval: 0.005198
Epoch: 25 	Ltrain: 0.005512 	Lval: 0.005292
Epoch 00027: reducing learning rate of group 0 to 2.9430e-04.
Epoch: 30 	Ltrain: 0.004852 	Lval: 0.004700
Epoch: 35 	Ltrain: 0.004835 	Lval: 0.004593
Epoch: 40 	Ltrain: 0.004478 	Lval: 0.004527
Epoch: 45 	Ltrain: 0.004531 	Lval: 0.004511
Epoch: 50 	Ltrain: 0.004691 	Lval: 0.004621
Epoch 00052: reducing learning rate of group 0 to 2.9430e-05.
Epoch: 55 	Ltrain: 0.004451 	Lval: 0.004485
Epoch: 60 	Ltrain: 0.004925 	Lval: 0.004469
Epoch: 65 	Ltrain: 0.004892 	Lval: 0.004462
Epoch 00069: reducing learning rate of group 0 to 2.9430e-06.
Epoch: 70 	Ltrain: 0.005457 	Lval: 0.004463
Epoch: 75 	Ltrain: 0.004587 	Lval: 0.004464
Epoch: 80 	Ltrain: 0.004932 	Lval: 0.004465
Epoch 00081: reducing learning rate of group 0 to 2.9430e-07.
Epoch: 85 	Ltrain: 0.005764 	Lval: 0.004465
EarlyStopper: stopping at epoch 87 with best_val_loss = 0.004464


	Fold 2/5
Epoch: 1 	Ltrain: 0.029757 	Lval: 0.013697
Epoch: 5 	Ltrain: 0.009979 	Lval: 0.010204
Epoch: 10 	Ltrain: 0.006309 	Lval: 0.004995
Epoch: 15 	Ltrain: 0.005851 	Lval: 0.004561
Epoch 00019: reducing learning rate of group 0 to 2.9430e-04.
Epoch: 20 	Ltrain: 0.004620 	Lval: 0.004427
Epoch: 25 	Ltrain: 0.004390 	Lval: 0.004215
Epoch: 30 	Ltrain: 0.004474 	Lval: 0.004193
Epoch: 35 	Ltrain: 0.004524 	Lval: 0.004183
Epoch: 40 	Ltrain: 0.004537 	Lval: 0.004129
Epoch 00042: reducing learning rate of group 0 to 2.9430e-05.
Epoch: 45 	Ltrain: 0.004277 	Lval: 0.004164
Epoch: 50 	Ltrain: 0.004182 	Lval: 0.004121
Epoch 00055: reducing learning rate of group 0 to 2.9430e-06.
Epoch: 55 	Ltrain: 0.004242 	Lval: 0.004165
Epoch: 60 	Ltrain: 0.004461 	Lval: 0.004154
Epoch: 65 	Ltrain: 0.004116 	Lval: 0.004153
Epoch 00067: reducing learning rate of group 0 to 2.9430e-07.
Epoch: 70 	Ltrain: 0.004431 	Lval: 0.004146
Epoch: 75 	Ltrain: 0.004293 	Lval: 0.004146
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.004108


	Fold 3/5
Epoch: 1 	Ltrain: 0.020906 	Lval: 0.016024
Epoch: 5 	Ltrain: 0.007776 	Lval: 0.007287
Epoch: 10 	Ltrain: 0.005212 	Lval: 0.004474
Epoch 00013: reducing learning rate of group 0 to 2.9430e-04.
Epoch: 15 	Ltrain: 0.004253 	Lval: 0.004181
Epoch: 20 	Ltrain: 0.004306 	Lval: 0.004193
Epoch: 25 	Ltrain: 0.004262 	Lval: 0.004132
Epoch: 30 	Ltrain: 0.004220 	Lval: 0.004052
Epoch 00031: reducing learning rate of group 0 to 2.9430e-05.
Epoch: 35 	Ltrain: 0.004137 	Lval: 0.004058
Epoch: 40 	Ltrain: 0.004110 	Lval: 0.004057
Epoch 00043: reducing learning rate of group 0 to 2.9430e-06.
Epoch: 45 	Ltrain: 0.004126 	Lval: 0.004047
Epoch: 50 	Ltrain: 0.004204 	Lval: 0.004052
EarlyStopper: stopping at epoch 49 with best_val_loss = 0.004049


	Fold 4/5
Epoch: 1 	Ltrain: 0.024084 	Lval: 0.012858
Epoch: 5 	Ltrain: 0.006544 	Lval: 0.005959
Epoch: 10 	Ltrain: 0.004550 	Lval: 0.004353
Epoch: 15 	Ltrain: 0.004148 	Lval: 0.004060
Epoch: 20 	Ltrain: 0.003931 	Lval: 0.003935
Epoch 00023: reducing learning rate of group 0 to 2.9430e-04.
Epoch: 25 	Ltrain: 0.003543 	Lval: 0.003721
Epoch: 30 	Ltrain: 0.003536 	Lval: 0.003710
Epoch: 35 	Ltrain: 0.003449 	Lval: 0.003697
Epoch 00036: reducing learning rate of group 0 to 2.9430e-05.
Epoch: 40 	Ltrain: 0.003381 	Lval: 0.003692
Epoch: 45 	Ltrain: 0.003389 	Lval: 0.003693
Epoch: 50 	Ltrain: 0.003441 	Lval: 0.003680
Epoch 00053: reducing learning rate of group 0 to 2.9430e-06.
Epoch: 55 	Ltrain: 0.003399 	Lval: 0.003680
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.003685


	Fold 5/5
Epoch: 1 	Ltrain: 0.021145 	Lval: 0.015057
Epoch: 5 	Ltrain: 0.006945 	Lval: 0.006839
Epoch: 10 	Ltrain: 0.004595 	Lval: 0.004758
Epoch: 15 	Ltrain: 0.004010 	Lval: 0.004446
Epoch 00018: reducing learning rate of group 0 to 2.9430e-04.
Epoch: 20 	Ltrain: 0.003751 	Lval: 0.004130
Epoch: 25 	Ltrain: 0.003661 	Lval: 0.004039
Epoch: 30 	Ltrain: 0.003661 	Lval: 0.004096
Epoch: 35 	Ltrain: 0.003593 	Lval: 0.004039
Epoch: 40 	Ltrain: 0.003653 	Lval: 0.004004
Epoch: 45 	Ltrain: 0.003578 	Lval: 0.003976
Epoch: 50 	Ltrain: 0.003527 	Lval: 0.003924
Epoch: 55 	Ltrain: 0.003517 	Lval: 0.004049
Epoch: 60 	Ltrain: 0.003437 	Lval: 0.003905
Epoch 00062: reducing learning rate of group 0 to 2.9430e-05.
Epoch: 65 	Ltrain: 0.003419 	Lval: 0.003873
Epoch: 70 	Ltrain: 0.003418 	Lval: 0.003872
Epoch: 75 	Ltrain: 0.003424 	Lval: 0.003868
Epoch 00076: reducing learning rate of group 0 to 2.9430e-06.
Epoch: 80 	Ltrain: 0.003393 	Lval: 0.003861
Epoch: 85 	Ltrain: 0.003396 	Lval: 0.003861
Epoch 00088: reducing learning rate of group 0 to 2.9430e-07.
Epoch: 90 	Ltrain: 0.003433 	Lval: 0.003860
Epoch: 95 	Ltrain: 0.003380 	Lval: 0.003860
EarlyStopper: stopping at epoch 98 with best_val_loss = 0.003860

Best hyperparameters: {'n_hours_u': 72, 'n_hours_y': 24, 'model_class': <class 'src.modelling.GRU.GRU'>, 'input_units': 8, 'hidden_layers': 5, 'hidden_units': 128, 'output_units': 2, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'lr_shared': 0.005552727450107967, 'scheduler': <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>, 'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}, 'w_decay': 9.0836872516287e-08, 'loss_fn': MSELoss(), 'epochs': 5000, 'early_stopper': <class 'src.modelling.EarlyStopper.EarlyStopper'>, 'patience': 19, 'batch_sz': 16, 'k_folds': 5}
Best validation loss: 0.0010648650135004168
