CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007194094541441001
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.731723105083079e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.137218 	Lval: 0.058265
Epoch: 5 	Ltrain: 0.013178 	Lval: 0.011231
Epoch: 10 	Ltrain: 0.007746 	Lval: 0.006819
Epoch: 15 	Ltrain: 0.006912 	Lval: 0.006154
Epoch 00016: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 20 	Ltrain: 0.006218 	Lval: 0.005713
Epoch: 25 	Ltrain: 0.005632 	Lval: 0.005418
Epoch 00028: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 30 	Ltrain: 0.005354 	Lval: 0.005396
Epoch: 35 	Ltrain: 0.005393 	Lval: 0.005373
Epoch 00040: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 40 	Ltrain: 0.005482 	Lval: 0.005443
Epoch: 45 	Ltrain: 0.005664 	Lval: 0.005429
Epoch: 50 	Ltrain: 0.005159 	Lval: 0.005410
Epoch 00052: reducing learning rate of group 0 to 7.1941e-07.
EarlyStopper: stopping at epoch 53 with best_val_loss = 0.005376


	Fold 2/5
Epoch: 1 	Ltrain: 0.146178 	Lval: 0.024765
Epoch: 5 	Ltrain: 0.009842 	Lval: 0.008195
Epoch: 10 	Ltrain: 0.005753 	Lval: 0.005263
Epoch: 15 	Ltrain: 0.004464 	Lval: 0.004925
Epoch: 20 	Ltrain: 0.004611 	Lval: 0.004587
Epoch 00024: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 25 	Ltrain: 0.004434 	Lval: 0.004824
Epoch: 30 	Ltrain: 0.003922 	Lval: 0.004432
Epoch: 35 	Ltrain: 0.003902 	Lval: 0.004470
Epoch 00037: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 40 	Ltrain: 0.004004 	Lval: 0.004454
Epoch: 45 	Ltrain: 0.003950 	Lval: 0.004441
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.004409


	Fold 3/5
Epoch: 1 	Ltrain: 0.179186 	Lval: 0.011890
Epoch: 5 	Ltrain: 0.007653 	Lval: 0.007169
Epoch: 10 	Ltrain: 0.005075 	Lval: 0.007158
Epoch: 15 	Ltrain: 0.004605 	Lval: 0.004548
Epoch: 20 	Ltrain: 0.004222 	Lval: 0.005182
Epoch 00023: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 25 	Ltrain: 0.003579 	Lval: 0.004217
Epoch: 30 	Ltrain: 0.003539 	Lval: 0.004369
Epoch: 35 	Ltrain: 0.003598 	Lval: 0.004209
Epoch 00037: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 40 	Ltrain: 0.003450 	Lval: 0.004194
Epoch: 45 	Ltrain: 0.003434 	Lval: 0.004208
Epoch 00049: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 50 	Ltrain: 0.003451 	Lval: 0.004196
Epoch: 55 	Ltrain: 0.003401 	Lval: 0.004197
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004191


	Fold 4/5
Epoch: 1 	Ltrain: 0.085689 	Lval: 0.009847
Epoch: 5 	Ltrain: 0.004919 	Lval: 0.005245
Epoch: 10 	Ltrain: 0.004081 	Lval: 0.005057
Epoch: 15 	Ltrain: 0.003883 	Lval: 0.004755
Epoch: 20 	Ltrain: 0.003821 	Lval: 0.004289
Epoch 00025: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 25 	Ltrain: 0.003331 	Lval: 0.004241
Epoch: 30 	Ltrain: 0.002810 	Lval: 0.003704
Epoch: 35 	Ltrain: 0.002840 	Lval: 0.003660
Epoch: 40 	Ltrain: 0.002732 	Lval: 0.003608
Epoch: 45 	Ltrain: 0.002779 	Lval: 0.003714
Epoch 00047: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 50 	Ltrain: 0.002653 	Lval: 0.003495
Epoch: 55 	Ltrain: 0.002745 	Lval: 0.003505
Epoch 00060: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 60 	Ltrain: 0.002644 	Lval: 0.003498
Epoch: 65 	Ltrain: 0.002680 	Lval: 0.003491
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.003498


	Fold 5/5
Epoch: 1 	Ltrain: 0.054957 	Lval: 0.014692
Epoch: 5 	Ltrain: 0.005227 	Lval: 0.005782
Epoch: 10 	Ltrain: 0.004694 	Lval: 0.005543
Epoch: 15 	Ltrain: 0.004548 	Lval: 0.005757
Epoch 00016: reducing learning rate of group 0 to 7.1941e-04.
Epoch: 20 	Ltrain: 0.003617 	Lval: 0.004721
Epoch: 25 	Ltrain: 0.003546 	Lval: 0.004654
Epoch 00028: reducing learning rate of group 0 to 7.1941e-05.
Epoch: 30 	Ltrain: 0.003453 	Lval: 0.004592
Epoch: 35 	Ltrain: 0.003451 	Lval: 0.004589
Epoch 00040: reducing learning rate of group 0 to 7.1941e-06.
Epoch: 40 	Ltrain: 0.003450 	Lval: 0.004577
Epoch: 45 	Ltrain: 0.003415 	Lval: 0.004577
EarlyStopper: stopping at epoch 45 with best_val_loss = 0.004575

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00012585356404335105
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.7667185122397169e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.042373 	Lval: 0.033672
Epoch: 5 	Ltrain: 0.013393 	Lval: 0.010957
Epoch: 10 	Ltrain: 0.009652 	Lval: 0.008936
Epoch: 15 	Ltrain: 0.008771 	Lval: 0.008276
Epoch: 20 	Ltrain: 0.008606 	Lval: 0.007773
Epoch: 25 	Ltrain: 0.007916 	Lval: 0.007319
Epoch: 30 	Ltrain: 0.007222 	Lval: 0.006879
Epoch: 35 	Ltrain: 0.007371 	Lval: 0.006522
Epoch: 40 	Ltrain: 0.007046 	Lval: 0.006233
Epoch: 45 	Ltrain: 0.006836 	Lval: 0.006060
Epoch: 50 	Ltrain: 0.006718 	Lval: 0.005928
Epoch: 55 	Ltrain: 0.006352 	Lval: 0.005850
Epoch 00060: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 60 	Ltrain: 0.006782 	Lval: 0.005846
Epoch: 65 	Ltrain: 0.005988 	Lval: 0.005804
Epoch: 70 	Ltrain: 0.005810 	Lval: 0.005793
Epoch 00072: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 75 	Ltrain: 0.006162 	Lval: 0.005804
Epoch: 80 	Ltrain: 0.005935 	Lval: 0.005800
Epoch 00084: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 85 	Ltrain: 0.006497 	Lval: 0.005791
Epoch: 90 	Ltrain: 0.005674 	Lval: 0.005791
EarlyStopper: stopping at epoch 93 with best_val_loss = 0.005788


	Fold 2/5
Epoch: 1 	Ltrain: 0.014313 	Lval: 0.010494
Epoch: 5 	Ltrain: 0.009008 	Lval: 0.007947
Epoch: 10 	Ltrain: 0.007429 	Lval: 0.006976
Epoch: 15 	Ltrain: 0.007077 	Lval: 0.006411
Epoch: 20 	Ltrain: 0.006538 	Lval: 0.006193
Epoch: 25 	Ltrain: 0.006110 	Lval: 0.005995
Epoch: 30 	Ltrain: 0.005936 	Lval: 0.005840
Epoch: 35 	Ltrain: 0.005547 	Lval: 0.005694
Epoch: 40 	Ltrain: 0.005866 	Lval: 0.005551
Epoch: 45 	Ltrain: 0.005312 	Lval: 0.005482
Epoch: 50 	Ltrain: 0.005157 	Lval: 0.005332
Epoch: 55 	Ltrain: 0.005090 	Lval: 0.005226
Epoch: 60 	Ltrain: 0.005028 	Lval: 0.005151
Epoch: 65 	Ltrain: 0.004846 	Lval: 0.005061
Epoch: 70 	Ltrain: 0.005196 	Lval: 0.005012
Epoch: 75 	Ltrain: 0.004695 	Lval: 0.004860
Epoch: 80 	Ltrain: 0.004594 	Lval: 0.004842
Epoch 00084: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 85 	Ltrain: 0.004621 	Lval: 0.004851
Epoch: 90 	Ltrain: 0.004511 	Lval: 0.004833
Epoch: 95 	Ltrain: 0.004496 	Lval: 0.004820
Epoch: 100 	Ltrain: 0.004304 	Lval: 0.004810
Epoch 00103: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 105 	Ltrain: 0.004365 	Lval: 0.004797
Epoch: 110 	Ltrain: 0.004443 	Lval: 0.004800
Epoch 00115: reducing learning rate of group 0 to 1.2585e-07.
Epoch: 115 	Ltrain: 0.004739 	Lval: 0.004802
Epoch: 120 	Ltrain: 0.004291 	Lval: 0.004802
Epoch: 125 	Ltrain: 0.004542 	Lval: 0.004802
EarlyStopper: stopping at epoch 124 with best_val_loss = 0.004797


	Fold 3/5
Epoch: 1 	Ltrain: 0.014171 	Lval: 0.009637
Epoch: 5 	Ltrain: 0.009288 	Lval: 0.007966
Epoch: 10 	Ltrain: 0.007268 	Lval: 0.006836
Epoch: 15 	Ltrain: 0.006811 	Lval: 0.006554
Epoch: 20 	Ltrain: 0.006255 	Lval: 0.006265
Epoch: 25 	Ltrain: 0.006037 	Lval: 0.006031
Epoch: 30 	Ltrain: 0.005726 	Lval: 0.005786
Epoch: 35 	Ltrain: 0.005315 	Lval: 0.005783
Epoch: 40 	Ltrain: 0.005494 	Lval: 0.005302
Epoch: 45 	Ltrain: 0.004818 	Lval: 0.005173
Epoch: 50 	Ltrain: 0.004834 	Lval: 0.004999
Epoch: 55 	Ltrain: 0.004612 	Lval: 0.004910
Epoch: 60 	Ltrain: 0.004541 	Lval: 0.004884
Epoch: 65 	Ltrain: 0.004558 	Lval: 0.004782
Epoch: 70 	Ltrain: 0.004377 	Lval: 0.004763
Epoch 00073: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 75 	Ltrain: 0.004274 	Lval: 0.004741
Epoch: 80 	Ltrain: 0.004283 	Lval: 0.004733
Epoch 00085: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 85 	Ltrain: 0.004278 	Lval: 0.004719
Epoch: 90 	Ltrain: 0.004251 	Lval: 0.004728
Epoch: 95 	Ltrain: 0.004223 	Lval: 0.004727
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.004702


	Fold 4/5
Epoch: 1 	Ltrain: 0.048087 	Lval: 0.030929
Epoch: 5 	Ltrain: 0.009144 	Lval: 0.008695
Epoch: 10 	Ltrain: 0.007278 	Lval: 0.007359
Epoch: 15 	Ltrain: 0.006486 	Lval: 0.006819
Epoch: 20 	Ltrain: 0.006062 	Lval: 0.006497
Epoch: 25 	Ltrain: 0.005783 	Lval: 0.006234
Epoch: 30 	Ltrain: 0.005583 	Lval: 0.006054
Epoch: 35 	Ltrain: 0.005266 	Lval: 0.005867
Epoch: 40 	Ltrain: 0.004899 	Lval: 0.005547
Epoch: 45 	Ltrain: 0.004653 	Lval: 0.005305
Epoch: 50 	Ltrain: 0.004501 	Lval: 0.005116
Epoch: 55 	Ltrain: 0.004307 	Lval: 0.005081
Epoch: 60 	Ltrain: 0.004205 	Lval: 0.004899
Epoch: 65 	Ltrain: 0.004214 	Lval: 0.004777
Epoch 00069: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 70 	Ltrain: 0.004039 	Lval: 0.004818
Epoch: 75 	Ltrain: 0.004032 	Lval: 0.004821
Epoch: 80 	Ltrain: 0.004037 	Lval: 0.004817
Epoch 00081: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 85 	Ltrain: 0.004030 	Lval: 0.004817
Epoch: 90 	Ltrain: 0.004036 	Lval: 0.004814
EarlyStopper: stopping at epoch 90 with best_val_loss = 0.004777


	Fold 5/5
Epoch: 1 	Ltrain: 0.030450 	Lval: 0.017758
Epoch: 5 	Ltrain: 0.009030 	Lval: 0.008879
Epoch: 10 	Ltrain: 0.007181 	Lval: 0.007555
Epoch: 15 	Ltrain: 0.006147 	Lval: 0.006812
Epoch: 20 	Ltrain: 0.005733 	Lval: 0.006497
Epoch: 25 	Ltrain: 0.005469 	Lval: 0.006219
Epoch: 30 	Ltrain: 0.005186 	Lval: 0.005960
Epoch: 35 	Ltrain: 0.004925 	Lval: 0.005738
Epoch: 40 	Ltrain: 0.004681 	Lval: 0.005579
Epoch: 45 	Ltrain: 0.004513 	Lval: 0.005374
Epoch: 50 	Ltrain: 0.004323 	Lval: 0.005234
Epoch: 55 	Ltrain: 0.004250 	Lval: 0.005126
Epoch: 60 	Ltrain: 0.004230 	Lval: 0.005114
Epoch: 65 	Ltrain: 0.004131 	Lval: 0.005086
Epoch 00070: reducing learning rate of group 0 to 1.2585e-05.
Epoch: 70 	Ltrain: 0.004074 	Lval: 0.004955
Epoch: 75 	Ltrain: 0.003918 	Lval: 0.004957
Epoch: 80 	Ltrain: 0.003979 	Lval: 0.004946
Epoch 00082: reducing learning rate of group 0 to 1.2585e-06.
Epoch: 85 	Ltrain: 0.004030 	Lval: 0.004947
Epoch: 90 	Ltrain: 0.003899 	Lval: 0.004946
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.004940

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.002622655022886898
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.390321341492203e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 27
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.031074 	Lval: 0.012054
Epoch: 5 	Ltrain: 0.012301 	Lval: 0.009722
Epoch: 10 	Ltrain: 0.007891 	Lval: 0.006834
Epoch: 15 	Ltrain: 0.007200 	Lval: 0.006449
Epoch: 20 	Ltrain: 0.006205 	Lval: 0.005151
Epoch 00024: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 25 	Ltrain: 0.005094 	Lval: 0.005146
Epoch: 30 	Ltrain: 0.004365 	Lval: 0.004981
Epoch: 35 	Ltrain: 0.004490 	Lval: 0.004691
Epoch 00037: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 40 	Ltrain: 0.004562 	Lval: 0.004867
Epoch: 45 	Ltrain: 0.004475 	Lval: 0.004746
Epoch 00049: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 50 	Ltrain: 0.004588 	Lval: 0.004729
Epoch: 55 	Ltrain: 0.004220 	Lval: 0.004726
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.004666


	Fold 2/5
Epoch: 1 	Ltrain: 0.014927 	Lval: 0.010360
Epoch: 5 	Ltrain: 0.006585 	Lval: 0.006125
Epoch: 10 	Ltrain: 0.005476 	Lval: 0.005266
Epoch: 15 	Ltrain: 0.004820 	Lval: 0.005191
Epoch: 20 	Ltrain: 0.004353 	Lval: 0.004681
Epoch: 25 	Ltrain: 0.003717 	Lval: 0.004595
Epoch: 30 	Ltrain: 0.004053 	Lval: 0.004944
Epoch 00032: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 35 	Ltrain: 0.003130 	Lval: 0.003860
Epoch: 40 	Ltrain: 0.003345 	Lval: 0.003868
Epoch: 45 	Ltrain: 0.003243 	Lval: 0.003787
Epoch: 50 	Ltrain: 0.003018 	Lval: 0.003719
Epoch: 55 	Ltrain: 0.003178 	Lval: 0.003706
Epoch: 60 	Ltrain: 0.002990 	Lval: 0.003667
Epoch 00063: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 65 	Ltrain: 0.002927 	Lval: 0.003651
Epoch: 70 	Ltrain: 0.002898 	Lval: 0.003643
Epoch 00075: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 75 	Ltrain: 0.003068 	Lval: 0.003650
Epoch: 80 	Ltrain: 0.002996 	Lval: 0.003647
Epoch: 85 	Ltrain: 0.002959 	Lval: 0.003648
EarlyStopper: stopping at epoch 85 with best_val_loss = 0.003648


	Fold 3/5
Epoch: 1 	Ltrain: 0.015616 	Lval: 0.009598
Epoch: 5 	Ltrain: 0.006017 	Lval: 0.005613
Epoch: 10 	Ltrain: 0.004552 	Lval: 0.006836
Epoch 00013: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 15 	Ltrain: 0.003846 	Lval: 0.004348
Epoch: 20 	Ltrain: 0.003959 	Lval: 0.004365
Epoch: 25 	Ltrain: 0.003803 	Lval: 0.004309
Epoch: 30 	Ltrain: 0.003685 	Lval: 0.004224
Epoch: 35 	Ltrain: 0.003716 	Lval: 0.004166
Epoch: 40 	Ltrain: 0.003590 	Lval: 0.004115
Epoch: 45 	Ltrain: 0.003511 	Lval: 0.004051
Epoch: 50 	Ltrain: 0.003472 	Lval: 0.003988
Epoch: 55 	Ltrain: 0.003399 	Lval: 0.004064
Epoch 00056: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 60 	Ltrain: 0.003309 	Lval: 0.003934
Epoch: 65 	Ltrain: 0.003413 	Lval: 0.003956
Epoch 00068: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 70 	Ltrain: 0.003337 	Lval: 0.003948
Epoch: 75 	Ltrain: 0.003293 	Lval: 0.003945
Epoch 00080: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 80 	Ltrain: 0.003363 	Lval: 0.003946
Epoch: 85 	Ltrain: 0.003298 	Lval: 0.003946
EarlyStopper: stopping at epoch 86 with best_val_loss = 0.003934


	Fold 4/5
Epoch: 1 	Ltrain: 0.018274 	Lval: 0.010273
Epoch: 5 	Ltrain: 0.005614 	Lval: 0.005684
Epoch: 10 	Ltrain: 0.004101 	Lval: 0.004852
Epoch 00015: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 15 	Ltrain: 0.003885 	Lval: 0.004668
Epoch: 20 	Ltrain: 0.003517 	Lval: 0.004306
Epoch: 25 	Ltrain: 0.003322 	Lval: 0.004219
Epoch: 30 	Ltrain: 0.003280 	Lval: 0.004109
Epoch: 35 	Ltrain: 0.003076 	Lval: 0.004040
Epoch 00040: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 40 	Ltrain: 0.002999 	Lval: 0.004011
Epoch: 45 	Ltrain: 0.002932 	Lval: 0.003877
Epoch: 50 	Ltrain: 0.002895 	Lval: 0.003867
Epoch: 55 	Ltrain: 0.002882 	Lval: 0.003854
Epoch: 60 	Ltrain: 0.002892 	Lval: 0.003842
Epoch: 65 	Ltrain: 0.002894 	Lval: 0.003837
Epoch: 70 	Ltrain: 0.002867 	Lval: 0.003819
Epoch: 75 	Ltrain: 0.002909 	Lval: 0.003821
Epoch 00078: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 80 	Ltrain: 0.002834 	Lval: 0.003817
Epoch: 85 	Ltrain: 0.002984 	Lval: 0.003814
Epoch 00090: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 90 	Ltrain: 0.002831 	Lval: 0.003813
Epoch: 95 	Ltrain: 0.002902 	Lval: 0.003813
Epoch: 100 	Ltrain: 0.002863 	Lval: 0.003813
EarlyStopper: stopping at epoch 100 with best_val_loss = 0.003810


	Fold 5/5
Epoch: 1 	Ltrain: 0.019783 	Lval: 0.010763
Epoch: 5 	Ltrain: 0.005362 	Lval: 0.005920
Epoch: 10 	Ltrain: 0.004035 	Lval: 0.004911
Epoch 00013: reducing learning rate of group 0 to 2.6227e-04.
Epoch: 15 	Ltrain: 0.003625 	Lval: 0.004613
Epoch: 20 	Ltrain: 0.003604 	Lval: 0.004560
Epoch: 25 	Ltrain: 0.003339 	Lval: 0.004389
Epoch: 30 	Ltrain: 0.003244 	Lval: 0.004306
Epoch: 35 	Ltrain: 0.003162 	Lval: 0.004187
Epoch: 40 	Ltrain: 0.003158 	Lval: 0.004154
Epoch: 45 	Ltrain: 0.003013 	Lval: 0.004018
Epoch: 50 	Ltrain: 0.002922 	Lval: 0.004003
Epoch: 55 	Ltrain: 0.002927 	Lval: 0.003948
Epoch: 60 	Ltrain: 0.002890 	Lval: 0.003849
Epoch: 65 	Ltrain: 0.002785 	Lval: 0.003785
Epoch: 70 	Ltrain: 0.002759 	Lval: 0.003739
Epoch 00074: reducing learning rate of group 0 to 2.6227e-05.
Epoch: 75 	Ltrain: 0.002755 	Lval: 0.003721
Epoch: 80 	Ltrain: 0.002709 	Lval: 0.003730
Epoch: 85 	Ltrain: 0.002685 	Lval: 0.003728
Epoch 00086: reducing learning rate of group 0 to 2.6227e-06.
Epoch: 90 	Ltrain: 0.002696 	Lval: 0.003728
Epoch: 95 	Ltrain: 0.002679 	Lval: 0.003724
Epoch 00098: reducing learning rate of group 0 to 2.6227e-07.
Epoch: 100 	Ltrain: 0.002700 	Lval: 0.003725
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.003721

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0036708403685720837
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 6.036796529397564e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.027339 	Lval: 0.012597
Epoch: 5 	Ltrain: 0.009466 	Lval: 0.006637
Epoch: 10 	Ltrain: 0.006690 	Lval: 0.006960
Epoch 00012: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 15 	Ltrain: 0.006158 	Lval: 0.006645
Epoch: 20 	Ltrain: 0.006158 	Lval: 0.006332
Epoch: 25 	Ltrain: 0.005887 	Lval: 0.006270
Epoch 00030: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 30 	Ltrain: 0.005942 	Lval: 0.006577
Epoch: 35 	Ltrain: 0.006362 	Lval: 0.006514
EarlyStopper: stopping at epoch 37 with best_val_loss = 0.006272


	Fold 2/5
Epoch: 1 	Ltrain: 0.025347 	Lval: 0.014881
Epoch: 5 	Ltrain: 0.007921 	Lval: 0.006631
Epoch: 10 	Ltrain: 0.005668 	Lval: 0.005634
Epoch: 15 	Ltrain: 0.005521 	Lval: 0.005368
Epoch 00020: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 20 	Ltrain: 0.005555 	Lval: 0.005244
Epoch: 25 	Ltrain: 0.004724 	Lval: 0.005036
Epoch: 30 	Ltrain: 0.005114 	Lval: 0.005139
Epoch 00032: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 35 	Ltrain: 0.004549 	Lval: 0.005023
Epoch: 40 	Ltrain: 0.004455 	Lval: 0.005010
EarlyStopper: stopping at epoch 42 with best_val_loss = 0.004992


	Fold 3/5
Epoch: 1 	Ltrain: 0.026654 	Lval: 0.010428
Epoch: 5 	Ltrain: 0.006958 	Lval: 0.007162
Epoch: 10 	Ltrain: 0.005289 	Lval: 0.005525
Epoch: 15 	Ltrain: 0.004594 	Lval: 0.005548
Epoch: 20 	Ltrain: 0.004328 	Lval: 0.004810
Epoch: 25 	Ltrain: 0.004006 	Lval: 0.004234
Epoch 00030: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 30 	Ltrain: 0.004002 	Lval: 0.004350
Epoch: 35 	Ltrain: 0.003789 	Lval: 0.004251
Epoch: 40 	Ltrain: 0.003754 	Lval: 0.004339
Epoch 00042: reducing learning rate of group 0 to 3.6708e-05.
EarlyStopper: stopping at epoch 41 with best_val_loss = 0.004205


	Fold 4/5
Epoch: 1 	Ltrain: 0.034967 	Lval: 0.010307
Epoch: 5 	Ltrain: 0.006258 	Lval: 0.005527
Epoch 00009: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 10 	Ltrain: 0.004986 	Lval: 0.005812
Epoch: 15 	Ltrain: 0.004841 	Lval: 0.005867
Epoch: 20 	Ltrain: 0.004807 	Lval: 0.005829
Epoch 00021: reducing learning rate of group 0 to 3.6708e-05.
EarlyStopper: stopping at epoch 20 with best_val_loss = 0.005527


	Fold 5/5
Epoch: 1 	Ltrain: 0.032204 	Lval: 0.010005
Epoch: 5 	Ltrain: 0.006421 	Lval: 0.006547
Epoch: 10 	Ltrain: 0.004940 	Lval: 0.006490
Epoch: 15 	Ltrain: 0.004318 	Lval: 0.005872
Epoch: 20 	Ltrain: 0.004132 	Lval: 0.005010
Epoch: 25 	Ltrain: 0.003959 	Lval: 0.005193
Epoch 00026: reducing learning rate of group 0 to 3.6708e-04.
Epoch: 30 	Ltrain: 0.003739 	Lval: 0.004964
Epoch: 35 	Ltrain: 0.003770 	Lval: 0.004917
Epoch 00038: reducing learning rate of group 0 to 3.6708e-05.
Epoch: 40 	Ltrain: 0.003754 	Lval: 0.004934
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.004904

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0028563448143027245
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.8400362376385822e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 10
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.033418 	Lval: 0.009949
Epoch: 5 	Ltrain: 0.007629 	Lval: 0.006833
Epoch: 10 	Ltrain: 0.006243 	Lval: 0.005727
Epoch: 15 	Ltrain: 0.006162 	Lval: 0.006946
Epoch: 20 	Ltrain: 0.005631 	Lval: 0.005214
Epoch 00024: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 25 	Ltrain: 0.005450 	Lval: 0.005179
Epoch: 30 	Ltrain: 0.005353 	Lval: 0.005044
Epoch: 35 	Ltrain: 0.005034 	Lval: 0.005014
Epoch: 40 	Ltrain: 0.005018 	Lval: 0.004993
Epoch: 45 	Ltrain: 0.005222 	Lval: 0.004992
Epoch: 50 	Ltrain: 0.005211 	Lval: 0.005004
Epoch: 55 	Ltrain: 0.005055 	Lval: 0.005003
Epoch: 60 	Ltrain: 0.004823 	Lval: 0.004891
Epoch: 65 	Ltrain: 0.004834 	Lval: 0.004873
Epoch: 70 	Ltrain: 0.004702 	Lval: 0.004862
Epoch: 75 	Ltrain: 0.004733 	Lval: 0.004840
Epoch 00078: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 80 	Ltrain: 0.004793 	Lval: 0.004825
Epoch: 85 	Ltrain: 0.005098 	Lval: 0.004814
Epoch 00090: reducing learning rate of group 0 to 2.8563e-06.
Epoch: 90 	Ltrain: 0.004715 	Lval: 0.004814
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.004806


	Fold 2/5
Epoch: 1 	Ltrain: 0.015039 	Lval: 0.009156
Epoch: 5 	Ltrain: 0.006148 	Lval: 0.006240
Epoch: 10 	Ltrain: 0.005623 	Lval: 0.005410
Epoch: 15 	Ltrain: 0.004759 	Lval: 0.005210
Epoch: 20 	Ltrain: 0.004544 	Lval: 0.004777
Epoch: 25 	Ltrain: 0.004336 	Lval: 0.004810
Epoch: 30 	Ltrain: 0.004249 	Lval: 0.005089
Epoch: 35 	Ltrain: 0.004363 	Lval: 0.004531
Epoch 00039: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 40 	Ltrain: 0.004265 	Lval: 0.004581
Epoch: 45 	Ltrain: 0.003961 	Lval: 0.004527
Epoch: 50 	Ltrain: 0.004029 	Lval: 0.004526
Epoch 00051: reducing learning rate of group 0 to 2.8563e-05.
EarlyStopper: stopping at epoch 51 with best_val_loss = 0.004497


	Fold 3/5
Epoch: 1 	Ltrain: 0.015807 	Lval: 0.008467
Epoch: 5 	Ltrain: 0.005707 	Lval: 0.007658
Epoch: 10 	Ltrain: 0.004457 	Lval: 0.004876
Epoch 00012: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 15 	Ltrain: 0.004147 	Lval: 0.004662
Epoch: 20 	Ltrain: 0.004078 	Lval: 0.004664
Epoch: 25 	Ltrain: 0.004069 	Lval: 0.004699
Epoch: 30 	Ltrain: 0.004044 	Lval: 0.004573
Epoch 00032: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 35 	Ltrain: 0.003989 	Lval: 0.004584
EarlyStopper: stopping at epoch 37 with best_val_loss = 0.004560


	Fold 4/5
Epoch: 1 	Ltrain: 0.013010 	Lval: 0.008827
Epoch: 5 	Ltrain: 0.004849 	Lval: 0.005738
Epoch: 10 	Ltrain: 0.004476 	Lval: 0.005692
Epoch: 15 	Ltrain: 0.004090 	Lval: 0.004658
Epoch 00017: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 20 	Ltrain: 0.003832 	Lval: 0.004631
EarlyStopper: stopping at epoch 22 with best_val_loss = 0.004641


	Fold 5/5
Epoch: 1 	Ltrain: 0.009870 	Lval: 0.008810
Epoch: 5 	Ltrain: 0.004792 	Lval: 0.005416
Epoch: 10 	Ltrain: 0.004237 	Lval: 0.005441
Epoch: 15 	Ltrain: 0.003956 	Lval: 0.004851
Epoch 00016: reducing learning rate of group 0 to 2.8563e-04.
Epoch: 20 	Ltrain: 0.003808 	Lval: 0.004828
Epoch: 25 	Ltrain: 0.003806 	Lval: 0.004753
Epoch 00028: reducing learning rate of group 0 to 2.8563e-05.
Epoch: 30 	Ltrain: 0.003772 	Lval: 0.004734
EarlyStopper: stopping at epoch 32 with best_val_loss = 0.004689

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00044590431323154054
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.414035159965753e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.046833 	Lval: 0.029780
Epoch: 5 	Ltrain: 0.011244 	Lval: 0.009287
Epoch 00008: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 10 	Ltrain: 0.011204 	Lval: 0.009764
Epoch: 15 	Ltrain: 0.011325 	Lval: 0.009154
Epoch: 20 	Ltrain: 0.009651 	Lval: 0.008804
Epoch: 25 	Ltrain: 0.010873 	Lval: 0.008664
Epoch: 30 	Ltrain: 0.008998 	Lval: 0.008628
Epoch: 35 	Ltrain: 0.010464 	Lval: 0.008564
Epoch: 40 	Ltrain: 0.014366 	Lval: 0.008511
Epoch: 45 	Ltrain: 0.011291 	Lval: 0.008504
Epoch 00047: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 50 	Ltrain: 0.009622 	Lval: 0.008513
Epoch: 55 	Ltrain: 0.012848 	Lval: 0.008514
Epoch 00059: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 60 	Ltrain: 0.009242 	Lval: 0.008510
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.008502


	Fold 2/5
Epoch: 1 	Ltrain: 0.086399 	Lval: 0.063066
Epoch: 5 	Ltrain: 0.011213 	Lval: 0.009948
Epoch: 10 	Ltrain: 0.010414 	Lval: 0.009328
Epoch 00011: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 15 	Ltrain: 0.009490 	Lval: 0.008667
Epoch: 20 	Ltrain: 0.010424 	Lval: 0.008401
Epoch: 25 	Ltrain: 0.009663 	Lval: 0.008238
Epoch: 30 	Ltrain: 0.009454 	Lval: 0.008192
Epoch: 35 	Ltrain: 0.008634 	Lval: 0.008140
Epoch: 40 	Ltrain: 0.008682 	Lval: 0.008041
Epoch: 45 	Ltrain: 0.009220 	Lval: 0.007995
Epoch 00048: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 50 	Ltrain: 0.008626 	Lval: 0.007993
Epoch: 55 	Ltrain: 0.009084 	Lval: 0.007984
Epoch: 60 	Ltrain: 0.008693 	Lval: 0.007974
Epoch: 65 	Ltrain: 0.008250 	Lval: 0.007962
Epoch: 70 	Ltrain: 0.009351 	Lval: 0.007954
Epoch: 75 	Ltrain: 0.008530 	Lval: 0.007950
Epoch: 80 	Ltrain: 0.008859 	Lval: 0.007938
Epoch: 85 	Ltrain: 0.009391 	Lval: 0.007916
Epoch: 90 	Ltrain: 0.008461 	Lval: 0.007902
Epoch: 95 	Ltrain: 0.008933 	Lval: 0.007889
Epoch: 100 	Ltrain: 0.008836 	Lval: 0.007884
Epoch: 105 	Ltrain: 0.008961 	Lval: 0.007877
Epoch 00109: reducing learning rate of group 0 to 4.4590e-07.
Epoch: 110 	Ltrain: 0.009372 	Lval: 0.007877
Epoch: 115 	Ltrain: 0.008611 	Lval: 0.007875
Epoch: 120 	Ltrain: 0.009677 	Lval: 0.007873
Epoch 00125: reducing learning rate of group 0 to 4.4590e-08.
Epoch: 125 	Ltrain: 0.008453 	Lval: 0.007873
EarlyStopper: stopping at epoch 124 with best_val_loss = 0.007878


	Fold 3/5
Epoch: 1 	Ltrain: 0.106226 	Lval: 0.070856
Epoch: 5 	Ltrain: 0.014428 	Lval: 0.009781
Epoch: 10 	Ltrain: 0.011408 	Lval: 0.008254
Epoch: 15 	Ltrain: 0.009628 	Lval: 0.007662
Epoch: 20 	Ltrain: 0.007217 	Lval: 0.007014
Epoch: 25 	Ltrain: 0.006722 	Lval: 0.006382
Epoch: 30 	Ltrain: 0.006288 	Lval: 0.006055
Epoch: 35 	Ltrain: 0.006425 	Lval: 0.006007
Epoch 00037: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 40 	Ltrain: 0.005980 	Lval: 0.005920
Epoch: 45 	Ltrain: 0.005745 	Lval: 0.005931
Epoch 00049: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 50 	Ltrain: 0.006084 	Lval: 0.005943
Epoch: 55 	Ltrain: 0.006043 	Lval: 0.005938
Epoch: 60 	Ltrain: 0.006015 	Lval: 0.005925
Epoch 00061: reducing learning rate of group 0 to 4.4590e-07.
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.005919


	Fold 4/5
Epoch: 1 	Ltrain: 0.012600 	Lval: 0.009324
Epoch: 5 	Ltrain: 0.010088 	Lval: 0.007521
Epoch: 10 	Ltrain: 0.006874 	Lval: 0.006468
Epoch: 15 	Ltrain: 0.006344 	Lval: 0.006284
Epoch: 20 	Ltrain: 0.005778 	Lval: 0.005912
Epoch: 25 	Ltrain: 0.005737 	Lval: 0.006006
Epoch: 30 	Ltrain: 0.005117 	Lval: 0.005701
Epoch: 35 	Ltrain: 0.004851 	Lval: 0.005515
Epoch: 40 	Ltrain: 0.004703 	Lval: 0.005103
Epoch: 45 	Ltrain: 0.004597 	Lval: 0.005134
Epoch: 50 	Ltrain: 0.004576 	Lval: 0.005001
Epoch: 55 	Ltrain: 0.004481 	Lval: 0.004834
Epoch: 60 	Ltrain: 0.004514 	Lval: 0.004871
Epoch: 65 	Ltrain: 0.004385 	Lval: 0.004810
Epoch: 70 	Ltrain: 0.004245 	Lval: 0.004789
Epoch 00072: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 75 	Ltrain: 0.004333 	Lval: 0.004825
Epoch: 80 	Ltrain: 0.004170 	Lval: 0.004793
Epoch 00084: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 85 	Ltrain: 0.004139 	Lval: 0.004793
Epoch: 90 	Ltrain: 0.004235 	Lval: 0.004799
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.004731


	Fold 5/5
Epoch: 1 	Ltrain: 0.012555 	Lval: 0.010649
Epoch: 5 	Ltrain: 0.008975 	Lval: 0.008811
Epoch: 10 	Ltrain: 0.007125 	Lval: 0.007499
Epoch: 15 	Ltrain: 0.006246 	Lval: 0.006841
Epoch: 20 	Ltrain: 0.005669 	Lval: 0.006553
Epoch: 25 	Ltrain: 0.005469 	Lval: 0.006326
Epoch: 30 	Ltrain: 0.005091 	Lval: 0.006219
Epoch: 35 	Ltrain: 0.004971 	Lval: 0.005835
Epoch: 40 	Ltrain: 0.004761 	Lval: 0.005868
Epoch: 45 	Ltrain: 0.004596 	Lval: 0.005706
Epoch: 50 	Ltrain: 0.004881 	Lval: 0.005607
Epoch 00052: reducing learning rate of group 0 to 4.4590e-05.
Epoch: 55 	Ltrain: 0.004518 	Lval: 0.005589
Epoch: 60 	Ltrain: 0.004319 	Lval: 0.005517
Epoch: 65 	Ltrain: 0.004462 	Lval: 0.005532
Epoch 00067: reducing learning rate of group 0 to 4.4590e-06.
Epoch: 70 	Ltrain: 0.004677 	Lval: 0.005514
Epoch: 75 	Ltrain: 0.004382 	Lval: 0.005514
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.005503

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 2
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0002488305190859649
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.13621701509778e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.019865 	Lval: 0.016426
Epoch: 5 	Ltrain: 0.010749 	Lval: 0.010024
Epoch: 10 	Ltrain: 0.009961 	Lval: 0.009482
Epoch: 15 	Ltrain: 0.009575 	Lval: 0.008938
Epoch: 20 	Ltrain: 0.009492 	Lval: 0.008407
Epoch: 25 	Ltrain: 0.009283 	Lval: 0.007972
Epoch: 30 	Ltrain: 0.007892 	Lval: 0.007189
Epoch: 35 	Ltrain: 0.006755 	Lval: 0.006625
Epoch: 40 	Ltrain: 0.006917 	Lval: 0.006239
Epoch: 45 	Ltrain: 0.006041 	Lval: 0.005883
Epoch: 50 	Ltrain: 0.005648 	Lval: 0.005706
Epoch: 55 	Ltrain: 0.005917 	Lval: 0.005632
Epoch: 60 	Ltrain: 0.005563 	Lval: 0.005628
Epoch: 65 	Ltrain: 0.005588 	Lval: 0.005678
Epoch: 70 	Ltrain: 0.005999 	Lval: 0.005579
Epoch 00071: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 75 	Ltrain: 0.005417 	Lval: 0.005510
Epoch: 80 	Ltrain: 0.005734 	Lval: 0.005485
Epoch: 85 	Ltrain: 0.005706 	Lval: 0.005458
Epoch 00087: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 90 	Ltrain: 0.005587 	Lval: 0.005460
Epoch: 95 	Ltrain: 0.005566 	Lval: 0.005459
Epoch 00099: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 100 	Ltrain: 0.006675 	Lval: 0.005457
Epoch: 105 	Ltrain: 0.005372 	Lval: 0.005457
EarlyStopper: stopping at epoch 104 with best_val_loss = 0.005464


	Fold 2/5
Epoch: 1 	Ltrain: 0.022428 	Lval: 0.014233
Epoch: 5 	Ltrain: 0.009728 	Lval: 0.009270
Epoch: 10 	Ltrain: 0.008310 	Lval: 0.008194
Epoch: 15 	Ltrain: 0.007465 	Lval: 0.007351
Epoch: 20 	Ltrain: 0.006449 	Lval: 0.006783
Epoch: 25 	Ltrain: 0.006365 	Lval: 0.006368
Epoch: 30 	Ltrain: 0.006048 	Lval: 0.006067
Epoch: 35 	Ltrain: 0.005802 	Lval: 0.006022
Epoch: 40 	Ltrain: 0.005688 	Lval: 0.005846
Epoch: 45 	Ltrain: 0.005849 	Lval: 0.005762
Epoch: 50 	Ltrain: 0.005469 	Lval: 0.005736
Epoch: 55 	Ltrain: 0.005271 	Lval: 0.005595
Epoch: 60 	Ltrain: 0.005269 	Lval: 0.005519
Epoch: 65 	Ltrain: 0.005171 	Lval: 0.005428
Epoch: 70 	Ltrain: 0.005178 	Lval: 0.005338
Epoch: 75 	Ltrain: 0.005251 	Lval: 0.005352
Epoch: 80 	Ltrain: 0.005019 	Lval: 0.005183
Epoch: 85 	Ltrain: 0.004799 	Lval: 0.005186
Epoch: 90 	Ltrain: 0.005164 	Lval: 0.005104
Epoch: 95 	Ltrain: 0.004692 	Lval: 0.005005
Epoch: 100 	Ltrain: 0.004555 	Lval: 0.004932
Epoch: 105 	Ltrain: 0.004604 	Lval: 0.004902
Epoch: 110 	Ltrain: 0.004414 	Lval: 0.004819
Epoch: 115 	Ltrain: 0.004467 	Lval: 0.004835
Epoch: 120 	Ltrain: 0.004262 	Lval: 0.004722
Epoch: 125 	Ltrain: 0.004261 	Lval: 0.004686
Epoch 00127: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 130 	Ltrain: 0.004255 	Lval: 0.004662
Epoch: 135 	Ltrain: 0.004276 	Lval: 0.004672
Epoch 00139: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 140 	Ltrain: 0.004177 	Lval: 0.004678
Epoch: 145 	Ltrain: 0.004291 	Lval: 0.004674
EarlyStopper: stopping at epoch 145 with best_val_loss = 0.004668


	Fold 3/5
Epoch: 1 	Ltrain: 0.064241 	Lval: 0.041850
Epoch: 5 	Ltrain: 0.010148 	Lval: 0.008396
Epoch: 10 	Ltrain: 0.008344 	Lval: 0.007237
Epoch: 15 	Ltrain: 0.006991 	Lval: 0.006389
Epoch: 20 	Ltrain: 0.006467 	Lval: 0.005998
Epoch: 25 	Ltrain: 0.005807 	Lval: 0.005867
Epoch: 30 	Ltrain: 0.005466 	Lval: 0.005639
Epoch: 35 	Ltrain: 0.005262 	Lval: 0.005503
Epoch: 40 	Ltrain: 0.005087 	Lval: 0.005305
Epoch: 45 	Ltrain: 0.004953 	Lval: 0.005240
Epoch: 50 	Ltrain: 0.004764 	Lval: 0.005208
Epoch: 55 	Ltrain: 0.004697 	Lval: 0.005050
Epoch: 60 	Ltrain: 0.004504 	Lval: 0.004923
Epoch: 65 	Ltrain: 0.004474 	Lval: 0.004889
Epoch: 70 	Ltrain: 0.004389 	Lval: 0.004772
Epoch: 75 	Ltrain: 0.004243 	Lval: 0.004742
Epoch 00076: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 80 	Ltrain: 0.004250 	Lval: 0.004720
Epoch: 85 	Ltrain: 0.004235 	Lval: 0.004711
Epoch 00088: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 90 	Ltrain: 0.004246 	Lval: 0.004702
Epoch: 95 	Ltrain: 0.004213 	Lval: 0.004701
Epoch 00100: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 100 	Ltrain: 0.004341 	Lval: 0.004702
EarlyStopper: stopping at epoch 100 with best_val_loss = 0.004703


	Fold 4/5
Epoch: 1 	Ltrain: 0.015797 	Lval: 0.009645
Epoch: 5 	Ltrain: 0.008494 	Lval: 0.007687
Epoch: 10 	Ltrain: 0.006108 	Lval: 0.006396
Epoch: 15 	Ltrain: 0.005693 	Lval: 0.006092
Epoch: 20 	Ltrain: 0.005303 	Lval: 0.005814
Epoch: 25 	Ltrain: 0.005025 	Lval: 0.005514
Epoch: 30 	Ltrain: 0.004757 	Lval: 0.005262
Epoch: 35 	Ltrain: 0.004498 	Lval: 0.005052
Epoch: 40 	Ltrain: 0.004400 	Lval: 0.005049
Epoch: 45 	Ltrain: 0.004156 	Lval: 0.004884
Epoch: 50 	Ltrain: 0.004148 	Lval: 0.004803
Epoch: 55 	Ltrain: 0.003995 	Lval: 0.004701
Epoch 00057: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 60 	Ltrain: 0.003911 	Lval: 0.004690
Epoch: 65 	Ltrain: 0.003926 	Lval: 0.004682
Epoch: 70 	Ltrain: 0.003896 	Lval: 0.004680
Epoch 00071: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 75 	Ltrain: 0.003882 	Lval: 0.004683
Epoch: 80 	Ltrain: 0.003878 	Lval: 0.004686
Epoch 00083: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 85 	Ltrain: 0.003915 	Lval: 0.004685
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.004678


	Fold 5/5
Epoch: 1 	Ltrain: 0.017232 	Lval: 0.011389
Epoch: 5 	Ltrain: 0.007911 	Lval: 0.007798
Epoch: 10 	Ltrain: 0.006315 	Lval: 0.006847
Epoch: 15 	Ltrain: 0.005677 	Lval: 0.006355
Epoch: 20 	Ltrain: 0.005315 	Lval: 0.005967
Epoch: 25 	Ltrain: 0.004974 	Lval: 0.005594
Epoch: 30 	Ltrain: 0.004650 	Lval: 0.005360
Epoch: 35 	Ltrain: 0.004561 	Lval: 0.005129
Epoch 00039: reducing learning rate of group 0 to 2.4883e-05.
Epoch: 40 	Ltrain: 0.004177 	Lval: 0.005107
Epoch: 45 	Ltrain: 0.004162 	Lval: 0.005103
Epoch: 50 	Ltrain: 0.004152 	Lval: 0.005092
Epoch 00052: reducing learning rate of group 0 to 2.4883e-06.
Epoch: 55 	Ltrain: 0.004144 	Lval: 0.005066
Epoch: 60 	Ltrain: 0.004198 	Lval: 0.005069
Epoch 00064: reducing learning rate of group 0 to 2.4883e-07.
Epoch: 65 	Ltrain: 0.004159 	Lval: 0.005068
Epoch: 70 	Ltrain: 0.004135 	Lval: 0.005067
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.005064

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0012757488347627612
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.30569702558947e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 26
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.012635 	Lval: 0.010660
Epoch: 5 	Ltrain: 0.006484 	Lval: 0.006157
Epoch: 10 	Ltrain: 0.005688 	Lval: 0.005539
Epoch: 15 	Ltrain: 0.005103 	Lval: 0.005011
Epoch: 20 	Ltrain: 0.004829 	Lval: 0.004701
Epoch: 25 	Ltrain: 0.004657 	Lval: 0.005400
Epoch 00028: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 30 	Ltrain: 0.004315 	Lval: 0.004349
Epoch: 35 	Ltrain: 0.004492 	Lval: 0.004356
Epoch: 40 	Ltrain: 0.004485 	Lval: 0.004292
Epoch: 45 	Ltrain: 0.004094 	Lval: 0.004261
Epoch: 50 	Ltrain: 0.004283 	Lval: 0.004247
Epoch: 55 	Ltrain: 0.004207 	Lval: 0.004228
Epoch: 60 	Ltrain: 0.004035 	Lval: 0.004203
Epoch 00063: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 65 	Ltrain: 0.003940 	Lval: 0.004200
Epoch: 70 	Ltrain: 0.003975 	Lval: 0.004190
Epoch 00075: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 75 	Ltrain: 0.004098 	Lval: 0.004188
Epoch: 80 	Ltrain: 0.004006 	Lval: 0.004188
Epoch: 85 	Ltrain: 0.003914 	Lval: 0.004191
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.004188


	Fold 2/5
Epoch: 1 	Ltrain: 0.012342 	Lval: 0.008215
Epoch: 5 	Ltrain: 0.005711 	Lval: 0.005662
Epoch: 10 	Ltrain: 0.004779 	Lval: 0.004901
Epoch: 15 	Ltrain: 0.004360 	Lval: 0.004682
Epoch: 20 	Ltrain: 0.004052 	Lval: 0.004378
Epoch: 25 	Ltrain: 0.003913 	Lval: 0.004461
Epoch: 30 	Ltrain: 0.003262 	Lval: 0.004498
Epoch 00033: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 35 	Ltrain: 0.003148 	Lval: 0.003757
Epoch: 40 	Ltrain: 0.003094 	Lval: 0.003661
Epoch: 45 	Ltrain: 0.003171 	Lval: 0.003602
Epoch: 50 	Ltrain: 0.003050 	Lval: 0.003597
Epoch: 55 	Ltrain: 0.002991 	Lval: 0.003570
Epoch: 60 	Ltrain: 0.002950 	Lval: 0.003542
Epoch: 65 	Ltrain: 0.002979 	Lval: 0.003534
Epoch 00068: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 70 	Ltrain: 0.002907 	Lval: 0.003512
Epoch: 75 	Ltrain: 0.002980 	Lval: 0.003509
Epoch: 80 	Ltrain: 0.002938 	Lval: 0.003509
Epoch 00082: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 85 	Ltrain: 0.002930 	Lval: 0.003509
Epoch: 90 	Ltrain: 0.002951 	Lval: 0.003508
Epoch 00094: reducing learning rate of group 0 to 1.2757e-07.
Epoch: 95 	Ltrain: 0.002917 	Lval: 0.003507
EarlyStopper: stopping at epoch 97 with best_val_loss = 0.003507


	Fold 3/5
Epoch: 1 	Ltrain: 0.010081 	Lval: 0.007639
Epoch: 5 	Ltrain: 0.005160 	Lval: 0.005832
Epoch: 10 	Ltrain: 0.004170 	Lval: 0.004669
Epoch 00013: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 15 	Ltrain: 0.003768 	Lval: 0.004408
Epoch: 20 	Ltrain: 0.003760 	Lval: 0.004359
Epoch: 25 	Ltrain: 0.003721 	Lval: 0.004286
Epoch 00029: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 30 	Ltrain: 0.003679 	Lval: 0.004307
Epoch: 35 	Ltrain: 0.003678 	Lval: 0.004290
Epoch: 40 	Ltrain: 0.003716 	Lval: 0.004288
Epoch: 45 	Ltrain: 0.003690 	Lval: 0.004285
Epoch 00046: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 50 	Ltrain: 0.003666 	Lval: 0.004281
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.004286


	Fold 4/5
Epoch: 1 	Ltrain: 0.016096 	Lval: 0.008112
Epoch: 5 	Ltrain: 0.005006 	Lval: 0.005338
Epoch: 10 	Ltrain: 0.004023 	Lval: 0.004901
Epoch 00013: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 15 	Ltrain: 0.003696 	Lval: 0.004538
Epoch: 20 	Ltrain: 0.003641 	Lval: 0.004535
Epoch: 25 	Ltrain: 0.003651 	Lval: 0.004422
Epoch 00029: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 30 	Ltrain: 0.003597 	Lval: 0.004434
Epoch: 35 	Ltrain: 0.003601 	Lval: 0.004432
Epoch: 40 	Ltrain: 0.003590 	Lval: 0.004433
Epoch 00041: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 45 	Ltrain: 0.003580 	Lval: 0.004427
Epoch: 50 	Ltrain: 0.003586 	Lval: 0.004430
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.004422


	Fold 5/5
Epoch: 1 	Ltrain: 0.012397 	Lval: 0.007655
Epoch: 5 	Ltrain: 0.004787 	Lval: 0.005495
Epoch: 10 	Ltrain: 0.003904 	Lval: 0.004769
Epoch: 15 	Ltrain: 0.003559 	Lval: 0.004436
Epoch: 20 	Ltrain: 0.003247 	Lval: 0.004345
Epoch: 25 	Ltrain: 0.002959 	Lval: 0.003775
Epoch: 30 	Ltrain: 0.002844 	Lval: 0.003689
Epoch 00031: reducing learning rate of group 0 to 1.2757e-04.
Epoch: 35 	Ltrain: 0.002667 	Lval: 0.003611
Epoch: 40 	Ltrain: 0.002686 	Lval: 0.003534
Epoch: 45 	Ltrain: 0.002661 	Lval: 0.003544
Epoch 00047: reducing learning rate of group 0 to 1.2757e-05.
Epoch: 50 	Ltrain: 0.002624 	Lval: 0.003522
Epoch: 55 	Ltrain: 0.002623 	Lval: 0.003514
Epoch 00059: reducing learning rate of group 0 to 1.2757e-06.
Epoch: 60 	Ltrain: 0.002619 	Lval: 0.003515
Epoch: 65 	Ltrain: 0.002620 	Lval: 0.003517
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.003518

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00030969089415100936
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.4813380065359772e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.044521 	Lval: 0.026279
Epoch: 5 	Ltrain: 0.010899 	Lval: 0.010447
Epoch: 10 	Ltrain: 0.011401 	Lval: 0.009928
Epoch: 15 	Ltrain: 0.009535 	Lval: 0.008894
Epoch: 20 	Ltrain: 0.007698 	Lval: 0.007256
Epoch: 25 	Ltrain: 0.006694 	Lval: 0.006635
Epoch: 30 	Ltrain: 0.006495 	Lval: 0.006491
Epoch: 35 	Ltrain: 0.006299 	Lval: 0.006142
Epoch: 40 	Ltrain: 0.006496 	Lval: 0.006473
Epoch: 45 	Ltrain: 0.005802 	Lval: 0.005872
Epoch: 50 	Ltrain: 0.005883 	Lval: 0.005821
Epoch: 55 	Ltrain: 0.006256 	Lval: 0.005741
Epoch 00057: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 60 	Ltrain: 0.005762 	Lval: 0.005525
Epoch: 65 	Ltrain: 0.005668 	Lval: 0.005569
Epoch: 70 	Ltrain: 0.005603 	Lval: 0.005498
Epoch 00074: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 75 	Ltrain: 0.005613 	Lval: 0.005501
Epoch: 80 	Ltrain: 0.005526 	Lval: 0.005499
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.005499


	Fold 2/5
Epoch: 1 	Ltrain: 0.022512 	Lval: 0.010662
Epoch: 5 	Ltrain: 0.010272 	Lval: 0.009688
Epoch: 10 	Ltrain: 0.007477 	Lval: 0.007268
Epoch: 15 	Ltrain: 0.006590 	Lval: 0.006467
Epoch: 20 	Ltrain: 0.005995 	Lval: 0.006106
Epoch: 25 	Ltrain: 0.005736 	Lval: 0.005831
Epoch: 30 	Ltrain: 0.005447 	Lval: 0.005482
Epoch: 35 	Ltrain: 0.005091 	Lval: 0.005685
Epoch: 40 	Ltrain: 0.004978 	Lval: 0.005362
Epoch: 45 	Ltrain: 0.004715 	Lval: 0.005119
Epoch: 50 	Ltrain: 0.004576 	Lval: 0.005011
Epoch: 55 	Ltrain: 0.004666 	Lval: 0.004909
Epoch 00057: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 60 	Ltrain: 0.004306 	Lval: 0.004827
Epoch: 65 	Ltrain: 0.004365 	Lval: 0.004850
Epoch: 70 	Ltrain: 0.004367 	Lval: 0.004825
Epoch 00071: reducing learning rate of group 0 to 3.0969e-06.
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.004827


	Fold 3/5
Epoch: 1 	Ltrain: 0.013665 	Lval: 0.010780
Epoch: 5 	Ltrain: 0.010405 	Lval: 0.009083
Epoch: 10 	Ltrain: 0.006552 	Lval: 0.006572
Epoch: 15 	Ltrain: 0.005785 	Lval: 0.005797
Epoch: 20 	Ltrain: 0.005102 	Lval: 0.005439
Epoch 00022: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 25 	Ltrain: 0.004616 	Lval: 0.005023
Epoch: 30 	Ltrain: 0.004574 	Lval: 0.005003
Epoch: 35 	Ltrain: 0.004566 	Lval: 0.004933
Epoch: 40 	Ltrain: 0.004463 	Lval: 0.004920
Epoch 00041: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 45 	Ltrain: 0.004459 	Lval: 0.004908
Epoch: 50 	Ltrain: 0.004463 	Lval: 0.004907
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.004914


	Fold 4/5
Epoch: 1 	Ltrain: 0.033734 	Lval: 0.011405
Epoch: 5 	Ltrain: 0.010512 	Lval: 0.009850
Epoch: 10 	Ltrain: 0.006428 	Lval: 0.006800
Epoch: 15 	Ltrain: 0.005593 	Lval: 0.006102
Epoch: 20 	Ltrain: 0.004818 	Lval: 0.005349
Epoch: 25 	Ltrain: 0.004437 	Lval: 0.005228
Epoch: 30 	Ltrain: 0.004250 	Lval: 0.004958
Epoch: 35 	Ltrain: 0.004162 	Lval: 0.004748
Epoch: 40 	Ltrain: 0.004016 	Lval: 0.004821
Epoch 00042: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 45 	Ltrain: 0.003846 	Lval: 0.004659
Epoch: 50 	Ltrain: 0.003808 	Lval: 0.004650
Epoch 00054: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 55 	Ltrain: 0.003795 	Lval: 0.004636
Epoch: 60 	Ltrain: 0.003791 	Lval: 0.004644
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.004635


	Fold 5/5
Epoch: 1 	Ltrain: 0.047284 	Lval: 0.013776
Epoch: 5 	Ltrain: 0.009576 	Lval: 0.009372
Epoch: 10 	Ltrain: 0.006597 	Lval: 0.007350
Epoch: 15 	Ltrain: 0.005800 	Lval: 0.006568
Epoch: 20 	Ltrain: 0.005057 	Lval: 0.005750
Epoch: 25 	Ltrain: 0.004579 	Lval: 0.005237
Epoch: 30 	Ltrain: 0.004324 	Lval: 0.005186
Epoch: 35 	Ltrain: 0.004205 	Lval: 0.005170
Epoch: 40 	Ltrain: 0.004051 	Lval: 0.004757
Epoch: 45 	Ltrain: 0.003960 	Lval: 0.004732
Epoch 00047: reducing learning rate of group 0 to 3.0969e-05.
Epoch: 50 	Ltrain: 0.003788 	Lval: 0.004713
Epoch: 55 	Ltrain: 0.003761 	Lval: 0.004634
Epoch 00059: reducing learning rate of group 0 to 3.0969e-06.
Epoch: 60 	Ltrain: 0.003758 	Lval: 0.004683
Epoch: 65 	Ltrain: 0.003749 	Lval: 0.004671
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.004641

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005745115971827479
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.22616303415697e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 23
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.045758 	Lval: 0.011121
Epoch: 5 	Ltrain: 0.007343 	Lval: 0.006025
Epoch: 10 	Ltrain: 0.005378 	Lval: 0.005514
Epoch: 15 	Ltrain: 0.004911 	Lval: 0.005235
Epoch 00016: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 20 	Ltrain: 0.004253 	Lval: 0.004472
Epoch: 25 	Ltrain: 0.004374 	Lval: 0.004432
Epoch: 30 	Ltrain: 0.004467 	Lval: 0.004357
Epoch: 35 	Ltrain: 0.004177 	Lval: 0.004316
Epoch: 40 	Ltrain: 0.004274 	Lval: 0.004246
Epoch 00044: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 45 	Ltrain: 0.004154 	Lval: 0.004230
Epoch: 50 	Ltrain: 0.004105 	Lval: 0.004246
Epoch: 55 	Ltrain: 0.004038 	Lval: 0.004194
Epoch: 60 	Ltrain: 0.003949 	Lval: 0.004184
Epoch: 65 	Ltrain: 0.004239 	Lval: 0.004183
Epoch: 70 	Ltrain: 0.004018 	Lval: 0.004182
Epoch 00073: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 75 	Ltrain: 0.003931 	Lval: 0.004170
Epoch: 80 	Ltrain: 0.004028 	Lval: 0.004169
Epoch: 85 	Ltrain: 0.003996 	Lval: 0.004168
Epoch 00086: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 90 	Ltrain: 0.004132 	Lval: 0.004169
Epoch: 95 	Ltrain: 0.004043 	Lval: 0.004169
Epoch 00098: reducing learning rate of group 0 to 5.7451e-08.
Epoch: 100 	Ltrain: 0.004038 	Lval: 0.004170
EarlyStopper: stopping at epoch 103 with best_val_loss = 0.004167


	Fold 2/5
Epoch: 1 	Ltrain: 0.039060 	Lval: 0.008577
Epoch: 5 	Ltrain: 0.005357 	Lval: 0.005299
Epoch: 10 	Ltrain: 0.004529 	Lval: 0.005820
Epoch: 15 	Ltrain: 0.003878 	Lval: 0.004523
Epoch 00018: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 20 	Ltrain: 0.003251 	Lval: 0.003901
Epoch: 25 	Ltrain: 0.003204 	Lval: 0.003759
Epoch: 30 	Ltrain: 0.003127 	Lval: 0.003761
Epoch 00033: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 35 	Ltrain: 0.003084 	Lval: 0.003681
Epoch: 40 	Ltrain: 0.003049 	Lval: 0.003675
Epoch: 45 	Ltrain: 0.003036 	Lval: 0.003661
Epoch: 50 	Ltrain: 0.003062 	Lval: 0.003662
Epoch: 55 	Ltrain: 0.002992 	Lval: 0.003648
Epoch: 60 	Ltrain: 0.003014 	Lval: 0.003650
Epoch: 65 	Ltrain: 0.003007 	Lval: 0.003652
Epoch: 70 	Ltrain: 0.003025 	Lval: 0.003627
Epoch 00074: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 75 	Ltrain: 0.002960 	Lval: 0.003627
Epoch: 80 	Ltrain: 0.002971 	Lval: 0.003626
Epoch: 85 	Ltrain: 0.002984 	Lval: 0.003625
Epoch 00086: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 90 	Ltrain: 0.002991 	Lval: 0.003626
EarlyStopper: stopping at epoch 90 with best_val_loss = 0.003628


	Fold 3/5
Epoch: 1 	Ltrain: 0.025554 	Lval: 0.007775
Epoch: 5 	Ltrain: 0.004576 	Lval: 0.005020
Epoch: 10 	Ltrain: 0.004489 	Lval: 0.004481
Epoch: 15 	Ltrain: 0.003911 	Lval: 0.004743
Epoch 00017: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 20 	Ltrain: 0.003359 	Lval: 0.004013
Epoch: 25 	Ltrain: 0.003107 	Lval: 0.003792
Epoch: 30 	Ltrain: 0.003075 	Lval: 0.003706
Epoch: 35 	Ltrain: 0.002919 	Lval: 0.003589
Epoch: 40 	Ltrain: 0.002921 	Lval: 0.003555
Epoch 00042: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 45 	Ltrain: 0.002813 	Lval: 0.003487
Epoch: 50 	Ltrain: 0.002771 	Lval: 0.003473
Epoch: 55 	Ltrain: 0.002780 	Lval: 0.003475
Epoch: 60 	Ltrain: 0.002774 	Lval: 0.003464
Epoch: 65 	Ltrain: 0.002774 	Lval: 0.003456
Epoch: 70 	Ltrain: 0.002789 	Lval: 0.003445
Epoch 00071: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 75 	Ltrain: 0.002749 	Lval: 0.003443
Epoch: 80 	Ltrain: 0.002746 	Lval: 0.003443
Epoch 00083: reducing learning rate of group 0 to 5.7451e-07.
Epoch: 85 	Ltrain: 0.002764 	Lval: 0.003446
EarlyStopper: stopping at epoch 85 with best_val_loss = 0.003452


	Fold 4/5
Epoch: 1 	Ltrain: 0.018523 	Lval: 0.007245
Epoch: 5 	Ltrain: 0.004342 	Lval: 0.004884
Epoch: 10 	Ltrain: 0.003800 	Lval: 0.004362
Epoch: 15 	Ltrain: 0.004115 	Lval: 0.004756
Epoch: 20 	Ltrain: 0.003216 	Lval: 0.003787
Epoch 00025: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 25 	Ltrain: 0.003228 	Lval: 0.003587
Epoch: 30 	Ltrain: 0.002527 	Lval: 0.003272
Epoch: 35 	Ltrain: 0.002499 	Lval: 0.003280
Epoch: 40 	Ltrain: 0.002470 	Lval: 0.003165
Epoch: 45 	Ltrain: 0.002447 	Lval: 0.003130
Epoch: 50 	Ltrain: 0.002424 	Lval: 0.003127
Epoch 00051: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 55 	Ltrain: 0.002355 	Lval: 0.003090
Epoch: 60 	Ltrain: 0.002354 	Lval: 0.003083
Epoch: 65 	Ltrain: 0.002344 	Lval: 0.003073
Epoch: 70 	Ltrain: 0.002336 	Lval: 0.003063
Epoch 00074: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 75 	Ltrain: 0.002331 	Lval: 0.003083
Epoch: 80 	Ltrain: 0.002352 	Lval: 0.003072
Epoch: 85 	Ltrain: 0.002325 	Lval: 0.003071
Epoch 00086: reducing learning rate of group 0 to 5.7451e-07.
EarlyStopper: stopping at epoch 87 with best_val_loss = 0.003073


	Fold 5/5
Epoch: 1 	Ltrain: 0.022727 	Lval: 0.009806
Epoch: 5 	Ltrain: 0.004662 	Lval: 0.005769
Epoch: 10 	Ltrain: 0.004090 	Lval: 0.004617
Epoch: 15 	Ltrain: 0.003459 	Lval: 0.004754
Epoch: 20 	Ltrain: 0.002991 	Lval: 0.003859
Epoch 00025: reducing learning rate of group 0 to 5.7451e-04.
Epoch: 25 	Ltrain: 0.003344 	Lval: 0.004065
Epoch: 30 	Ltrain: 0.002582 	Lval: 0.003516
Epoch: 35 	Ltrain: 0.002550 	Lval: 0.003417
Epoch: 40 	Ltrain: 0.002540 	Lval: 0.003349
Epoch: 45 	Ltrain: 0.002490 	Lval: 0.003288
Epoch 00049: reducing learning rate of group 0 to 5.7451e-05.
Epoch: 50 	Ltrain: 0.002482 	Lval: 0.003257
Epoch: 55 	Ltrain: 0.002434 	Lval: 0.003273
Epoch: 60 	Ltrain: 0.002431 	Lval: 0.003264
Epoch: 65 	Ltrain: 0.002429 	Lval: 0.003255
Epoch 00068: reducing learning rate of group 0 to 5.7451e-06.
Epoch: 70 	Ltrain: 0.002426 	Lval: 0.003252
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.003257

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007072780301199806
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.023976847523278e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.070117 	Lval: 0.014071
Epoch: 5 	Ltrain: 0.010002 	Lval: 0.007219
Epoch: 10 	Ltrain: 0.005858 	Lval: 0.005379
Epoch 00015: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 15 	Ltrain: 0.005146 	Lval: 0.005373
Epoch: 20 	Ltrain: 0.004464 	Lval: 0.004579
Epoch: 25 	Ltrain: 0.004496 	Lval: 0.004494
Epoch: 30 	Ltrain: 0.004369 	Lval: 0.004467
Epoch: 35 	Ltrain: 0.004345 	Lval: 0.004371
Epoch: 40 	Ltrain: 0.004326 	Lval: 0.004328
Epoch 00045: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 45 	Ltrain: 0.004399 	Lval: 0.004478
Epoch: 50 	Ltrain: 0.004127 	Lval: 0.004265
Epoch: 55 	Ltrain: 0.004158 	Lval: 0.004258
Epoch 00057: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 60 	Ltrain: 0.004177 	Lval: 0.004264
Epoch: 65 	Ltrain: 0.004219 	Lval: 0.004261
Epoch 00069: reducing learning rate of group 0 to 7.0728e-07.
EarlyStopper: stopping at epoch 68 with best_val_loss = 0.004256


	Fold 2/5
Epoch: 1 	Ltrain: 0.078642 	Lval: 0.010819
Epoch: 5 	Ltrain: 0.005520 	Lval: 0.005397
Epoch 00010: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 10 	Ltrain: 0.004601 	Lval: 0.005169
Epoch: 15 	Ltrain: 0.003769 	Lval: 0.004388
Epoch: 20 	Ltrain: 0.003734 	Lval: 0.004291
Epoch: 25 	Ltrain: 0.003576 	Lval: 0.004139
Epoch: 30 	Ltrain: 0.003414 	Lval: 0.004050
Epoch: 35 	Ltrain: 0.003356 	Lval: 0.003845
Epoch: 40 	Ltrain: 0.003064 	Lval: 0.003788
Epoch: 45 	Ltrain: 0.003049 	Lval: 0.003683
Epoch: 50 	Ltrain: 0.003018 	Lval: 0.003535
Epoch 00051: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 55 	Ltrain: 0.002807 	Lval: 0.003446
Epoch: 60 	Ltrain: 0.002788 	Lval: 0.003437
Epoch: 65 	Ltrain: 0.002832 	Lval: 0.003409
Epoch: 70 	Ltrain: 0.002754 	Lval: 0.003397
Epoch: 75 	Ltrain: 0.002817 	Lval: 0.003393
Epoch: 80 	Ltrain: 0.002806 	Lval: 0.003366
Epoch: 85 	Ltrain: 0.002738 	Lval: 0.003360
Epoch 00088: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 90 	Ltrain: 0.002759 	Lval: 0.003344
Epoch: 95 	Ltrain: 0.002785 	Lval: 0.003342
Epoch: 100 	Ltrain: 0.002790 	Lval: 0.003338
Epoch 00102: reducing learning rate of group 0 to 7.0728e-07.
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.003342


	Fold 3/5
Epoch: 1 	Ltrain: 0.028887 	Lval: 0.009253
Epoch: 5 	Ltrain: 0.004812 	Lval: 0.005595
Epoch: 10 	Ltrain: 0.003854 	Lval: 0.004284
Epoch: 15 	Ltrain: 0.003508 	Lval: 0.003914
Epoch: 20 	Ltrain: 0.002822 	Lval: 0.003373
Epoch 00024: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 25 	Ltrain: 0.002874 	Lval: 0.003205
Epoch: 30 	Ltrain: 0.002390 	Lval: 0.003085
Epoch: 35 	Ltrain: 0.002302 	Lval: 0.002887
Epoch: 40 	Ltrain: 0.002250 	Lval: 0.002802
Epoch: 45 	Ltrain: 0.002214 	Lval: 0.002834
Epoch: 50 	Ltrain: 0.002135 	Lval: 0.002688
Epoch: 55 	Ltrain: 0.002068 	Lval: 0.002587
Epoch: 60 	Ltrain: 0.002062 	Lval: 0.002557
Epoch: 65 	Ltrain: 0.001964 	Lval: 0.002487
Epoch: 70 	Ltrain: 0.001916 	Lval: 0.002348
Epoch: 75 	Ltrain: 0.001843 	Lval: 0.002375
Epoch: 80 	Ltrain: 0.001744 	Lval: 0.002149
Epoch: 85 	Ltrain: 0.001744 	Lval: 0.002042
Epoch: 90 	Ltrain: 0.001635 	Lval: 0.001941
Epoch: 95 	Ltrain: 0.001554 	Lval: 0.001824
Epoch: 100 	Ltrain: 0.001544 	Lval: 0.001819
Epoch: 105 	Ltrain: 0.001428 	Lval: 0.001675
Epoch: 110 	Ltrain: 0.001369 	Lval: 0.001518
Epoch: 115 	Ltrain: 0.001289 	Lval: 0.001364
Epoch: 120 	Ltrain: 0.001165 	Lval: 0.001274
Epoch: 125 	Ltrain: 0.001174 	Lval: 0.001202
Epoch: 130 	Ltrain: 0.001085 	Lval: 0.001101
Epoch: 135 	Ltrain: 0.001031 	Lval: 0.001122
Epoch: 140 	Ltrain: 0.000918 	Lval: 0.000949
Epoch: 145 	Ltrain: 0.000817 	Lval: 0.000842
Epoch: 150 	Ltrain: 0.000819 	Lval: 0.000820
Epoch 00153: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 155 	Ltrain: 0.000654 	Lval: 0.000675
Epoch: 160 	Ltrain: 0.000621 	Lval: 0.000651
Epoch: 165 	Ltrain: 0.000607 	Lval: 0.000638
Epoch: 170 	Ltrain: 0.000600 	Lval: 0.000628
Epoch: 175 	Ltrain: 0.000597 	Lval: 0.000617
Epoch: 180 	Ltrain: 0.000582 	Lval: 0.000608
Epoch: 185 	Ltrain: 0.000575 	Lval: 0.000597
Epoch: 190 	Ltrain: 0.000564 	Lval: 0.000587
Epoch: 195 	Ltrain: 0.000561 	Lval: 0.000579
Epoch: 200 	Ltrain: 0.000553 	Lval: 0.000570
Epoch: 205 	Ltrain: 0.000543 	Lval: 0.000561
Epoch: 210 	Ltrain: 0.000538 	Lval: 0.000552
Epoch: 215 	Ltrain: 0.000524 	Lval: 0.000540
Epoch: 220 	Ltrain: 0.000516 	Lval: 0.000531
Epoch: 225 	Ltrain: 0.000505 	Lval: 0.000520
Epoch: 230 	Ltrain: 0.000499 	Lval: 0.000513
Epoch: 235 	Ltrain: 0.000488 	Lval: 0.000503
Epoch: 240 	Ltrain: 0.000485 	Lval: 0.000495
Epoch: 245 	Ltrain: 0.000476 	Lval: 0.000485
Epoch: 250 	Ltrain: 0.000461 	Lval: 0.000471
Epoch: 255 	Ltrain: 0.000452 	Lval: 0.000467
Epoch: 260 	Ltrain: 0.000447 	Lval: 0.000458
Epoch: 265 	Ltrain: 0.000434 	Lval: 0.000446
Epoch: 270 	Ltrain: 0.000424 	Lval: 0.000437
Epoch: 275 	Ltrain: 0.000416 	Lval: 0.000429
Epoch: 280 	Ltrain: 0.000411 	Lval: 0.000419
Epoch: 285 	Ltrain: 0.000404 	Lval: 0.000414
Epoch: 290 	Ltrain: 0.000395 	Lval: 0.000402
Epoch: 295 	Ltrain: 0.000385 	Lval: 0.000394
Epoch: 300 	Ltrain: 0.000375 	Lval: 0.000386
Epoch: 305 	Ltrain: 0.000370 	Lval: 0.000376
Epoch: 310 	Ltrain: 0.000360 	Lval: 0.000368
Epoch: 315 	Ltrain: 0.000353 	Lval: 0.000360
Epoch: 320 	Ltrain: 0.000345 	Lval: 0.000350
Epoch: 325 	Ltrain: 0.000337 	Lval: 0.000343
Epoch: 330 	Ltrain: 0.000335 	Lval: 0.000340
Epoch: 335 	Ltrain: 0.000324 	Lval: 0.000332
Epoch: 340 	Ltrain: 0.000315 	Lval: 0.000322
Epoch: 345 	Ltrain: 0.000309 	Lval: 0.000314
Epoch: 350 	Ltrain: 0.000304 	Lval: 0.000307
Epoch: 355 	Ltrain: 0.000298 	Lval: 0.000300
Epoch: 360 	Ltrain: 0.000290 	Lval: 0.000292
Epoch: 365 	Ltrain: 0.000283 	Lval: 0.000291
Epoch 00368: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 370 	Ltrain: 0.000271 	Lval: 0.000275
Epoch: 375 	Ltrain: 0.000269 	Lval: 0.000273
Epoch: 380 	Ltrain: 0.000267 	Lval: 0.000272
Epoch: 385 	Ltrain: 0.000267 	Lval: 0.000271
EarlyStopper: stopping at epoch 386 with best_val_loss = 0.000277


	Fold 4/5
Epoch: 1 	Ltrain: 0.026404 	Lval: 0.007056
Epoch: 5 	Ltrain: 0.004255 	Lval: 0.004886
Epoch: 10 	Ltrain: 0.003715 	Lval: 0.004235
Epoch: 15 	Ltrain: 0.003198 	Lval: 0.003731
Epoch: 20 	Ltrain: 0.002946 	Lval: 0.003528
Epoch: 25 	Ltrain: 0.002850 	Lval: 0.003568
Epoch: 30 	Ltrain: 0.002329 	Lval: 0.003304
Epoch 00031: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 35 	Ltrain: 0.001937 	Lval: 0.002527
Epoch: 40 	Ltrain: 0.001856 	Lval: 0.002450
Epoch: 45 	Ltrain: 0.001787 	Lval: 0.002320
Epoch: 50 	Ltrain: 0.001739 	Lval: 0.002270
Epoch: 55 	Ltrain: 0.001693 	Lval: 0.002173
Epoch: 60 	Ltrain: 0.001643 	Lval: 0.002066
Epoch: 65 	Ltrain: 0.001589 	Lval: 0.002042
Epoch: 70 	Ltrain: 0.001551 	Lval: 0.001961
Epoch: 75 	Ltrain: 0.001480 	Lval: 0.001792
Epoch: 80 	Ltrain: 0.001387 	Lval: 0.001723
Epoch: 85 	Ltrain: 0.001344 	Lval: 0.001588
Epoch: 90 	Ltrain: 0.001246 	Lval: 0.001571
Epoch: 95 	Ltrain: 0.001199 	Lval: 0.001462
Epoch: 100 	Ltrain: 0.001109 	Lval: 0.001288
Epoch: 105 	Ltrain: 0.001032 	Lval: 0.001146
Epoch: 110 	Ltrain: 0.000983 	Lval: 0.001100
Epoch: 115 	Ltrain: 0.000926 	Lval: 0.000994
Epoch 00118: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 120 	Ltrain: 0.000759 	Lval: 0.000844
Epoch: 125 	Ltrain: 0.000732 	Lval: 0.000819
Epoch: 130 	Ltrain: 0.000718 	Lval: 0.000803
Epoch: 135 	Ltrain: 0.000710 	Lval: 0.000787
Epoch: 140 	Ltrain: 0.000698 	Lval: 0.000774
Epoch: 145 	Ltrain: 0.000686 	Lval: 0.000759
Epoch: 150 	Ltrain: 0.000678 	Lval: 0.000746
Epoch: 155 	Ltrain: 0.000670 	Lval: 0.000737
Epoch: 160 	Ltrain: 0.000660 	Lval: 0.000717
Epoch: 165 	Ltrain: 0.000651 	Lval: 0.000707
Epoch: 170 	Ltrain: 0.000636 	Lval: 0.000695
Epoch: 175 	Ltrain: 0.000626 	Lval: 0.000679
Epoch: 180 	Ltrain: 0.000618 	Lval: 0.000672
Epoch: 185 	Ltrain: 0.000606 	Lval: 0.000649
Epoch: 190 	Ltrain: 0.000595 	Lval: 0.000638
Epoch: 195 	Ltrain: 0.000585 	Lval: 0.000622
Epoch: 200 	Ltrain: 0.000574 	Lval: 0.000607
Epoch: 205 	Ltrain: 0.000562 	Lval: 0.000592
Epoch: 210 	Ltrain: 0.000553 	Lval: 0.000582
Epoch: 215 	Ltrain: 0.000543 	Lval: 0.000571
Epoch: 220 	Ltrain: 0.000533 	Lval: 0.000558
Epoch: 225 	Ltrain: 0.000519 	Lval: 0.000542
Epoch: 230 	Ltrain: 0.000510 	Lval: 0.000531
Epoch: 235 	Ltrain: 0.000496 	Lval: 0.000516
Epoch: 240 	Ltrain: 0.000488 	Lval: 0.000508
Epoch: 245 	Ltrain: 0.000478 	Lval: 0.000496
Epoch: 250 	Ltrain: 0.000466 	Lval: 0.000482
Epoch: 255 	Ltrain: 0.000456 	Lval: 0.000470
Epoch: 260 	Ltrain: 0.000449 	Lval: 0.000458
Epoch: 265 	Ltrain: 0.000435 	Lval: 0.000446
Epoch: 270 	Ltrain: 0.000428 	Lval: 0.000441
Epoch: 275 	Ltrain: 0.000414 	Lval: 0.000425
Epoch: 280 	Ltrain: 0.000408 	Lval: 0.000417
Epoch: 285 	Ltrain: 0.000397 	Lval: 0.000403
Epoch: 290 	Ltrain: 0.000388 	Lval: 0.000396
Epoch: 295 	Ltrain: 0.000381 	Lval: 0.000390
Epoch: 300 	Ltrain: 0.000371 	Lval: 0.000377
Epoch: 305 	Ltrain: 0.000363 	Lval: 0.000370
Epoch: 310 	Ltrain: 0.000352 	Lval: 0.000358
Epoch: 315 	Ltrain: 0.000345 	Lval: 0.000354
Epoch: 320 	Ltrain: 0.000336 	Lval: 0.000344
Epoch: 325 	Ltrain: 0.000329 	Lval: 0.000341
Epoch: 330 	Ltrain: 0.000319 	Lval: 0.000323
Epoch: 335 	Ltrain: 0.000311 	Lval: 0.000313
Epoch: 340 	Ltrain: 0.000304 	Lval: 0.000309
Epoch: 345 	Ltrain: 0.000299 	Lval: 0.000301
Epoch: 350 	Ltrain: 0.000292 	Lval: 0.000294
Epoch: 355 	Ltrain: 0.000285 	Lval: 0.000288
Epoch: 360 	Ltrain: 0.000278 	Lval: 0.000281
Epoch: 365 	Ltrain: 0.000269 	Lval: 0.000275
Epoch 00368: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 370 	Ltrain: 0.000256 	Lval: 0.000261
Epoch: 375 	Ltrain: 0.000252 	Lval: 0.000259
Epoch: 380 	Ltrain: 0.000252 	Lval: 0.000258
Epoch: 385 	Ltrain: 0.000251 	Lval: 0.000257
EarlyStopper: stopping at epoch 386 with best_val_loss = 0.000265


	Fold 5/5
Epoch: 1 	Ltrain: 0.035349 	Lval: 0.011039
Epoch: 5 	Ltrain: 0.004228 	Lval: 0.005075
Epoch: 10 	Ltrain: 0.003172 	Lval: 0.004106
Epoch: 15 	Ltrain: 0.002957 	Lval: 0.003500
Epoch 00020: reducing learning rate of group 0 to 7.0728e-04.
Epoch: 20 	Ltrain: 0.002859 	Lval: 0.004795
Epoch: 25 	Ltrain: 0.002202 	Lval: 0.002877
Epoch: 30 	Ltrain: 0.002158 	Lval: 0.002929
Epoch: 35 	Ltrain: 0.002088 	Lval: 0.002877
Epoch: 40 	Ltrain: 0.002002 	Lval: 0.002577
Epoch: 45 	Ltrain: 0.001948 	Lval: 0.002465
Epoch: 50 	Ltrain: 0.001892 	Lval: 0.002367
Epoch: 55 	Ltrain: 0.001831 	Lval: 0.002366
Epoch: 60 	Ltrain: 0.001749 	Lval: 0.002087
Epoch 00064: reducing learning rate of group 0 to 7.0728e-05.
Epoch: 65 	Ltrain: 0.001641 	Lval: 0.002042
Epoch: 70 	Ltrain: 0.001582 	Lval: 0.001984
Epoch: 75 	Ltrain: 0.001575 	Lval: 0.001989
Epoch: 80 	Ltrain: 0.001560 	Lval: 0.001965
Epoch: 85 	Ltrain: 0.001552 	Lval: 0.001920
Epoch: 90 	Ltrain: 0.001536 	Lval: 0.001903
Epoch: 95 	Ltrain: 0.001524 	Lval: 0.001892
Epoch 00097: reducing learning rate of group 0 to 7.0728e-06.
Epoch: 100 	Ltrain: 0.001506 	Lval: 0.001876
Epoch: 105 	Ltrain: 0.001497 	Lval: 0.001873
Epoch: 110 	Ltrain: 0.001501 	Lval: 0.001870
Epoch: 115 	Ltrain: 0.001498 	Lval: 0.001869
EarlyStopper: stopping at epoch 116 with best_val_loss = 0.001876

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009975264079239824
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.9689216151938953e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.140443 	Lval: 0.022893
Epoch: 5 	Ltrain: 0.008687 	Lval: 0.007126
Epoch: 10 	Ltrain: 0.006665 	Lval: 0.005600
Epoch: 15 	Ltrain: 0.005268 	Lval: 0.004932
Epoch 00020: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 20 	Ltrain: 0.005044 	Lval: 0.005806
Epoch: 25 	Ltrain: 0.004034 	Lval: 0.004197
Epoch: 30 	Ltrain: 0.003901 	Lval: 0.004127
Epoch: 35 	Ltrain: 0.004016 	Lval: 0.004149
Epoch 00036: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 40 	Ltrain: 0.003830 	Lval: 0.004088
Epoch: 45 	Ltrain: 0.003849 	Lval: 0.004063
Epoch 00048: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 50 	Ltrain: 0.004264 	Lval: 0.004066
Epoch: 55 	Ltrain: 0.003879 	Lval: 0.004066
Epoch 00060: reducing learning rate of group 0 to 9.9753e-07.
Epoch: 60 	Ltrain: 0.003736 	Lval: 0.004065
EarlyStopper: stopping at epoch 61 with best_val_loss = 0.004063


	Fold 2/5
Epoch: 1 	Ltrain: 0.230629 	Lval: 0.033676
Epoch: 5 	Ltrain: 0.007836 	Lval: 0.009274
Epoch: 10 	Ltrain: 0.005721 	Lval: 0.005760
Epoch: 15 	Ltrain: 0.004561 	Lval: 0.004775
Epoch: 20 	Ltrain: 0.004175 	Lval: 0.004696
Epoch 00024: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 25 	Ltrain: 0.003870 	Lval: 0.004423
Epoch: 30 	Ltrain: 0.003783 	Lval: 0.004322
Epoch: 35 	Ltrain: 0.003728 	Lval: 0.004310
Epoch 00038: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 40 	Ltrain: 0.003640 	Lval: 0.004288
Epoch: 45 	Ltrain: 0.003686 	Lval: 0.004275
Epoch: 50 	Ltrain: 0.003711 	Lval: 0.004275
Epoch 00052: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 55 	Ltrain: 0.003738 	Lval: 0.004273
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.004278


	Fold 3/5
Epoch: 1 	Ltrain: 0.183521 	Lval: 0.011097
Epoch: 5 	Ltrain: 0.010549 	Lval: 0.009787
Epoch: 10 	Ltrain: 0.005423 	Lval: 0.005353
Epoch: 15 	Ltrain: 0.004447 	Lval: 0.004810
Epoch: 20 	Ltrain: 0.004041 	Lval: 0.004451
Epoch: 25 	Ltrain: 0.003561 	Lval: 0.004124
Epoch 00028: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 30 	Ltrain: 0.003281 	Lval: 0.004021
Epoch: 35 	Ltrain: 0.003216 	Lval: 0.003900
Epoch: 40 	Ltrain: 0.003207 	Lval: 0.003852
Epoch 00045: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 45 	Ltrain: 0.003172 	Lval: 0.003869
Epoch: 50 	Ltrain: 0.003091 	Lval: 0.003797
Epoch: 55 	Ltrain: 0.003114 	Lval: 0.003795
Epoch: 60 	Ltrain: 0.003074 	Lval: 0.003782
Epoch: 65 	Ltrain: 0.003087 	Lval: 0.003772
Epoch 00066: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 70 	Ltrain: 0.003085 	Lval: 0.003771
Epoch: 75 	Ltrain: 0.003089 	Lval: 0.003770
Epoch 00078: reducing learning rate of group 0 to 9.9753e-07.
Epoch: 80 	Ltrain: 0.003070 	Lval: 0.003770
Epoch: 85 	Ltrain: 0.003059 	Lval: 0.003770
EarlyStopper: stopping at epoch 88 with best_val_loss = 0.003770


	Fold 4/5
Epoch: 1 	Ltrain: 0.049849 	Lval: 0.008496
Epoch: 5 	Ltrain: 0.004132 	Lval: 0.005245
Epoch: 10 	Ltrain: 0.003480 	Lval: 0.004042
Epoch: 15 	Ltrain: 0.002712 	Lval: 0.003542
Epoch: 20 	Ltrain: 0.002761 	Lval: 0.003904
Epoch: 25 	Ltrain: 0.002596 	Lval: 0.002861
Epoch 00029: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 30 	Ltrain: 0.002207 	Lval: 0.002719
Epoch: 35 	Ltrain: 0.001943 	Lval: 0.002575
Epoch: 40 	Ltrain: 0.001886 	Lval: 0.002469
Epoch: 45 	Ltrain: 0.001863 	Lval: 0.002501
Epoch: 50 	Ltrain: 0.001792 	Lval: 0.002297
Epoch: 55 	Ltrain: 0.001782 	Lval: 0.002270
Epoch: 60 	Ltrain: 0.001723 	Lval: 0.002203
Epoch: 65 	Ltrain: 0.001658 	Lval: 0.002066
Epoch 00069: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 70 	Ltrain: 0.001563 	Lval: 0.001968
Epoch: 75 	Ltrain: 0.001505 	Lval: 0.001919
Epoch: 80 	Ltrain: 0.001505 	Lval: 0.001908
Epoch: 85 	Ltrain: 0.001490 	Lval: 0.001895
Epoch: 90 	Ltrain: 0.001474 	Lval: 0.001847
Epoch: 95 	Ltrain: 0.001466 	Lval: 0.001849
Epoch: 100 	Ltrain: 0.001460 	Lval: 0.001816
Epoch: 105 	Ltrain: 0.001448 	Lval: 0.001836
Epoch: 110 	Ltrain: 0.001435 	Lval: 0.001777
Epoch: 115 	Ltrain: 0.001418 	Lval: 0.001756
Epoch: 120 	Ltrain: 0.001408 	Lval: 0.001736
Epoch 00121: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 125 	Ltrain: 0.001389 	Lval: 0.001730
Epoch: 130 	Ltrain: 0.001385 	Lval: 0.001725
Epoch 00133: reducing learning rate of group 0 to 9.9753e-07.
Epoch: 135 	Ltrain: 0.001381 	Lval: 0.001725
Epoch: 140 	Ltrain: 0.001386 	Lval: 0.001724
Epoch: 145 	Ltrain: 0.001382 	Lval: 0.001724
EarlyStopper: stopping at epoch 144 with best_val_loss = 0.001725


	Fold 5/5
Epoch: 1 	Ltrain: 0.116483 	Lval: 0.007628
Epoch: 5 	Ltrain: 0.004058 	Lval: 0.004833
Epoch: 10 	Ltrain: 0.003496 	Lval: 0.004077
Epoch: 15 	Ltrain: 0.002881 	Lval: 0.003802
Epoch: 20 	Ltrain: 0.002930 	Lval: 0.003437
Epoch: 25 	Ltrain: 0.002939 	Lval: 0.003161
Epoch 00030: reducing learning rate of group 0 to 9.9753e-04.
Epoch: 30 	Ltrain: 0.002550 	Lval: 0.003892
Epoch: 35 	Ltrain: 0.002027 	Lval: 0.002715
Epoch: 40 	Ltrain: 0.001976 	Lval: 0.002674
Epoch: 45 	Ltrain: 0.001931 	Lval: 0.002681
Epoch 00048: reducing learning rate of group 0 to 9.9753e-05.
Epoch: 50 	Ltrain: 0.001845 	Lval: 0.002518
Epoch: 55 	Ltrain: 0.001839 	Lval: 0.002507
Epoch: 60 	Ltrain: 0.001831 	Lval: 0.002497
Epoch 00062: reducing learning rate of group 0 to 9.9753e-06.
Epoch: 65 	Ltrain: 0.001822 	Lval: 0.002492
Epoch: 70 	Ltrain: 0.001816 	Lval: 0.002489
Epoch 00074: reducing learning rate of group 0 to 9.9753e-07.
Epoch: 75 	Ltrain: 0.001817 	Lval: 0.002488
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.002495

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009681872024514883
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.0585572347001232e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 17
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.168170 	Lval: 0.032059
Epoch: 5 	Ltrain: 0.008108 	Lval: 0.006388
Epoch: 10 	Ltrain: 0.005340 	Lval: 0.004910
Epoch: 15 	Ltrain: 0.004588 	Lval: 0.008605
Epoch: 20 	Ltrain: 0.005013 	Lval: 0.004407
Epoch: 25 	Ltrain: 0.004200 	Lval: 0.004503
Epoch 00027: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 30 	Ltrain: 0.003743 	Lval: 0.003860
Epoch: 35 	Ltrain: 0.003733 	Lval: 0.003747
Epoch: 40 	Ltrain: 0.003635 	Lval: 0.003656
Epoch: 45 	Ltrain: 0.003545 	Lval: 0.003616
Epoch: 50 	Ltrain: 0.003359 	Lval: 0.003562
Epoch: 55 	Ltrain: 0.003198 	Lval: 0.003470
Epoch 00059: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 60 	Ltrain: 0.003663 	Lval: 0.003465
Epoch: 65 	Ltrain: 0.003101 	Lval: 0.003370
Epoch: 70 	Ltrain: 0.003075 	Lval: 0.003385
Epoch 00071: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 75 	Ltrain: 0.003504 	Lval: 0.003372
Epoch: 80 	Ltrain: 0.003161 	Lval: 0.003376
Epoch 00083: reducing learning rate of group 0 to 9.6819e-07.
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.003365


	Fold 2/5
Epoch: 1 	Ltrain: 0.124474 	Lval: 0.020475
Epoch: 5 	Ltrain: 0.005603 	Lval: 0.005714
Epoch: 10 	Ltrain: 0.004726 	Lval: 0.004806
Epoch: 15 	Ltrain: 0.004187 	Lval: 0.004515
Epoch 00019: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 20 	Ltrain: 0.003908 	Lval: 0.004413
Epoch: 25 	Ltrain: 0.003579 	Lval: 0.004216
Epoch: 30 	Ltrain: 0.003511 	Lval: 0.004136
Epoch: 35 	Ltrain: 0.003454 	Lval: 0.004089
Epoch 00036: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 40 	Ltrain: 0.003365 	Lval: 0.004036
Epoch: 45 	Ltrain: 0.003354 	Lval: 0.004030
Epoch: 50 	Ltrain: 0.003363 	Lval: 0.004015
Epoch: 55 	Ltrain: 0.003328 	Lval: 0.004003
Epoch: 60 	Ltrain: 0.003356 	Lval: 0.003985
Epoch 00064: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 65 	Ltrain: 0.003305 	Lval: 0.003991
Epoch: 70 	Ltrain: 0.003303 	Lval: 0.003987
Epoch: 75 	Ltrain: 0.003374 	Lval: 0.003985
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.003992


	Fold 3/5
Epoch: 1 	Ltrain: 0.105771 	Lval: 0.010149
Epoch: 5 	Ltrain: 0.004520 	Lval: 0.004670
Epoch: 10 	Ltrain: 0.004036 	Lval: 0.004884
Epoch 00013: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 15 	Ltrain: 0.003303 	Lval: 0.004007
Epoch: 20 	Ltrain: 0.003180 	Lval: 0.003895
Epoch: 25 	Ltrain: 0.003023 	Lval: 0.003728
Epoch: 30 	Ltrain: 0.002890 	Lval: 0.003581
Epoch: 35 	Ltrain: 0.002719 	Lval: 0.003437
Epoch: 40 	Ltrain: 0.002655 	Lval: 0.003539
Epoch: 45 	Ltrain: 0.002481 	Lval: 0.003218
Epoch: 50 	Ltrain: 0.002525 	Lval: 0.003322
Epoch: 55 	Ltrain: 0.002392 	Lval: 0.003043
Epoch: 60 	Ltrain: 0.002276 	Lval: 0.002929
Epoch: 65 	Ltrain: 0.002244 	Lval: 0.002808
Epoch 00070: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 70 	Ltrain: 0.002308 	Lval: 0.003117
Epoch: 75 	Ltrain: 0.002023 	Lval: 0.002624
Epoch: 80 	Ltrain: 0.001992 	Lval: 0.002609
Epoch: 85 	Ltrain: 0.001994 	Lval: 0.002588
Epoch: 90 	Ltrain: 0.001985 	Lval: 0.002575
Epoch: 95 	Ltrain: 0.001967 	Lval: 0.002526
Epoch: 100 	Ltrain: 0.001972 	Lval: 0.002521
Epoch: 105 	Ltrain: 0.001953 	Lval: 0.002509
Epoch: 110 	Ltrain: 0.001929 	Lval: 0.002489
Epoch: 115 	Ltrain: 0.001910 	Lval: 0.002453
Epoch: 120 	Ltrain: 0.001923 	Lval: 0.002467
Epoch 00122: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 125 	Ltrain: 0.001889 	Lval: 0.002431
Epoch: 130 	Ltrain: 0.001897 	Lval: 0.002433
Epoch: 135 	Ltrain: 0.001900 	Lval: 0.002435
EarlyStopper: stopping at epoch 134 with best_val_loss = 0.002436


	Fold 4/5
Epoch: 1 	Ltrain: 0.046917 	Lval: 0.008961
Epoch: 5 	Ltrain: 0.004194 	Lval: 0.005108
Epoch: 10 	Ltrain: 0.003432 	Lval: 0.004173
Epoch: 15 	Ltrain: 0.003048 	Lval: 0.003765
Epoch: 20 	Ltrain: 0.002664 	Lval: 0.003434
Epoch 00023: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 25 	Ltrain: 0.002120 	Lval: 0.002787
Epoch: 30 	Ltrain: 0.002022 	Lval: 0.002624
Epoch: 35 	Ltrain: 0.001976 	Lval: 0.002551
Epoch: 40 	Ltrain: 0.001889 	Lval: 0.002508
Epoch: 45 	Ltrain: 0.001854 	Lval: 0.002421
Epoch: 50 	Ltrain: 0.001826 	Lval: 0.002330
Epoch: 55 	Ltrain: 0.001739 	Lval: 0.002235
Epoch: 60 	Ltrain: 0.001666 	Lval: 0.002040
Epoch: 65 	Ltrain: 0.001621 	Lval: 0.001944
Epoch: 70 	Ltrain: 0.001579 	Lval: 0.001928
Epoch: 75 	Ltrain: 0.001454 	Lval: 0.001676
Epoch: 80 	Ltrain: 0.001425 	Lval: 0.001531
Epoch: 85 	Ltrain: 0.001300 	Lval: 0.001394
Epoch: 90 	Ltrain: 0.001284 	Lval: 0.001520
Epoch: 95 	Ltrain: 0.001143 	Lval: 0.001212
Epoch: 100 	Ltrain: 0.001077 	Lval: 0.001100
Epoch 00104: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 105 	Ltrain: 0.000923 	Lval: 0.000980
Epoch: 110 	Ltrain: 0.000836 	Lval: 0.000928
Epoch: 115 	Ltrain: 0.000823 	Lval: 0.000916
Epoch: 120 	Ltrain: 0.000812 	Lval: 0.000900
Epoch: 125 	Ltrain: 0.000799 	Lval: 0.000883
Epoch: 130 	Ltrain: 0.000789 	Lval: 0.000864
Epoch: 135 	Ltrain: 0.000777 	Lval: 0.000849
Epoch: 140 	Ltrain: 0.000764 	Lval: 0.000836
Epoch: 145 	Ltrain: 0.000752 	Lval: 0.000817
Epoch: 150 	Ltrain: 0.000738 	Lval: 0.000803
Epoch: 155 	Ltrain: 0.000726 	Lval: 0.000784
Epoch: 160 	Ltrain: 0.000711 	Lval: 0.000771
Epoch: 165 	Ltrain: 0.000704 	Lval: 0.000758
Epoch: 170 	Ltrain: 0.000687 	Lval: 0.000740
Epoch: 175 	Ltrain: 0.000674 	Lval: 0.000720
Epoch: 180 	Ltrain: 0.000661 	Lval: 0.000704
Epoch: 185 	Ltrain: 0.000648 	Lval: 0.000688
Epoch: 190 	Ltrain: 0.000637 	Lval: 0.000674
Epoch: 195 	Ltrain: 0.000619 	Lval: 0.000660
Epoch: 200 	Ltrain: 0.000610 	Lval: 0.000643
Epoch: 205 	Ltrain: 0.000598 	Lval: 0.000624
Epoch: 210 	Ltrain: 0.000582 	Lval: 0.000610
Epoch: 215 	Ltrain: 0.000573 	Lval: 0.000594
Epoch: 220 	Ltrain: 0.000558 	Lval: 0.000582
Epoch: 225 	Ltrain: 0.000545 	Lval: 0.000568
Epoch: 230 	Ltrain: 0.000532 	Lval: 0.000563
Epoch: 235 	Ltrain: 0.000522 	Lval: 0.000540
Epoch: 240 	Ltrain: 0.000505 	Lval: 0.000528
Epoch: 245 	Ltrain: 0.000500 	Lval: 0.000517
Epoch: 250 	Ltrain: 0.000489 	Lval: 0.000506
Epoch: 255 	Ltrain: 0.000473 	Lval: 0.000494
Epoch: 260 	Ltrain: 0.000461 	Lval: 0.000477
Epoch: 265 	Ltrain: 0.000455 	Lval: 0.000475
Epoch 00266: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 270 	Ltrain: 0.000429 	Lval: 0.000448
Epoch: 275 	Ltrain: 0.000426 	Lval: 0.000446
Epoch: 280 	Ltrain: 0.000425 	Lval: 0.000445
Epoch: 285 	Ltrain: 0.000423 	Lval: 0.000443
Epoch: 290 	Ltrain: 0.000422 	Lval: 0.000441
Epoch: 295 	Ltrain: 0.000420 	Lval: 0.000440
Epoch: 300 	Ltrain: 0.000420 	Lval: 0.000439
EarlyStopper: stopping at epoch 299 with best_val_loss = 0.000443


	Fold 5/5
Epoch: 1 	Ltrain: 0.067341 	Lval: 0.011235
Epoch: 5 	Ltrain: 0.004051 	Lval: 0.005367
Epoch: 10 	Ltrain: 0.004007 	Lval: 0.004854
Epoch: 15 	Ltrain: 0.003127 	Lval: 0.003748
Epoch: 20 	Ltrain: 0.002898 	Lval: 0.003737
Epoch 00021: reducing learning rate of group 0 to 9.6819e-04.
Epoch: 25 	Ltrain: 0.002241 	Lval: 0.003014
Epoch: 30 	Ltrain: 0.002182 	Lval: 0.002933
Epoch: 35 	Ltrain: 0.002129 	Lval: 0.002830
Epoch: 40 	Ltrain: 0.002089 	Lval: 0.002714
Epoch: 45 	Ltrain: 0.002068 	Lval: 0.002603
Epoch 00050: reducing learning rate of group 0 to 9.6819e-05.
Epoch: 50 	Ltrain: 0.002006 	Lval: 0.002627
Epoch: 55 	Ltrain: 0.001900 	Lval: 0.002548
Epoch: 60 	Ltrain: 0.001900 	Lval: 0.002520
Epoch: 65 	Ltrain: 0.001890 	Lval: 0.002513
Epoch 00070: reducing learning rate of group 0 to 9.6819e-06.
Epoch: 70 	Ltrain: 0.001875 	Lval: 0.002500
Epoch: 75 	Ltrain: 0.001868 	Lval: 0.002488
Epoch: 80 	Ltrain: 0.001862 	Lval: 0.002488
Epoch 00082: reducing learning rate of group 0 to 9.6819e-07.
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.002490

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0012363506550085658
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.5349677455963487e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 13
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.014934 	Lval: 0.009677
Epoch: 5 	Ltrain: 0.006886 	Lval: 0.007406
Epoch 00006: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 10 	Ltrain: 0.006466 	Lval: 0.006012
Epoch: 15 	Ltrain: 0.005884 	Lval: 0.005782
Epoch: 20 	Ltrain: 0.005673 	Lval: 0.005626
Epoch: 25 	Ltrain: 0.005578 	Lval: 0.005479
Epoch: 30 	Ltrain: 0.005612 	Lval: 0.005328
Epoch: 35 	Ltrain: 0.005308 	Lval: 0.005250
Epoch: 40 	Ltrain: 0.004927 	Lval: 0.005076
Epoch: 45 	Ltrain: 0.005030 	Lval: 0.005035
Epoch 00046: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 50 	Ltrain: 0.004980 	Lval: 0.004935
Epoch: 55 	Ltrain: 0.004780 	Lval: 0.004912
Epoch: 60 	Ltrain: 0.005005 	Lval: 0.004894
Epoch: 65 	Ltrain: 0.004872 	Lval: 0.004892
Epoch: 70 	Ltrain: 0.004688 	Lval: 0.004881
Epoch: 75 	Ltrain: 0.004751 	Lval: 0.004872
Epoch 00078: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 80 	Ltrain: 0.005360 	Lval: 0.004876
Epoch: 85 	Ltrain: 0.004769 	Lval: 0.004866
EarlyStopper: stopping at epoch 86 with best_val_loss = 0.004869


	Fold 2/5
Epoch: 1 	Ltrain: 0.019497 	Lval: 0.010368
Epoch: 5 	Ltrain: 0.005779 	Lval: 0.005789
Epoch: 10 	Ltrain: 0.004564 	Lval: 0.005103
Epoch: 15 	Ltrain: 0.004207 	Lval: 0.004493
Epoch: 20 	Ltrain: 0.003976 	Lval: 0.004379
Epoch: 25 	Ltrain: 0.003696 	Lval: 0.004463
Epoch: 30 	Ltrain: 0.003292 	Lval: 0.003703
Epoch: 35 	Ltrain: 0.003217 	Lval: 0.003571
Epoch: 40 	Ltrain: 0.003202 	Lval: 0.004000
Epoch 00042: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 45 	Ltrain: 0.002736 	Lval: 0.003272
Epoch: 50 	Ltrain: 0.002699 	Lval: 0.003241
Epoch: 55 	Ltrain: 0.002655 	Lval: 0.003230
Epoch 00058: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 60 	Ltrain: 0.002645 	Lval: 0.003191
Epoch: 65 	Ltrain: 0.002667 	Lval: 0.003191
Epoch 00070: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 70 	Ltrain: 0.002751 	Lval: 0.003193
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.003191


	Fold 3/5
Epoch: 1 	Ltrain: 0.014551 	Lval: 0.007907
Epoch: 5 	Ltrain: 0.004849 	Lval: 0.005731
Epoch: 10 	Ltrain: 0.004080 	Lval: 0.004820
Epoch: 15 	Ltrain: 0.003706 	Lval: 0.004528
Epoch: 20 	Ltrain: 0.003290 	Lval: 0.003731
Epoch: 25 	Ltrain: 0.003327 	Lval: 0.003863
Epoch: 30 	Ltrain: 0.003136 	Lval: 0.003490
Epoch: 35 	Ltrain: 0.002894 	Lval: 0.003522
Epoch: 40 	Ltrain: 0.002872 	Lval: 0.003368
Epoch: 45 	Ltrain: 0.002686 	Lval: 0.003130
Epoch 00050: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 50 	Ltrain: 0.002694 	Lval: 0.003317
Epoch: 55 	Ltrain: 0.002280 	Lval: 0.002901
Epoch: 60 	Ltrain: 0.002248 	Lval: 0.002859
Epoch: 65 	Ltrain: 0.002248 	Lval: 0.002875
Epoch 00068: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 70 	Ltrain: 0.002212 	Lval: 0.002831
Epoch: 75 	Ltrain: 0.002242 	Lval: 0.002834
Epoch: 80 	Ltrain: 0.002208 	Lval: 0.002829
Epoch 00083: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 85 	Ltrain: 0.002195 	Lval: 0.002827
Epoch: 90 	Ltrain: 0.002196 	Lval: 0.002827
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.002827


	Fold 4/5
Epoch: 1 	Ltrain: 0.011159 	Lval: 0.007046
Epoch: 5 	Ltrain: 0.004406 	Lval: 0.005648
Epoch: 10 	Ltrain: 0.003743 	Lval: 0.004399
Epoch: 15 	Ltrain: 0.003111 	Lval: 0.003934
Epoch: 20 	Ltrain: 0.002941 	Lval: 0.003494
Epoch: 25 	Ltrain: 0.002705 	Lval: 0.003540
Epoch: 30 	Ltrain: 0.002650 	Lval: 0.003530
Epoch: 35 	Ltrain: 0.002592 	Lval: 0.003271
Epoch: 40 	Ltrain: 0.002494 	Lval: 0.003380
Epoch 00043: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 45 	Ltrain: 0.002208 	Lval: 0.002940
Epoch: 50 	Ltrain: 0.002169 	Lval: 0.002899
Epoch: 55 	Ltrain: 0.002138 	Lval: 0.002840
Epoch: 60 	Ltrain: 0.002130 	Lval: 0.002813
Epoch: 65 	Ltrain: 0.002115 	Lval: 0.002796
Epoch: 70 	Ltrain: 0.002098 	Lval: 0.002801
Epoch: 75 	Ltrain: 0.002081 	Lval: 0.002733
Epoch: 80 	Ltrain: 0.002061 	Lval: 0.002736
Epoch: 85 	Ltrain: 0.002054 	Lval: 0.002719
Epoch: 90 	Ltrain: 0.002046 	Lval: 0.002668
Epoch 00094: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 95 	Ltrain: 0.001976 	Lval: 0.002667
Epoch: 100 	Ltrain: 0.001964 	Lval: 0.002651
Epoch: 105 	Ltrain: 0.001963 	Lval: 0.002657
Epoch 00106: reducing learning rate of group 0 to 1.2364e-06.
Epoch: 110 	Ltrain: 0.001961 	Lval: 0.002657
EarlyStopper: stopping at epoch 112 with best_val_loss = 0.002651


	Fold 5/5
Epoch: 1 	Ltrain: 0.010011 	Lval: 0.007395
Epoch: 5 	Ltrain: 0.004584 	Lval: 0.004882
Epoch: 10 	Ltrain: 0.003929 	Lval: 0.004794
Epoch: 15 	Ltrain: 0.003362 	Lval: 0.003920
Epoch: 20 	Ltrain: 0.002863 	Lval: 0.004004
Epoch: 25 	Ltrain: 0.002790 	Lval: 0.003479
Epoch: 30 	Ltrain: 0.002905 	Lval: 0.004157
Epoch 00032: reducing learning rate of group 0 to 1.2364e-04.
Epoch: 35 	Ltrain: 0.002378 	Lval: 0.003219
Epoch: 40 	Ltrain: 0.002348 	Lval: 0.003173
Epoch: 45 	Ltrain: 0.002329 	Lval: 0.003152
Epoch: 50 	Ltrain: 0.002305 	Lval: 0.003088
Epoch: 55 	Ltrain: 0.002282 	Lval: 0.003035
Epoch: 60 	Ltrain: 0.002261 	Lval: 0.003037
Epoch: 65 	Ltrain: 0.002256 	Lval: 0.002995
Epoch: 70 	Ltrain: 0.002223 	Lval: 0.002959
Epoch: 75 	Ltrain: 0.002209 	Lval: 0.002927
Epoch: 80 	Ltrain: 0.002191 	Lval: 0.002991
Epoch: 85 	Ltrain: 0.002181 	Lval: 0.002957
Epoch: 90 	Ltrain: 0.002153 	Lval: 0.002913
Epoch 00095: reducing learning rate of group 0 to 1.2364e-05.
Epoch: 95 	Ltrain: 0.002133 	Lval: 0.002876
Epoch: 100 	Ltrain: 0.002089 	Lval: 0.002843
Epoch: 105 	Ltrain: 0.002087 	Lval: 0.002856
Epoch: 110 	Ltrain: 0.002080 	Lval: 0.002840
EarlyStopper: stopping at epoch 112 with best_val_loss = 0.002843

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004668373855365965
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.0047955890831617e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 19
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.030084 	Lval: 0.014675
Epoch: 5 	Ltrain: 0.005821 	Lval: 0.005476
Epoch 00009: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 10 	Ltrain: 0.005652 	Lval: 0.005092
Epoch: 15 	Ltrain: 0.004726 	Lval: 0.004776
Epoch: 20 	Ltrain: 0.004796 	Lval: 0.004756
Epoch 00023: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 25 	Ltrain: 0.004597 	Lval: 0.004624
Epoch: 30 	Ltrain: 0.004460 	Lval: 0.004636
Epoch: 35 	Ltrain: 0.004569 	Lval: 0.004619
Epoch: 40 	Ltrain: 0.004474 	Lval: 0.004599
Epoch 00042: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 45 	Ltrain: 0.004352 	Lval: 0.004591
Epoch: 50 	Ltrain: 0.004645 	Lval: 0.004589
Epoch: 55 	Ltrain: 0.004831 	Lval: 0.004584
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.004593


	Fold 2/5
Epoch: 1 	Ltrain: 0.021994 	Lval: 0.008130
Epoch: 5 	Ltrain: 0.005145 	Lval: 0.004929
Epoch: 10 	Ltrain: 0.004414 	Lval: 0.004646
Epoch 00014: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 15 	Ltrain: 0.003913 	Lval: 0.004412
Epoch: 20 	Ltrain: 0.003630 	Lval: 0.004151
Epoch: 25 	Ltrain: 0.003402 	Lval: 0.003951
Epoch: 30 	Ltrain: 0.003282 	Lval: 0.003803
Epoch: 35 	Ltrain: 0.003175 	Lval: 0.003712
Epoch: 40 	Ltrain: 0.003095 	Lval: 0.003613
Epoch: 45 	Ltrain: 0.003003 	Lval: 0.003534
Epoch: 50 	Ltrain: 0.002902 	Lval: 0.003431
Epoch: 55 	Ltrain: 0.002852 	Lval: 0.003389
Epoch 00057: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 60 	Ltrain: 0.002696 	Lval: 0.003291
Epoch: 65 	Ltrain: 0.002688 	Lval: 0.003262
Epoch: 70 	Ltrain: 0.002654 	Lval: 0.003235
Epoch: 75 	Ltrain: 0.002689 	Lval: 0.003225
Epoch: 80 	Ltrain: 0.002729 	Lval: 0.003218
Epoch 00082: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 85 	Ltrain: 0.002669 	Lval: 0.003216
Epoch: 90 	Ltrain: 0.002648 	Lval: 0.003215
Epoch 00094: reducing learning rate of group 0 to 4.6684e-07.
Epoch: 95 	Ltrain: 0.002704 	Lval: 0.003214
EarlyStopper: stopping at epoch 96 with best_val_loss = 0.003212


	Fold 3/5
Epoch: 1 	Ltrain: 0.019668 	Lval: 0.008421
Epoch: 5 	Ltrain: 0.004732 	Lval: 0.005102
Epoch: 10 	Ltrain: 0.004031 	Lval: 0.004273
Epoch: 15 	Ltrain: 0.003573 	Lval: 0.003858
Epoch: 20 	Ltrain: 0.002954 	Lval: 0.003318
Epoch: 25 	Ltrain: 0.002644 	Lval: 0.003201
Epoch 00028: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 30 	Ltrain: 0.002311 	Lval: 0.002855
Epoch: 35 	Ltrain: 0.002173 	Lval: 0.002780
Epoch: 40 	Ltrain: 0.002115 	Lval: 0.002703
Epoch: 45 	Ltrain: 0.002073 	Lval: 0.002588
Epoch: 50 	Ltrain: 0.002030 	Lval: 0.002550
Epoch: 55 	Ltrain: 0.001998 	Lval: 0.002471
Epoch: 60 	Ltrain: 0.001919 	Lval: 0.002432
Epoch: 65 	Ltrain: 0.001864 	Lval: 0.002380
Epoch: 70 	Ltrain: 0.001875 	Lval: 0.002304
Epoch: 75 	Ltrain: 0.001785 	Lval: 0.002191
Epoch: 80 	Ltrain: 0.001734 	Lval: 0.002118
Epoch: 85 	Ltrain: 0.001689 	Lval: 0.002034
Epoch: 90 	Ltrain: 0.001631 	Lval: 0.001935
Epoch 00092: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 95 	Ltrain: 0.001510 	Lval: 0.001816
Epoch: 100 	Ltrain: 0.001494 	Lval: 0.001789
Epoch: 105 	Ltrain: 0.001497 	Lval: 0.001780
Epoch: 110 	Ltrain: 0.001488 	Lval: 0.001757
Epoch: 115 	Ltrain: 0.001475 	Lval: 0.001741
Epoch: 120 	Ltrain: 0.001478 	Lval: 0.001725
Epoch: 125 	Ltrain: 0.001464 	Lval: 0.001702
Epoch 00129: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 130 	Ltrain: 0.001438 	Lval: 0.001704
Epoch: 135 	Ltrain: 0.001438 	Lval: 0.001695
Epoch: 140 	Ltrain: 0.001438 	Lval: 0.001694
Epoch 00141: reducing learning rate of group 0 to 4.6684e-07.
Epoch: 145 	Ltrain: 0.001444 	Lval: 0.001694
Epoch: 150 	Ltrain: 0.001431 	Lval: 0.001694
EarlyStopper: stopping at epoch 149 with best_val_loss = 0.001700


	Fold 4/5
Epoch: 1 	Ltrain: 0.013543 	Lval: 0.006590
Epoch: 5 	Ltrain: 0.004688 	Lval: 0.005592
Epoch: 10 	Ltrain: 0.003247 	Lval: 0.004154
Epoch: 15 	Ltrain: 0.002951 	Lval: 0.004301
Epoch: 20 	Ltrain: 0.002610 	Lval: 0.003287
Epoch 00023: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 25 	Ltrain: 0.002180 	Lval: 0.002886
Epoch: 30 	Ltrain: 0.002125 	Lval: 0.002747
Epoch: 35 	Ltrain: 0.002063 	Lval: 0.002689
Epoch: 40 	Ltrain: 0.002030 	Lval: 0.002660
Epoch: 45 	Ltrain: 0.001985 	Lval: 0.002576
Epoch: 50 	Ltrain: 0.001954 	Lval: 0.002596
Epoch 00051: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 55 	Ltrain: 0.001880 	Lval: 0.002509
Epoch: 60 	Ltrain: 0.001868 	Lval: 0.002484
Epoch 00064: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 65 	Ltrain: 0.001857 	Lval: 0.002489
Epoch: 70 	Ltrain: 0.001854 	Lval: 0.002484
Epoch: 75 	Ltrain: 0.001850 	Lval: 0.002482
Epoch 00076: reducing learning rate of group 0 to 4.6684e-07.
EarlyStopper: stopping at epoch 78 with best_val_loss = 0.002484


	Fold 5/5
Epoch: 1 	Ltrain: 0.014971 	Lval: 0.007217
Epoch: 5 	Ltrain: 0.004430 	Lval: 0.004860
Epoch: 10 	Ltrain: 0.003393 	Lval: 0.004106
Epoch: 15 	Ltrain: 0.003315 	Lval: 0.004622
Epoch: 20 	Ltrain: 0.002795 	Lval: 0.003396
Epoch: 25 	Ltrain: 0.002464 	Lval: 0.003135
Epoch: 30 	Ltrain: 0.002316 	Lval: 0.003183
Epoch: 35 	Ltrain: 0.002150 	Lval: 0.002582
Epoch 00039: reducing learning rate of group 0 to 4.6684e-04.
Epoch: 40 	Ltrain: 0.001875 	Lval: 0.002365
Epoch: 45 	Ltrain: 0.001652 	Lval: 0.002186
Epoch: 50 	Ltrain: 0.001600 	Lval: 0.002095
Epoch: 55 	Ltrain: 0.001552 	Lval: 0.002059
Epoch: 60 	Ltrain: 0.001507 	Lval: 0.001933
Epoch 00065: reducing learning rate of group 0 to 4.6684e-05.
Epoch: 65 	Ltrain: 0.001456 	Lval: 0.002028
Epoch: 70 	Ltrain: 0.001376 	Lval: 0.001834
Epoch: 75 	Ltrain: 0.001370 	Lval: 0.001820
Epoch: 80 	Ltrain: 0.001362 	Lval: 0.001809
Epoch: 85 	Ltrain: 0.001354 	Lval: 0.001791
Epoch: 90 	Ltrain: 0.001345 	Lval: 0.001777
Epoch: 95 	Ltrain: 0.001336 	Lval: 0.001764
Epoch: 100 	Ltrain: 0.001334 	Lval: 0.001756
Epoch 00105: reducing learning rate of group 0 to 4.6684e-06.
Epoch: 105 	Ltrain: 0.001325 	Lval: 0.001751
Epoch: 110 	Ltrain: 0.001314 	Lval: 0.001739
Epoch: 115 	Ltrain: 0.001309 	Lval: 0.001734
Epoch 00117: reducing learning rate of group 0 to 4.6684e-07.
Epoch: 120 	Ltrain: 0.001311 	Lval: 0.001734
EarlyStopper: stopping at epoch 119 with best_val_loss = 0.001741

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0018725447193571648
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.882180362114456e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 30
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.030048 	Lval: 0.008857
Epoch: 5 	Ltrain: 0.011440 	Lval: 0.008549
Epoch: 10 	Ltrain: 0.008452 	Lval: 0.006637
Epoch: 15 	Ltrain: 0.007296 	Lval: 0.005918
Epoch: 20 	Ltrain: 0.005776 	Lval: 0.005808
Epoch: 25 	Ltrain: 0.006179 	Lval: 0.008017
Epoch: 30 	Ltrain: 0.006638 	Lval: 0.005787
Epoch: 35 	Ltrain: 0.004232 	Lval: 0.004819
Epoch 00038: reducing learning rate of group 0 to 1.8725e-04.
Epoch: 40 	Ltrain: 0.004912 	Lval: 0.005035
Epoch: 45 	Ltrain: 0.005929 	Lval: 0.004609
Epoch 00050: reducing learning rate of group 0 to 1.8725e-05.
Epoch: 50 	Ltrain: 0.004457 	Lval: 0.004650
Epoch: 55 	Ltrain: 0.004337 	Lval: 0.004712
Epoch: 60 	Ltrain: 0.005628 	Lval: 0.004800
Epoch 00062: reducing learning rate of group 0 to 1.8725e-06.
Epoch: 65 	Ltrain: 0.004717 	Lval: 0.004804
Epoch: 70 	Ltrain: 0.004735 	Lval: 0.004798
Epoch 00074: reducing learning rate of group 0 to 1.8725e-07.
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.004607


	Fold 2/5
Epoch: 1 	Ltrain: 0.020771 	Lval: 0.010503
Epoch: 5 	Ltrain: 0.008192 	Lval: 0.007971
Epoch: 10 	Ltrain: 0.005906 	Lval: 0.006171
Epoch: 15 	Ltrain: 0.005643 	Lval: 0.004942
Epoch 00019: reducing learning rate of group 0 to 1.8725e-04.
Epoch: 20 	Ltrain: 0.004922 	Lval: 0.004892
Epoch: 25 	Ltrain: 0.004167 	Lval: 0.004793
Epoch: 30 	Ltrain: 0.004552 	Lval: 0.004709
Epoch: 35 	Ltrain: 0.004102 	Lval: 0.004805
Epoch 00037: reducing learning rate of group 0 to 1.8725e-05.
Epoch: 40 	Ltrain: 0.004290 	Lval: 0.004707
Epoch: 45 	Ltrain: 0.004466 	Lval: 0.004689
Epoch: 50 	Ltrain: 0.004846 	Lval: 0.004668
Epoch 00052: reducing learning rate of group 0 to 1.8725e-06.
Epoch: 55 	Ltrain: 0.004455 	Lval: 0.004696
Epoch: 60 	Ltrain: 0.004367 	Lval: 0.004688
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.004657


	Fold 3/5
Epoch: 1 	Ltrain: 0.018268 	Lval: 0.012066
Epoch: 5 	Ltrain: 0.007335 	Lval: 0.006853
Epoch: 10 	Ltrain: 0.004994 	Lval: 0.005038
Epoch 00015: reducing learning rate of group 0 to 1.8725e-04.
Epoch: 15 	Ltrain: 0.004424 	Lval: 0.005384
Epoch: 20 	Ltrain: 0.004293 	Lval: 0.004715
Epoch: 25 	Ltrain: 0.003988 	Lval: 0.004529
Epoch 00027: reducing learning rate of group 0 to 1.8725e-05.
Epoch: 30 	Ltrain: 0.003939 	Lval: 0.004454
Epoch: 35 	Ltrain: 0.003947 	Lval: 0.004489
Epoch 00039: reducing learning rate of group 0 to 1.8725e-06.
Epoch: 40 	Ltrain: 0.004206 	Lval: 0.004495
Epoch: 45 	Ltrain: 0.003882 	Lval: 0.004481
Epoch: 50 	Ltrain: 0.003883 	Lval: 0.004482
Epoch 00051: reducing learning rate of group 0 to 1.8725e-07.
Epoch: 55 	Ltrain: 0.004072 	Lval: 0.004485
Epoch: 60 	Ltrain: 0.003884 	Lval: 0.004484
EarlyStopper: stopping at epoch 59 with best_val_loss = 0.004454


	Fold 4/5
Epoch: 1 	Ltrain: 0.013871 	Lval: 0.008254
Epoch: 5 	Ltrain: 0.005824 	Lval: 0.007445
Epoch: 10 	Ltrain: 0.004587 	Lval: 0.005314
Epoch: 15 	Ltrain: 0.004205 	Lval: 0.004647
Epoch: 20 	Ltrain: 0.003934 	Lval: 0.004452
Epoch: 25 	Ltrain: 0.003427 	Lval: 0.004041
Epoch: 30 	Ltrain: 0.003393 	Lval: 0.004114
Epoch 00035: reducing learning rate of group 0 to 1.8725e-04.
Epoch: 35 	Ltrain: 0.003040 	Lval: 0.004159
Epoch: 40 	Ltrain: 0.002759 	Lval: 0.003575
Epoch: 45 	Ltrain: 0.002694 	Lval: 0.003546
Epoch: 50 	Ltrain: 0.002765 	Lval: 0.003559
Epoch: 55 	Ltrain: 0.002675 	Lval: 0.003456
Epoch: 60 	Ltrain: 0.002752 	Lval: 0.003460
Epoch 00061: reducing learning rate of group 0 to 1.8725e-05.
Epoch: 65 	Ltrain: 0.002801 	Lval: 0.003436
Epoch: 70 	Ltrain: 0.002580 	Lval: 0.003445
Epoch: 75 	Ltrain: 0.002638 	Lval: 0.003453
Epoch 00076: reducing learning rate of group 0 to 1.8725e-06.
Epoch: 80 	Ltrain: 0.002777 	Lval: 0.003443
Epoch: 85 	Ltrain: 0.002657 	Lval: 0.003442
EarlyStopper: stopping at epoch 86 with best_val_loss = 0.003433


	Fold 5/5
Epoch: 1 	Ltrain: 0.016344 	Lval: 0.009239
Epoch: 5 	Ltrain: 0.006447 	Lval: 0.007664
Epoch: 10 	Ltrain: 0.004770 	Lval: 0.005630
Epoch: 15 	Ltrain: 0.003943 	Lval: 0.004897
Epoch 00019: reducing learning rate of group 0 to 1.8725e-04.
Epoch: 20 	Ltrain: 0.003847 	Lval: 0.004807
Epoch: 25 	Ltrain: 0.003657 	Lval: 0.004808
Epoch: 30 	Ltrain: 0.003672 	Lval: 0.004697
Epoch: 35 	Ltrain: 0.003615 	Lval: 0.004768
Epoch: 40 	Ltrain: 0.003484 	Lval: 0.004714
Epoch: 45 	Ltrain: 0.003579 	Lval: 0.004654
Epoch: 50 	Ltrain: 0.003502 	Lval: 0.004514
Epoch: 55 	Ltrain: 0.003356 	Lval: 0.004447
Epoch: 60 	Ltrain: 0.003095 	Lval: 0.004319
Epoch: 65 	Ltrain: 0.003189 	Lval: 0.004273
Epoch: 70 	Ltrain: 0.003201 	Lval: 0.004263
Epoch: 75 	Ltrain: 0.003072 	Lval: 0.004110
Epoch: 80 	Ltrain: 0.002946 	Lval: 0.004074
Epoch 00085: reducing learning rate of group 0 to 1.8725e-05.
Epoch: 85 	Ltrain: 0.002974 	Lval: 0.004055
Epoch: 90 	Ltrain: 0.002886 	Lval: 0.004056
Epoch: 95 	Ltrain: 0.002887 	Lval: 0.004055
Epoch 00097: reducing learning rate of group 0 to 1.8725e-06.
Epoch: 100 	Ltrain: 0.002870 	Lval: 0.004060
Epoch: 105 	Ltrain: 0.002971 	Lval: 0.004055
Epoch 00109: reducing learning rate of group 0 to 1.8725e-07.
Epoch: 110 	Ltrain: 0.002894 	Lval: 0.004053
EarlyStopper: stopping at epoch 110 with best_val_loss = 0.004048

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009576152419483777
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.3319784092097704e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.207685 	Lval: 0.045522
Epoch: 5 	Ltrain: 0.009664 	Lval: 0.008634
Epoch: 10 	Ltrain: 0.006268 	Lval: 0.005769
Epoch: 15 	Ltrain: 0.004994 	Lval: 0.005469
Epoch: 20 	Ltrain: 0.005151 	Lval: 0.004666
Epoch: 25 	Ltrain: 0.004628 	Lval: 0.005125
Epoch 00026: reducing learning rate of group 0 to 9.5762e-04.
Epoch: 30 	Ltrain: 0.004411 	Lval: 0.004116
Epoch: 35 	Ltrain: 0.003852 	Lval: 0.004023
Epoch: 40 	Ltrain: 0.003745 	Lval: 0.003962
Epoch: 45 	Ltrain: 0.003946 	Lval: 0.003926
Epoch: 50 	Ltrain: 0.003769 	Lval: 0.003870
Epoch: 55 	Ltrain: 0.003636 	Lval: 0.003837
Epoch 00060: reducing learning rate of group 0 to 9.5762e-05.
Epoch: 60 	Ltrain: 0.003722 	Lval: 0.003789
Epoch: 65 	Ltrain: 0.003553 	Lval: 0.003764
Epoch: 70 	Ltrain: 0.003491 	Lval: 0.003734
Epoch: 75 	Ltrain: 0.003630 	Lval: 0.003745
Epoch: 80 	Ltrain: 0.003668 	Lval: 0.003717
Epoch 00085: reducing learning rate of group 0 to 9.5762e-06.
Epoch: 85 	Ltrain: 0.003534 	Lval: 0.003714
Epoch: 90 	Ltrain: 0.003464 	Lval: 0.003709
Epoch: 95 	Ltrain: 0.003584 	Lval: 0.003704
EarlyStopper: stopping at epoch 96 with best_val_loss = 0.003702


	Fold 2/5
Epoch: 1 	Ltrain: 0.139220 	Lval: 0.018606
Epoch: 5 	Ltrain: 0.007521 	Lval: 0.006550
Epoch: 10 	Ltrain: 0.005258 	Lval: 0.005803
Epoch 00015: reducing learning rate of group 0 to 9.5762e-04.
Epoch: 15 	Ltrain: 0.004650 	Lval: 0.005257
Epoch: 20 	Ltrain: 0.004095 	Lval: 0.004702
Epoch: 25 	Ltrain: 0.004123 	Lval: 0.004591
Epoch: 30 	Ltrain: 0.003991 	Lval: 0.004552
Epoch: 35 	Ltrain: 0.003933 	Lval: 0.004502
Epoch: 40 	Ltrain: 0.004082 	Lval: 0.004386
Epoch: 45 	Ltrain: 0.003709 	Lval: 0.004332
Epoch 00050: reducing learning rate of group 0 to 9.5762e-05.
Epoch: 50 	Ltrain: 0.003568 	Lval: 0.004453
Epoch: 55 	Ltrain: 0.003536 	Lval: 0.004149
Epoch: 60 	Ltrain: 0.003466 	Lval: 0.004138
Epoch: 65 	Ltrain: 0.003466 	Lval: 0.004119
Epoch: 70 	Ltrain: 0.003584 	Lval: 0.004101
Epoch: 75 	Ltrain: 0.003502 	Lval: 0.004089
Epoch: 80 	Ltrain: 0.003449 	Lval: 0.004084
Epoch 00083: reducing learning rate of group 0 to 9.5762e-06.
Epoch: 85 	Ltrain: 0.003490 	Lval: 0.004078
Epoch: 90 	Ltrain: 0.003431 	Lval: 0.004071
Epoch 00095: reducing learning rate of group 0 to 9.5762e-07.
Epoch: 95 	Ltrain: 0.003440 	Lval: 0.004071
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.004079


	Fold 3/5
Epoch: 1 	Ltrain: 0.069850 	Lval: 0.013398
Epoch: 5 	Ltrain: 0.005357 	Lval: 0.006023
Epoch: 10 	Ltrain: 0.004458 	Lval: 0.004728
Epoch: 15 	Ltrain: 0.004008 	Lval: 0.004566
Epoch 00017: reducing learning rate of group 0 to 9.5762e-04.
Epoch: 20 	Ltrain: 0.003360 	Lval: 0.004029
Epoch: 25 	Ltrain: 0.003207 	Lval: 0.003841
Epoch: 30 	Ltrain: 0.003134 	Lval: 0.003761
Epoch: 35 	Ltrain: 0.003029 	Lval: 0.003712
Epoch: 40 	Ltrain: 0.002944 	Lval: 0.003647
Epoch: 45 	Ltrain: 0.002854 	Lval: 0.003568
Epoch: 50 	Ltrain: 0.002879 	Lval: 0.003572
Epoch: 55 	Ltrain: 0.002767 	Lval: 0.003346
Epoch: 60 	Ltrain: 0.002636 	Lval: 0.003365
Epoch 00063: reducing learning rate of group 0 to 9.5762e-05.
Epoch: 65 	Ltrain: 0.002524 	Lval: 0.003196
Epoch: 70 	Ltrain: 0.002530 	Lval: 0.003188
Epoch: 75 	Ltrain: 0.002527 	Lval: 0.003160
Epoch: 80 	Ltrain: 0.002496 	Lval: 0.003151
Epoch: 85 	Ltrain: 0.002492 	Lval: 0.003131
Epoch: 90 	Ltrain: 0.002458 	Lval: 0.003129
Epoch: 95 	Ltrain: 0.002459 	Lval: 0.003087
Epoch 00099: reducing learning rate of group 0 to 9.5762e-06.
Epoch: 100 	Ltrain: 0.002445 	Lval: 0.003093
Epoch: 105 	Ltrain: 0.002421 	Lval: 0.003083
Epoch: 110 	Ltrain: 0.002427 	Lval: 0.003081
Epoch 00111: reducing learning rate of group 0 to 9.5762e-07.
Epoch: 115 	Ltrain: 0.002433 	Lval: 0.003081
EarlyStopper: stopping at epoch 118 with best_val_loss = 0.003081


	Fold 4/5
Epoch: 1 	Ltrain: 0.054883 	Lval: 0.011207
Epoch: 5 	Ltrain: 0.004295 	Lval: 0.005246
Epoch: 10 	Ltrain: 0.003947 	Lval: 0.005185
Epoch: 15 	Ltrain: 0.003259 	Lval: 0.004429
Epoch: 20 	Ltrain: 0.003003 	Lval: 0.003638
Epoch: 25 	Ltrain: 0.002381 	Lval: 0.002868
Epoch 00029: reducing learning rate of group 0 to 9.5762e-04.
Epoch: 30 	Ltrain: 0.002004 	Lval: 0.002530
Epoch: 35 	Ltrain: 0.001748 	Lval: 0.002312
Epoch: 40 	Ltrain: 0.001714 	Lval: 0.002323
Epoch: 45 	Ltrain: 0.001590 	Lval: 0.002038
Epoch: 50 	Ltrain: 0.001511 	Lval: 0.001955
Epoch: 55 	Ltrain: 0.001422 	Lval: 0.001792
Epoch: 60 	Ltrain: 0.001339 	Lval: 0.001645
Epoch: 65 	Ltrain: 0.001283 	Lval: 0.001518
Epoch: 70 	Ltrain: 0.001146 	Lval: 0.001339
Epoch: 75 	Ltrain: 0.001092 	Lval: 0.001216
Epoch: 80 	Ltrain: 0.000984 	Lval: 0.001111
Epoch: 85 	Ltrain: 0.000890 	Lval: 0.000949
Epoch: 90 	Ltrain: 0.000836 	Lval: 0.000887
Epoch: 95 	Ltrain: 0.000783 	Lval: 0.000808
Epoch: 100 	Ltrain: 0.000702 	Lval: 0.000772
Epoch: 105 	Ltrain: 0.000648 	Lval: 0.000664
Epoch: 110 	Ltrain: 0.000596 	Lval: 0.000637
Epoch 00115: reducing learning rate of group 0 to 9.5762e-05.
Epoch: 115 	Ltrain: 0.000583 	Lval: 0.000614
Epoch: 120 	Ltrain: 0.000464 	Lval: 0.000491
Epoch: 125 	Ltrain: 0.000451 	Lval: 0.000479
Epoch: 130 	Ltrain: 0.000445 	Lval: 0.000472
Epoch: 135 	Ltrain: 0.000436 	Lval: 0.000461
Epoch: 140 	Ltrain: 0.000430 	Lval: 0.000455
Epoch: 145 	Ltrain: 0.000423 	Lval: 0.000446
Epoch: 150 	Ltrain: 0.000416 	Lval: 0.000439
Epoch: 155 	Ltrain: 0.000410 	Lval: 0.000430
Epoch: 160 	Ltrain: 0.000403 	Lval: 0.000422
Epoch: 165 	Ltrain: 0.000396 	Lval: 0.000415
Epoch: 170 	Ltrain: 0.000387 	Lval: 0.000404
Epoch: 175 	Ltrain: 0.000380 	Lval: 0.000397
Epoch: 180 	Ltrain: 0.000372 	Lval: 0.000389
Epoch: 185 	Ltrain: 0.000364 	Lval: 0.000378
Epoch: 190 	Ltrain: 0.000355 	Lval: 0.000370
Epoch: 195 	Ltrain: 0.000348 	Lval: 0.000359
Epoch: 200 	Ltrain: 0.000340 	Lval: 0.000353
Epoch: 205 	Ltrain: 0.000332 	Lval: 0.000342
Epoch: 210 	Ltrain: 0.000325 	Lval: 0.000338
Epoch: 215 	Ltrain: 0.000316 	Lval: 0.000328
Epoch: 220 	Ltrain: 0.000308 	Lval: 0.000320
Epoch: 225 	Ltrain: 0.000302 	Lval: 0.000312
Epoch: 230 	Ltrain: 0.000295 	Lval: 0.000304
Epoch: 235 	Ltrain: 0.000287 	Lval: 0.000293
Epoch: 240 	Ltrain: 0.000280 	Lval: 0.000291
Epoch: 245 	Ltrain: 0.000273 	Lval: 0.000277
Epoch: 250 	Ltrain: 0.000265 	Lval: 0.000275
Epoch: 255 	Ltrain: 0.000258 	Lval: 0.000262
Epoch 00259: reducing learning rate of group 0 to 9.5762e-06.
Epoch: 260 	Ltrain: 0.000250 	Lval: 0.000252
Epoch: 265 	Ltrain: 0.000242 	Lval: 0.000249
Epoch: 270 	Ltrain: 0.000241 	Lval: 0.000248
Epoch: 275 	Ltrain: 0.000240 	Lval: 0.000247
EarlyStopper: stopping at epoch 275 with best_val_loss = 0.000252


	Fold 5/5
Epoch: 1 	Ltrain: 0.055131 	Lval: 0.007634
Epoch: 5 	Ltrain: 0.004480 	Lval: 0.005030
Epoch: 10 	Ltrain: 0.003916 	Lval: 0.004852
Epoch: 15 	Ltrain: 0.003261 	Lval: 0.003864
Epoch: 20 	Ltrain: 0.002808 	Lval: 0.004260
Epoch: 25 	Ltrain: 0.002748 	Lval: 0.003663
Epoch 00026: reducing learning rate of group 0 to 9.5762e-04.
Epoch: 30 	Ltrain: 0.002072 	Lval: 0.002773
Epoch: 35 	Ltrain: 0.002035 	Lval: 0.002734
Epoch: 40 	Ltrain: 0.001975 	Lval: 0.002570
Epoch 00044: reducing learning rate of group 0 to 9.5762e-05.
Epoch: 45 	Ltrain: 0.001887 	Lval: 0.002517
Epoch: 50 	Ltrain: 0.001867 	Lval: 0.002501
Epoch: 55 	Ltrain: 0.001862 	Lval: 0.002487
Epoch: 60 	Ltrain: 0.001857 	Lval: 0.002472
Epoch: 65 	Ltrain: 0.001843 	Lval: 0.002458
Epoch: 70 	Ltrain: 0.001834 	Lval: 0.002440
Epoch: 75 	Ltrain: 0.001830 	Lval: 0.002429
Epoch: 80 	Ltrain: 0.001822 	Lval: 0.002416
Epoch 00083: reducing learning rate of group 0 to 9.5762e-06.
Epoch: 85 	Ltrain: 0.001802 	Lval: 0.002399
Epoch: 90 	Ltrain: 0.001803 	Lval: 0.002397
Epoch: 95 	Ltrain: 0.001803 	Lval: 0.002397
EarlyStopper: stopping at epoch 94 with best_val_loss = 0.002405

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0008070606406819337
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.813820283202916e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 11
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.016487 	Lval: 0.012384
Epoch: 5 	Ltrain: 0.007707 	Lval: 0.007536
Epoch: 10 	Ltrain: 0.006539 	Lval: 0.005818
Epoch: 15 	Ltrain: 0.005538 	Lval: 0.005228
Epoch 00019: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 20 	Ltrain: 0.005572 	Lval: 0.005064
Epoch: 25 	Ltrain: 0.005364 	Lval: 0.004815
Epoch: 30 	Ltrain: 0.004640 	Lval: 0.004755
Epoch: 35 	Ltrain: 0.004651 	Lval: 0.004715
Epoch 00039: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 40 	Ltrain: 0.004631 	Lval: 0.004749
Epoch: 45 	Ltrain: 0.004619 	Lval: 0.004696
Epoch: 50 	Ltrain: 0.004627 	Lval: 0.004693
Epoch 00054: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 55 	Ltrain: 0.004484 	Lval: 0.004700
Epoch: 60 	Ltrain: 0.005012 	Lval: 0.004699
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.004693


	Fold 2/5
Epoch: 1 	Ltrain: 0.012232 	Lval: 0.009045
Epoch: 5 	Ltrain: 0.006103 	Lval: 0.005952
Epoch: 10 	Ltrain: 0.005009 	Lval: 0.005098
Epoch: 15 	Ltrain: 0.004892 	Lval: 0.004797
Epoch 00020: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 20 	Ltrain: 0.004396 	Lval: 0.004737
Epoch: 25 	Ltrain: 0.003945 	Lval: 0.004435
Epoch: 30 	Ltrain: 0.003832 	Lval: 0.004397
Epoch 00032: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 35 	Ltrain: 0.003831 	Lval: 0.004382
Epoch: 40 	Ltrain: 0.003795 	Lval: 0.004379
Epoch: 45 	Ltrain: 0.003828 	Lval: 0.004374
Epoch 00050: reducing learning rate of group 0 to 8.0706e-07.
Epoch: 50 	Ltrain: 0.003877 	Lval: 0.004368
Epoch: 55 	Ltrain: 0.003761 	Lval: 0.004367
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004371


	Fold 3/5
Epoch: 1 	Ltrain: 0.014600 	Lval: 0.009809
Epoch: 5 	Ltrain: 0.005429 	Lval: 0.005811
Epoch: 10 	Ltrain: 0.004349 	Lval: 0.004853
Epoch 00013: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 15 	Ltrain: 0.003855 	Lval: 0.004433
Epoch: 20 	Ltrain: 0.003789 	Lval: 0.004404
Epoch: 25 	Ltrain: 0.003765 	Lval: 0.004330
Epoch 00027: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 30 	Ltrain: 0.003719 	Lval: 0.004316
EarlyStopper: stopping at epoch 33 with best_val_loss = 0.004319


	Fold 4/5
Epoch: 1 	Ltrain: 0.014791 	Lval: 0.008550
Epoch: 5 	Ltrain: 0.005058 	Lval: 0.005624
Epoch: 10 	Ltrain: 0.004072 	Lval: 0.004732
Epoch: 15 	Ltrain: 0.003838 	Lval: 0.004832
Epoch: 20 	Ltrain: 0.003161 	Lval: 0.003935
Epoch: 25 	Ltrain: 0.002984 	Lval: 0.003693
Epoch: 30 	Ltrain: 0.002838 	Lval: 0.003690
Epoch 00032: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 35 	Ltrain: 0.002541 	Lval: 0.003343
Epoch: 40 	Ltrain: 0.002537 	Lval: 0.003317
Epoch: 45 	Ltrain: 0.002492 	Lval: 0.003311
Epoch 00046: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 50 	Ltrain: 0.002477 	Lval: 0.003279
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.003282


	Fold 5/5
Epoch: 1 	Ltrain: 0.012398 	Lval: 0.009019
Epoch: 5 	Ltrain: 0.004989 	Lval: 0.006197
Epoch: 10 	Ltrain: 0.004073 	Lval: 0.004798
Epoch: 15 	Ltrain: 0.003467 	Lval: 0.004191
Epoch: 20 	Ltrain: 0.003153 	Lval: 0.004028
Epoch: 25 	Ltrain: 0.002935 	Lval: 0.003660
Epoch: 30 	Ltrain: 0.002890 	Lval: 0.003473
Epoch: 35 	Ltrain: 0.002653 	Lval: 0.003720
Epoch: 40 	Ltrain: 0.002667 	Lval: 0.003272
Epoch: 45 	Ltrain: 0.002480 	Lval: 0.003149
Epoch: 50 	Ltrain: 0.002343 	Lval: 0.003082
Epoch: 55 	Ltrain: 0.002403 	Lval: 0.003126
Epoch 00056: reducing learning rate of group 0 to 8.0706e-05.
Epoch: 60 	Ltrain: 0.002036 	Lval: 0.002753
Epoch: 65 	Ltrain: 0.002024 	Lval: 0.002771
Epoch: 70 	Ltrain: 0.002001 	Lval: 0.002719
Epoch: 75 	Ltrain: 0.001988 	Lval: 0.002663
Epoch: 80 	Ltrain: 0.001962 	Lval: 0.002705
Epoch 00083: reducing learning rate of group 0 to 8.0706e-06.
Epoch: 85 	Ltrain: 0.001931 	Lval: 0.002646
Epoch: 90 	Ltrain: 0.001928 	Lval: 0.002633
Epoch: 95 	Ltrain: 0.001927 	Lval: 0.002632
Epoch: 100 	Ltrain: 0.001939 	Lval: 0.002630
EarlyStopper: stopping at epoch 101 with best_val_loss = 0.002631

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 3
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005043960024360002
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 4.428487065439827e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.035308 	Lval: 0.008802
Epoch: 5 	Ltrain: 0.006256 	Lval: 0.005702
Epoch: 10 	Ltrain: 0.005730 	Lval: 0.006452
Epoch 00011: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 15 	Ltrain: 0.004880 	Lval: 0.004709
Epoch: 20 	Ltrain: 0.004853 	Lval: 0.004651
Epoch: 25 	Ltrain: 0.004666 	Lval: 0.004598
Epoch: 30 	Ltrain: 0.004468 	Lval: 0.004532
Epoch: 35 	Ltrain: 0.005165 	Lval: 0.004533
Epoch: 40 	Ltrain: 0.004904 	Lval: 0.004460
Epoch: 45 	Ltrain: 0.004662 	Lval: 0.004413
Epoch 00047: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 50 	Ltrain: 0.004162 	Lval: 0.004393
Epoch: 55 	Ltrain: 0.004235 	Lval: 0.004381
Epoch: 60 	Ltrain: 0.004315 	Lval: 0.004383
Epoch 00061: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 65 	Ltrain: 0.004255 	Lval: 0.004372
Epoch: 70 	Ltrain: 0.004432 	Lval: 0.004365
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.004374


	Fold 2/5
Epoch: 1 	Ltrain: 0.021439 	Lval: 0.008095
Epoch: 5 	Ltrain: 0.004937 	Lval: 0.005831
Epoch: 10 	Ltrain: 0.004198 	Lval: 0.004553
Epoch: 15 	Ltrain: 0.004021 	Lval: 0.004509
Epoch 00016: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 20 	Ltrain: 0.003106 	Lval: 0.003828
Epoch: 25 	Ltrain: 0.003013 	Lval: 0.003702
Epoch: 30 	Ltrain: 0.003086 	Lval: 0.003635
Epoch: 35 	Ltrain: 0.002992 	Lval: 0.003554
Epoch: 40 	Ltrain: 0.002978 	Lval: 0.003451
Epoch: 45 	Ltrain: 0.002812 	Lval: 0.003430
Epoch: 50 	Ltrain: 0.002880 	Lval: 0.003419
Epoch: 55 	Ltrain: 0.002837 	Lval: 0.003430
Epoch 00058: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 60 	Ltrain: 0.002702 	Lval: 0.003288
Epoch: 65 	Ltrain: 0.002677 	Lval: 0.003271
Epoch: 70 	Ltrain: 0.002660 	Lval: 0.003257
Epoch 00072: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 75 	Ltrain: 0.002621 	Lval: 0.003257
Epoch: 80 	Ltrain: 0.002681 	Lval: 0.003256
EarlyStopper: stopping at epoch 79 with best_val_loss = 0.003264


	Fold 3/5
Epoch: 1 	Ltrain: 0.015052 	Lval: 0.007700
Epoch: 5 	Ltrain: 0.004307 	Lval: 0.005471
Epoch: 10 	Ltrain: 0.003567 	Lval: 0.003876
Epoch: 15 	Ltrain: 0.003404 	Lval: 0.003841
Epoch: 20 	Ltrain: 0.002932 	Lval: 0.003421
Epoch: 25 	Ltrain: 0.002963 	Lval: 0.004437
Epoch: 30 	Ltrain: 0.002793 	Lval: 0.003167
Epoch 00035: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 35 	Ltrain: 0.002638 	Lval: 0.003163
Epoch: 40 	Ltrain: 0.002264 	Lval: 0.002858
Epoch: 45 	Ltrain: 0.002229 	Lval: 0.002820
Epoch: 50 	Ltrain: 0.002190 	Lval: 0.002696
Epoch: 55 	Ltrain: 0.002140 	Lval: 0.002683
Epoch 00056: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 60 	Ltrain: 0.002118 	Lval: 0.002655
Epoch: 65 	Ltrain: 0.002126 	Lval: 0.002654
Epoch: 70 	Ltrain: 0.002097 	Lval: 0.002647
Epoch: 75 	Ltrain: 0.002087 	Lval: 0.002635
Epoch 00079: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 80 	Ltrain: 0.002109 	Lval: 0.002639
Epoch: 85 	Ltrain: 0.002082 	Lval: 0.002637
Epoch: 90 	Ltrain: 0.002074 	Lval: 0.002637
Epoch 00091: reducing learning rate of group 0 to 5.0440e-07.
EarlyStopper: stopping at epoch 90 with best_val_loss = 0.002635


	Fold 4/5
Epoch: 1 	Ltrain: 0.019314 	Lval: 0.006155
Epoch: 5 	Ltrain: 0.004133 	Lval: 0.004855
Epoch: 10 	Ltrain: 0.003516 	Lval: 0.004169
Epoch: 15 	Ltrain: 0.002893 	Lval: 0.003631
Epoch 00018: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 20 	Ltrain: 0.002542 	Lval: 0.003306
Epoch: 25 	Ltrain: 0.002498 	Lval: 0.003260
Epoch: 30 	Ltrain: 0.002484 	Lval: 0.003175
Epoch 00034: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 35 	Ltrain: 0.002416 	Lval: 0.003183
Epoch: 40 	Ltrain: 0.002402 	Lval: 0.003161
Epoch: 45 	Ltrain: 0.002393 	Lval: 0.003142
Epoch: 50 	Ltrain: 0.002385 	Lval: 0.003140
Epoch 00053: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 55 	Ltrain: 0.002376 	Lval: 0.003138
Epoch: 60 	Ltrain: 0.002377 	Lval: 0.003134
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.003141


	Fold 5/5
Epoch: 1 	Ltrain: 0.015458 	Lval: 0.007252
Epoch: 5 	Ltrain: 0.003936 	Lval: 0.004806
Epoch: 10 	Ltrain: 0.003169 	Lval: 0.004096
Epoch: 15 	Ltrain: 0.003054 	Lval: 0.004068
Epoch: 20 	Ltrain: 0.002804 	Lval: 0.003369
Epoch: 25 	Ltrain: 0.002720 	Lval: 0.003421
Epoch 00028: reducing learning rate of group 0 to 5.0440e-04.
Epoch: 30 	Ltrain: 0.002356 	Lval: 0.003084
Epoch: 35 	Ltrain: 0.002329 	Lval: 0.003091
Epoch: 40 	Ltrain: 0.002277 	Lval: 0.003009
Epoch: 45 	Ltrain: 0.002245 	Lval: 0.002958
Epoch 00048: reducing learning rate of group 0 to 5.0440e-05.
Epoch: 50 	Ltrain: 0.002202 	Lval: 0.002952
Epoch: 55 	Ltrain: 0.002190 	Lval: 0.002956
Epoch: 60 	Ltrain: 0.002192 	Lval: 0.002936
Epoch 00064: reducing learning rate of group 0 to 5.0440e-06.
Epoch: 65 	Ltrain: 0.002184 	Lval: 0.002943
Epoch: 70 	Ltrain: 0.002179 	Lval: 0.002941
Epoch: 75 	Ltrain: 0.002182 	Lval: 0.002941
Epoch 00076: reducing learning rate of group 0 to 5.0440e-07.
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.002936

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0021438653190657695
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.1609382310136867e-05
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.021275 	Lval: 0.009084
Epoch: 5 	Ltrain: 0.014580 	Lval: 0.008696
Epoch: 10 	Ltrain: 0.009675 	Lval: 0.006534
Epoch: 15 	Ltrain: 0.007734 	Lval: 0.006082
Epoch: 20 	Ltrain: 0.007013 	Lval: 0.006013
Epoch: 25 	Ltrain: 0.007549 	Lval: 0.005291
Epoch: 30 	Ltrain: 0.008250 	Lval: 0.006647
Epoch 00031: reducing learning rate of group 0 to 2.1439e-04.
Epoch: 35 	Ltrain: 0.008078 	Lval: 0.006743
Epoch: 40 	Ltrain: 0.007326 	Lval: 0.005894
Epoch 00043: reducing learning rate of group 0 to 2.1439e-05.
Epoch: 45 	Ltrain: 0.005561 	Lval: 0.005548
EarlyStopper: stopping at epoch 46 with best_val_loss = 0.005260


	Fold 2/5
Epoch: 1 	Ltrain: 0.024536 	Lval: 0.014831
Epoch: 5 	Ltrain: 0.008020 	Lval: 0.007216
Epoch: 10 	Ltrain: 0.006335 	Lval: 0.005626
Epoch: 15 	Ltrain: 0.005138 	Lval: 0.005297
Epoch: 20 	Ltrain: 0.004849 	Lval: 0.005187
Epoch 00025: reducing learning rate of group 0 to 2.1439e-04.
Epoch: 25 	Ltrain: 0.005792 	Lval: 0.005195
Epoch: 30 	Ltrain: 0.004785 	Lval: 0.005157
Epoch: 35 	Ltrain: 0.004252 	Lval: 0.004761
Epoch 00040: reducing learning rate of group 0 to 2.1439e-05.
Epoch: 40 	Ltrain: 0.004647 	Lval: 0.004653
Epoch: 45 	Ltrain: 0.004468 	Lval: 0.004624
Epoch: 50 	Ltrain: 0.004605 	Lval: 0.004600
Epoch 00052: reducing learning rate of group 0 to 2.1439e-06.
Epoch: 55 	Ltrain: 0.004260 	Lval: 0.004604
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.004596


	Fold 3/5
Epoch: 1 	Ltrain: 0.017650 	Lval: 0.008059
Epoch: 5 	Ltrain: 0.006993 	Lval: 0.006088
Epoch: 10 	Ltrain: 0.004943 	Lval: 0.005168
Epoch: 15 	Ltrain: 0.004672 	Lval: 0.004781
Epoch 00018: reducing learning rate of group 0 to 2.1439e-04.
Epoch: 20 	Ltrain: 0.004072 	Lval: 0.004647
Epoch: 25 	Ltrain: 0.003992 	Lval: 0.004565
Epoch 00030: reducing learning rate of group 0 to 2.1439e-05.
Epoch: 30 	Ltrain: 0.004093 	Lval: 0.004549
EarlyStopper: stopping at epoch 33 with best_val_loss = 0.004494


	Fold 4/5
Epoch: 1 	Ltrain: 0.016097 	Lval: 0.008767
Epoch: 5 	Ltrain: 0.005897 	Lval: 0.006868
Epoch: 10 	Ltrain: 0.004880 	Lval: 0.005048
Epoch: 15 	Ltrain: 0.004627 	Lval: 0.004878
Epoch: 20 	Ltrain: 0.004018 	Lval: 0.004655
Epoch 00022: reducing learning rate of group 0 to 2.1439e-04.
Epoch: 25 	Ltrain: 0.003937 	Lval: 0.004757
Epoch: 30 	Ltrain: 0.003847 	Lval: 0.004698
Epoch 00034: reducing learning rate of group 0 to 2.1439e-05.
Epoch: 35 	Ltrain: 0.003834 	Lval: 0.004635
EarlyStopper: stopping at epoch 37 with best_val_loss = 0.004529


	Fold 5/5
Epoch: 1 	Ltrain: 0.024487 	Lval: 0.009285
Epoch: 5 	Ltrain: 0.006358 	Lval: 0.007251
Epoch: 10 	Ltrain: 0.004818 	Lval: 0.005869
Epoch: 15 	Ltrain: 0.004256 	Lval: 0.005195
Epoch 00020: reducing learning rate of group 0 to 2.1439e-04.
Epoch: 20 	Ltrain: 0.004342 	Lval: 0.005439
Epoch: 25 	Ltrain: 0.003855 	Lval: 0.005045
Epoch: 30 	Ltrain: 0.003892 	Lval: 0.005042
Epoch 00032: reducing learning rate of group 0 to 2.1439e-05.
Epoch: 35 	Ltrain: 0.003934 	Lval: 0.005055
Epoch: 40 	Ltrain: 0.003850 	Lval: 0.005052
Epoch 00044: reducing learning rate of group 0 to 2.1439e-06.
Epoch: 45 	Ltrain: 0.003898 	Lval: 0.005047
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.005004

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0006969127036536701
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.314635931599511e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 18
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.014926 	Lval: 0.010606
Epoch: 5 	Ltrain: 0.009789 	Lval: 0.011913
Epoch: 10 	Ltrain: 0.007207 	Lval: 0.007360
Epoch: 15 	Ltrain: 0.006623 	Lval: 0.006143
Epoch: 20 	Ltrain: 0.005749 	Lval: 0.005993
Epoch: 25 	Ltrain: 0.005652 	Lval: 0.005386
Epoch: 30 	Ltrain: 0.005164 	Lval: 0.004996
Epoch: 35 	Ltrain: 0.005307 	Lval: 0.005011
Epoch: 40 	Ltrain: 0.004770 	Lval: 0.004738
Epoch: 45 	Ltrain: 0.004515 	Lval: 0.004454
Epoch: 50 	Ltrain: 0.004474 	Lval: 0.004403
Epoch: 55 	Ltrain: 0.004536 	Lval: 0.004433
Epoch 00060: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 60 	Ltrain: 0.004265 	Lval: 0.004460
Epoch: 65 	Ltrain: 0.003905 	Lval: 0.004119
Epoch: 70 	Ltrain: 0.004328 	Lval: 0.004150
Epoch: 75 	Ltrain: 0.004167 	Lval: 0.004137
Epoch 00078: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 80 	Ltrain: 0.004359 	Lval: 0.004084
Epoch: 85 	Ltrain: 0.003938 	Lval: 0.004082
Epoch: 90 	Ltrain: 0.003790 	Lval: 0.004069
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.004074


	Fold 2/5
Epoch: 1 	Ltrain: 0.015915 	Lval: 0.011993
Epoch: 5 	Ltrain: 0.006895 	Lval: 0.006860
Epoch: 10 	Ltrain: 0.005448 	Lval: 0.005486
Epoch: 15 	Ltrain: 0.004869 	Lval: 0.005234
Epoch: 20 	Ltrain: 0.004443 	Lval: 0.004673
Epoch 00024: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 25 	Ltrain: 0.004226 	Lval: 0.004578
Epoch: 30 	Ltrain: 0.004089 	Lval: 0.004546
Epoch: 35 	Ltrain: 0.004062 	Lval: 0.004530
Epoch: 40 	Ltrain: 0.003959 	Lval: 0.004505
Epoch: 45 	Ltrain: 0.003961 	Lval: 0.004481
Epoch 00049: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 50 	Ltrain: 0.003918 	Lval: 0.004483
Epoch: 55 	Ltrain: 0.004016 	Lval: 0.004467
Epoch: 60 	Ltrain: 0.003921 	Lval: 0.004465
Epoch 00061: reducing learning rate of group 0 to 6.9691e-07.
Epoch: 65 	Ltrain: 0.003882 	Lval: 0.004460
Epoch: 70 	Ltrain: 0.003871 	Lval: 0.004461
Epoch 00073: reducing learning rate of group 0 to 6.9691e-08.
EarlyStopper: stopping at epoch 73 with best_val_loss = 0.004462


	Fold 3/5
Epoch: 1 	Ltrain: 0.013224 	Lval: 0.010610
Epoch: 5 	Ltrain: 0.006374 	Lval: 0.006462
Epoch: 10 	Ltrain: 0.004793 	Lval: 0.004924
Epoch: 15 	Ltrain: 0.004290 	Lval: 0.004875
Epoch 00016: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 20 	Ltrain: 0.003993 	Lval: 0.004564
Epoch: 25 	Ltrain: 0.003988 	Lval: 0.004543
Epoch: 30 	Ltrain: 0.003911 	Lval: 0.004536
Epoch 00035: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 35 	Ltrain: 0.003915 	Lval: 0.004532
Epoch: 40 	Ltrain: 0.003866 	Lval: 0.004490
Epoch: 45 	Ltrain: 0.003907 	Lval: 0.004484
Epoch 00047: reducing learning rate of group 0 to 6.9691e-07.
Epoch: 50 	Ltrain: 0.003896 	Lval: 0.004483
Epoch: 55 	Ltrain: 0.003892 	Lval: 0.004481
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.004480


	Fold 4/5
Epoch: 1 	Ltrain: 0.017266 	Lval: 0.010660
Epoch: 5 	Ltrain: 0.005925 	Lval: 0.006247
Epoch: 10 	Ltrain: 0.004524 	Lval: 0.005062
Epoch: 15 	Ltrain: 0.004137 	Lval: 0.004787
Epoch: 20 	Ltrain: 0.003762 	Lval: 0.004394
Epoch: 25 	Ltrain: 0.003139 	Lval: 0.003915
Epoch 00029: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 30 	Ltrain: 0.002946 	Lval: 0.003702
Epoch: 35 	Ltrain: 0.002818 	Lval: 0.003647
Epoch: 40 	Ltrain: 0.002818 	Lval: 0.003658
Epoch: 45 	Ltrain: 0.002789 	Lval: 0.003616
Epoch 00047: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 50 	Ltrain: 0.002759 	Lval: 0.003604
Epoch: 55 	Ltrain: 0.002758 	Lval: 0.003603
Epoch: 60 	Ltrain: 0.002766 	Lval: 0.003600
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.003603


	Fold 5/5
Epoch: 1 	Ltrain: 0.015113 	Lval: 0.010714
Epoch: 5 	Ltrain: 0.005601 	Lval: 0.006267
Epoch: 10 	Ltrain: 0.004504 	Lval: 0.005160
Epoch: 15 	Ltrain: 0.004095 	Lval: 0.004742
Epoch: 20 	Ltrain: 0.003796 	Lval: 0.004482
Epoch: 25 	Ltrain: 0.003726 	Lval: 0.004275
Epoch: 30 	Ltrain: 0.003174 	Lval: 0.003937
Epoch 00033: reducing learning rate of group 0 to 6.9691e-05.
Epoch: 35 	Ltrain: 0.002841 	Lval: 0.003753
Epoch: 40 	Ltrain: 0.002806 	Lval: 0.003733
Epoch: 45 	Ltrain: 0.002803 	Lval: 0.003666
Epoch 00049: reducing learning rate of group 0 to 6.9691e-06.
Epoch: 50 	Ltrain: 0.002761 	Lval: 0.003690
Epoch: 55 	Ltrain: 0.002753 	Lval: 0.003669
Epoch: 60 	Ltrain: 0.002746 	Lval: 0.003677
Epoch 00061: reducing learning rate of group 0 to 6.9691e-07.
EarlyStopper: stopping at epoch 62 with best_val_loss = 0.003666

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.009835906648160307
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.2273142890562989e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 16
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.254701 	Lval: 0.037338
Epoch: 5 	Ltrain: 0.015366 	Lval: 0.012027
Epoch: 10 	Ltrain: 0.011966 	Lval: 0.013613
Epoch: 15 	Ltrain: 0.011457 	Lval: 0.010718
Epoch: 20 	Ltrain: 0.010588 	Lval: 0.010289
Epoch: 25 	Ltrain: 0.011874 	Lval: 0.010770
Epoch 00028: reducing learning rate of group 0 to 9.8359e-04.
Epoch: 30 	Ltrain: 0.011019 	Lval: 0.010093
Epoch: 35 	Ltrain: 0.010276 	Lval: 0.009943
Epoch 00040: reducing learning rate of group 0 to 9.8359e-05.
Epoch: 40 	Ltrain: 0.010242 	Lval: 0.010047
Epoch: 45 	Ltrain: 0.010300 	Lval: 0.009910
Epoch: 50 	Ltrain: 0.010687 	Lval: 0.009885
Epoch: 55 	Ltrain: 0.010208 	Lval: 0.009865
Epoch: 60 	Ltrain: 0.010971 	Lval: 0.009861
Epoch 00061: reducing learning rate of group 0 to 9.8359e-06.
Epoch: 65 	Ltrain: 0.010392 	Lval: 0.009857
Epoch: 70 	Ltrain: 0.010728 	Lval: 0.009851
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.009854


	Fold 2/5
Epoch: 1 	Ltrain: 0.080503 	Lval: 0.012636
Epoch: 5 	Ltrain: 0.006490 	Lval: 0.007296
Epoch: 10 	Ltrain: 0.004410 	Lval: 0.005068
Epoch: 15 	Ltrain: 0.004158 	Lval: 0.004565
Epoch: 20 	Ltrain: 0.003387 	Lval: 0.003886
Epoch: 25 	Ltrain: 0.002989 	Lval: 0.003544
Epoch: 30 	Ltrain: 0.002964 	Lval: 0.003819
Epoch 00031: reducing learning rate of group 0 to 9.8359e-04.
Epoch: 35 	Ltrain: 0.002286 	Lval: 0.002740
Epoch: 40 	Ltrain: 0.002229 	Lval: 0.002703
Epoch: 45 	Ltrain: 0.002155 	Lval: 0.002487
Epoch: 50 	Ltrain: 0.002105 	Lval: 0.002387
Epoch: 55 	Ltrain: 0.002109 	Lval: 0.002333
Epoch: 60 	Ltrain: 0.001938 	Lval: 0.002179
Epoch: 65 	Ltrain: 0.001880 	Lval: 0.002083
Epoch: 70 	Ltrain: 0.001862 	Lval: 0.001978
Epoch: 75 	Ltrain: 0.001720 	Lval: 0.001845
Epoch: 80 	Ltrain: 0.001586 	Lval: 0.001690
Epoch: 85 	Ltrain: 0.001569 	Lval: 0.001650
Epoch: 90 	Ltrain: 0.001375 	Lval: 0.001465
Epoch: 95 	Ltrain: 0.001292 	Lval: 0.001377
Epoch: 100 	Ltrain: 0.001268 	Lval: 0.001315
Epoch: 105 	Ltrain: 0.001112 	Lval: 0.001157
Epoch: 110 	Ltrain: 0.001078 	Lval: 0.001082
Epoch: 115 	Ltrain: 0.001045 	Lval: 0.001092
Epoch: 120 	Ltrain: 0.001041 	Lval: 0.001097
Epoch 00121: reducing learning rate of group 0 to 9.8359e-05.
Epoch: 125 	Ltrain: 0.000884 	Lval: 0.000893
Epoch: 130 	Ltrain: 0.000835 	Lval: 0.000874
Epoch: 135 	Ltrain: 0.000827 	Lval: 0.000864
Epoch: 140 	Ltrain: 0.000823 	Lval: 0.000853
Epoch: 145 	Ltrain: 0.000816 	Lval: 0.000843
Epoch: 150 	Ltrain: 0.000797 	Lval: 0.000832
Epoch: 155 	Ltrain: 0.000795 	Lval: 0.000823
Epoch: 160 	Ltrain: 0.000793 	Lval: 0.000813
Epoch: 165 	Ltrain: 0.000794 	Lval: 0.000806
Epoch: 170 	Ltrain: 0.000770 	Lval: 0.000796
Epoch: 175 	Ltrain: 0.000756 	Lval: 0.000786
Epoch: 180 	Ltrain: 0.000746 	Lval: 0.000776
Epoch: 185 	Ltrain: 0.000742 	Lval: 0.000765
Epoch: 190 	Ltrain: 0.000740 	Lval: 0.000754
Epoch: 195 	Ltrain: 0.000737 	Lval: 0.000745
Epoch: 200 	Ltrain: 0.000721 	Lval: 0.000736
Epoch: 205 	Ltrain: 0.000703 	Lval: 0.000725
Epoch: 210 	Ltrain: 0.000698 	Lval: 0.000714
Epoch: 215 	Ltrain: 0.000682 	Lval: 0.000705
Epoch: 220 	Ltrain: 0.000670 	Lval: 0.000693
Epoch: 225 	Ltrain: 0.000668 	Lval: 0.000684
Epoch: 230 	Ltrain: 0.000668 	Lval: 0.000675
Epoch: 235 	Ltrain: 0.000649 	Lval: 0.000661
Epoch: 240 	Ltrain: 0.000645 	Lval: 0.000650
Epoch: 245 	Ltrain: 0.000630 	Lval: 0.000640
Epoch: 250 	Ltrain: 0.000612 	Lval: 0.000629
Epoch: 255 	Ltrain: 0.000607 	Lval: 0.000620
Epoch: 260 	Ltrain: 0.000592 	Lval: 0.000607
Epoch: 265 	Ltrain: 0.000584 	Lval: 0.000601
Epoch: 270 	Ltrain: 0.000569 	Lval: 0.000590
Epoch: 275 	Ltrain: 0.000574 	Lval: 0.000574
Epoch: 280 	Ltrain: 0.000561 	Lval: 0.000563
Epoch: 285 	Ltrain: 0.000546 	Lval: 0.000557
Epoch: 290 	Ltrain: 0.000529 	Lval: 0.000546
Epoch: 295 	Ltrain: 0.000520 	Lval: 0.000529
Epoch: 300 	Ltrain: 0.000523 	Lval: 0.000520
Epoch: 305 	Ltrain: 0.000504 	Lval: 0.000510
Epoch: 310 	Ltrain: 0.000489 	Lval: 0.000500
Epoch: 315 	Ltrain: 0.000484 	Lval: 0.000491
Epoch 00318: reducing learning rate of group 0 to 9.8359e-06.
Epoch: 320 	Ltrain: 0.000467 	Lval: 0.000479
Epoch: 325 	Ltrain: 0.000473 	Lval: 0.000477
Epoch: 330 	Ltrain: 0.000465 	Lval: 0.000475
Epoch: 335 	Ltrain: 0.000454 	Lval: 0.000474
EarlyStopper: stopping at epoch 334 with best_val_loss = 0.000481


	Fold 3/5
Epoch: 1 	Ltrain: 0.192006 	Lval: 0.036676
Epoch: 5 	Ltrain: 0.012917 	Lval: 0.011628
Epoch: 10 	Ltrain: 0.011969 	Lval: 0.012458
Epoch 00013: reducing learning rate of group 0 to 9.8359e-04.
Epoch: 15 	Ltrain: 0.010766 	Lval: 0.010174
Epoch: 20 	Ltrain: 0.010530 	Lval: 0.010248
Epoch 00025: reducing learning rate of group 0 to 9.8359e-05.
Epoch: 25 	Ltrain: 0.010526 	Lval: 0.010019
EarlyStopper: stopping at epoch 24 with best_val_loss = 0.009979


	Fold 4/5
Epoch: 1 	Ltrain: 0.148056 	Lval: 0.029239
Epoch: 5 	Ltrain: 0.012699 	Lval: 0.012942
Epoch: 10 	Ltrain: 0.012844 	Lval: 0.011407
Epoch 00012: reducing learning rate of group 0 to 9.8359e-04.
Epoch: 15 	Ltrain: 0.009486 	Lval: 0.009286
Epoch: 20 	Ltrain: 0.009029 	Lval: 0.009783
Epoch 00024: reducing learning rate of group 0 to 9.8359e-05.
Epoch: 25 	Ltrain: 0.008667 	Lval: 0.009085
Epoch: 30 	Ltrain: 0.008513 	Lval: 0.009034
Epoch: 35 	Ltrain: 0.008505 	Lval: 0.009006
Epoch: 40 	Ltrain: 0.008382 	Lval: 0.009022
Epoch 00041: reducing learning rate of group 0 to 9.8359e-06.
Epoch: 45 	Ltrain: 0.008345 	Lval: 0.009003
Epoch: 50 	Ltrain: 0.008292 	Lval: 0.009002
Epoch 00053: reducing learning rate of group 0 to 9.8359e-07.
EarlyStopper: stopping at epoch 52 with best_val_loss = 0.008993


	Fold 5/5
Epoch: 1 	Ltrain: 0.090133 	Lval: 0.010461
Epoch: 5 	Ltrain: 0.005137 	Lval: 0.005617
Epoch: 10 	Ltrain: 0.004380 	Lval: 0.004886
Epoch: 15 	Ltrain: 0.003417 	Lval: 0.004243
Epoch: 20 	Ltrain: 0.003169 	Lval: 0.004066
Epoch: 25 	Ltrain: 0.002853 	Lval: 0.003709
Epoch: 30 	Ltrain: 0.002619 	Lval: 0.003526
Epoch: 35 	Ltrain: 0.002697 	Lval: 0.003021
Epoch: 40 	Ltrain: 0.002259 	Lval: 0.002719
Epoch: 45 	Ltrain: 0.002079 	Lval: 0.002472
Epoch 00050: reducing learning rate of group 0 to 9.8359e-04.
Epoch: 50 	Ltrain: 0.002241 	Lval: 0.002380
Epoch: 55 	Ltrain: 0.001491 	Lval: 0.001842
Epoch: 60 	Ltrain: 0.001405 	Lval: 0.001739
Epoch: 65 	Ltrain: 0.001354 	Lval: 0.001608
Epoch: 70 	Ltrain: 0.001293 	Lval: 0.001543
Epoch: 75 	Ltrain: 0.001252 	Lval: 0.001442
Epoch: 80 	Ltrain: 0.001186 	Lval: 0.001389
Epoch: 85 	Ltrain: 0.001116 	Lval: 0.001258
Epoch: 90 	Ltrain: 0.001063 	Lval: 0.001199
Epoch: 95 	Ltrain: 0.000992 	Lval: 0.001097
Epoch: 100 	Ltrain: 0.000928 	Lval: 0.001021
Epoch: 105 	Ltrain: 0.000844 	Lval: 0.000923
Epoch: 110 	Ltrain: 0.000815 	Lval: 0.000853
Epoch: 115 	Ltrain: 0.000756 	Lval: 0.000814
Epoch: 120 	Ltrain: 0.000693 	Lval: 0.000692
Epoch: 125 	Ltrain: 0.000667 	Lval: 0.000663
Epoch: 130 	Ltrain: 0.000616 	Lval: 0.000622
Epoch: 135 	Ltrain: 0.000563 	Lval: 0.000591
Epoch: 140 	Ltrain: 0.000537 	Lval: 0.000538
Epoch: 145 	Ltrain: 0.000488 	Lval: 0.000473
Epoch 00149: reducing learning rate of group 0 to 9.8359e-05.
Epoch: 150 	Ltrain: 0.000436 	Lval: 0.000409
Epoch: 155 	Ltrain: 0.000371 	Lval: 0.000372
Epoch: 160 	Ltrain: 0.000360 	Lval: 0.000360
Epoch: 165 	Ltrain: 0.000353 	Lval: 0.000352
Epoch: 170 	Ltrain: 0.000345 	Lval: 0.000345
Epoch: 175 	Ltrain: 0.000339 	Lval: 0.000338
Epoch: 180 	Ltrain: 0.000333 	Lval: 0.000332
Epoch: 185 	Ltrain: 0.000326 	Lval: 0.000325
Epoch: 190 	Ltrain: 0.000321 	Lval: 0.000318
Epoch: 195 	Ltrain: 0.000314 	Lval: 0.000312
Epoch: 200 	Ltrain: 0.000308 	Lval: 0.000304
Epoch: 205 	Ltrain: 0.000301 	Lval: 0.000298
Epoch: 210 	Ltrain: 0.000293 	Lval: 0.000290
Epoch: 215 	Ltrain: 0.000286 	Lval: 0.000282
Epoch: 220 	Ltrain: 0.000279 	Lval: 0.000276
Epoch: 225 	Ltrain: 0.000272 	Lval: 0.000267
Epoch: 230 	Ltrain: 0.000265 	Lval: 0.000260
Epoch: 235 	Ltrain: 0.000257 	Lval: 0.000251
Epoch: 240 	Ltrain: 0.000250 	Lval: 0.000244
Epoch: 245 	Ltrain: 0.000241 	Lval: 0.000235
Epoch: 250 	Ltrain: 0.000234 	Lval: 0.000226
Epoch: 255 	Ltrain: 0.000226 	Lval: 0.000219
Epoch: 260 	Ltrain: 0.000220 	Lval: 0.000212
Epoch: 265 	Ltrain: 0.000211 	Lval: 0.000202
Epoch: 270 	Ltrain: 0.000205 	Lval: 0.000195
Epoch: 275 	Ltrain: 0.000197 	Lval: 0.000187
Epoch: 280 	Ltrain: 0.000192 	Lval: 0.000182
Epoch: 285 	Ltrain: 0.000185 	Lval: 0.000174
Epoch: 290 	Ltrain: 0.000178 	Lval: 0.000168
Epoch: 295 	Ltrain: 0.000174 	Lval: 0.000161
Epoch: 300 	Ltrain: 0.000171 	Lval: 0.000158
Epoch: 305 	Ltrain: 0.000163 	Lval: 0.000157
Epoch: 310 	Ltrain: 0.000157 	Lval: 0.000148
Epoch 00315: reducing learning rate of group 0 to 9.8359e-06.
Epoch: 315 	Ltrain: 0.000156 	Lval: 0.000147
Epoch: 320 	Ltrain: 0.000140 	Lval: 0.000132
Epoch: 325 	Ltrain: 0.000139 	Lval: 0.000131
Epoch: 330 	Ltrain: 0.000138 	Lval: 0.000130
EarlyStopper: stopping at epoch 331 with best_val_loss = 0.000136

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007309325476965149
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.121727854382293e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 13
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.110675 	Lval: 0.019211
Epoch: 5 	Ltrain: 0.010362 	Lval: 0.009497
Epoch: 10 	Ltrain: 0.006404 	Lval: 0.005840
Epoch: 15 	Ltrain: 0.004763 	Lval: 0.004807
Epoch: 20 	Ltrain: 0.004379 	Lval: 0.005266
Epoch 00025: reducing learning rate of group 0 to 7.3093e-04.
Epoch: 25 	Ltrain: 0.004485 	Lval: 0.004571
Epoch: 30 	Ltrain: 0.003823 	Lval: 0.004005
Epoch: 35 	Ltrain: 0.003768 	Lval: 0.003904
Epoch: 40 	Ltrain: 0.003671 	Lval: 0.003807
Epoch: 45 	Ltrain: 0.003572 	Lval: 0.003699
Epoch: 50 	Ltrain: 0.003664 	Lval: 0.003646
Epoch: 55 	Ltrain: 0.003456 	Lval: 0.003564
Epoch: 60 	Ltrain: 0.003423 	Lval: 0.003481
Epoch: 65 	Ltrain: 0.003432 	Lval: 0.003412
Epoch 00069: reducing learning rate of group 0 to 7.3093e-05.
Epoch: 70 	Ltrain: 0.003343 	Lval: 0.003398
Epoch: 75 	Ltrain: 0.003223 	Lval: 0.003339
Epoch: 80 	Ltrain: 0.003150 	Lval: 0.003330
Epoch: 85 	Ltrain: 0.003320 	Lval: 0.003340
Epoch: 90 	Ltrain: 0.003136 	Lval: 0.003326
Epoch: 95 	Ltrain: 0.003075 	Lval: 0.003289
Epoch: 100 	Ltrain: 0.003074 	Lval: 0.003284
Epoch: 105 	Ltrain: 0.003061 	Lval: 0.003264
Epoch: 110 	Ltrain: 0.003066 	Lval: 0.003268
Epoch: 115 	Ltrain: 0.003191 	Lval: 0.003250
Epoch: 120 	Ltrain: 0.003100 	Lval: 0.003235
Epoch: 125 	Ltrain: 0.003011 	Lval: 0.003227
Epoch: 130 	Ltrain: 0.003214 	Lval: 0.003248
Epoch: 135 	Ltrain: 0.003114 	Lval: 0.003211
Epoch 00137: reducing learning rate of group 0 to 7.3093e-06.
Epoch: 140 	Ltrain: 0.003020 	Lval: 0.003209
Epoch: 145 	Ltrain: 0.003133 	Lval: 0.003208
EarlyStopper: stopping at epoch 144 with best_val_loss = 0.003213


	Fold 2/5
Epoch: 1 	Ltrain: 0.236098 	Lval: 0.023664
Epoch: 5 	Ltrain: 0.010885 	Lval: 0.011396
Epoch: 10 	Ltrain: 0.008145 	Lval: 0.007074
Epoch: 15 	Ltrain: 0.005299 	Lval: 0.005343
Epoch: 20 	Ltrain: 0.004636 	Lval: 0.005003
Epoch 00025: reducing learning rate of group 0 to 7.3093e-04.
Epoch: 25 	Ltrain: 0.004212 	Lval: 0.004778
Epoch: 30 	Ltrain: 0.003857 	Lval: 0.004426
Epoch: 35 	Ltrain: 0.003806 	Lval: 0.004397
Epoch 00038: reducing learning rate of group 0 to 7.3093e-05.
Epoch: 40 	Ltrain: 0.003767 	Lval: 0.004382
Epoch: 45 	Ltrain: 0.003822 	Lval: 0.004378
Epoch: 50 	Ltrain: 0.003764 	Lval: 0.004372
Epoch: 55 	Ltrain: 0.003776 	Lval: 0.004371
Epoch 00058: reducing learning rate of group 0 to 7.3093e-06.
Epoch: 60 	Ltrain: 0.003781 	Lval: 0.004363
Epoch: 65 	Ltrain: 0.003761 	Lval: 0.004363
EarlyStopper: stopping at epoch 66 with best_val_loss = 0.004363


	Fold 3/5
Epoch: 1 	Ltrain: 0.093344 	Lval: 0.009114
Epoch: 5 	Ltrain: 0.004469 	Lval: 0.004733
Epoch: 10 	Ltrain: 0.004129 	Lval: 0.004667
Epoch: 15 	Ltrain: 0.003968 	Lval: 0.004201
Epoch: 20 	Ltrain: 0.003367 	Lval: 0.004044
Epoch: 25 	Ltrain: 0.002911 	Lval: 0.003538
Epoch: 30 	Ltrain: 0.002746 	Lval: 0.003317
Epoch: 35 	Ltrain: 0.002436 	Lval: 0.002872
Epoch 00040: reducing learning rate of group 0 to 7.3093e-04.
Epoch: 40 	Ltrain: 0.002415 	Lval: 0.003182
Epoch: 45 	Ltrain: 0.001816 	Lval: 0.002355
Epoch: 50 	Ltrain: 0.001743 	Lval: 0.002223
Epoch: 55 	Ltrain: 0.001689 	Lval: 0.002133
Epoch: 60 	Ltrain: 0.001611 	Lval: 0.002055
Epoch: 65 	Ltrain: 0.001567 	Lval: 0.001952
Epoch: 70 	Ltrain: 0.001528 	Lval: 0.001884
Epoch: 75 	Ltrain: 0.001448 	Lval: 0.001773
Epoch: 80 	Ltrain: 0.001403 	Lval: 0.001693
Epoch: 85 	Ltrain: 0.001339 	Lval: 0.001602
Epoch: 90 	Ltrain: 0.001267 	Lval: 0.001517
Epoch: 95 	Ltrain: 0.001232 	Lval: 0.001403
Epoch: 100 	Ltrain: 0.001177 	Lval: 0.001331
Epoch: 105 	Ltrain: 0.001085 	Lval: 0.001217
Epoch: 110 	Ltrain: 0.001045 	Lval: 0.001134
Epoch: 115 	Ltrain: 0.001005 	Lval: 0.001054
Epoch: 120 	Ltrain: 0.000919 	Lval: 0.000981
Epoch: 125 	Ltrain: 0.000860 	Lval: 0.000879
Epoch: 130 	Ltrain: 0.000851 	Lval: 0.000862
Epoch: 135 	Ltrain: 0.000766 	Lval: 0.000773
Epoch: 140 	Ltrain: 0.000729 	Lval: 0.000712
Epoch: 145 	Ltrain: 0.000709 	Lval: 0.000708
Epoch: 150 	Ltrain: 0.000657 	Lval: 0.000642
Epoch: 155 	Ltrain: 0.000626 	Lval: 0.000614
Epoch: 160 	Ltrain: 0.000618 	Lval: 0.000591
Epoch: 165 	Ltrain: 0.000708 	Lval: 0.000654
Epoch 00166: reducing learning rate of group 0 to 7.3093e-05.
Epoch: 170 	Ltrain: 0.000487 	Lval: 0.000490
Epoch: 175 	Ltrain: 0.000472 	Lval: 0.000474
Epoch: 180 	Ltrain: 0.000464 	Lval: 0.000465
Epoch: 185 	Ltrain: 0.000456 	Lval: 0.000458
Epoch: 190 	Ltrain: 0.000450 	Lval: 0.000453
Epoch: 195 	Ltrain: 0.000446 	Lval: 0.000447
Epoch: 200 	Ltrain: 0.000441 	Lval: 0.000441
Epoch: 205 	Ltrain: 0.000433 	Lval: 0.000436
Epoch: 210 	Ltrain: 0.000428 	Lval: 0.000430
Epoch: 215 	Ltrain: 0.000422 	Lval: 0.000425
Epoch: 220 	Ltrain: 0.000418 	Lval: 0.000421
Epoch: 225 	Ltrain: 0.000412 	Lval: 0.000414
Epoch: 230 	Ltrain: 0.000410 	Lval: 0.000409
Epoch: 235 	Ltrain: 0.000403 	Lval: 0.000403
Epoch: 240 	Ltrain: 0.000397 	Lval: 0.000397
Epoch: 245 	Ltrain: 0.000390 	Lval: 0.000393
Epoch: 250 	Ltrain: 0.000383 	Lval: 0.000385
Epoch: 255 	Ltrain: 0.000379 	Lval: 0.000378
Epoch: 260 	Ltrain: 0.000372 	Lval: 0.000372
Epoch: 265 	Ltrain: 0.000363 	Lval: 0.000365
Epoch: 270 	Ltrain: 0.000359 	Lval: 0.000359
Epoch: 275 	Ltrain: 0.000354 	Lval: 0.000353
Epoch: 280 	Ltrain: 0.000346 	Lval: 0.000346
Epoch: 285 	Ltrain: 0.000339 	Lval: 0.000338
Epoch: 290 	Ltrain: 0.000335 	Lval: 0.000332
Epoch: 295 	Ltrain: 0.000329 	Lval: 0.000323
Epoch: 300 	Ltrain: 0.000322 	Lval: 0.000318
Epoch: 305 	Ltrain: 0.000311 	Lval: 0.000310
Epoch: 310 	Ltrain: 0.000307 	Lval: 0.000304
Epoch: 315 	Ltrain: 0.000297 	Lval: 0.000295
Epoch: 320 	Ltrain: 0.000295 	Lval: 0.000290
Epoch: 325 	Ltrain: 0.000288 	Lval: 0.000284
Epoch: 330 	Ltrain: 0.000285 	Lval: 0.000276
Epoch: 335 	Ltrain: 0.000274 	Lval: 0.000270
Epoch: 340 	Ltrain: 0.000271 	Lval: 0.000266
Epoch: 345 	Ltrain: 0.000262 	Lval: 0.000260
Epoch: 350 	Ltrain: 0.000261 	Lval: 0.000253
Epoch: 355 	Ltrain: 0.000248 	Lval: 0.000242
Epoch: 360 	Ltrain: 0.000248 	Lval: 0.000235
Epoch: 365 	Ltrain: 0.000239 	Lval: 0.000231
Epoch: 370 	Ltrain: 0.000236 	Lval: 0.000230
Epoch: 375 	Ltrain: 0.000233 	Lval: 0.000232
Epoch 00376: reducing learning rate of group 0 to 7.3093e-06.
Epoch: 380 	Ltrain: 0.000220 	Lval: 0.000213
Epoch: 385 	Ltrain: 0.000218 	Lval: 0.000212
Epoch: 390 	Ltrain: 0.000217 	Lval: 0.000211
EarlyStopper: stopping at epoch 389 with best_val_loss = 0.000216


	Fold 4/5
Epoch: 1 	Ltrain: 0.036791 	Lval: 0.007497
Epoch: 5 	Ltrain: 0.003843 	Lval: 0.004452
Epoch: 10 	Ltrain: 0.003278 	Lval: 0.004139
Epoch: 15 	Ltrain: 0.003022 	Lval: 0.006391
Epoch: 20 	Ltrain: 0.002711 	Lval: 0.003305
Epoch: 25 	Ltrain: 0.002391 	Lval: 0.003595
Epoch: 30 	Ltrain: 0.002219 	Lval: 0.003161
Epoch 00035: reducing learning rate of group 0 to 7.3093e-04.
Epoch: 35 	Ltrain: 0.002201 	Lval: 0.002564
Epoch: 40 	Ltrain: 0.001538 	Lval: 0.001970
Epoch: 45 	Ltrain: 0.001460 	Lval: 0.001837
Epoch: 50 	Ltrain: 0.001387 	Lval: 0.001700
Epoch: 55 	Ltrain: 0.001319 	Lval: 0.001611
Epoch: 60 	Ltrain: 0.001254 	Lval: 0.001542
Epoch: 65 	Ltrain: 0.001166 	Lval: 0.001347
Epoch: 70 	Ltrain: 0.001091 	Lval: 0.001234
Epoch: 75 	Ltrain: 0.001003 	Lval: 0.001110
Epoch: 80 	Ltrain: 0.000924 	Lval: 0.001008
Epoch: 85 	Ltrain: 0.000826 	Lval: 0.000874
Epoch: 90 	Ltrain: 0.000762 	Lval: 0.000760
Epoch: 95 	Ltrain: 0.000681 	Lval: 0.000694
Epoch: 100 	Ltrain: 0.000640 	Lval: 0.000635
Epoch: 105 	Ltrain: 0.000578 	Lval: 0.000577
Epoch: 110 	Ltrain: 0.000544 	Lval: 0.000531
Epoch: 115 	Ltrain: 0.000517 	Lval: 0.000626
Epoch 00118: reducing learning rate of group 0 to 7.3093e-05.
Epoch: 120 	Ltrain: 0.000415 	Lval: 0.000415
Epoch: 125 	Ltrain: 0.000390 	Lval: 0.000394
Epoch: 130 	Ltrain: 0.000381 	Lval: 0.000384
Epoch: 135 	Ltrain: 0.000376 	Lval: 0.000376
Epoch: 140 	Ltrain: 0.000368 	Lval: 0.000370
Epoch: 145 	Ltrain: 0.000361 	Lval: 0.000364
Epoch: 150 	Ltrain: 0.000356 	Lval: 0.000357
Epoch: 155 	Ltrain: 0.000349 	Lval: 0.000350
Epoch: 160 	Ltrain: 0.000344 	Lval: 0.000344
Epoch: 165 	Ltrain: 0.000337 	Lval: 0.000338
Epoch: 170 	Ltrain: 0.000331 	Lval: 0.000331
Epoch: 175 	Ltrain: 0.000324 	Lval: 0.000325
Epoch: 180 	Ltrain: 0.000317 	Lval: 0.000319
Epoch: 185 	Ltrain: 0.000310 	Lval: 0.000312
Epoch: 190 	Ltrain: 0.000302 	Lval: 0.000304
Epoch: 195 	Ltrain: 0.000296 	Lval: 0.000297
Epoch: 200 	Ltrain: 0.000289 	Lval: 0.000291
Epoch: 205 	Ltrain: 0.000281 	Lval: 0.000283
Epoch: 210 	Ltrain: 0.000274 	Lval: 0.000277
Epoch: 215 	Ltrain: 0.000268 	Lval: 0.000270
Epoch: 220 	Ltrain: 0.000260 	Lval: 0.000262
Epoch: 225 	Ltrain: 0.000253 	Lval: 0.000255
Epoch: 230 	Ltrain: 0.000246 	Lval: 0.000248
Epoch: 235 	Ltrain: 0.000239 	Lval: 0.000241
Epoch: 240 	Ltrain: 0.000232 	Lval: 0.000235
Epoch: 245 	Ltrain: 0.000226 	Lval: 0.000228
Epoch: 250 	Ltrain: 0.000219 	Lval: 0.000223
Epoch: 255 	Ltrain: 0.000212 	Lval: 0.000213
Epoch: 260 	Ltrain: 0.000206 	Lval: 0.000207
Epoch: 265 	Ltrain: 0.000202 	Lval: 0.000204
Epoch: 270 	Ltrain: 0.000194 	Lval: 0.000196
Epoch: 275 	Ltrain: 0.000192 	Lval: 0.000192
Epoch: 280 	Ltrain: 0.000185 	Lval: 0.000186
Epoch: 285 	Ltrain: 0.000179 	Lval: 0.000182
Epoch: 290 	Ltrain: 0.000175 	Lval: 0.000175
Epoch: 295 	Ltrain: 0.000170 	Lval: 0.000171
Epoch: 300 	Ltrain: 0.000165 	Lval: 0.000167
Epoch: 305 	Ltrain: 0.000161 	Lval: 0.000162
Epoch 00310: reducing learning rate of group 0 to 7.3093e-06.
Epoch: 310 	Ltrain: 0.000163 	Lval: 0.000169
Epoch: 315 	Ltrain: 0.000148 	Lval: 0.000149
EarlyStopper: stopping at epoch 318 with best_val_loss = 0.000158


	Fold 5/5
Epoch: 1 	Ltrain: 0.105236 	Lval: 0.011849
Epoch: 5 	Ltrain: 0.004804 	Lval: 0.005342
Epoch: 10 	Ltrain: 0.004119 	Lval: 0.004800
Epoch 00014: reducing learning rate of group 0 to 7.3093e-04.
Epoch: 15 	Ltrain: 0.003570 	Lval: 0.004608
Epoch: 20 	Ltrain: 0.003406 	Lval: 0.004543
Epoch: 25 	Ltrain: 0.003362 	Lval: 0.004456
Epoch: 30 	Ltrain: 0.003252 	Lval: 0.004330
Epoch: 35 	Ltrain: 0.003149 	Lval: 0.004095
Epoch: 40 	Ltrain: 0.003025 	Lval: 0.003971
Epoch: 45 	Ltrain: 0.003020 	Lval: 0.003798
Epoch: 50 	Ltrain: 0.002890 	Lval: 0.003809
Epoch: 55 	Ltrain: 0.002814 	Lval: 0.003753
Epoch: 60 	Ltrain: 0.002642 	Lval: 0.003502
Epoch 00065: reducing learning rate of group 0 to 7.3093e-05.
Epoch: 65 	Ltrain: 0.002700 	Lval: 0.003544
Epoch: 70 	Ltrain: 0.002481 	Lval: 0.003379
Epoch: 75 	Ltrain: 0.002468 	Lval: 0.003393
Epoch 00077: reducing learning rate of group 0 to 7.3093e-06.
Epoch: 80 	Ltrain: 0.002448 	Lval: 0.003352
Epoch: 85 	Ltrain: 0.002441 	Lval: 0.003345
Epoch 00089: reducing learning rate of group 0 to 7.3093e-07.
Epoch: 90 	Ltrain: 0.002448 	Lval: 0.003349
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.003349

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006684626591266993
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.334835160500724e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 12
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.086277 	Lval: 0.013648
Epoch: 5 	Ltrain: 0.009269 	Lval: 0.009667
Epoch: 10 	Ltrain: 0.005998 	Lval: 0.005447
Epoch: 15 	Ltrain: 0.005570 	Lval: 0.004824
Epoch: 20 	Ltrain: 0.004951 	Lval: 0.004575
Epoch: 25 	Ltrain: 0.004614 	Lval: 0.004366
Epoch 00027: reducing learning rate of group 0 to 6.6846e-04.
Epoch: 30 	Ltrain: 0.003903 	Lval: 0.003771
Epoch: 35 	Ltrain: 0.003554 	Lval: 0.003607
Epoch: 40 	Ltrain: 0.003400 	Lval: 0.003542
Epoch: 45 	Ltrain: 0.003772 	Lval: 0.003424
Epoch: 50 	Ltrain: 0.003392 	Lval: 0.003350
Epoch: 55 	Ltrain: 0.003302 	Lval: 0.003273
Epoch: 60 	Ltrain: 0.003143 	Lval: 0.003160
Epoch: 65 	Ltrain: 0.003309 	Lval: 0.003176
Epoch: 70 	Ltrain: 0.002957 	Lval: 0.002981
Epoch: 75 	Ltrain: 0.002894 	Lval: 0.002864
Epoch: 80 	Ltrain: 0.002700 	Lval: 0.002739
Epoch 00084: reducing learning rate of group 0 to 6.6846e-05.
Epoch: 85 	Ltrain: 0.002967 	Lval: 0.002703
Epoch: 90 	Ltrain: 0.002698 	Lval: 0.002638
Epoch: 95 	Ltrain: 0.002591 	Lval: 0.002630
Epoch: 100 	Ltrain: 0.002722 	Lval: 0.002609
Epoch: 105 	Ltrain: 0.002656 	Lval: 0.002595
Epoch: 110 	Ltrain: 0.002576 	Lval: 0.002586
Epoch: 115 	Ltrain: 0.002523 	Lval: 0.002575
Epoch: 120 	Ltrain: 0.002587 	Lval: 0.002568
Epoch: 125 	Ltrain: 0.002685 	Lval: 0.002547
Epoch: 130 	Ltrain: 0.002532 	Lval: 0.002539
Epoch: 135 	Ltrain: 0.002533 	Lval: 0.002527
Epoch: 140 	Ltrain: 0.002552 	Lval: 0.002525
Epoch: 145 	Ltrain: 0.002468 	Lval: 0.002500
Epoch: 150 	Ltrain: 0.002534 	Lval: 0.002484
Epoch: 155 	Ltrain: 0.002442 	Lval: 0.002472
Epoch: 160 	Ltrain: 0.002540 	Lval: 0.002488
Epoch: 165 	Ltrain: 0.002438 	Lval: 0.002453
Epoch: 170 	Ltrain: 0.002453 	Lval: 0.002441
Epoch: 175 	Ltrain: 0.002374 	Lval: 0.002419
Epoch: 180 	Ltrain: 0.002408 	Lval: 0.002456
Epoch: 185 	Ltrain: 0.002444 	Lval: 0.002405
Epoch 00188: reducing learning rate of group 0 to 6.6846e-06.
Epoch: 190 	Ltrain: 0.002390 	Lval: 0.002386
Epoch: 195 	Ltrain: 0.002492 	Lval: 0.002382
Epoch: 200 	Ltrain: 0.002451 	Lval: 0.002382
Epoch 00201: reducing learning rate of group 0 to 6.6846e-07.
EarlyStopper: stopping at epoch 200 with best_val_loss = 0.002390


	Fold 2/5
Epoch: 1 	Ltrain: 0.074254 	Lval: 0.010074
Epoch: 5 	Ltrain: 0.004874 	Lval: 0.004719
Epoch 00009: reducing learning rate of group 0 to 6.6846e-04.
Epoch: 10 	Ltrain: 0.004292 	Lval: 0.004596
Epoch: 15 	Ltrain: 0.003877 	Lval: 0.004493
Epoch: 20 	Ltrain: 0.003730 	Lval: 0.004475
Epoch: 25 	Ltrain: 0.003706 	Lval: 0.004402
Epoch: 30 	Ltrain: 0.003718 	Lval: 0.004321
Epoch: 35 	Ltrain: 0.003445 	Lval: 0.004129
Epoch: 40 	Ltrain: 0.003347 	Lval: 0.003917
Epoch: 45 	Ltrain: 0.003263 	Lval: 0.003730
Epoch: 50 	Ltrain: 0.003198 	Lval: 0.003670
Epoch: 55 	Ltrain: 0.003178 	Lval: 0.003775
Epoch: 60 	Ltrain: 0.003040 	Lval: 0.003586
Epoch 00061: reducing learning rate of group 0 to 6.6846e-05.
Epoch: 65 	Ltrain: 0.002845 	Lval: 0.003418
Epoch: 70 	Ltrain: 0.002834 	Lval: 0.003402
Epoch: 75 	Ltrain: 0.002787 	Lval: 0.003386
Epoch: 80 	Ltrain: 0.002786 	Lval: 0.003369
Epoch: 85 	Ltrain: 0.002779 	Lval: 0.003366
Epoch: 90 	Ltrain: 0.002773 	Lval: 0.003347
Epoch: 95 	Ltrain: 0.002762 	Lval: 0.003344
Epoch: 100 	Ltrain: 0.002774 	Lval: 0.003329
Epoch: 105 	Ltrain: 0.002740 	Lval: 0.003302
Epoch: 110 	Ltrain: 0.002722 	Lval: 0.003291
Epoch 00113: reducing learning rate of group 0 to 6.6846e-06.
Epoch: 115 	Ltrain: 0.002796 	Lval: 0.003277
Epoch: 120 	Ltrain: 0.002696 	Lval: 0.003277
EarlyStopper: stopping at epoch 120 with best_val_loss = 0.003281


	Fold 3/5
Epoch: 1 	Ltrain: 0.034397 	Lval: 0.009879
Epoch: 5 	Ltrain: 0.004868 	Lval: 0.005245
Epoch: 10 	Ltrain: 0.004084 	Lval: 0.005069
Epoch: 15 	Ltrain: 0.003158 	Lval: 0.003649
Epoch 00019: reducing learning rate of group 0 to 6.6846e-04.
Epoch: 20 	Ltrain: 0.003042 	Lval: 0.003537
Epoch: 25 	Ltrain: 0.002565 	Lval: 0.003418
Epoch: 30 	Ltrain: 0.002488 	Lval: 0.003198
Epoch: 35 	Ltrain: 0.002450 	Lval: 0.003069
Epoch: 40 	Ltrain: 0.002380 	Lval: 0.003004
Epoch: 45 	Ltrain: 0.002314 	Lval: 0.002910
Epoch: 50 	Ltrain: 0.002199 	Lval: 0.002884
Epoch: 55 	Ltrain: 0.002165 	Lval: 0.002763
Epoch 00060: reducing learning rate of group 0 to 6.6846e-05.
Epoch: 60 	Ltrain: 0.002161 	Lval: 0.002741
Epoch: 65 	Ltrain: 0.002004 	Lval: 0.002619
Epoch: 70 	Ltrain: 0.002006 	Lval: 0.002616
Epoch: 75 	Ltrain: 0.002012 	Lval: 0.002607
Epoch: 80 	Ltrain: 0.001986 	Lval: 0.002590
Epoch: 85 	Ltrain: 0.001963 	Lval: 0.002576
Epoch: 90 	Ltrain: 0.001966 	Lval: 0.002590
Epoch: 95 	Ltrain: 0.001955 	Lval: 0.002556
Epoch: 100 	Ltrain: 0.001939 	Lval: 0.002508
Epoch 00104: reducing learning rate of group 0 to 6.6846e-06.
Epoch: 105 	Ltrain: 0.001923 	Lval: 0.002510
Epoch: 110 	Ltrain: 0.001920 	Lval: 0.002512
EarlyStopper: stopping at epoch 111 with best_val_loss = 0.002508


	Fold 4/5
Epoch: 1 	Ltrain: 0.021588 	Lval: 0.006737
Epoch: 5 	Ltrain: 0.004302 	Lval: 0.004943
Epoch: 10 	Ltrain: 0.003657 	Lval: 0.004254
Epoch: 15 	Ltrain: 0.002927 	Lval: 0.004020
Epoch: 20 	Ltrain: 0.002627 	Lval: 0.003131
Epoch: 25 	Ltrain: 0.002348 	Lval: 0.002968
Epoch: 30 	Ltrain: 0.002491 	Lval: 0.003003
Epoch 00031: reducing learning rate of group 0 to 6.6846e-04.
Epoch: 35 	Ltrain: 0.001751 	Lval: 0.002260
Epoch: 40 	Ltrain: 0.001660 	Lval: 0.002121
Epoch: 45 	Ltrain: 0.001596 	Lval: 0.002037
Epoch: 50 	Ltrain: 0.001517 	Lval: 0.001987
Epoch: 55 	Ltrain: 0.001442 	Lval: 0.001824
Epoch: 60 	Ltrain: 0.001359 	Lval: 0.001660
Epoch: 65 	Ltrain: 0.001253 	Lval: 0.001509
Epoch: 70 	Ltrain: 0.001187 	Lval: 0.001399
Epoch: 75 	Ltrain: 0.001094 	Lval: 0.001251
Epoch: 80 	Ltrain: 0.001018 	Lval: 0.001128
Epoch: 85 	Ltrain: 0.000932 	Lval: 0.001033
Epoch: 90 	Ltrain: 0.000863 	Lval: 0.000928
Epoch: 95 	Ltrain: 0.000822 	Lval: 0.000830
Epoch: 100 	Ltrain: 0.000751 	Lval: 0.000781
Epoch: 105 	Ltrain: 0.000674 	Lval: 0.000676
Epoch: 110 	Ltrain: 0.000651 	Lval: 0.000660
Epoch: 115 	Ltrain: 0.000582 	Lval: 0.000611
Epoch: 120 	Ltrain: 0.000556 	Lval: 0.000580
Epoch: 125 	Ltrain: 0.000501 	Lval: 0.000472
Epoch: 130 	Ltrain: 0.000491 	Lval: 0.000463
Epoch 00133: reducing learning rate of group 0 to 6.6846e-05.
Epoch: 135 	Ltrain: 0.000365 	Lval: 0.000364
Epoch: 140 	Ltrain: 0.000342 	Lval: 0.000342
Epoch: 145 	Ltrain: 0.000334 	Lval: 0.000334
Epoch: 150 	Ltrain: 0.000326 	Lval: 0.000327
Epoch: 155 	Ltrain: 0.000321 	Lval: 0.000323
Epoch: 160 	Ltrain: 0.000314 	Lval: 0.000315
Epoch: 165 	Ltrain: 0.000309 	Lval: 0.000310
Epoch: 170 	Ltrain: 0.000304 	Lval: 0.000305
Epoch: 175 	Ltrain: 0.000298 	Lval: 0.000299
Epoch: 180 	Ltrain: 0.000292 	Lval: 0.000293
Epoch: 185 	Ltrain: 0.000285 	Lval: 0.000287
Epoch: 190 	Ltrain: 0.000280 	Lval: 0.000281
Epoch: 195 	Ltrain: 0.000274 	Lval: 0.000275
Epoch: 200 	Ltrain: 0.000267 	Lval: 0.000269
Epoch: 205 	Ltrain: 0.000261 	Lval: 0.000264
Epoch: 210 	Ltrain: 0.000254 	Lval: 0.000256
Epoch: 215 	Ltrain: 0.000248 	Lval: 0.000251
Epoch: 220 	Ltrain: 0.000241 	Lval: 0.000244
Epoch: 225 	Ltrain: 0.000235 	Lval: 0.000238
Epoch: 230 	Ltrain: 0.000230 	Lval: 0.000232
Epoch: 235 	Ltrain: 0.000224 	Lval: 0.000226
Epoch: 240 	Ltrain: 0.000217 	Lval: 0.000218
Epoch: 245 	Ltrain: 0.000211 	Lval: 0.000214
Epoch: 250 	Ltrain: 0.000206 	Lval: 0.000207
Epoch: 255 	Ltrain: 0.000201 	Lval: 0.000202
Epoch: 260 	Ltrain: 0.000195 	Lval: 0.000196
Epoch: 265 	Ltrain: 0.000189 	Lval: 0.000190
Epoch: 270 	Ltrain: 0.000185 	Lval: 0.000187
Epoch: 275 	Ltrain: 0.000179 	Lval: 0.000180
Epoch: 280 	Ltrain: 0.000173 	Lval: 0.000175
Epoch: 285 	Ltrain: 0.000169 	Lval: 0.000172
Epoch: 290 	Ltrain: 0.000165 	Lval: 0.000166
Epoch: 295 	Ltrain: 0.000160 	Lval: 0.000160
Epoch: 300 	Ltrain: 0.000156 	Lval: 0.000156
EarlyStopper: stopping at epoch 300 with best_val_loss = 0.000166


	Fold 5/5
Epoch: 1 	Ltrain: 0.031604 	Lval: 0.010956
Epoch: 5 	Ltrain: 0.004603 	Lval: 0.005896
Epoch: 10 	Ltrain: 0.003601 	Lval: 0.004357
Epoch: 15 	Ltrain: 0.003126 	Lval: 0.004060
Epoch: 20 	Ltrain: 0.002876 	Lval: 0.003892
Epoch 00025: reducing learning rate of group 0 to 6.6846e-04.
Epoch: 25 	Ltrain: 0.002486 	Lval: 0.003488
Epoch: 30 	Ltrain: 0.002061 	Lval: 0.002754
Epoch: 35 	Ltrain: 0.001991 	Lval: 0.002670
Epoch: 40 	Ltrain: 0.001936 	Lval: 0.002603
Epoch: 45 	Ltrain: 0.001894 	Lval: 0.002546
Epoch: 50 	Ltrain: 0.001851 	Lval: 0.002470
Epoch: 55 	Ltrain: 0.001764 	Lval: 0.002304
Epoch: 60 	Ltrain: 0.001697 	Lval: 0.002263
Epoch: 65 	Ltrain: 0.001642 	Lval: 0.002037
Epoch: 70 	Ltrain: 0.001558 	Lval: 0.001850
Epoch: 75 	Ltrain: 0.001473 	Lval: 0.001712
Epoch: 80 	Ltrain: 0.001418 	Lval: 0.001707
Epoch: 85 	Ltrain: 0.001335 	Lval: 0.001482
Epoch: 90 	Ltrain: 0.001245 	Lval: 0.001352
Epoch: 95 	Ltrain: 0.001120 	Lval: 0.001216
Epoch: 100 	Ltrain: 0.001059 	Lval: 0.001146
Epoch 00103: reducing learning rate of group 0 to 6.6846e-05.
Epoch: 105 	Ltrain: 0.000922 	Lval: 0.000990
Epoch: 110 	Ltrain: 0.000886 	Lval: 0.000954
Epoch: 115 	Ltrain: 0.000868 	Lval: 0.000932
Epoch: 120 	Ltrain: 0.000857 	Lval: 0.000921
Epoch: 125 	Ltrain: 0.000846 	Lval: 0.000909
Epoch: 130 	Ltrain: 0.000835 	Lval: 0.000896
Epoch: 135 	Ltrain: 0.000825 	Lval: 0.000877
Epoch: 140 	Ltrain: 0.000812 	Lval: 0.000867
Epoch: 145 	Ltrain: 0.000798 	Lval: 0.000858
Epoch: 150 	Ltrain: 0.000788 	Lval: 0.000836
Epoch: 155 	Ltrain: 0.000779 	Lval: 0.000823
Epoch: 160 	Ltrain: 0.000765 	Lval: 0.000807
Epoch: 165 	Ltrain: 0.000754 	Lval: 0.000795
Epoch: 170 	Ltrain: 0.000739 	Lval: 0.000784
Epoch: 175 	Ltrain: 0.000730 	Lval: 0.000764
Epoch: 180 	Ltrain: 0.000717 	Lval: 0.000749
Epoch: 185 	Ltrain: 0.000704 	Lval: 0.000735
Epoch: 190 	Ltrain: 0.000694 	Lval: 0.000726
Epoch: 195 	Ltrain: 0.000679 	Lval: 0.000711
Epoch: 200 	Ltrain: 0.000667 	Lval: 0.000700
Epoch: 205 	Ltrain: 0.000658 	Lval: 0.000685
Epoch: 210 	Ltrain: 0.000643 	Lval: 0.000668
Epoch: 215 	Ltrain: 0.000632 	Lval: 0.000661
Epoch: 220 	Ltrain: 0.000616 	Lval: 0.000636
Epoch: 225 	Ltrain: 0.000607 	Lval: 0.000621
Epoch: 230 	Ltrain: 0.000594 	Lval: 0.000612
Epoch: 235 	Ltrain: 0.000580 	Lval: 0.000593
Epoch: 240 	Ltrain: 0.000572 	Lval: 0.000590
Epoch: 245 	Ltrain: 0.000556 	Lval: 0.000576
Epoch: 250 	Ltrain: 0.000544 	Lval: 0.000559
Epoch: 255 	Ltrain: 0.000532 	Lval: 0.000555
Epoch: 260 	Ltrain: 0.000520 	Lval: 0.000535
Epoch: 265 	Ltrain: 0.000512 	Lval: 0.000523
Epoch: 270 	Ltrain: 0.000498 	Lval: 0.000507
Epoch: 275 	Ltrain: 0.000486 	Lval: 0.000493
Epoch: 280 	Ltrain: 0.000477 	Lval: 0.000483
Epoch: 285 	Ltrain: 0.000464 	Lval: 0.000468
Epoch: 290 	Ltrain: 0.000454 	Lval: 0.000463
Epoch: 295 	Ltrain: 0.000447 	Lval: 0.000458
Epoch: 300 	Ltrain: 0.000435 	Lval: 0.000436
Epoch: 305 	Ltrain: 0.000423 	Lval: 0.000431
Epoch: 310 	Ltrain: 0.000413 	Lval: 0.000413
Epoch: 315 	Ltrain: 0.000404 	Lval: 0.000405
Epoch: 320 	Ltrain: 0.000398 	Lval: 0.000397
Epoch: 325 	Ltrain: 0.000391 	Lval: 0.000386
Epoch: 330 	Ltrain: 0.000378 	Lval: 0.000375
Epoch: 335 	Ltrain: 0.000369 	Lval: 0.000369
Epoch: 340 	Ltrain: 0.000362 	Lval: 0.000365
Epoch 00345: reducing learning rate of group 0 to 6.6846e-06.
Epoch: 345 	Ltrain: 0.000366 	Lval: 0.000366
Epoch: 350 	Ltrain: 0.000342 	Lval: 0.000344
Epoch: 355 	Ltrain: 0.000341 	Lval: 0.000343
EarlyStopper: stopping at epoch 357 with best_val_loss = 0.000349

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003694478109386515
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.223999424722098e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 13
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.029156 	Lval: 0.013225
Epoch: 5 	Ltrain: 0.007480 	Lval: 0.008218
Epoch: 10 	Ltrain: 0.007015 	Lval: 0.006009
Epoch: 15 	Ltrain: 0.005171 	Lval: 0.005026
Epoch: 20 	Ltrain: 0.004480 	Lval: 0.004377
Epoch 00024: reducing learning rate of group 0 to 3.6945e-04.
Epoch: 25 	Ltrain: 0.004624 	Lval: 0.004206
Epoch: 30 	Ltrain: 0.004150 	Lval: 0.003842
Epoch: 35 	Ltrain: 0.003932 	Lval: 0.003749
Epoch: 40 	Ltrain: 0.003523 	Lval: 0.003671
Epoch: 45 	Ltrain: 0.003463 	Lval: 0.003627
Epoch: 50 	Ltrain: 0.003821 	Lval: 0.003584
Epoch: 55 	Ltrain: 0.003397 	Lval: 0.003500
Epoch: 60 	Ltrain: 0.003220 	Lval: 0.003399
Epoch: 65 	Ltrain: 0.003252 	Lval: 0.003437
Epoch: 70 	Ltrain: 0.003210 	Lval: 0.003201
Epoch: 75 	Ltrain: 0.003219 	Lval: 0.003118
Epoch: 80 	Ltrain: 0.003215 	Lval: 0.003032
Epoch: 85 	Ltrain: 0.003067 	Lval: 0.003039
Epoch: 90 	Ltrain: 0.003196 	Lval: 0.002950
Epoch: 95 	Ltrain: 0.002912 	Lval: 0.002834
Epoch: 100 	Ltrain: 0.002650 	Lval: 0.002563
Epoch: 105 	Ltrain: 0.002645 	Lval: 0.002515
Epoch: 110 	Ltrain: 0.002469 	Lval: 0.002415
Epoch: 115 	Ltrain: 0.002680 	Lval: 0.002548
Epoch 00116: reducing learning rate of group 0 to 3.6945e-05.
Epoch: 120 	Ltrain: 0.002139 	Lval: 0.002222
Epoch: 125 	Ltrain: 0.002189 	Lval: 0.002184
Epoch: 130 	Ltrain: 0.002105 	Lval: 0.002167
Epoch: 135 	Ltrain: 0.002136 	Lval: 0.002156
Epoch: 140 	Ltrain: 0.002079 	Lval: 0.002145
Epoch: 145 	Ltrain: 0.002074 	Lval: 0.002125
Epoch: 150 	Ltrain: 0.002100 	Lval: 0.002112
Epoch: 155 	Ltrain: 0.002136 	Lval: 0.002097
Epoch: 160 	Ltrain: 0.002153 	Lval: 0.002100
Epoch: 165 	Ltrain: 0.002025 	Lval: 0.002077
Epoch: 170 	Ltrain: 0.002132 	Lval: 0.002065
Epoch: 175 	Ltrain: 0.002017 	Lval: 0.002047
Epoch: 180 	Ltrain: 0.001932 	Lval: 0.002034
Epoch: 185 	Ltrain: 0.001892 	Lval: 0.002018
Epoch: 190 	Ltrain: 0.001969 	Lval: 0.002012
Epoch: 195 	Ltrain: 0.001968 	Lval: 0.001991
Epoch: 200 	Ltrain: 0.001896 	Lval: 0.001978
Epoch: 205 	Ltrain: 0.001921 	Lval: 0.001976
Epoch: 210 	Ltrain: 0.001853 	Lval: 0.001952
Epoch: 215 	Ltrain: 0.001999 	Lval: 0.001934
Epoch: 220 	Ltrain: 0.002053 	Lval: 0.001937
Epoch: 225 	Ltrain: 0.001935 	Lval: 0.001907
Epoch: 230 	Ltrain: 0.001806 	Lval: 0.001894
Epoch: 235 	Ltrain: 0.001846 	Lval: 0.001879
Epoch: 240 	Ltrain: 0.001893 	Lval: 0.001870
Epoch: 245 	Ltrain: 0.001779 	Lval: 0.001852
Epoch: 250 	Ltrain: 0.001954 	Lval: 0.001847
Epoch: 255 	Ltrain: 0.001827 	Lval: 0.001832
Epoch: 260 	Ltrain: 0.001778 	Lval: 0.001808
Epoch: 265 	Ltrain: 0.001784 	Lval: 0.001796
Epoch: 270 	Ltrain: 0.001723 	Lval: 0.001779
Epoch: 275 	Ltrain: 0.001815 	Lval: 0.001769
Epoch: 280 	Ltrain: 0.001844 	Lval: 0.001754
Epoch: 285 	Ltrain: 0.001720 	Lval: 0.001739
Epoch: 290 	Ltrain: 0.001699 	Lval: 0.001727
Epoch: 295 	Ltrain: 0.001873 	Lval: 0.001713
Epoch: 300 	Ltrain: 0.001704 	Lval: 0.001700
Epoch: 305 	Ltrain: 0.001630 	Lval: 0.001696
Epoch: 310 	Ltrain: 0.001725 	Lval: 0.001676
Epoch: 315 	Ltrain: 0.001694 	Lval: 0.001665
Epoch: 320 	Ltrain: 0.001597 	Lval: 0.001658
Epoch: 325 	Ltrain: 0.001637 	Lval: 0.001631
Epoch: 330 	Ltrain: 0.001591 	Lval: 0.001621
Epoch: 335 	Ltrain: 0.001663 	Lval: 0.001606
Epoch: 340 	Ltrain: 0.001588 	Lval: 0.001596
Epoch: 345 	Ltrain: 0.001589 	Lval: 0.001583
Epoch: 350 	Ltrain: 0.001593 	Lval: 0.001577
Epoch: 355 	Ltrain: 0.001551 	Lval: 0.001562
Epoch: 360 	Ltrain: 0.001506 	Lval: 0.001543
Epoch: 365 	Ltrain: 0.001649 	Lval: 0.001534
Epoch: 370 	Ltrain: 0.001594 	Lval: 0.001526
Epoch: 375 	Ltrain: 0.001479 	Lval: 0.001502
Epoch: 380 	Ltrain: 0.001524 	Lval: 0.001490
Epoch: 385 	Ltrain: 0.001490 	Lval: 0.001478
Epoch: 390 	Ltrain: 0.001533 	Lval: 0.001464
Epoch: 395 	Ltrain: 0.001439 	Lval: 0.001452
Epoch: 400 	Ltrain: 0.001446 	Lval: 0.001443
Epoch: 405 	Ltrain: 0.001422 	Lval: 0.001430
Epoch: 410 	Ltrain: 0.001401 	Lval: 0.001421
Epoch: 415 	Ltrain: 0.001389 	Lval: 0.001405
Epoch: 420 	Ltrain: 0.001497 	Lval: 0.001394
Epoch: 425 	Ltrain: 0.001392 	Lval: 0.001385
Epoch: 430 	Ltrain: 0.001325 	Lval: 0.001371
Epoch: 435 	Ltrain: 0.001435 	Lval: 0.001363
Epoch: 440 	Ltrain: 0.001392 	Lval: 0.001348
Epoch: 445 	Ltrain: 0.001434 	Lval: 0.001337
Epoch: 450 	Ltrain: 0.001320 	Lval: 0.001326
Epoch: 455 	Ltrain: 0.001393 	Lval: 0.001318
Epoch: 460 	Ltrain: 0.001349 	Lval: 0.001312
Epoch: 465 	Ltrain: 0.001361 	Lval: 0.001294
Epoch: 470 	Ltrain: 0.001279 	Lval: 0.001286
Epoch: 475 	Ltrain: 0.001278 	Lval: 0.001269
Epoch: 480 	Ltrain: 0.001259 	Lval: 0.001273
Epoch: 485 	Ltrain: 0.001301 	Lval: 0.001254
Epoch: 490 	Ltrain: 0.001296 	Lval: 0.001241
Epoch: 495 	Ltrain: 0.001253 	Lval: 0.001234
Epoch: 500 	Ltrain: 0.001188 	Lval: 0.001219
Epoch: 505 	Ltrain: 0.001280 	Lval: 0.001211
Epoch: 510 	Ltrain: 0.001222 	Lval: 0.001202
Epoch: 515 	Ltrain: 0.001226 	Lval: 0.001189
Epoch: 520 	Ltrain: 0.001196 	Lval: 0.001187
Epoch: 525 	Ltrain: 0.001171 	Lval: 0.001171
Epoch: 530 	Ltrain: 0.001161 	Lval: 0.001159
Epoch: 535 	Ltrain: 0.001157 	Lval: 0.001158
Epoch: 540 	Ltrain: 0.001209 	Lval: 0.001146
Epoch: 545 	Ltrain: 0.001191 	Lval: 0.001132
Epoch: 550 	Ltrain: 0.001121 	Lval: 0.001123
Epoch: 555 	Ltrain: 0.001107 	Lval: 0.001113
Epoch: 560 	Ltrain: 0.001175 	Lval: 0.001106
Epoch: 565 	Ltrain: 0.001148 	Lval: 0.001101
Epoch: 570 	Ltrain: 0.001123 	Lval: 0.001091
Epoch: 575 	Ltrain: 0.001108 	Lval: 0.001079
Epoch: 580 	Ltrain: 0.001079 	Lval: 0.001069
Epoch: 585 	Ltrain: 0.001072 	Lval: 0.001065
Epoch: 590 	Ltrain: 0.001106 	Lval: 0.001057
Epoch: 595 	Ltrain: 0.001010 	Lval: 0.001042
Epoch: 600 	Ltrain: 0.001081 	Lval: 0.001043
Epoch 00602: reducing learning rate of group 0 to 3.6945e-06.
Epoch: 605 	Ltrain: 0.001039 	Lval: 0.001027
Epoch: 610 	Ltrain: 0.001033 	Lval: 0.001023
Epoch: 615 	Ltrain: 0.000979 	Lval: 0.001021
EarlyStopper: stopping at epoch 616 with best_val_loss = 0.001030


	Fold 2/5
Epoch: 1 	Ltrain: 0.020878 	Lval: 0.010981
Epoch: 5 	Ltrain: 0.006678 	Lval: 0.007395
Epoch: 10 	Ltrain: 0.004358 	Lval: 0.004753
Epoch: 15 	Ltrain: 0.004056 	Lval: 0.004470
Epoch: 20 	Ltrain: 0.003303 	Lval: 0.003600
Epoch 00025: reducing learning rate of group 0 to 3.6945e-04.
Epoch: 25 	Ltrain: 0.003063 	Lval: 0.003694
Epoch: 30 	Ltrain: 0.002615 	Lval: 0.003108
Epoch: 35 	Ltrain: 0.002520 	Lval: 0.002984
Epoch: 40 	Ltrain: 0.002499 	Lval: 0.002967
Epoch: 45 	Ltrain: 0.002414 	Lval: 0.002884
Epoch: 50 	Ltrain: 0.002369 	Lval: 0.002836
Epoch: 55 	Ltrain: 0.002344 	Lval: 0.002785
Epoch: 60 	Ltrain: 0.002295 	Lval: 0.002667
Epoch: 65 	Ltrain: 0.002233 	Lval: 0.002571
Epoch: 70 	Ltrain: 0.002142 	Lval: 0.002485
Epoch: 75 	Ltrain: 0.002145 	Lval: 0.002390
Epoch: 80 	Ltrain: 0.002084 	Lval: 0.002321
Epoch: 85 	Ltrain: 0.001963 	Lval: 0.002208
Epoch: 90 	Ltrain: 0.001841 	Lval: 0.002036
Epoch: 95 	Ltrain: 0.001751 	Lval: 0.001959
Epoch: 100 	Ltrain: 0.001626 	Lval: 0.001806
Epoch: 105 	Ltrain: 0.001549 	Lval: 0.001602
Epoch: 110 	Ltrain: 0.001438 	Lval: 0.001468
Epoch: 115 	Ltrain: 0.001336 	Lval: 0.001388
Epoch: 120 	Ltrain: 0.001172 	Lval: 0.001235
Epoch: 125 	Ltrain: 0.001112 	Lval: 0.001291
Epoch: 130 	Ltrain: 0.001012 	Lval: 0.001069
Epoch: 135 	Ltrain: 0.000964 	Lval: 0.001015
Epoch: 140 	Ltrain: 0.000868 	Lval: 0.000933
Epoch 00144: reducing learning rate of group 0 to 3.6945e-05.
Epoch: 145 	Ltrain: 0.000833 	Lval: 0.000853
Epoch: 150 	Ltrain: 0.000741 	Lval: 0.000815
Epoch: 155 	Ltrain: 0.000729 	Lval: 0.000805
Epoch: 160 	Ltrain: 0.000725 	Lval: 0.000796
Epoch: 165 	Ltrain: 0.000717 	Lval: 0.000788
Epoch: 170 	Ltrain: 0.000726 	Lval: 0.000779
Epoch: 175 	Ltrain: 0.000701 	Lval: 0.000770
Epoch: 180 	Ltrain: 0.000691 	Lval: 0.000763
Epoch: 185 	Ltrain: 0.000692 	Lval: 0.000754
Epoch: 190 	Ltrain: 0.000680 	Lval: 0.000747
Epoch: 195 	Ltrain: 0.000683 	Lval: 0.000736
Epoch: 200 	Ltrain: 0.000664 	Lval: 0.000729
Epoch: 205 	Ltrain: 0.000655 	Lval: 0.000720
Epoch: 210 	Ltrain: 0.000648 	Lval: 0.000713
Epoch: 215 	Ltrain: 0.000636 	Lval: 0.000705
Epoch: 220 	Ltrain: 0.000633 	Lval: 0.000696
Epoch: 225 	Ltrain: 0.000625 	Lval: 0.000687
Epoch: 230 	Ltrain: 0.000618 	Lval: 0.000678
Epoch: 235 	Ltrain: 0.000619 	Lval: 0.000671
Epoch: 240 	Ltrain: 0.000617 	Lval: 0.000664
Epoch: 245 	Ltrain: 0.000601 	Lval: 0.000655
Epoch: 250 	Ltrain: 0.000587 	Lval: 0.000646
Epoch: 255 	Ltrain: 0.000587 	Lval: 0.000638
Epoch: 260 	Ltrain: 0.000574 	Lval: 0.000630
Epoch: 265 	Ltrain: 0.000584 	Lval: 0.000622
Epoch: 270 	Ltrain: 0.000558 	Lval: 0.000612
Epoch: 275 	Ltrain: 0.000562 	Lval: 0.000606
Epoch: 280 	Ltrain: 0.000562 	Lval: 0.000597
Epoch: 285 	Ltrain: 0.000542 	Lval: 0.000590
Epoch: 290 	Ltrain: 0.000539 	Lval: 0.000581
Epoch: 295 	Ltrain: 0.000528 	Lval: 0.000572
Epoch: 300 	Ltrain: 0.000524 	Lval: 0.000564
Epoch: 305 	Ltrain: 0.000521 	Lval: 0.000559
Epoch: 310 	Ltrain: 0.000511 	Lval: 0.000550
Epoch: 315 	Ltrain: 0.000501 	Lval: 0.000544
Epoch: 320 	Ltrain: 0.000496 	Lval: 0.000536
Epoch: 325 	Ltrain: 0.000483 	Lval: 0.000528
Epoch: 330 	Ltrain: 0.000475 	Lval: 0.000521
Epoch: 335 	Ltrain: 0.000481 	Lval: 0.000514
Epoch: 340 	Ltrain: 0.000468 	Lval: 0.000509
Epoch: 345 	Ltrain: 0.000462 	Lval: 0.000501
Epoch: 350 	Ltrain: 0.000453 	Lval: 0.000494
Epoch: 355 	Ltrain: 0.000451 	Lval: 0.000490
Epoch: 360 	Ltrain: 0.000444 	Lval: 0.000480
Epoch: 365 	Ltrain: 0.000440 	Lval: 0.000474
Epoch: 370 	Ltrain: 0.000435 	Lval: 0.000470
Epoch: 375 	Ltrain: 0.000425 	Lval: 0.000461
Epoch: 380 	Ltrain: 0.000421 	Lval: 0.000458
Epoch: 385 	Ltrain: 0.000412 	Lval: 0.000451
Epoch: 390 	Ltrain: 0.000406 	Lval: 0.000448
Epoch: 395 	Ltrain: 0.000401 	Lval: 0.000438
Epoch: 400 	Ltrain: 0.000390 	Lval: 0.000432
Epoch: 405 	Ltrain: 0.000391 	Lval: 0.000425
Epoch: 410 	Ltrain: 0.000392 	Lval: 0.000420
Epoch: 415 	Ltrain: 0.000381 	Lval: 0.000414
Epoch: 420 	Ltrain: 0.000377 	Lval: 0.000407
Epoch: 425 	Ltrain: 0.000364 	Lval: 0.000403
Epoch: 430 	Ltrain: 0.000362 	Lval: 0.000395
Epoch: 435 	Ltrain: 0.000358 	Lval: 0.000391
Epoch: 440 	Ltrain: 0.000350 	Lval: 0.000384
Epoch: 445 	Ltrain: 0.000344 	Lval: 0.000378
Epoch: 450 	Ltrain: 0.000347 	Lval: 0.000373
Epoch: 455 	Ltrain: 0.000337 	Lval: 0.000367
Epoch: 460 	Ltrain: 0.000330 	Lval: 0.000363
Epoch: 465 	Ltrain: 0.000330 	Lval: 0.000358
Epoch: 470 	Ltrain: 0.000325 	Lval: 0.000354
Epoch: 475 	Ltrain: 0.000315 	Lval: 0.000345
Epoch: 480 	Ltrain: 0.000308 	Lval: 0.000341
Epoch: 485 	Ltrain: 0.000308 	Lval: 0.000336
Epoch: 490 	Ltrain: 0.000303 	Lval: 0.000331
Epoch: 495 	Ltrain: 0.000304 	Lval: 0.000326
Epoch: 500 	Ltrain: 0.000297 	Lval: 0.000322
Epoch: 505 	Ltrain: 0.000288 	Lval: 0.000316
Epoch: 510 	Ltrain: 0.000286 	Lval: 0.000312
Epoch: 515 	Ltrain: 0.000290 	Lval: 0.000306
Epoch: 520 	Ltrain: 0.000274 	Lval: 0.000301
Epoch: 525 	Ltrain: 0.000273 	Lval: 0.000297
Epoch: 530 	Ltrain: 0.000267 	Lval: 0.000293
EarlyStopper: stopping at epoch 532 with best_val_loss = 0.000301


	Fold 3/5
Epoch: 1 	Ltrain: 0.015473 	Lval: 0.009138
Epoch: 5 	Ltrain: 0.004826 	Lval: 0.004836
Epoch: 10 	Ltrain: 0.004050 	Lval: 0.004371
Epoch: 15 	Ltrain: 0.004240 	Lval: 0.004279
Epoch: 20 	Ltrain: 0.002935 	Lval: 0.003482
Epoch 00025: reducing learning rate of group 0 to 3.6945e-04.
Epoch: 25 	Ltrain: 0.003038 	Lval: 0.003608
Epoch: 30 	Ltrain: 0.002267 	Lval: 0.002872
Epoch: 35 	Ltrain: 0.002201 	Lval: 0.002817
Epoch: 40 	Ltrain: 0.002118 	Lval: 0.002696
Epoch: 45 	Ltrain: 0.002079 	Lval: 0.002591
Epoch: 50 	Ltrain: 0.002038 	Lval: 0.002473
Epoch: 55 	Ltrain: 0.001940 	Lval: 0.002415
Epoch: 60 	Ltrain: 0.001874 	Lval: 0.002324
Epoch: 65 	Ltrain: 0.001806 	Lval: 0.002234
Epoch: 70 	Ltrain: 0.001711 	Lval: 0.002124
Epoch: 75 	Ltrain: 0.001698 	Lval: 0.002030
Epoch: 80 	Ltrain: 0.001604 	Lval: 0.001981
Epoch: 85 	Ltrain: 0.001508 	Lval: 0.001722
Epoch: 90 	Ltrain: 0.001402 	Lval: 0.001547
Epoch: 95 	Ltrain: 0.001319 	Lval: 0.001443
Epoch: 100 	Ltrain: 0.001290 	Lval: 0.001459
Epoch: 105 	Ltrain: 0.001170 	Lval: 0.001253
Epoch: 110 	Ltrain: 0.001063 	Lval: 0.001129
Epoch: 115 	Ltrain: 0.001028 	Lval: 0.001035
Epoch: 120 	Ltrain: 0.000907 	Lval: 0.000956
Epoch: 125 	Ltrain: 0.000881 	Lval: 0.000915
Epoch 00127: reducing learning rate of group 0 to 3.6945e-05.
Epoch: 130 	Ltrain: 0.000714 	Lval: 0.000755
Epoch: 135 	Ltrain: 0.000697 	Lval: 0.000740
Epoch: 140 	Ltrain: 0.000686 	Lval: 0.000728
Epoch: 145 	Ltrain: 0.000681 	Lval: 0.000719
Epoch: 150 	Ltrain: 0.000672 	Lval: 0.000711
Epoch: 155 	Ltrain: 0.000662 	Lval: 0.000699
Epoch: 160 	Ltrain: 0.000655 	Lval: 0.000688
Epoch: 165 	Ltrain: 0.000646 	Lval: 0.000680
Epoch: 170 	Ltrain: 0.000640 	Lval: 0.000670
Epoch: 175 	Ltrain: 0.000628 	Lval: 0.000658
Epoch: 180 	Ltrain: 0.000620 	Lval: 0.000650
Epoch: 185 	Ltrain: 0.000611 	Lval: 0.000638
Epoch: 190 	Ltrain: 0.000602 	Lval: 0.000628
Epoch: 195 	Ltrain: 0.000591 	Lval: 0.000618
Epoch: 200 	Ltrain: 0.000587 	Lval: 0.000611
Epoch: 205 	Ltrain: 0.000576 	Lval: 0.000597
Epoch: 210 	Ltrain: 0.000568 	Lval: 0.000590
Epoch: 215 	Ltrain: 0.000562 	Lval: 0.000580
Epoch: 220 	Ltrain: 0.000549 	Lval: 0.000569
Epoch: 225 	Ltrain: 0.000542 	Lval: 0.000559
Epoch: 230 	Ltrain: 0.000533 	Lval: 0.000550
Epoch: 235 	Ltrain: 0.000527 	Lval: 0.000541
Epoch: 240 	Ltrain: 0.000518 	Lval: 0.000527
Epoch: 245 	Ltrain: 0.000511 	Lval: 0.000520
Epoch: 250 	Ltrain: 0.000499 	Lval: 0.000510
Epoch: 255 	Ltrain: 0.000493 	Lval: 0.000503
Epoch: 260 	Ltrain: 0.000484 	Lval: 0.000496
Epoch: 265 	Ltrain: 0.000475 	Lval: 0.000484
Epoch: 270 	Ltrain: 0.000469 	Lval: 0.000477
Epoch: 275 	Ltrain: 0.000460 	Lval: 0.000471
Epoch: 280 	Ltrain: 0.000456 	Lval: 0.000458
Epoch: 285 	Ltrain: 0.000447 	Lval: 0.000450
Epoch: 290 	Ltrain: 0.000439 	Lval: 0.000441
Epoch: 295 	Ltrain: 0.000433 	Lval: 0.000436
Epoch: 300 	Ltrain: 0.000423 	Lval: 0.000426
Epoch: 305 	Ltrain: 0.000419 	Lval: 0.000421
Epoch: 310 	Ltrain: 0.000413 	Lval: 0.000414
Epoch: 315 	Ltrain: 0.000407 	Lval: 0.000404
Epoch: 320 	Ltrain: 0.000398 	Lval: 0.000396
Epoch: 325 	Ltrain: 0.000395 	Lval: 0.000388
Epoch: 330 	Ltrain: 0.000388 	Lval: 0.000381
Epoch: 335 	Ltrain: 0.000377 	Lval: 0.000377
Epoch: 340 	Ltrain: 0.000374 	Lval: 0.000369
Epoch: 345 	Ltrain: 0.000367 	Lval: 0.000361
Epoch: 350 	Ltrain: 0.000359 	Lval: 0.000355
Epoch: 355 	Ltrain: 0.000354 	Lval: 0.000349
Epoch: 360 	Ltrain: 0.000349 	Lval: 0.000343
Epoch: 365 	Ltrain: 0.000342 	Lval: 0.000336
Epoch: 370 	Ltrain: 0.000334 	Lval: 0.000332
Epoch: 375 	Ltrain: 0.000331 	Lval: 0.000323
Epoch: 380 	Ltrain: 0.000323 	Lval: 0.000318
Epoch: 385 	Ltrain: 0.000321 	Lval: 0.000311
Epoch: 390 	Ltrain: 0.000314 	Lval: 0.000306
Epoch: 395 	Ltrain: 0.000308 	Lval: 0.000301
Epoch: 400 	Ltrain: 0.000303 	Lval: 0.000295
Epoch: 405 	Ltrain: 0.000299 	Lval: 0.000291
Epoch: 410 	Ltrain: 0.000291 	Lval: 0.000285
Epoch: 415 	Ltrain: 0.000288 	Lval: 0.000283
Epoch: 420 	Ltrain: 0.000282 	Lval: 0.000275
Epoch: 425 	Ltrain: 0.000277 	Lval: 0.000271
Epoch: 430 	Ltrain: 0.000271 	Lval: 0.000267
Epoch: 435 	Ltrain: 0.000265 	Lval: 0.000260
Epoch: 440 	Ltrain: 0.000263 	Lval: 0.000258
Epoch: 445 	Ltrain: 0.000258 	Lval: 0.000252
EarlyStopper: stopping at epoch 447 with best_val_loss = 0.000260


	Fold 4/5
Epoch: 1 	Ltrain: 0.013383 	Lval: 0.008484
Epoch: 5 	Ltrain: 0.004194 	Lval: 0.005324
Epoch: 10 	Ltrain: 0.003629 	Lval: 0.003920
Epoch: 15 	Ltrain: 0.002934 	Lval: 0.003488
Epoch: 20 	Ltrain: 0.002666 	Lval: 0.003814
Epoch: 25 	Ltrain: 0.002502 	Lval: 0.003097
Epoch: 30 	Ltrain: 0.002299 	Lval: 0.003131
Epoch: 35 	Ltrain: 0.002321 	Lval: 0.002536
Epoch: 40 	Ltrain: 0.001828 	Lval: 0.002195
Epoch 00044: reducing learning rate of group 0 to 3.6945e-04.
Epoch: 45 	Ltrain: 0.001758 	Lval: 0.001831
Epoch: 50 	Ltrain: 0.001286 	Lval: 0.001557
Epoch: 55 	Ltrain: 0.001171 	Lval: 0.001386
Epoch: 60 	Ltrain: 0.001077 	Lval: 0.001245
Epoch: 65 	Ltrain: 0.000988 	Lval: 0.001132
Epoch: 70 	Ltrain: 0.000916 	Lval: 0.001010
Epoch: 75 	Ltrain: 0.000826 	Lval: 0.000911
Epoch: 80 	Ltrain: 0.000747 	Lval: 0.000822
Epoch: 85 	Ltrain: 0.000688 	Lval: 0.000740
Epoch: 90 	Ltrain: 0.000623 	Lval: 0.000667
Epoch: 95 	Ltrain: 0.000571 	Lval: 0.000601
Epoch: 100 	Ltrain: 0.000510 	Lval: 0.000548
Epoch: 105 	Ltrain: 0.000481 	Lval: 0.000530
Epoch: 110 	Ltrain: 0.000461 	Lval: 0.000475
Epoch 00112: reducing learning rate of group 0 to 3.6945e-05.
Epoch: 115 	Ltrain: 0.000371 	Lval: 0.000387
Epoch: 120 	Ltrain: 0.000358 	Lval: 0.000374
Epoch: 125 	Ltrain: 0.000353 	Lval: 0.000367
Epoch: 130 	Ltrain: 0.000346 	Lval: 0.000360
Epoch: 135 	Ltrain: 0.000341 	Lval: 0.000353
Epoch: 140 	Ltrain: 0.000336 	Lval: 0.000347
Epoch: 145 	Ltrain: 0.000331 	Lval: 0.000341
Epoch: 150 	Ltrain: 0.000326 	Lval: 0.000335
Epoch: 155 	Ltrain: 0.000320 	Lval: 0.000328
Epoch: 160 	Ltrain: 0.000315 	Lval: 0.000323
Epoch: 165 	Ltrain: 0.000310 	Lval: 0.000316
Epoch: 170 	Ltrain: 0.000305 	Lval: 0.000310
Epoch: 175 	Ltrain: 0.000298 	Lval: 0.000305
Epoch: 180 	Ltrain: 0.000294 	Lval: 0.000298
Epoch: 185 	Ltrain: 0.000287 	Lval: 0.000292
Epoch: 190 	Ltrain: 0.000282 	Lval: 0.000286
Epoch: 195 	Ltrain: 0.000277 	Lval: 0.000280
Epoch: 200 	Ltrain: 0.000271 	Lval: 0.000274
Epoch: 205 	Ltrain: 0.000266 	Lval: 0.000268
Epoch: 210 	Ltrain: 0.000260 	Lval: 0.000263
Epoch: 215 	Ltrain: 0.000255 	Lval: 0.000257
Epoch: 220 	Ltrain: 0.000250 	Lval: 0.000252
Epoch: 225 	Ltrain: 0.000244 	Lval: 0.000247
Epoch: 230 	Ltrain: 0.000240 	Lval: 0.000241
Epoch: 235 	Ltrain: 0.000234 	Lval: 0.000236
Epoch: 240 	Ltrain: 0.000230 	Lval: 0.000231
Epoch: 245 	Ltrain: 0.000225 	Lval: 0.000225
Epoch: 250 	Ltrain: 0.000219 	Lval: 0.000221
Epoch: 255 	Ltrain: 0.000215 	Lval: 0.000216
Epoch: 260 	Ltrain: 0.000210 	Lval: 0.000211
Epoch: 265 	Ltrain: 0.000205 	Lval: 0.000207
Epoch: 270 	Ltrain: 0.000201 	Lval: 0.000203
Epoch: 275 	Ltrain: 0.000197 	Lval: 0.000198
Epoch: 280 	Ltrain: 0.000192 	Lval: 0.000193
Epoch: 285 	Ltrain: 0.000187 	Lval: 0.000189
Epoch: 290 	Ltrain: 0.000184 	Lval: 0.000185
Epoch: 295 	Ltrain: 0.000180 	Lval: 0.000181
EarlyStopper: stopping at epoch 297 with best_val_loss = 0.000189


	Fold 5/5
Epoch: 1 	Ltrain: 0.014042 	Lval: 0.007270
Epoch: 5 	Ltrain: 0.004675 	Lval: 0.005449
Epoch: 10 	Ltrain: 0.003426 	Lval: 0.003935
Epoch: 15 	Ltrain: 0.003104 	Lval: 0.003732
Epoch: 20 	Ltrain: 0.002827 	Lval: 0.003259
Epoch: 25 	Ltrain: 0.002691 	Lval: 0.003125
Epoch: 30 	Ltrain: 0.002749 	Lval: 0.003460
Epoch 00031: reducing learning rate of group 0 to 3.6945e-04.
Epoch: 35 	Ltrain: 0.001935 	Lval: 0.002623
Epoch: 40 	Ltrain: 0.001895 	Lval: 0.002546
Epoch: 45 	Ltrain: 0.001850 	Lval: 0.002460
Epoch: 50 	Ltrain: 0.001778 	Lval: 0.002408
Epoch: 55 	Ltrain: 0.001735 	Lval: 0.002413
Epoch: 60 	Ltrain: 0.001703 	Lval: 0.002341
Epoch: 65 	Ltrain: 0.001648 	Lval: 0.002135
Epoch: 70 	Ltrain: 0.001595 	Lval: 0.002112
Epoch: 75 	Ltrain: 0.001522 	Lval: 0.002020
Epoch: 80 	Ltrain: 0.001458 	Lval: 0.001929
Epoch: 85 	Ltrain: 0.001398 	Lval: 0.001783
Epoch: 90 	Ltrain: 0.001343 	Lval: 0.001717
Epoch: 95 	Ltrain: 0.001270 	Lval: 0.001526
Epoch: 100 	Ltrain: 0.001184 	Lval: 0.001435
Epoch: 105 	Ltrain: 0.001074 	Lval: 0.001267
Epoch: 110 	Ltrain: 0.000995 	Lval: 0.001106
Epoch: 115 	Ltrain: 0.000909 	Lval: 0.001030
Epoch: 120 	Ltrain: 0.000833 	Lval: 0.000917
Epoch: 125 	Ltrain: 0.000790 	Lval: 0.000810
Epoch: 130 	Ltrain: 0.000720 	Lval: 0.000752
Epoch: 135 	Ltrain: 0.000691 	Lval: 0.000735
Epoch 00137: reducing learning rate of group 0 to 3.6945e-05.
Epoch: 140 	Ltrain: 0.000565 	Lval: 0.000599
Epoch: 145 	Ltrain: 0.000548 	Lval: 0.000583
Epoch: 150 	Ltrain: 0.000537 	Lval: 0.000574
Epoch: 155 	Ltrain: 0.000530 	Lval: 0.000564
Epoch: 160 	Ltrain: 0.000521 	Lval: 0.000558
Epoch: 165 	Ltrain: 0.000515 	Lval: 0.000548
Epoch: 170 	Ltrain: 0.000508 	Lval: 0.000540
Epoch: 175 	Ltrain: 0.000499 	Lval: 0.000531
Epoch: 180 	Ltrain: 0.000492 	Lval: 0.000523
Epoch: 185 	Ltrain: 0.000484 	Lval: 0.000514
Epoch: 190 	Ltrain: 0.000477 	Lval: 0.000505
Epoch: 195 	Ltrain: 0.000470 	Lval: 0.000498
Epoch: 200 	Ltrain: 0.000461 	Lval: 0.000489
Epoch: 205 	Ltrain: 0.000454 	Lval: 0.000479
Epoch: 210 	Ltrain: 0.000445 	Lval: 0.000471
Epoch: 215 	Ltrain: 0.000438 	Lval: 0.000462
Epoch: 220 	Ltrain: 0.000430 	Lval: 0.000454
Epoch: 225 	Ltrain: 0.000422 	Lval: 0.000446
Epoch: 230 	Ltrain: 0.000414 	Lval: 0.000437
Epoch: 235 	Ltrain: 0.000406 	Lval: 0.000428
Epoch: 240 	Ltrain: 0.000398 	Lval: 0.000421
Epoch: 245 	Ltrain: 0.000391 	Lval: 0.000411
Epoch: 250 	Ltrain: 0.000382 	Lval: 0.000402
Epoch: 255 	Ltrain: 0.000376 	Lval: 0.000395
Epoch: 260 	Ltrain: 0.000367 	Lval: 0.000385
Epoch: 265 	Ltrain: 0.000359 	Lval: 0.000379
Epoch: 270 	Ltrain: 0.000352 	Lval: 0.000370
Epoch: 275 	Ltrain: 0.000345 	Lval: 0.000361
Epoch: 280 	Ltrain: 0.000340 	Lval: 0.000354
Epoch: 285 	Ltrain: 0.000332 	Lval: 0.000348
Epoch: 290 	Ltrain: 0.000324 	Lval: 0.000337
Epoch: 295 	Ltrain: 0.000318 	Lval: 0.000330
Epoch: 300 	Ltrain: 0.000311 	Lval: 0.000323
Epoch: 305 	Ltrain: 0.000305 	Lval: 0.000316
Epoch: 310 	Ltrain: 0.000298 	Lval: 0.000308
Epoch: 315 	Ltrain: 0.000292 	Lval: 0.000301
Epoch: 320 	Ltrain: 0.000286 	Lval: 0.000295
Epoch: 325 	Ltrain: 0.000279 	Lval: 0.000288
Epoch: 330 	Ltrain: 0.000273 	Lval: 0.000282
Epoch: 335 	Ltrain: 0.000268 	Lval: 0.000275
Epoch: 340 	Ltrain: 0.000262 	Lval: 0.000269
Epoch: 345 	Ltrain: 0.000257 	Lval: 0.000263
Epoch: 350 	Ltrain: 0.000252 	Lval: 0.000257
Epoch: 355 	Ltrain: 0.000247 	Lval: 0.000252
Epoch: 360 	Ltrain: 0.000241 	Lval: 0.000247
Epoch: 365 	Ltrain: 0.000237 	Lval: 0.000240
Epoch: 370 	Ltrain: 0.000232 	Lval: 0.000235
Epoch: 375 	Ltrain: 0.000225 	Lval: 0.000230
Epoch: 380 	Ltrain: 0.000223 	Lval: 0.000227
Epoch: 385 	Ltrain: 0.000218 	Lval: 0.000221
Epoch: 390 	Ltrain: 0.000214 	Lval: 0.000216
Epoch: 395 	Ltrain: 0.000208 	Lval: 0.000211
Epoch: 400 	Ltrain: 0.000205 	Lval: 0.000207
Epoch: 405 	Ltrain: 0.000200 	Lval: 0.000202
Epoch: 410 	Ltrain: 0.000197 	Lval: 0.000198
Epoch: 415 	Ltrain: 0.000192 	Lval: 0.000194
Epoch: 420 	Ltrain: 0.000187 	Lval: 0.000188
Epoch: 425 	Ltrain: 0.000184 	Lval: 0.000187
EarlyStopper: stopping at epoch 427 with best_val_loss = 0.000194

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004430933574533289
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.830043265160412e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 12
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.034257 	Lval: 0.019244
Epoch: 5 	Ltrain: 0.006451 	Lval: 0.009504
Epoch: 10 	Ltrain: 0.004920 	Lval: 0.005567
Epoch 00013: reducing learning rate of group 0 to 4.4309e-04.
Epoch: 15 	Ltrain: 0.005468 	Lval: 0.004497
Epoch: 20 	Ltrain: 0.004474 	Lval: 0.004483
Epoch: 25 	Ltrain: 0.004392 	Lval: 0.004437
Epoch: 30 	Ltrain: 0.004429 	Lval: 0.004405
Epoch 00034: reducing learning rate of group 0 to 4.4309e-05.
Epoch: 35 	Ltrain: 0.004410 	Lval: 0.004404
Epoch: 40 	Ltrain: 0.004592 	Lval: 0.004369
Epoch: 45 	Ltrain: 0.004146 	Lval: 0.004369
Epoch: 50 	Ltrain: 0.004730 	Lval: 0.004355
Epoch: 55 	Ltrain: 0.004406 	Lval: 0.004341
Epoch: 60 	Ltrain: 0.004338 	Lval: 0.004336
Epoch 00063: reducing learning rate of group 0 to 4.4309e-06.
Epoch: 65 	Ltrain: 0.004289 	Lval: 0.004348
Epoch: 70 	Ltrain: 0.004310 	Lval: 0.004342
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.004332


	Fold 2/5
Epoch: 1 	Ltrain: 0.024259 	Lval: 0.010377
Epoch: 5 	Ltrain: 0.005033 	Lval: 0.004993
Epoch 00009: reducing learning rate of group 0 to 4.4309e-04.
Epoch: 10 	Ltrain: 0.004325 	Lval: 0.004723
Epoch: 15 	Ltrain: 0.003925 	Lval: 0.004564
Epoch: 20 	Ltrain: 0.003984 	Lval: 0.004405
Epoch: 25 	Ltrain: 0.003716 	Lval: 0.004277
Epoch: 30 	Ltrain: 0.003442 	Lval: 0.004079
Epoch: 35 	Ltrain: 0.003262 	Lval: 0.003780
Epoch: 40 	Ltrain: 0.003060 	Lval: 0.003683
Epoch: 45 	Ltrain: 0.003071 	Lval: 0.003552
Epoch: 50 	Ltrain: 0.002897 	Lval: 0.003328
Epoch: 55 	Ltrain: 0.002877 	Lval: 0.003362
Epoch: 60 	Ltrain: 0.002745 	Lval: 0.003202
Epoch: 65 	Ltrain: 0.002701 	Lval: 0.003160
Epoch 00068: reducing learning rate of group 0 to 4.4309e-05.
Epoch: 70 	Ltrain: 0.002550 	Lval: 0.003070
Epoch: 75 	Ltrain: 0.002534 	Lval: 0.003033
Epoch: 80 	Ltrain: 0.002531 	Lval: 0.003011
Epoch: 85 	Ltrain: 0.002528 	Lval: 0.002994
Epoch 00089: reducing learning rate of group 0 to 4.4309e-06.
Epoch: 90 	Ltrain: 0.002464 	Lval: 0.002993
Epoch: 95 	Ltrain: 0.002534 	Lval: 0.002988
EarlyStopper: stopping at epoch 96 with best_val_loss = 0.002994


	Fold 3/5
Epoch: 1 	Ltrain: 0.018339 	Lval: 0.008009
Epoch: 5 	Ltrain: 0.004501 	Lval: 0.004704
Epoch: 10 	Ltrain: 0.004356 	Lval: 0.004795
Epoch: 15 	Ltrain: 0.003214 	Lval: 0.004205
Epoch: 20 	Ltrain: 0.002919 	Lval: 0.003162
Epoch: 25 	Ltrain: 0.002810 	Lval: 0.003017
Epoch 00029: reducing learning rate of group 0 to 4.4309e-04.
Epoch: 30 	Ltrain: 0.002376 	Lval: 0.002862
Epoch: 35 	Ltrain: 0.002226 	Lval: 0.002722
Epoch: 40 	Ltrain: 0.002125 	Lval: 0.002664
Epoch: 45 	Ltrain: 0.002103 	Lval: 0.002601
Epoch: 50 	Ltrain: 0.002084 	Lval: 0.002622
Epoch: 55 	Ltrain: 0.002006 	Lval: 0.002505
Epoch: 60 	Ltrain: 0.001961 	Lval: 0.002500
Epoch: 65 	Ltrain: 0.001962 	Lval: 0.002504
Epoch: 70 	Ltrain: 0.001873 	Lval: 0.002380
Epoch: 75 	Ltrain: 0.001844 	Lval: 0.002263
Epoch: 80 	Ltrain: 0.001784 	Lval: 0.002296
Epoch: 85 	Ltrain: 0.001706 	Lval: 0.002117
Epoch: 90 	Ltrain: 0.001646 	Lval: 0.001979
Epoch: 95 	Ltrain: 0.001589 	Lval: 0.001922
Epoch: 100 	Ltrain: 0.001520 	Lval: 0.001810
Epoch 00104: reducing learning rate of group 0 to 4.4309e-05.
Epoch: 105 	Ltrain: 0.001445 	Lval: 0.001745
Epoch: 110 	Ltrain: 0.001348 	Lval: 0.001670
Epoch: 115 	Ltrain: 0.001334 	Lval: 0.001652
Epoch: 120 	Ltrain: 0.001327 	Lval: 0.001619
Epoch: 125 	Ltrain: 0.001333 	Lval: 0.001618
Epoch: 130 	Ltrain: 0.001302 	Lval: 0.001596
Epoch: 135 	Ltrain: 0.001278 	Lval: 0.001572
Epoch: 140 	Ltrain: 0.001270 	Lval: 0.001562
Epoch: 145 	Ltrain: 0.001264 	Lval: 0.001543
Epoch: 150 	Ltrain: 0.001251 	Lval: 0.001514
Epoch 00154: reducing learning rate of group 0 to 4.4309e-06.
Epoch: 155 	Ltrain: 0.001240 	Lval: 0.001519
Epoch: 160 	Ltrain: 0.001222 	Lval: 0.001502
Epoch: 165 	Ltrain: 0.001217 	Lval: 0.001500
Epoch: 170 	Ltrain: 0.001219 	Lval: 0.001500
EarlyStopper: stopping at epoch 169 with best_val_loss = 0.001504


	Fold 4/5
Epoch: 1 	Ltrain: 0.019448 	Lval: 0.007487
Epoch: 5 	Ltrain: 0.004226 	Lval: 0.005224
Epoch: 10 	Ltrain: 0.003492 	Lval: 0.004085
Epoch: 15 	Ltrain: 0.003038 	Lval: 0.003538
Epoch 00020: reducing learning rate of group 0 to 4.4309e-04.
Epoch: 20 	Ltrain: 0.003191 	Lval: 0.003356
Epoch: 25 	Ltrain: 0.002288 	Lval: 0.003068
Epoch: 30 	Ltrain: 0.002252 	Lval: 0.002936
Epoch: 35 	Ltrain: 0.002197 	Lval: 0.002944
Epoch: 40 	Ltrain: 0.002180 	Lval: 0.002879
Epoch: 45 	Ltrain: 0.002121 	Lval: 0.002786
Epoch: 50 	Ltrain: 0.002072 	Lval: 0.002689
Epoch 00054: reducing learning rate of group 0 to 4.4309e-05.
Epoch: 55 	Ltrain: 0.002002 	Lval: 0.002674
Epoch: 60 	Ltrain: 0.001961 	Lval: 0.002641
Epoch: 65 	Ltrain: 0.001956 	Lval: 0.002639
Epoch 00070: reducing learning rate of group 0 to 4.4309e-06.
Epoch: 70 	Ltrain: 0.001949 	Lval: 0.002637
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.002641


	Fold 5/5
Epoch: 1 	Ltrain: 0.021816 	Lval: 0.007066
Epoch: 5 	Ltrain: 0.004426 	Lval: 0.005003
Epoch: 10 	Ltrain: 0.003614 	Lval: 0.004717
Epoch: 15 	Ltrain: 0.003081 	Lval: 0.003779
Epoch 00016: reducing learning rate of group 0 to 4.4309e-04.
Epoch: 20 	Ltrain: 0.002482 	Lval: 0.003338
Epoch: 25 	Ltrain: 0.002450 	Lval: 0.003217
Epoch: 30 	Ltrain: 0.002387 	Lval: 0.003162
Epoch: 35 	Ltrain: 0.002341 	Lval: 0.003145
Epoch: 40 	Ltrain: 0.002321 	Lval: 0.002999
Epoch: 45 	Ltrain: 0.002204 	Lval: 0.002922
Epoch 00050: reducing learning rate of group 0 to 4.4309e-05.
Epoch: 50 	Ltrain: 0.002181 	Lval: 0.002933
Epoch: 55 	Ltrain: 0.002095 	Lval: 0.002789
Epoch: 60 	Ltrain: 0.002087 	Lval: 0.002803
Epoch: 65 	Ltrain: 0.002080 	Lval: 0.002770
Epoch: 70 	Ltrain: 0.002072 	Lval: 0.002758
Epoch 00074: reducing learning rate of group 0 to 4.4309e-06.
Epoch: 75 	Ltrain: 0.002058 	Lval: 0.002783
Epoch: 80 	Ltrain: 0.002058 	Lval: 0.002768
EarlyStopper: stopping at epoch 81 with best_val_loss = 0.002758

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.003560859913604446
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.247033588713008e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.033616 	Lval: 0.010739
Epoch: 5 	Ltrain: 0.010186 	Lval: 0.007825
Epoch: 10 	Ltrain: 0.006761 	Lval: 0.005814
Epoch: 15 	Ltrain: 0.005387 	Lval: 0.005104
Epoch 00018: reducing learning rate of group 0 to 3.5609e-04.
Epoch: 20 	Ltrain: 0.004846 	Lval: 0.004660
Epoch: 25 	Ltrain: 0.004763 	Lval: 0.004571
Epoch: 30 	Ltrain: 0.004566 	Lval: 0.004575
Epoch: 35 	Ltrain: 0.004302 	Lval: 0.004473
Epoch: 40 	Ltrain: 0.004593 	Lval: 0.004385
Epoch: 45 	Ltrain: 0.004361 	Lval: 0.004314
Epoch: 50 	Ltrain: 0.004357 	Lval: 0.004274
Epoch: 55 	Ltrain: 0.004066 	Lval: 0.004384
Epoch 00058: reducing learning rate of group 0 to 3.5609e-05.
Epoch: 60 	Ltrain: 0.003950 	Lval: 0.004124
Epoch: 65 	Ltrain: 0.004406 	Lval: 0.004104
Epoch 00070: reducing learning rate of group 0 to 3.5609e-06.
Epoch: 70 	Ltrain: 0.003844 	Lval: 0.004110
Epoch: 75 	Ltrain: 0.003847 	Lval: 0.004099
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.004098


	Fold 2/5
Epoch: 1 	Ltrain: 0.024557 	Lval: 0.010057
Epoch: 5 	Ltrain: 0.005149 	Lval: 0.005185
Epoch: 10 	Ltrain: 0.004510 	Lval: 0.004600
Epoch: 15 	Ltrain: 0.003589 	Lval: 0.004596
Epoch 00020: reducing learning rate of group 0 to 3.5609e-04.
Epoch: 20 	Ltrain: 0.003761 	Lval: 0.004163
Epoch: 25 	Ltrain: 0.002905 	Lval: 0.003450
Epoch: 30 	Ltrain: 0.002832 	Lval: 0.003363
Epoch: 35 	Ltrain: 0.002769 	Lval: 0.003327
Epoch: 40 	Ltrain: 0.002748 	Lval: 0.003273
Epoch: 45 	Ltrain: 0.002747 	Lval: 0.003286
Epoch 00046: reducing learning rate of group 0 to 3.5609e-05.
Epoch: 50 	Ltrain: 0.002641 	Lval: 0.003164
Epoch: 55 	Ltrain: 0.002646 	Lval: 0.003152
Epoch: 60 	Ltrain: 0.002625 	Lval: 0.003154
Epoch: 65 	Ltrain: 0.002593 	Lval: 0.003137
Epoch 00067: reducing learning rate of group 0 to 3.5609e-06.
Epoch: 70 	Ltrain: 0.002607 	Lval: 0.003136
Epoch: 75 	Ltrain: 0.002659 	Lval: 0.003134
EarlyStopper: stopping at epoch 76 with best_val_loss = 0.003135


	Fold 3/5
Epoch: 1 	Ltrain: 0.015327 	Lval: 0.007839
Epoch: 5 	Ltrain: 0.005196 	Lval: 0.005066
Epoch: 10 	Ltrain: 0.003817 	Lval: 0.004115
Epoch: 15 	Ltrain: 0.003693 	Lval: 0.004411
Epoch: 20 	Ltrain: 0.003308 	Lval: 0.003559
Epoch: 25 	Ltrain: 0.002702 	Lval: 0.003422
Epoch: 30 	Ltrain: 0.002763 	Lval: 0.003080
Epoch: 35 	Ltrain: 0.002524 	Lval: 0.002865
Epoch: 40 	Ltrain: 0.002318 	Lval: 0.002755
Epoch: 45 	Ltrain: 0.002108 	Lval: 0.002374
Epoch: 50 	Ltrain: 0.001926 	Lval: 0.002326
Epoch: 55 	Ltrain: 0.001781 	Lval: 0.001966
Epoch: 60 	Ltrain: 0.001554 	Lval: 0.001767
Epoch: 65 	Ltrain: 0.001641 	Lval: 0.001627
Epoch: 70 	Ltrain: 0.001096 	Lval: 0.001015
Epoch: 75 	Ltrain: 0.000794 	Lval: 0.000757
Epoch: 80 	Ltrain: 0.000645 	Lval: 0.000672
Epoch: 85 	Ltrain: 0.000604 	Lval: 0.000573
Epoch: 90 	Ltrain: 0.000506 	Lval: 0.000521
Epoch 00092: reducing learning rate of group 0 to 3.5609e-04.
Epoch: 95 	Ltrain: 0.000288 	Lval: 0.000283
Epoch: 100 	Ltrain: 0.000243 	Lval: 0.000238
Epoch: 105 	Ltrain: 0.000221 	Lval: 0.000217
Epoch: 110 	Ltrain: 0.000205 	Lval: 0.000201
Epoch: 115 	Ltrain: 0.000194 	Lval: 0.000187
Epoch: 120 	Ltrain: 0.000181 	Lval: 0.000175
Epoch: 125 	Ltrain: 0.000168 	Lval: 0.000163
Epoch: 130 	Ltrain: 0.000157 	Lval: 0.000151
Epoch: 135 	Ltrain: 0.000146 	Lval: 0.000141
Epoch: 140 	Ltrain: 0.000137 	Lval: 0.000131
Epoch: 145 	Ltrain: 0.000126 	Lval: 0.000120
Epoch: 150 	Ltrain: 0.000117 	Lval: 0.000112
Epoch: 155 	Ltrain: 0.000108 	Lval: 0.000103
Epoch: 160 	Ltrain: 0.000100 	Lval: 0.000095
Epoch: 165 	Ltrain: 0.000091 	Lval: 0.000087
Epoch: 170 	Ltrain: 0.000085 	Lval: 0.000081
Epoch: 175 	Ltrain: 0.000078 	Lval: 0.000074
Epoch: 180 	Ltrain: 0.000072 	Lval: 0.000069
Epoch: 185 	Ltrain: 0.000067 	Lval: 0.000066
Epoch: 190 	Ltrain: 0.000061 	Lval: 0.000058
Epoch: 195 	Ltrain: 0.000055 	Lval: 0.000054
Epoch: 200 	Ltrain: 0.000052 	Lval: 0.000052
Epoch 00205: reducing learning rate of group 0 to 3.5609e-05.
Epoch: 205 	Ltrain: 0.000054 	Lval: 0.000052
EarlyStopper: stopping at epoch 205 with best_val_loss = 0.000055


	Fold 4/5
Epoch: 1 	Ltrain: 0.014352 	Lval: 0.006644
Epoch: 5 	Ltrain: 0.004264 	Lval: 0.004922
Epoch: 10 	Ltrain: 0.003631 	Lval: 0.003903
Epoch: 15 	Ltrain: 0.002954 	Lval: 0.004636
Epoch 00018: reducing learning rate of group 0 to 3.5609e-04.
Epoch: 20 	Ltrain: 0.002461 	Lval: 0.003296
Epoch: 25 	Ltrain: 0.002429 	Lval: 0.003203
Epoch: 30 	Ltrain: 0.002367 	Lval: 0.003114
Epoch: 35 	Ltrain: 0.002305 	Lval: 0.003104
Epoch: 40 	Ltrain: 0.002272 	Lval: 0.003048
Epoch: 45 	Ltrain: 0.002215 	Lval: 0.002826
Epoch 00049: reducing learning rate of group 0 to 3.5609e-05.
Epoch: 50 	Ltrain: 0.002123 	Lval: 0.002789
Epoch: 55 	Ltrain: 0.002081 	Lval: 0.002804
Epoch: 60 	Ltrain: 0.002078 	Lval: 0.002775
Epoch: 65 	Ltrain: 0.002075 	Lval: 0.002777
Epoch: 70 	Ltrain: 0.002053 	Lval: 0.002748
Epoch: 75 	Ltrain: 0.002057 	Lval: 0.002748
Epoch: 80 	Ltrain: 0.002042 	Lval: 0.002726
Epoch 00081: reducing learning rate of group 0 to 3.5609e-06.
Epoch: 85 	Ltrain: 0.002027 	Lval: 0.002711
Epoch: 90 	Ltrain: 0.002028 	Lval: 0.002714
EarlyStopper: stopping at epoch 90 with best_val_loss = 0.002711


	Fold 5/5
Epoch: 1 	Ltrain: 0.014423 	Lval: 0.007619
Epoch: 5 	Ltrain: 0.004472 	Lval: 0.005487
Epoch: 10 	Ltrain: 0.003441 	Lval: 0.004571
Epoch: 15 	Ltrain: 0.003413 	Lval: 0.004160
Epoch: 20 	Ltrain: 0.002761 	Lval: 0.003495
Epoch: 25 	Ltrain: 0.002748 	Lval: 0.003214
Epoch: 30 	Ltrain: 0.002334 	Lval: 0.002893
Epoch: 35 	Ltrain: 0.002342 	Lval: 0.002768
Epoch: 40 	Ltrain: 0.002244 	Lval: 0.002872
Epoch: 45 	Ltrain: 0.002080 	Lval: 0.002603
Epoch: 50 	Ltrain: 0.001979 	Lval: 0.002277
Epoch: 55 	Ltrain: 0.001829 	Lval: 0.001998
Epoch: 60 	Ltrain: 0.001648 	Lval: 0.001952
Epoch: 65 	Ltrain: 0.001503 	Lval: 0.001740
Epoch: 70 	Ltrain: 0.001299 	Lval: 0.001415
Epoch: 75 	Ltrain: 0.001089 	Lval: 0.001057
Epoch: 80 	Ltrain: 0.000888 	Lval: 0.000941
Epoch: 85 	Ltrain: 0.000730 	Lval: 0.000836
Epoch 00086: reducing learning rate of group 0 to 3.5609e-04.
Epoch: 90 	Ltrain: 0.000438 	Lval: 0.000449
Epoch: 95 	Ltrain: 0.000382 	Lval: 0.000391
Epoch: 100 	Ltrain: 0.000349 	Lval: 0.000357
Epoch: 105 	Ltrain: 0.000325 	Lval: 0.000329
Epoch: 110 	Ltrain: 0.000301 	Lval: 0.000302
Epoch: 115 	Ltrain: 0.000279 	Lval: 0.000278
Epoch: 120 	Ltrain: 0.000256 	Lval: 0.000257
Epoch: 125 	Ltrain: 0.000235 	Lval: 0.000233
Epoch: 130 	Ltrain: 0.000215 	Lval: 0.000213
Epoch: 135 	Ltrain: 0.000197 	Lval: 0.000192
Epoch: 140 	Ltrain: 0.000179 	Lval: 0.000174
Epoch: 145 	Ltrain: 0.000163 	Lval: 0.000158
Epoch: 150 	Ltrain: 0.000150 	Lval: 0.000145
Epoch: 155 	Ltrain: 0.000139 	Lval: 0.000131
Epoch: 160 	Ltrain: 0.000127 	Lval: 0.000125
Epoch: 165 	Ltrain: 0.000126 	Lval: 0.000122
Epoch: 170 	Ltrain: 0.000116 	Lval: 0.000117
Epoch: 175 	Ltrain: 0.000107 	Lval: 0.000107
Epoch 00176: reducing learning rate of group 0 to 3.5609e-05.
Epoch: 180 	Ltrain: 0.000087 	Lval: 0.000084
Epoch: 185 	Ltrain: 0.000084 	Lval: 0.000082
Epoch: 190 	Ltrain: 0.000082 	Lval: 0.000080
Epoch: 195 	Ltrain: 0.000081 	Lval: 0.000079
EarlyStopper: stopping at epoch 197 with best_val_loss = 0.000082

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.001740438321755252
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.222754158285766e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 10
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.019231 	Lval: 0.012138
Epoch: 5 	Ltrain: 0.007128 	Lval: 0.006330
Epoch: 10 	Ltrain: 0.006275 	Lval: 0.005643
Epoch: 15 	Ltrain: 0.006302 	Lval: 0.006409
Epoch 00016: reducing learning rate of group 0 to 1.7404e-04.
Epoch: 20 	Ltrain: 0.004982 	Lval: 0.004769
Epoch: 25 	Ltrain: 0.004623 	Lval: 0.004705
Epoch: 30 	Ltrain: 0.004499 	Lval: 0.004658
Epoch: 35 	Ltrain: 0.004541 	Lval: 0.004552
Epoch: 40 	Ltrain: 0.004468 	Lval: 0.004550
Epoch: 45 	Ltrain: 0.004907 	Lval: 0.004416
Epoch: 50 	Ltrain: 0.004257 	Lval: 0.004369
Epoch: 55 	Ltrain: 0.004191 	Lval: 0.004301
Epoch: 60 	Ltrain: 0.004361 	Lval: 0.004338
Epoch 00062: reducing learning rate of group 0 to 1.7404e-05.
Epoch: 65 	Ltrain: 0.003977 	Lval: 0.004284
Epoch: 70 	Ltrain: 0.003949 	Lval: 0.004258
Epoch 00074: reducing learning rate of group 0 to 1.7404e-06.
Epoch: 75 	Ltrain: 0.004105 	Lval: 0.004288
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.004266


	Fold 2/5
Epoch: 1 	Ltrain: 0.012765 	Lval: 0.008800
Epoch: 5 	Ltrain: 0.005808 	Lval: 0.006587
Epoch: 10 	Ltrain: 0.004463 	Lval: 0.004669
Epoch 00015: reducing learning rate of group 0 to 1.7404e-04.
Epoch: 15 	Ltrain: 0.004568 	Lval: 0.004710
Epoch: 20 	Ltrain: 0.003854 	Lval: 0.004217
Epoch: 25 	Ltrain: 0.003498 	Lval: 0.004195
Epoch: 30 	Ltrain: 0.003336 	Lval: 0.004009
Epoch: 35 	Ltrain: 0.003294 	Lval: 0.003946
Epoch: 40 	Ltrain: 0.003207 	Lval: 0.003831
Epoch: 45 	Ltrain: 0.003209 	Lval: 0.003825
Epoch 00047: reducing learning rate of group 0 to 1.7404e-05.
Epoch: 50 	Ltrain: 0.003139 	Lval: 0.003730
Epoch: 55 	Ltrain: 0.003136 	Lval: 0.003722
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.003732


	Fold 3/5
Epoch: 1 	Ltrain: 0.011932 	Lval: 0.007833
Epoch: 5 	Ltrain: 0.004823 	Lval: 0.004795
Epoch: 10 	Ltrain: 0.004200 	Lval: 0.005144
Epoch: 15 	Ltrain: 0.003703 	Lval: 0.004060
Epoch: 20 	Ltrain: 0.003058 	Lval: 0.003688
Epoch 00023: reducing learning rate of group 0 to 1.7404e-04.
Epoch: 25 	Ltrain: 0.002798 	Lval: 0.003411
Epoch: 30 	Ltrain: 0.002773 	Lval: 0.003407
Epoch 00035: reducing learning rate of group 0 to 1.7404e-05.
Epoch: 35 	Ltrain: 0.002741 	Lval: 0.003381
Epoch: 40 	Ltrain: 0.002745 	Lval: 0.003349
Epoch: 45 	Ltrain: 0.002736 	Lval: 0.003342
EarlyStopper: stopping at epoch 46 with best_val_loss = 0.003349


	Fold 4/5
Epoch: 1 	Ltrain: 0.014020 	Lval: 0.008403
Epoch: 5 	Ltrain: 0.004759 	Lval: 0.005144
Epoch: 10 	Ltrain: 0.003836 	Lval: 0.005232
Epoch: 15 	Ltrain: 0.003228 	Lval: 0.004209
Epoch: 20 	Ltrain: 0.003109 	Lval: 0.003628
Epoch: 25 	Ltrain: 0.002944 	Lval: 0.003512
Epoch: 30 	Ltrain: 0.002803 	Lval: 0.003767
Epoch: 35 	Ltrain: 0.002714 	Lval: 0.003491
Epoch: 40 	Ltrain: 0.002586 	Lval: 0.003328
Epoch 00045: reducing learning rate of group 0 to 1.7404e-04.
Epoch: 45 	Ltrain: 0.002583 	Lval: 0.003258
Epoch: 50 	Ltrain: 0.002288 	Lval: 0.002983
Epoch: 55 	Ltrain: 0.002267 	Lval: 0.002972
Epoch 00057: reducing learning rate of group 0 to 1.7404e-05.
EarlyStopper: stopping at epoch 58 with best_val_loss = 0.002945


	Fold 5/5
Epoch: 1 	Ltrain: 0.010858 	Lval: 0.007209
Epoch: 5 	Ltrain: 0.004395 	Lval: 0.005583
Epoch: 10 	Ltrain: 0.003888 	Lval: 0.005205
Epoch: 15 	Ltrain: 0.003397 	Lval: 0.004267
Epoch: 20 	Ltrain: 0.002852 	Lval: 0.003651
Epoch: 25 	Ltrain: 0.002865 	Lval: 0.003739
Epoch: 30 	Ltrain: 0.002749 	Lval: 0.003445
Epoch 00034: reducing learning rate of group 0 to 1.7404e-04.
Epoch: 35 	Ltrain: 0.002531 	Lval: 0.003262
Epoch: 40 	Ltrain: 0.002432 	Lval: 0.003223
Epoch: 45 	Ltrain: 0.002402 	Lval: 0.003247
Epoch 00046: reducing learning rate of group 0 to 1.7404e-05.
Epoch: 50 	Ltrain: 0.002366 	Lval: 0.003189
Epoch: 55 	Ltrain: 0.002359 	Lval: 0.003186
Epoch 00058: reducing learning rate of group 0 to 1.7404e-06.
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.003188

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00646628563234717
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.1923497177837952e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.086680 	Lval: 0.019375
Epoch: 5 	Ltrain: 0.009278 	Lval: 0.006714
Epoch: 10 	Ltrain: 0.005468 	Lval: 0.006828
Epoch 00013: reducing learning rate of group 0 to 6.4663e-04.
Epoch: 15 	Ltrain: 0.005650 	Lval: 0.005335
Epoch: 20 	Ltrain: 0.005934 	Lval: 0.005052
Epoch: 25 	Ltrain: 0.004558 	Lval: 0.004738
Epoch: 30 	Ltrain: 0.004564 	Lval: 0.004676
Epoch: 35 	Ltrain: 0.005499 	Lval: 0.004747
Epoch 00036: reducing learning rate of group 0 to 6.4663e-05.
Epoch: 40 	Ltrain: 0.004846 	Lval: 0.004785
Epoch: 45 	Ltrain: 0.004940 	Lval: 0.004724
EarlyStopper: stopping at epoch 45 with best_val_loss = 0.004661


	Fold 2/5
Epoch: 1 	Ltrain: 0.049103 	Lval: 0.011497
Epoch: 5 	Ltrain: 0.005381 	Lval: 0.005177
Epoch: 10 	Ltrain: 0.005072 	Lval: 0.007356
Epoch 00011: reducing learning rate of group 0 to 6.4663e-04.
Epoch: 15 	Ltrain: 0.004308 	Lval: 0.004548
Epoch: 20 	Ltrain: 0.004142 	Lval: 0.004568
Epoch 00023: reducing learning rate of group 0 to 6.4663e-05.
Epoch: 25 	Ltrain: 0.004043 	Lval: 0.004573
EarlyStopper: stopping at epoch 28 with best_val_loss = 0.004548


	Fold 3/5
Epoch: 1 	Ltrain: 0.053606 	Lval: 0.009523
Epoch: 5 	Ltrain: 0.004822 	Lval: 0.004840
Epoch: 10 	Ltrain: 0.004137 	Lval: 0.004646
Epoch 00015: reducing learning rate of group 0 to 6.4663e-04.
Epoch: 15 	Ltrain: 0.004070 	Lval: 0.004368
Epoch: 20 	Ltrain: 0.003330 	Lval: 0.004061
Epoch: 25 	Ltrain: 0.003284 	Lval: 0.003968
Epoch: 30 	Ltrain: 0.003212 	Lval: 0.004075
Epoch 00031: reducing learning rate of group 0 to 6.4663e-05.
Epoch: 35 	Ltrain: 0.003085 	Lval: 0.003828
Epoch: 40 	Ltrain: 0.003075 	Lval: 0.003831
Epoch: 45 	Ltrain: 0.003068 	Lval: 0.003800
Epoch: 50 	Ltrain: 0.003045 	Lval: 0.003800
Epoch: 55 	Ltrain: 0.003154 	Lval: 0.003790
Epoch: 60 	Ltrain: 0.003068 	Lval: 0.003768
Epoch 00062: reducing learning rate of group 0 to 6.4663e-06.
Epoch: 65 	Ltrain: 0.003033 	Lval: 0.003779
Epoch: 70 	Ltrain: 0.002993 	Lval: 0.003777
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.003764


	Fold 4/5
Epoch: 1 	Ltrain: 0.031076 	Lval: 0.010203
Epoch: 5 	Ltrain: 0.004665 	Lval: 0.005048
Epoch: 10 	Ltrain: 0.003796 	Lval: 0.004831
Epoch: 15 	Ltrain: 0.003858 	Lval: 0.004621
Epoch: 20 	Ltrain: 0.002986 	Lval: 0.003850
Epoch 00025: reducing learning rate of group 0 to 6.4663e-04.
Epoch: 25 	Ltrain: 0.002783 	Lval: 0.003380
Epoch: 30 	Ltrain: 0.002295 	Lval: 0.002973
Epoch: 35 	Ltrain: 0.002238 	Lval: 0.002942
Epoch: 40 	Ltrain: 0.002165 	Lval: 0.002895
Epoch: 45 	Ltrain: 0.002126 	Lval: 0.002865
Epoch: 50 	Ltrain: 0.002107 	Lval: 0.002783
Epoch: 55 	Ltrain: 0.002070 	Lval: 0.002774
Epoch: 60 	Ltrain: 0.002100 	Lval: 0.002843
Epoch 00061: reducing learning rate of group 0 to 6.4663e-05.
Epoch: 65 	Ltrain: 0.001962 	Lval: 0.002669
Epoch: 70 	Ltrain: 0.001962 	Lval: 0.002658
Epoch: 75 	Ltrain: 0.001968 	Lval: 0.002653
Epoch 00078: reducing learning rate of group 0 to 6.4663e-06.
Epoch: 80 	Ltrain: 0.001951 	Lval: 0.002644
Epoch: 85 	Ltrain: 0.001944 	Lval: 0.002644
EarlyStopper: stopping at epoch 87 with best_val_loss = 0.002639


	Fold 5/5
Epoch: 1 	Ltrain: 0.044694 	Lval: 0.009678
Epoch: 5 	Ltrain: 0.004400 	Lval: 0.005552
Epoch: 10 	Ltrain: 0.003745 	Lval: 0.004740
Epoch: 15 	Ltrain: 0.003243 	Lval: 0.004233
Epoch: 20 	Ltrain: 0.002847 	Lval: 0.003857
Epoch: 25 	Ltrain: 0.002640 	Lval: 0.003702
Epoch: 30 	Ltrain: 0.002385 	Lval: 0.003836
Epoch 00033: reducing learning rate of group 0 to 6.4663e-04.
Epoch: 35 	Ltrain: 0.002099 	Lval: 0.002894
Epoch: 40 	Ltrain: 0.002021 	Lval: 0.002805
Epoch: 45 	Ltrain: 0.002016 	Lval: 0.002779
Epoch: 50 	Ltrain: 0.001983 	Lval: 0.002775
Epoch: 55 	Ltrain: 0.001950 	Lval: 0.002671
Epoch: 60 	Ltrain: 0.001923 	Lval: 0.002613
Epoch 00065: reducing learning rate of group 0 to 6.4663e-05.
Epoch: 65 	Ltrain: 0.001885 	Lval: 0.002602
Epoch: 70 	Ltrain: 0.001859 	Lval: 0.002544
Epoch: 75 	Ltrain: 0.001836 	Lval: 0.002531
Epoch: 80 	Ltrain: 0.001834 	Lval: 0.002524
Epoch 00082: reducing learning rate of group 0 to 6.4663e-06.
Epoch: 85 	Ltrain: 0.001828 	Lval: 0.002524
Epoch: 90 	Ltrain: 0.001854 	Lval: 0.002524
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.002519

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006558508259785599
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 9.488721640734299e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 12
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.060831 	Lval: 0.016770
Epoch: 5 	Ltrain: 0.010666 	Lval: 0.008847
Epoch 00008: reducing learning rate of group 0 to 6.5585e-04.
Epoch: 10 	Ltrain: 0.012597 	Lval: 0.008366
Epoch: 15 	Ltrain: 0.008861 	Lval: 0.006650
Epoch: 20 	Ltrain: 0.006580 	Lval: 0.006170
Epoch: 25 	Ltrain: 0.006905 	Lval: 0.005876
Epoch: 30 	Ltrain: 0.006360 	Lval: 0.006140
Epoch 00031: reducing learning rate of group 0 to 6.5585e-05.
Epoch: 35 	Ltrain: 0.006257 	Lval: 0.005893
Epoch: 40 	Ltrain: 0.006556 	Lval: 0.005741
Epoch: 45 	Ltrain: 0.007031 	Lval: 0.005715
Epoch 00048: reducing learning rate of group 0 to 6.5585e-06.
Epoch: 50 	Ltrain: 0.006964 	Lval: 0.005753
Epoch: 55 	Ltrain: 0.006615 	Lval: 0.005755
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.005715


	Fold 2/5
Epoch: 1 	Ltrain: 0.041594 	Lval: 0.009079
Epoch: 5 	Ltrain: 0.008712 	Lval: 0.006463
Epoch: 10 	Ltrain: 0.006393 	Lval: 0.005001
Epoch: 15 	Ltrain: 0.005209 	Lval: 0.005168
Epoch 00016: reducing learning rate of group 0 to 6.5585e-04.
Epoch: 20 	Ltrain: 0.005282 	Lval: 0.004526
Epoch: 25 	Ltrain: 0.004438 	Lval: 0.004666
Epoch 00028: reducing learning rate of group 0 to 6.5585e-05.
Epoch: 30 	Ltrain: 0.004038 	Lval: 0.004611
EarlyStopper: stopping at epoch 32 with best_val_loss = 0.004490


	Fold 3/5
Epoch: 1 	Ltrain: 0.026077 	Lval: 0.015894
Epoch: 5 	Ltrain: 0.008396 	Lval: 0.006543
Epoch: 10 	Ltrain: 0.004581 	Lval: 0.005407
Epoch: 15 	Ltrain: 0.004059 	Lval: 0.004523
Epoch: 20 	Ltrain: 0.003966 	Lval: 0.004704
Epoch: 25 	Ltrain: 0.003968 	Lval: 0.004477
Epoch: 30 	Ltrain: 0.003398 	Lval: 0.004053
Epoch 00031: reducing learning rate of group 0 to 6.5585e-04.
Epoch: 35 	Ltrain: 0.003017 	Lval: 0.003612
Epoch: 40 	Ltrain: 0.003034 	Lval: 0.003568
Epoch: 45 	Ltrain: 0.003070 	Lval: 0.003607
Epoch 00048: reducing learning rate of group 0 to 6.5585e-05.
Epoch: 50 	Ltrain: 0.002842 	Lval: 0.003544
Epoch: 55 	Ltrain: 0.002814 	Lval: 0.003517
EarlyStopper: stopping at epoch 55 with best_val_loss = 0.003505


	Fold 4/5
Epoch: 1 	Ltrain: 0.020574 	Lval: 0.009883
Epoch: 5 	Ltrain: 0.006138 	Lval: 0.005985
Epoch: 10 	Ltrain: 0.004118 	Lval: 0.004881
Epoch 00015: reducing learning rate of group 0 to 6.5585e-04.
Epoch: 15 	Ltrain: 0.003833 	Lval: 0.004365
Epoch: 20 	Ltrain: 0.003435 	Lval: 0.004149
Epoch: 25 	Ltrain: 0.003188 	Lval: 0.004116
Epoch 00027: reducing learning rate of group 0 to 6.5585e-05.
Epoch: 30 	Ltrain: 0.003366 	Lval: 0.004097
EarlyStopper: stopping at epoch 32 with best_val_loss = 0.004056


	Fold 5/5
Epoch: 1 	Ltrain: 0.018977 	Lval: 0.010111
Epoch: 5 	Ltrain: 0.005750 	Lval: 0.006271
Epoch: 10 	Ltrain: 0.004811 	Lval: 0.005594
Epoch 00015: reducing learning rate of group 0 to 6.5585e-04.
Epoch: 15 	Ltrain: 0.004077 	Lval: 0.005332
Epoch: 20 	Ltrain: 0.003536 	Lval: 0.004569
Epoch: 25 	Ltrain: 0.003572 	Lval: 0.004516
Epoch: 30 	Ltrain: 0.003561 	Lval: 0.004457
Epoch: 35 	Ltrain: 0.003581 	Lval: 0.004372
Epoch: 40 	Ltrain: 0.003322 	Lval: 0.004387
Epoch: 45 	Ltrain: 0.003398 	Lval: 0.004294
Epoch: 50 	Ltrain: 0.003264 	Lval: 0.004282
Epoch: 55 	Ltrain: 0.002997 	Lval: 0.004184
Epoch 00056: reducing learning rate of group 0 to 6.5585e-05.
Epoch: 60 	Ltrain: 0.003122 	Lval: 0.004179
Epoch: 65 	Ltrain: 0.003020 	Lval: 0.004192
Epoch 00068: reducing learning rate of group 0 to 6.5585e-06.
Epoch: 70 	Ltrain: 0.002932 	Lval: 0.004176
Epoch: 75 	Ltrain: 0.003066 	Lval: 0.004171
EarlyStopper: stopping at epoch 74 with best_val_loss = 0.004174

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0033260188808193615
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.0446598200230224e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 15
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.026825 	Lval: 0.011847
Epoch: 5 	Ltrain: 0.008504 	Lval: 0.006949
Epoch: 10 	Ltrain: 0.005548 	Lval: 0.005670
Epoch 00015: reducing learning rate of group 0 to 3.3260e-04.
Epoch: 15 	Ltrain: 0.005521 	Lval: 0.005365
Epoch: 20 	Ltrain: 0.005763 	Lval: 0.004890
Epoch: 25 	Ltrain: 0.004749 	Lval: 0.004871
Epoch: 30 	Ltrain: 0.005771 	Lval: 0.005111
Epoch 00033: reducing learning rate of group 0 to 3.3260e-05.
Epoch: 35 	Ltrain: 0.005816 	Lval: 0.004874
Epoch: 40 	Ltrain: 0.004501 	Lval: 0.004872
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.004814


	Fold 2/5
Epoch: 1 	Ltrain: 0.024129 	Lval: 0.017303
Epoch: 5 	Ltrain: 0.006483 	Lval: 0.005876
Epoch: 10 	Ltrain: 0.004807 	Lval: 0.005030
Epoch: 15 	Ltrain: 0.004141 	Lval: 0.004725
Epoch 00018: reducing learning rate of group 0 to 3.3260e-04.
Epoch: 20 	Ltrain: 0.003616 	Lval: 0.004256
Epoch: 25 	Ltrain: 0.003432 	Lval: 0.004134
Epoch: 30 	Ltrain: 0.003457 	Lval: 0.003981
Epoch: 35 	Ltrain: 0.003345 	Lval: 0.003878
Epoch: 40 	Ltrain: 0.003220 	Lval: 0.003816
Epoch: 45 	Ltrain: 0.003144 	Lval: 0.003851
Epoch 00050: reducing learning rate of group 0 to 3.3260e-05.
Epoch: 50 	Ltrain: 0.003231 	Lval: 0.003755
Epoch: 55 	Ltrain: 0.003106 	Lval: 0.003735
Epoch: 60 	Ltrain: 0.003057 	Lval: 0.003718
Epoch: 65 	Ltrain: 0.002972 	Lval: 0.003713
Epoch: 70 	Ltrain: 0.003089 	Lval: 0.003696
Epoch 00073: reducing learning rate of group 0 to 3.3260e-06.
Epoch: 75 	Ltrain: 0.003190 	Lval: 0.003693
Epoch: 80 	Ltrain: 0.003245 	Lval: 0.003693
EarlyStopper: stopping at epoch 83 with best_val_loss = 0.003693


	Fold 3/5
Epoch: 1 	Ltrain: 0.017716 	Lval: 0.008571
Epoch: 5 	Ltrain: 0.004951 	Lval: 0.005209
Epoch: 10 	Ltrain: 0.003950 	Lval: 0.005043
Epoch: 15 	Ltrain: 0.003830 	Lval: 0.005242
Epoch: 20 	Ltrain: 0.003270 	Lval: 0.003940
Epoch: 25 	Ltrain: 0.003117 	Lval: 0.004017
Epoch 00027: reducing learning rate of group 0 to 3.3260e-04.
Epoch: 30 	Ltrain: 0.002580 	Lval: 0.003190
Epoch: 35 	Ltrain: 0.002631 	Lval: 0.003197
Epoch: 40 	Ltrain: 0.002571 	Lval: 0.003183
Epoch 00043: reducing learning rate of group 0 to 3.3260e-05.
Epoch: 45 	Ltrain: 0.002508 	Lval: 0.003143
Epoch: 50 	Ltrain: 0.002471 	Lval: 0.003133
EarlyStopper: stopping at epoch 53 with best_val_loss = 0.003126


	Fold 4/5
Epoch: 1 	Ltrain: 0.017247 	Lval: 0.008240
Epoch: 5 	Ltrain: 0.004682 	Lval: 0.006616
Epoch: 10 	Ltrain: 0.003849 	Lval: 0.004669
Epoch: 15 	Ltrain: 0.003352 	Lval: 0.004088
Epoch: 20 	Ltrain: 0.002956 	Lval: 0.003734
Epoch: 25 	Ltrain: 0.002668 	Lval: 0.003149
Epoch 00029: reducing learning rate of group 0 to 3.3260e-04.
Epoch: 30 	Ltrain: 0.002418 	Lval: 0.003100
Epoch: 35 	Ltrain: 0.002295 	Lval: 0.003075
Epoch: 40 	Ltrain: 0.002260 	Lval: 0.003007
Epoch: 45 	Ltrain: 0.002240 	Lval: 0.002954
Epoch: 50 	Ltrain: 0.002244 	Lval: 0.002931
Epoch: 55 	Ltrain: 0.002202 	Lval: 0.002921
Epoch: 60 	Ltrain: 0.002211 	Lval: 0.002903
Epoch: 65 	Ltrain: 0.002139 	Lval: 0.002886
Epoch: 70 	Ltrain: 0.002129 	Lval: 0.002763
Epoch: 75 	Ltrain: 0.002093 	Lval: 0.002801
Epoch 00078: reducing learning rate of group 0 to 3.3260e-05.
Epoch: 80 	Ltrain: 0.002048 	Lval: 0.002732
Epoch: 85 	Ltrain: 0.002022 	Lval: 0.002708
Epoch: 90 	Ltrain: 0.002108 	Lval: 0.002716
Epoch 00095: reducing learning rate of group 0 to 3.3260e-06.
Epoch: 95 	Ltrain: 0.002023 	Lval: 0.002707
Epoch: 100 	Ltrain: 0.002016 	Lval: 0.002704
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.002708


	Fold 5/5
Epoch: 1 	Ltrain: 0.019026 	Lval: 0.010473
Epoch: 5 	Ltrain: 0.005151 	Lval: 0.006163
Epoch: 10 	Ltrain: 0.003921 	Lval: 0.004647
Epoch: 15 	Ltrain: 0.003451 	Lval: 0.004526
Epoch: 20 	Ltrain: 0.002770 	Lval: 0.004074
Epoch: 25 	Ltrain: 0.002752 	Lval: 0.003435
Epoch: 30 	Ltrain: 0.002954 	Lval: 0.004889
Epoch 00031: reducing learning rate of group 0 to 3.3260e-04.
Epoch: 35 	Ltrain: 0.002289 	Lval: 0.003039
Epoch: 40 	Ltrain: 0.002267 	Lval: 0.002989
Epoch: 45 	Ltrain: 0.002227 	Lval: 0.002966
Epoch: 50 	Ltrain: 0.002273 	Lval: 0.002891
Epoch 00053: reducing learning rate of group 0 to 3.3260e-05.
Epoch: 55 	Ltrain: 0.002149 	Lval: 0.002864
Epoch: 60 	Ltrain: 0.002145 	Lval: 0.002858
Epoch: 65 	Ltrain: 0.002148 	Lval: 0.002860
Epoch: 70 	Ltrain: 0.002202 	Lval: 0.002857
Epoch: 75 	Ltrain: 0.002162 	Lval: 0.002846
Epoch: 80 	Ltrain: 0.002152 	Lval: 0.002833
Epoch 00084: reducing learning rate of group 0 to 3.3260e-06.
Epoch: 85 	Ltrain: 0.002125 	Lval: 0.002841
Epoch: 90 	Ltrain: 0.002126 	Lval: 0.002839
EarlyStopper: stopping at epoch 91 with best_val_loss = 0.002838

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0039041805229986393
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 5.192301432873277e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 14
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.025002 	Lval: 0.011654
Epoch: 5 	Ltrain: 0.006482 	Lval: 0.006273
Epoch: 10 	Ltrain: 0.005799 	Lval: 0.005589
Epoch: 15 	Ltrain: 0.004926 	Lval: 0.004925
Epoch: 20 	Ltrain: 0.005251 	Lval: 0.005121
Epoch 00021: reducing learning rate of group 0 to 3.9042e-04.
Epoch: 25 	Ltrain: 0.004790 	Lval: 0.004453
Epoch: 30 	Ltrain: 0.004461 	Lval: 0.004469
Epoch: 35 	Ltrain: 0.004270 	Lval: 0.004284
Epoch: 40 	Ltrain: 0.004116 	Lval: 0.004161
Epoch: 45 	Ltrain: 0.004069 	Lval: 0.004091
Epoch: 50 	Ltrain: 0.004284 	Lval: 0.004024
Epoch 00053: reducing learning rate of group 0 to 3.9042e-05.
Epoch: 55 	Ltrain: 0.003773 	Lval: 0.003936
Epoch: 60 	Ltrain: 0.003812 	Lval: 0.003935
Epoch: 65 	Ltrain: 0.003935 	Lval: 0.003929
Epoch: 70 	Ltrain: 0.003910 	Lval: 0.003918
Epoch: 75 	Ltrain: 0.003670 	Lval: 0.003905
Epoch 00078: reducing learning rate of group 0 to 3.9042e-06.
Epoch: 80 	Ltrain: 0.003866 	Lval: 0.003907
Epoch: 85 	Ltrain: 0.003812 	Lval: 0.003904
EarlyStopper: stopping at epoch 87 with best_val_loss = 0.003904


	Fold 2/5
Epoch: 1 	Ltrain: 0.023128 	Lval: 0.010888
Epoch: 5 	Ltrain: 0.007651 	Lval: 0.006773
Epoch: 10 	Ltrain: 0.004523 	Lval: 0.004578
Epoch: 15 	Ltrain: 0.003930 	Lval: 0.004402
Epoch: 20 	Ltrain: 0.003915 	Lval: 0.004162
Epoch: 25 	Ltrain: 0.003180 	Lval: 0.003867
Epoch: 30 	Ltrain: 0.002998 	Lval: 0.003534
Epoch: 35 	Ltrain: 0.002636 	Lval: 0.003081
Epoch: 40 	Ltrain: 0.002727 	Lval: 0.003826
Epoch: 45 	Ltrain: 0.002641 	Lval: 0.002753
Epoch: 50 	Ltrain: 0.002401 	Lval: 0.002853
Epoch: 55 	Ltrain: 0.002445 	Lval: 0.002586
Epoch 00056: reducing learning rate of group 0 to 3.9042e-04.
Epoch: 60 	Ltrain: 0.001712 	Lval: 0.001911
Epoch: 65 	Ltrain: 0.001647 	Lval: 0.001780
Epoch: 70 	Ltrain: 0.001527 	Lval: 0.001655
Epoch: 75 	Ltrain: 0.001436 	Lval: 0.001562
Epoch: 80 	Ltrain: 0.001348 	Lval: 0.001474
Epoch: 85 	Ltrain: 0.001275 	Lval: 0.001364
Epoch: 90 	Ltrain: 0.001195 	Lval: 0.001305
Epoch: 95 	Ltrain: 0.001139 	Lval: 0.001180
Epoch: 100 	Ltrain: 0.001043 	Lval: 0.001100
Epoch: 105 	Ltrain: 0.000987 	Lval: 0.001035
Epoch: 110 	Ltrain: 0.000923 	Lval: 0.000965
Epoch: 115 	Ltrain: 0.000894 	Lval: 0.000918
Epoch: 120 	Ltrain: 0.000803 	Lval: 0.000846
Epoch: 125 	Ltrain: 0.000777 	Lval: 0.000792
Epoch: 130 	Ltrain: 0.000703 	Lval: 0.000728
Epoch: 135 	Ltrain: 0.000666 	Lval: 0.000701
Epoch 00138: reducing learning rate of group 0 to 3.9042e-05.
Epoch: 140 	Ltrain: 0.000601 	Lval: 0.000630
Epoch: 145 	Ltrain: 0.000577 	Lval: 0.000615
Epoch: 150 	Ltrain: 0.000566 	Lval: 0.000607
Epoch: 155 	Ltrain: 0.000570 	Lval: 0.000603
Epoch: 160 	Ltrain: 0.000553 	Lval: 0.000597
Epoch: 165 	Ltrain: 0.000549 	Lval: 0.000592
Epoch: 170 	Ltrain: 0.000546 	Lval: 0.000586
Epoch: 175 	Ltrain: 0.000538 	Lval: 0.000581
Epoch: 180 	Ltrain: 0.000544 	Lval: 0.000575
Epoch: 185 	Ltrain: 0.000532 	Lval: 0.000570
Epoch: 190 	Ltrain: 0.000523 	Lval: 0.000564
Epoch: 195 	Ltrain: 0.000519 	Lval: 0.000559
Epoch: 200 	Ltrain: 0.000513 	Lval: 0.000553
Epoch: 205 	Ltrain: 0.000506 	Lval: 0.000548
Epoch: 210 	Ltrain: 0.000514 	Lval: 0.000541
Epoch: 215 	Ltrain: 0.000506 	Lval: 0.000535
Epoch: 220 	Ltrain: 0.000486 	Lval: 0.000530
Epoch: 225 	Ltrain: 0.000487 	Lval: 0.000524
Epoch: 230 	Ltrain: 0.000488 	Lval: 0.000518
Epoch: 235 	Ltrain: 0.000474 	Lval: 0.000513
Epoch: 240 	Ltrain: 0.000474 	Lval: 0.000507
Epoch: 245 	Ltrain: 0.000464 	Lval: 0.000502
Epoch: 250 	Ltrain: 0.000467 	Lval: 0.000495
Epoch: 255 	Ltrain: 0.000453 	Lval: 0.000491
Epoch: 260 	Ltrain: 0.000452 	Lval: 0.000485
Epoch: 265 	Ltrain: 0.000446 	Lval: 0.000480
Epoch: 270 	Ltrain: 0.000439 	Lval: 0.000473
Epoch: 275 	Ltrain: 0.000438 	Lval: 0.000467
Epoch: 280 	Ltrain: 0.000432 	Lval: 0.000462
Epoch: 285 	Ltrain: 0.000426 	Lval: 0.000457
Epoch: 290 	Ltrain: 0.000426 	Lval: 0.000450
Epoch: 295 	Ltrain: 0.000406 	Lval: 0.000446
Epoch: 300 	Ltrain: 0.000415 	Lval: 0.000441
Epoch: 305 	Ltrain: 0.000406 	Lval: 0.000436
Epoch: 310 	Ltrain: 0.000398 	Lval: 0.000430
Epoch: 315 	Ltrain: 0.000392 	Lval: 0.000424
Epoch: 320 	Ltrain: 0.000382 	Lval: 0.000421
Epoch: 325 	Ltrain: 0.000384 	Lval: 0.000414
Epoch: 330 	Ltrain: 0.000376 	Lval: 0.000408
Epoch: 335 	Ltrain: 0.000373 	Lval: 0.000404
Epoch: 340 	Ltrain: 0.000365 	Lval: 0.000397
Epoch: 345 	Ltrain: 0.000369 	Lval: 0.000394
Epoch: 350 	Ltrain: 0.000353 	Lval: 0.000389
Epoch: 355 	Ltrain: 0.000349 	Lval: 0.000383
Epoch: 360 	Ltrain: 0.000351 	Lval: 0.000377
Epoch: 365 	Ltrain: 0.000337 	Lval: 0.000373
Epoch: 370 	Ltrain: 0.000333 	Lval: 0.000368
Epoch: 375 	Ltrain: 0.000329 	Lval: 0.000363
Epoch: 380 	Ltrain: 0.000322 	Lval: 0.000357
Epoch: 385 	Ltrain: 0.000330 	Lval: 0.000353
Epoch: 390 	Ltrain: 0.000317 	Lval: 0.000349
Epoch: 395 	Ltrain: 0.000313 	Lval: 0.000343
Epoch: 400 	Ltrain: 0.000309 	Lval: 0.000339
Epoch: 405 	Ltrain: 0.000308 	Lval: 0.000337
EarlyStopper: stopping at epoch 408 with best_val_loss = 0.000343


	Fold 3/5
Epoch: 1 	Ltrain: 0.020202 	Lval: 0.009064
Epoch: 5 	Ltrain: 0.005165 	Lval: 0.004889
Epoch: 10 	Ltrain: 0.004279 	Lval: 0.004692
Epoch: 15 	Ltrain: 0.003547 	Lval: 0.003792
Epoch: 20 	Ltrain: 0.003291 	Lval: 0.003530
Epoch: 25 	Ltrain: 0.002789 	Lval: 0.003841
Epoch 00026: reducing learning rate of group 0 to 3.9042e-04.
Epoch: 30 	Ltrain: 0.002249 	Lval: 0.002841
Epoch: 35 	Ltrain: 0.002206 	Lval: 0.002867
Epoch: 40 	Ltrain: 0.002147 	Lval: 0.002723
Epoch 00045: reducing learning rate of group 0 to 3.9042e-05.
Epoch: 45 	Ltrain: 0.002115 	Lval: 0.002662
Epoch: 50 	Ltrain: 0.002057 	Lval: 0.002632
Epoch: 55 	Ltrain: 0.002042 	Lval: 0.002623
Epoch: 60 	Ltrain: 0.002042 	Lval: 0.002612
Epoch: 65 	Ltrain: 0.002037 	Lval: 0.002606
Epoch: 70 	Ltrain: 0.002020 	Lval: 0.002592
Epoch 00074: reducing learning rate of group 0 to 3.9042e-06.
Epoch: 75 	Ltrain: 0.002007 	Lval: 0.002595
Epoch: 80 	Ltrain: 0.002007 	Lval: 0.002588
EarlyStopper: stopping at epoch 83 with best_val_loss = 0.002592


	Fold 4/5
Epoch: 1 	Ltrain: 0.014794 	Lval: 0.007750
Epoch: 5 	Ltrain: 0.004463 	Lval: 0.005996
Epoch: 10 	Ltrain: 0.003693 	Lval: 0.004497
Epoch: 15 	Ltrain: 0.003091 	Lval: 0.003783
Epoch 00018: reducing learning rate of group 0 to 3.9042e-04.
Epoch: 20 	Ltrain: 0.002399 	Lval: 0.003248
Epoch: 25 	Ltrain: 0.002355 	Lval: 0.003036
Epoch: 30 	Ltrain: 0.002283 	Lval: 0.003016
Epoch: 35 	Ltrain: 0.002256 	Lval: 0.002982
Epoch 00040: reducing learning rate of group 0 to 3.9042e-05.
Epoch: 40 	Ltrain: 0.002181 	Lval: 0.002882
Epoch: 45 	Ltrain: 0.002116 	Lval: 0.002829
Epoch: 50 	Ltrain: 0.002114 	Lval: 0.002823
Epoch: 55 	Ltrain: 0.002096 	Lval: 0.002819
Epoch: 60 	Ltrain: 0.002090 	Lval: 0.002798
Epoch: 65 	Ltrain: 0.002079 	Lval: 0.002783
Epoch: 70 	Ltrain: 0.002079 	Lval: 0.002793
Epoch: 75 	Ltrain: 0.002063 	Lval: 0.002760
Epoch 00076: reducing learning rate of group 0 to 3.9042e-06.
Epoch: 80 	Ltrain: 0.002051 	Lval: 0.002755
EarlyStopper: stopping at epoch 82 with best_val_loss = 0.002759


	Fold 5/5
Epoch: 1 	Ltrain: 0.015765 	Lval: 0.006774
Epoch: 5 	Ltrain: 0.004314 	Lval: 0.004857
Epoch: 10 	Ltrain: 0.003261 	Lval: 0.004066
Epoch: 15 	Ltrain: 0.003333 	Lval: 0.004238
Epoch: 20 	Ltrain: 0.002812 	Lval: 0.003731
Epoch 00022: reducing learning rate of group 0 to 3.9042e-04.
Epoch: 25 	Ltrain: 0.002284 	Lval: 0.003041
Epoch: 30 	Ltrain: 0.002223 	Lval: 0.002963
Epoch: 35 	Ltrain: 0.002188 	Lval: 0.002890
Epoch: 40 	Ltrain: 0.002152 	Lval: 0.002878
Epoch: 45 	Ltrain: 0.002094 	Lval: 0.002761
Epoch: 50 	Ltrain: 0.002054 	Lval: 0.002752
Epoch: 55 	Ltrain: 0.001997 	Lval: 0.002808
Epoch: 60 	Ltrain: 0.001967 	Lval: 0.002587
Epoch 00062: reducing learning rate of group 0 to 3.9042e-05.
Epoch: 65 	Ltrain: 0.001864 	Lval: 0.002527
Epoch: 70 	Ltrain: 0.001844 	Lval: 0.002488
Epoch: 75 	Ltrain: 0.001841 	Lval: 0.002490
Epoch: 80 	Ltrain: 0.001841 	Lval: 0.002474
Epoch: 85 	Ltrain: 0.001826 	Lval: 0.002473
Epoch: 90 	Ltrain: 0.001813 	Lval: 0.002435
Epoch 00094: reducing learning rate of group 0 to 3.9042e-06.
Epoch: 95 	Ltrain: 0.001800 	Lval: 0.002432
Epoch: 100 	Ltrain: 0.001796 	Lval: 0.002433
EarlyStopper: stopping at epoch 103 with best_val_loss = 0.002435

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0027916491392573316
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 3.526601570716035e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 12
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.020140 	Lval: 0.010332
Epoch: 5 	Ltrain: 0.007377 	Lval: 0.007955
Epoch: 10 	Ltrain: 0.005730 	Lval: 0.005282
Epoch: 15 	Ltrain: 0.005581 	Lval: 0.004795
Epoch: 20 	Ltrain: 0.004723 	Lval: 0.005998
Epoch 00022: reducing learning rate of group 0 to 2.7916e-04.
Epoch: 25 	Ltrain: 0.004173 	Lval: 0.004381
Epoch: 30 	Ltrain: 0.004127 	Lval: 0.004223
Epoch: 35 	Ltrain: 0.004078 	Lval: 0.004149
Epoch 00037: reducing learning rate of group 0 to 2.7916e-05.
Epoch: 40 	Ltrain: 0.004402 	Lval: 0.004052
Epoch: 45 	Ltrain: 0.004076 	Lval: 0.004047
Epoch: 50 	Ltrain: 0.003855 	Lval: 0.004039
Epoch 00055: reducing learning rate of group 0 to 2.7916e-06.
Epoch: 55 	Ltrain: 0.004233 	Lval: 0.004039
Epoch: 60 	Ltrain: 0.003925 	Lval: 0.004038
EarlyStopper: stopping at epoch 61 with best_val_loss = 0.004039


	Fold 2/5
Epoch: 1 	Ltrain: 0.016348 	Lval: 0.009015
Epoch: 5 	Ltrain: 0.005875 	Lval: 0.006278
Epoch: 10 	Ltrain: 0.004448 	Lval: 0.004644
Epoch: 15 	Ltrain: 0.003951 	Lval: 0.004501
Epoch: 20 	Ltrain: 0.003238 	Lval: 0.004568
Epoch 00022: reducing learning rate of group 0 to 2.7916e-04.
Epoch: 25 	Ltrain: 0.002785 	Lval: 0.003357
Epoch: 30 	Ltrain: 0.002725 	Lval: 0.003269
Epoch: 35 	Ltrain: 0.002681 	Lval: 0.003178
Epoch: 40 	Ltrain: 0.002630 	Lval: 0.003166
Epoch: 45 	Ltrain: 0.002641 	Lval: 0.003125
Epoch: 50 	Ltrain: 0.002581 	Lval: 0.003059
Epoch: 55 	Ltrain: 0.002563 	Lval: 0.002966
Epoch: 60 	Ltrain: 0.002527 	Lval: 0.002942
Epoch 00062: reducing learning rate of group 0 to 2.7916e-05.
Epoch: 65 	Ltrain: 0.002431 	Lval: 0.002872
Epoch: 70 	Ltrain: 0.002399 	Lval: 0.002866
Epoch: 75 	Ltrain: 0.002447 	Lval: 0.002852
Epoch: 80 	Ltrain: 0.002387 	Lval: 0.002845
Epoch: 85 	Ltrain: 0.002362 	Lval: 0.002827
Epoch: 90 	Ltrain: 0.002443 	Lval: 0.002814
Epoch: 95 	Ltrain: 0.002479 	Lval: 0.002824
Epoch: 100 	Ltrain: 0.002383 	Lval: 0.002798
Epoch 00101: reducing learning rate of group 0 to 2.7916e-06.
Epoch: 105 	Ltrain: 0.002447 	Lval: 0.002796
EarlyStopper: stopping at epoch 107 with best_val_loss = 0.002802


	Fold 3/5
Epoch: 1 	Ltrain: 0.014209 	Lval: 0.008378
Epoch: 5 	Ltrain: 0.004808 	Lval: 0.004820
Epoch: 10 	Ltrain: 0.003941 	Lval: 0.004685
Epoch: 15 	Ltrain: 0.003222 	Lval: 0.003603
Epoch: 20 	Ltrain: 0.003159 	Lval: 0.003793
Epoch: 25 	Ltrain: 0.002947 	Lval: 0.003309
Epoch 00027: reducing learning rate of group 0 to 2.7916e-04.
Epoch: 30 	Ltrain: 0.002323 	Lval: 0.002949
Epoch: 35 	Ltrain: 0.002242 	Lval: 0.002902
Epoch: 40 	Ltrain: 0.002223 	Lval: 0.002851
Epoch: 45 	Ltrain: 0.002209 	Lval: 0.002788
Epoch: 50 	Ltrain: 0.002153 	Lval: 0.002707
Epoch: 55 	Ltrain: 0.002080 	Lval: 0.002609
Epoch: 60 	Ltrain: 0.002050 	Lval: 0.002558
Epoch: 65 	Ltrain: 0.001995 	Lval: 0.002461
Epoch: 70 	Ltrain: 0.001962 	Lval: 0.002555
Epoch 00072: reducing learning rate of group 0 to 2.7916e-05.
Epoch: 75 	Ltrain: 0.001816 	Lval: 0.002363
Epoch: 80 	Ltrain: 0.001803 	Lval: 0.002337
Epoch: 85 	Ltrain: 0.001792 	Lval: 0.002313
Epoch: 90 	Ltrain: 0.001797 	Lval: 0.002296
Epoch: 95 	Ltrain: 0.001779 	Lval: 0.002293
Epoch: 100 	Ltrain: 0.001753 	Lval: 0.002266
Epoch: 105 	Ltrain: 0.001755 	Lval: 0.002249
Epoch: 110 	Ltrain: 0.001765 	Lval: 0.002242
Epoch: 115 	Ltrain: 0.001745 	Lval: 0.002228
Epoch: 120 	Ltrain: 0.001723 	Lval: 0.002211
Epoch: 125 	Ltrain: 0.001730 	Lval: 0.002205
Epoch: 130 	Ltrain: 0.001717 	Lval: 0.002187
Epoch: 135 	Ltrain: 0.001728 	Lval: 0.002167
Epoch: 140 	Ltrain: 0.001683 	Lval: 0.002148
Epoch: 145 	Ltrain: 0.001672 	Lval: 0.002135
Epoch 00148: reducing learning rate of group 0 to 2.7916e-06.
Epoch: 150 	Ltrain: 0.001653 	Lval: 0.002122
Epoch: 155 	Ltrain: 0.001658 	Lval: 0.002118
EarlyStopper: stopping at epoch 155 with best_val_loss = 0.002125


	Fold 4/5
Epoch: 1 	Ltrain: 0.013973 	Lval: 0.007944
Epoch: 5 	Ltrain: 0.004187 	Lval: 0.004624
Epoch: 10 	Ltrain: 0.003797 	Lval: 0.004961
Epoch: 15 	Ltrain: 0.003196 	Lval: 0.004073
Epoch: 20 	Ltrain: 0.002872 	Lval: 0.003280
Epoch: 25 	Ltrain: 0.002782 	Lval: 0.003665
Epoch 00027: reducing learning rate of group 0 to 2.7916e-04.
Epoch: 30 	Ltrain: 0.002233 	Lval: 0.002942
Epoch: 35 	Ltrain: 0.002134 	Lval: 0.002885
Epoch: 40 	Ltrain: 0.002079 	Lval: 0.002803
Epoch: 45 	Ltrain: 0.002031 	Lval: 0.002702
Epoch: 50 	Ltrain: 0.001997 	Lval: 0.002623
Epoch: 55 	Ltrain: 0.001940 	Lval: 0.002644
Epoch: 60 	Ltrain: 0.001901 	Lval: 0.002577
Epoch: 65 	Ltrain: 0.001852 	Lval: 0.002508
Epoch 00067: reducing learning rate of group 0 to 2.7916e-05.
Epoch: 70 	Ltrain: 0.001752 	Lval: 0.002340
Epoch: 75 	Ltrain: 0.001741 	Lval: 0.002339
Epoch: 80 	Ltrain: 0.001733 	Lval: 0.002330
Epoch 00081: reducing learning rate of group 0 to 2.7916e-06.
Epoch: 85 	Ltrain: 0.001723 	Lval: 0.002316
EarlyStopper: stopping at epoch 88 with best_val_loss = 0.002312


	Fold 5/5
Epoch: 1 	Ltrain: 0.012769 	Lval: 0.008259
Epoch: 5 	Ltrain: 0.004429 	Lval: 0.004964
Epoch: 10 	Ltrain: 0.003252 	Lval: 0.003947
Epoch: 15 	Ltrain: 0.003037 	Lval: 0.004278
Epoch 00018: reducing learning rate of group 0 to 2.7916e-04.
Epoch: 20 	Ltrain: 0.002513 	Lval: 0.003402
Epoch: 25 	Ltrain: 0.002411 	Lval: 0.003256
Epoch: 30 	Ltrain: 0.002363 	Lval: 0.003184
Epoch: 35 	Ltrain: 0.002322 	Lval: 0.003273
Epoch 00038: reducing learning rate of group 0 to 2.7916e-05.
Epoch: 40 	Ltrain: 0.002263 	Lval: 0.003075
Epoch: 45 	Ltrain: 0.002251 	Lval: 0.003063
Epoch 00050: reducing learning rate of group 0 to 2.7916e-06.
Epoch: 50 	Ltrain: 0.002241 	Lval: 0.003066
EarlyStopper: stopping at epoch 50 with best_val_loss = 0.003062

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007599283466262749
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 7.753337189411605e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.100316 	Lval: 0.025366
Epoch: 5 	Ltrain: 0.008700 	Lval: 0.006980
Epoch: 10 	Ltrain: 0.005029 	Lval: 0.006638
Epoch: 15 	Ltrain: 0.005225 	Lval: 0.005683
Epoch: 20 	Ltrain: 0.004544 	Lval: 0.004150
Epoch 00024: reducing learning rate of group 0 to 7.5993e-04.
Epoch: 25 	Ltrain: 0.003845 	Lval: 0.003864
Epoch: 30 	Ltrain: 0.003508 	Lval: 0.003673
Epoch: 35 	Ltrain: 0.003707 	Lval: 0.003636
Epoch: 40 	Ltrain: 0.003404 	Lval: 0.003513
Epoch: 45 	Ltrain: 0.003343 	Lval: 0.003452
Epoch: 50 	Ltrain: 0.003329 	Lval: 0.003416
Epoch: 55 	Ltrain: 0.003348 	Lval: 0.003345
Epoch: 60 	Ltrain: 0.003193 	Lval: 0.003202
Epoch: 65 	Ltrain: 0.003155 	Lval: 0.003112
Epoch: 70 	Ltrain: 0.002980 	Lval: 0.002987
Epoch: 75 	Ltrain: 0.003006 	Lval: 0.002971
Epoch: 80 	Ltrain: 0.002785 	Lval: 0.002821
Epoch: 85 	Ltrain: 0.003051 	Lval: 0.002963
Epoch 00086: reducing learning rate of group 0 to 7.5993e-05.
Epoch: 90 	Ltrain: 0.002588 	Lval: 0.002658
Epoch: 95 	Ltrain: 0.002533 	Lval: 0.002636
Epoch: 100 	Ltrain: 0.002528 	Lval: 0.002614
Epoch: 105 	Ltrain: 0.002576 	Lval: 0.002603
Epoch: 110 	Ltrain: 0.002473 	Lval: 0.002587
Epoch: 115 	Ltrain: 0.002498 	Lval: 0.002618
Epoch: 120 	Ltrain: 0.002509 	Lval: 0.002565
Epoch: 125 	Ltrain: 0.002509 	Lval: 0.002541
Epoch: 130 	Ltrain: 0.002563 	Lval: 0.002534
Epoch: 135 	Ltrain: 0.002444 	Lval: 0.002512
Epoch: 140 	Ltrain: 0.002789 	Lval: 0.002492
Epoch: 145 	Ltrain: 0.002545 	Lval: 0.002471
Epoch: 150 	Ltrain: 0.002399 	Lval: 0.002455
Epoch: 155 	Ltrain: 0.002643 	Lval: 0.002443
Epoch: 160 	Ltrain: 0.002368 	Lval: 0.002414
Epoch: 165 	Ltrain: 0.002449 	Lval: 0.002412
Epoch: 170 	Ltrain: 0.002472 	Lval: 0.002393
Epoch 00171: reducing learning rate of group 0 to 7.5993e-06.
Epoch: 175 	Ltrain: 0.002443 	Lval: 0.002381
Epoch: 180 	Ltrain: 0.002317 	Lval: 0.002374
Epoch: 185 	Ltrain: 0.002348 	Lval: 0.002370
Epoch: 190 	Ltrain: 0.002413 	Lval: 0.002369
Epoch: 195 	Ltrain: 0.002302 	Lval: 0.002368
Epoch: 200 	Ltrain: 0.002451 	Lval: 0.002364
Epoch: 205 	Ltrain: 0.002358 	Lval: 0.002367
Epoch 00207: reducing learning rate of group 0 to 7.5993e-07.
Epoch: 210 	Ltrain: 0.002299 	Lval: 0.002363
Epoch: 215 	Ltrain: 0.002606 	Lval: 0.002363
Epoch 00219: reducing learning rate of group 0 to 7.5993e-08.
Epoch: 220 	Ltrain: 0.002324 	Lval: 0.002362
EarlyStopper: stopping at epoch 219 with best_val_loss = 0.002366


	Fold 2/5
Epoch: 1 	Ltrain: 0.063375 	Lval: 0.010046
Epoch: 5 	Ltrain: 0.005648 	Lval: 0.005886
Epoch: 10 	Ltrain: 0.004634 	Lval: 0.005624
Epoch: 15 	Ltrain: 0.003880 	Lval: 0.004538
Epoch: 20 	Ltrain: 0.003375 	Lval: 0.004341
Epoch: 25 	Ltrain: 0.003208 	Lval: 0.003926
Epoch 00026: reducing learning rate of group 0 to 7.5993e-04.
Epoch: 30 	Ltrain: 0.002564 	Lval: 0.003081
Epoch: 35 	Ltrain: 0.002501 	Lval: 0.002988
Epoch: 40 	Ltrain: 0.002441 	Lval: 0.002899
Epoch: 45 	Ltrain: 0.002503 	Lval: 0.002827
Epoch: 50 	Ltrain: 0.002254 	Lval: 0.002688
Epoch: 55 	Ltrain: 0.002223 	Lval: 0.002548
Epoch: 60 	Ltrain: 0.002172 	Lval: 0.002466
Epoch: 65 	Ltrain: 0.002078 	Lval: 0.002304
Epoch: 70 	Ltrain: 0.001916 	Lval: 0.002139
Epoch: 75 	Ltrain: 0.001885 	Lval: 0.002082
Epoch: 80 	Ltrain: 0.001732 	Lval: 0.001900
Epoch: 85 	Ltrain: 0.001628 	Lval: 0.001827
Epoch: 90 	Ltrain: 0.001564 	Lval: 0.001673
Epoch: 95 	Ltrain: 0.001397 	Lval: 0.001548
Epoch: 100 	Ltrain: 0.001278 	Lval: 0.001395
Epoch: 105 	Ltrain: 0.001195 	Lval: 0.001320
Epoch: 110 	Ltrain: 0.001110 	Lval: 0.001151
Epoch: 115 	Ltrain: 0.001025 	Lval: 0.001110
Epoch: 120 	Ltrain: 0.000986 	Lval: 0.001005
Epoch: 125 	Ltrain: 0.000855 	Lval: 0.000914
Epoch: 130 	Ltrain: 0.000812 	Lval: 0.000881
Epoch: 135 	Ltrain: 0.000775 	Lval: 0.000786
Epoch 00139: reducing learning rate of group 0 to 7.5993e-05.
Epoch: 140 	Ltrain: 0.000734 	Lval: 0.000729
Epoch: 145 	Ltrain: 0.000624 	Lval: 0.000680
Epoch: 150 	Ltrain: 0.000614 	Lval: 0.000668
Epoch: 155 	Ltrain: 0.000601 	Lval: 0.000658
Epoch: 160 	Ltrain: 0.000599 	Lval: 0.000651
Epoch: 165 	Ltrain: 0.000589 	Lval: 0.000643
Epoch: 170 	Ltrain: 0.000587 	Lval: 0.000636
Epoch: 175 	Ltrain: 0.000579 	Lval: 0.000629
Epoch: 180 	Ltrain: 0.000572 	Lval: 0.000622
Epoch: 185 	Ltrain: 0.000567 	Lval: 0.000614
Epoch: 190 	Ltrain: 0.000558 	Lval: 0.000607
Epoch: 195 	Ltrain: 0.000555 	Lval: 0.000600
Epoch: 200 	Ltrain: 0.000551 	Lval: 0.000592
Epoch: 205 	Ltrain: 0.000540 	Lval: 0.000584
Epoch: 210 	Ltrain: 0.000536 	Lval: 0.000576
Epoch: 215 	Ltrain: 0.000532 	Lval: 0.000568
Epoch: 220 	Ltrain: 0.000514 	Lval: 0.000561
Epoch: 225 	Ltrain: 0.000513 	Lval: 0.000552
Epoch: 230 	Ltrain: 0.000505 	Lval: 0.000546
Epoch: 235 	Ltrain: 0.000493 	Lval: 0.000536
Epoch: 240 	Ltrain: 0.000490 	Lval: 0.000530
Epoch: 245 	Ltrain: 0.000478 	Lval: 0.000522
Epoch: 250 	Ltrain: 0.000475 	Lval: 0.000515
Epoch: 255 	Ltrain: 0.000465 	Lval: 0.000506
Epoch: 260 	Ltrain: 0.000461 	Lval: 0.000497
Epoch: 265 	Ltrain: 0.000448 	Lval: 0.000490
Epoch: 270 	Ltrain: 0.000448 	Lval: 0.000483
Epoch: 275 	Ltrain: 0.000438 	Lval: 0.000474
Epoch: 280 	Ltrain: 0.000427 	Lval: 0.000466
Epoch: 285 	Ltrain: 0.000428 	Lval: 0.000458
Epoch: 290 	Ltrain: 0.000411 	Lval: 0.000451
Epoch: 295 	Ltrain: 0.000404 	Lval: 0.000443
Epoch: 300 	Ltrain: 0.000401 	Lval: 0.000436
Epoch: 305 	Ltrain: 0.000393 	Lval: 0.000428
Epoch: 310 	Ltrain: 0.000385 	Lval: 0.000421
Epoch: 315 	Ltrain: 0.000376 	Lval: 0.000414
Epoch: 320 	Ltrain: 0.000371 	Lval: 0.000408
Epoch: 325 	Ltrain: 0.000368 	Lval: 0.000408
Epoch: 330 	Ltrain: 0.000357 	Lval: 0.000395
Epoch: 335 	Ltrain: 0.000352 	Lval: 0.000387
Epoch: 340 	Ltrain: 0.000344 	Lval: 0.000381
Epoch: 345 	Ltrain: 0.000332 	Lval: 0.000373
Epoch: 350 	Ltrain: 0.000325 	Lval: 0.000367
Epoch: 355 	Ltrain: 0.000323 	Lval: 0.000363
Epoch: 360 	Ltrain: 0.000318 	Lval: 0.000354
Epoch: 365 	Ltrain: 0.000307 	Lval: 0.000347
Epoch: 370 	Ltrain: 0.000304 	Lval: 0.000342
Epoch: 375 	Ltrain: 0.000299 	Lval: 0.000335
Epoch: 380 	Ltrain: 0.000297 	Lval: 0.000333
Epoch: 385 	Ltrain: 0.000291 	Lval: 0.000328
Epoch 00390: reducing learning rate of group 0 to 7.5993e-06.
Epoch: 390 	Ltrain: 0.000302 	Lval: 0.000328
Epoch: 395 	Ltrain: 0.000274 	Lval: 0.000313
Epoch: 400 	Ltrain: 0.000275 	Lval: 0.000312
Epoch: 405 	Ltrain: 0.000268 	Lval: 0.000311
Epoch: 410 	Ltrain: 0.000270 	Lval: 0.000310
EarlyStopper: stopping at epoch 412 with best_val_loss = 0.000315


	Fold 3/5
Epoch: 1 	Ltrain: 0.093255 	Lval: 0.010314
Epoch: 5 	Ltrain: 0.004894 	Lval: 0.005033
Epoch: 10 	Ltrain: 0.004271 	Lval: 0.004966
Epoch: 15 	Ltrain: 0.003556 	Lval: 0.004266
Epoch: 20 	Ltrain: 0.003288 	Lval: 0.003592
Epoch: 25 	Ltrain: 0.002883 	Lval: 0.003709
Epoch: 30 	Ltrain: 0.002628 	Lval: 0.003034
Epoch: 35 	Ltrain: 0.002461 	Lval: 0.002678
Epoch 00039: reducing learning rate of group 0 to 7.5993e-04.
Epoch: 40 	Ltrain: 0.002091 	Lval: 0.002490
Epoch: 45 	Ltrain: 0.001879 	Lval: 0.002333
Epoch: 50 	Ltrain: 0.001830 	Lval: 0.002234
Epoch: 55 	Ltrain: 0.001779 	Lval: 0.002181
Epoch: 60 	Ltrain: 0.001763 	Lval: 0.002123
Epoch: 65 	Ltrain: 0.001695 	Lval: 0.002071
Epoch: 70 	Ltrain: 0.001645 	Lval: 0.001941
Epoch: 75 	Ltrain: 0.001604 	Lval: 0.001891
Epoch: 80 	Ltrain: 0.001559 	Lval: 0.001765
Epoch: 85 	Ltrain: 0.001489 	Lval: 0.001693
Epoch: 90 	Ltrain: 0.001435 	Lval: 0.001604
Epoch: 95 	Ltrain: 0.001425 	Lval: 0.001565
Epoch: 100 	Ltrain: 0.001346 	Lval: 0.001435
Epoch: 105 	Ltrain: 0.001249 	Lval: 0.001357
Epoch: 110 	Ltrain: 0.001185 	Lval: 0.001251
Epoch: 115 	Ltrain: 0.001127 	Lval: 0.001160
Epoch: 120 	Ltrain: 0.001067 	Lval: 0.001206
Epoch: 125 	Ltrain: 0.001000 	Lval: 0.001029
Epoch: 130 	Ltrain: 0.000951 	Lval: 0.000976
Epoch: 135 	Ltrain: 0.001000 	Lval: 0.000949
Epoch: 140 	Ltrain: 0.000855 	Lval: 0.000837
Epoch 00144: reducing learning rate of group 0 to 7.5993e-05.
Epoch: 145 	Ltrain: 0.000787 	Lval: 0.000769
Epoch: 150 	Ltrain: 0.000720 	Lval: 0.000739
Epoch: 155 	Ltrain: 0.000712 	Lval: 0.000731
Epoch: 160 	Ltrain: 0.000707 	Lval: 0.000722
Epoch: 165 	Ltrain: 0.000697 	Lval: 0.000714
Epoch: 170 	Ltrain: 0.000690 	Lval: 0.000708
Epoch: 175 	Ltrain: 0.000685 	Lval: 0.000699
Epoch: 180 	Ltrain: 0.000674 	Lval: 0.000692
Epoch: 185 	Ltrain: 0.000672 	Lval: 0.000683
Epoch: 190 	Ltrain: 0.000661 	Lval: 0.000677
Epoch: 195 	Ltrain: 0.000654 	Lval: 0.000669
Epoch: 200 	Ltrain: 0.000650 	Lval: 0.000662
Epoch: 205 	Ltrain: 0.000638 	Lval: 0.000656
Epoch: 210 	Ltrain: 0.000629 	Lval: 0.000646
Epoch: 215 	Ltrain: 0.000623 	Lval: 0.000638
Epoch: 220 	Ltrain: 0.000619 	Lval: 0.000628
Epoch: 225 	Ltrain: 0.000611 	Lval: 0.000618
Epoch: 230 	Ltrain: 0.000598 	Lval: 0.000611
Epoch: 235 	Ltrain: 0.000592 	Lval: 0.000602
Epoch: 240 	Ltrain: 0.000587 	Lval: 0.000594
Epoch: 245 	Ltrain: 0.000573 	Lval: 0.000583
Epoch: 250 	Ltrain: 0.000568 	Lval: 0.000574
Epoch: 255 	Ltrain: 0.000558 	Lval: 0.000566
Epoch: 260 	Ltrain: 0.000547 	Lval: 0.000555
Epoch: 265 	Ltrain: 0.000539 	Lval: 0.000547
Epoch: 270 	Ltrain: 0.000529 	Lval: 0.000536
Epoch: 275 	Ltrain: 0.000522 	Lval: 0.000533
Epoch: 280 	Ltrain: 0.000511 	Lval: 0.000518
Epoch: 285 	Ltrain: 0.000501 	Lval: 0.000509
Epoch: 290 	Ltrain: 0.000493 	Lval: 0.000501
Epoch: 295 	Ltrain: 0.000483 	Lval: 0.000493
Epoch: 300 	Ltrain: 0.000480 	Lval: 0.000485
Epoch: 305 	Ltrain: 0.000465 	Lval: 0.000470
Epoch: 310 	Ltrain: 0.000462 	Lval: 0.000464
Epoch: 315 	Ltrain: 0.000446 	Lval: 0.000452
Epoch: 320 	Ltrain: 0.000440 	Lval: 0.000445
Epoch: 325 	Ltrain: 0.000433 	Lval: 0.000435
Epoch: 330 	Ltrain: 0.000422 	Lval: 0.000426
Epoch: 335 	Ltrain: 0.000415 	Lval: 0.000422
Epoch: 340 	Ltrain: 0.000401 	Lval: 0.000409
Epoch: 345 	Ltrain: 0.000398 	Lval: 0.000400
Epoch: 350 	Ltrain: 0.000390 	Lval: 0.000389
Epoch: 355 	Ltrain: 0.000381 	Lval: 0.000383
Epoch: 360 	Ltrain: 0.000373 	Lval: 0.000383
Epoch: 365 	Ltrain: 0.000372 	Lval: 0.000372
Epoch: 370 	Ltrain: 0.000357 	Lval: 0.000358
Epoch 00374: reducing learning rate of group 0 to 7.5993e-06.
Epoch: 375 	Ltrain: 0.000346 	Lval: 0.000349
Epoch: 380 	Ltrain: 0.000339 	Lval: 0.000344
Epoch: 385 	Ltrain: 0.000338 	Lval: 0.000342
Epoch: 390 	Ltrain: 0.000337 	Lval: 0.000341
Epoch: 395 	Ltrain: 0.000336 	Lval: 0.000340
EarlyStopper: stopping at epoch 395 with best_val_loss = 0.000349


	Fold 4/5
Epoch: 1 	Ltrain: 0.050648 	Lval: 0.009779
Epoch: 5 	Ltrain: 0.004339 	Lval: 0.004914
Epoch: 10 	Ltrain: 0.003753 	Lval: 0.004417
Epoch: 15 	Ltrain: 0.003298 	Lval: 0.004104
Epoch: 20 	Ltrain: 0.002838 	Lval: 0.003628
Epoch: 25 	Ltrain: 0.002572 	Lval: 0.003357
Epoch: 30 	Ltrain: 0.002528 	Lval: 0.003742
Epoch: 35 	Ltrain: 0.002195 	Lval: 0.002692
Epoch: 40 	Ltrain: 0.002300 	Lval: 0.002795
Epoch: 45 	Ltrain: 0.001982 	Lval: 0.002224
Epoch: 50 	Ltrain: 0.001817 	Lval: 0.002103
Epoch: 55 	Ltrain: 0.001672 	Lval: 0.001711
Epoch: 60 	Ltrain: 0.001544 	Lval: 0.001570
Epoch: 65 	Ltrain: 0.001346 	Lval: 0.001506
Epoch: 70 	Ltrain: 0.001079 	Lval: 0.001081
Epoch: 75 	Ltrain: 0.001113 	Lval: 0.001105
Epoch 00076: reducing learning rate of group 0 to 7.5993e-04.
Epoch: 80 	Ltrain: 0.000607 	Lval: 0.000635
Epoch: 85 	Ltrain: 0.000537 	Lval: 0.000559
Epoch: 90 	Ltrain: 0.000494 	Lval: 0.000511
Epoch: 95 	Ltrain: 0.000458 	Lval: 0.000470
Epoch: 100 	Ltrain: 0.000425 	Lval: 0.000434
Epoch: 105 	Ltrain: 0.000393 	Lval: 0.000398
Epoch: 110 	Ltrain: 0.000361 	Lval: 0.000366
Epoch: 115 	Ltrain: 0.000332 	Lval: 0.000333
Epoch: 120 	Ltrain: 0.000302 	Lval: 0.000301
Epoch: 125 	Ltrain: 0.000275 	Lval: 0.000271
Epoch: 130 	Ltrain: 0.000249 	Lval: 0.000244
Epoch: 135 	Ltrain: 0.000228 	Lval: 0.000220
Epoch: 140 	Ltrain: 0.000209 	Lval: 0.000202
Epoch: 145 	Ltrain: 0.000196 	Lval: 0.000201
Epoch 00150: reducing learning rate of group 0 to 7.5993e-05.
Epoch: 150 	Ltrain: 0.000192 	Lval: 0.000189
Epoch: 155 	Ltrain: 0.000157 	Lval: 0.000158
Epoch: 160 	Ltrain: 0.000153 	Lval: 0.000155
Epoch: 165 	Ltrain: 0.000150 	Lval: 0.000152
Epoch: 170 	Ltrain: 0.000148 	Lval: 0.000150
Epoch: 175 	Ltrain: 0.000146 	Lval: 0.000148
Epoch: 180 	Ltrain: 0.000144 	Lval: 0.000146
Epoch: 185 	Ltrain: 0.000142 	Lval: 0.000144
Epoch: 190 	Ltrain: 0.000140 	Lval: 0.000141
Epoch: 195 	Ltrain: 0.000138 	Lval: 0.000139
EarlyStopper: stopping at epoch 196 with best_val_loss = 0.000148


	Fold 5/5
Epoch: 1 	Ltrain: 0.043864 	Lval: 0.007452
Epoch: 5 	Ltrain: 0.004349 	Lval: 0.004730
Epoch: 10 	Ltrain: 0.003612 	Lval: 0.004891
Epoch: 15 	Ltrain: 0.003279 	Lval: 0.004007
Epoch: 20 	Ltrain: 0.002839 	Lval: 0.003852
Epoch: 25 	Ltrain: 0.002686 	Lval: 0.003097
Epoch: 30 	Ltrain: 0.002559 	Lval: 0.003015
Epoch 00031: reducing learning rate of group 0 to 7.5993e-04.
Epoch: 35 	Ltrain: 0.001829 	Lval: 0.002410
Epoch: 40 	Ltrain: 0.001780 	Lval: 0.002330
Epoch: 45 	Ltrain: 0.001717 	Lval: 0.002186
Epoch: 50 	Ltrain: 0.001675 	Lval: 0.002144
Epoch: 55 	Ltrain: 0.001643 	Lval: 0.002200
Epoch: 60 	Ltrain: 0.001550 	Lval: 0.001900
Epoch: 65 	Ltrain: 0.001505 	Lval: 0.001872
Epoch: 70 	Ltrain: 0.001404 	Lval: 0.001632
Epoch: 75 	Ltrain: 0.001337 	Lval: 0.001566
Epoch: 80 	Ltrain: 0.001246 	Lval: 0.001418
Epoch: 85 	Ltrain: 0.001160 	Lval: 0.001314
Epoch: 90 	Ltrain: 0.001071 	Lval: 0.001235
Epoch: 95 	Ltrain: 0.001000 	Lval: 0.001082
Epoch: 100 	Ltrain: 0.000934 	Lval: 0.000987
Epoch: 105 	Ltrain: 0.000822 	Lval: 0.000880
Epoch: 110 	Ltrain: 0.000764 	Lval: 0.000835
Epoch: 115 	Ltrain: 0.000727 	Lval: 0.000765
Epoch: 120 	Ltrain: 0.000737 	Lval: 0.000742
Epoch 00122: reducing learning rate of group 0 to 7.5993e-05.
Epoch: 125 	Ltrain: 0.000542 	Lval: 0.000581
Epoch: 130 	Ltrain: 0.000526 	Lval: 0.000563
Epoch: 135 	Ltrain: 0.000516 	Lval: 0.000552
Epoch: 140 	Ltrain: 0.000508 	Lval: 0.000545
Epoch: 145 	Ltrain: 0.000499 	Lval: 0.000532
Epoch: 150 	Ltrain: 0.000492 	Lval: 0.000523
Epoch: 155 	Ltrain: 0.000484 	Lval: 0.000513
Epoch: 160 	Ltrain: 0.000476 	Lval: 0.000504
Epoch: 165 	Ltrain: 0.000467 	Lval: 0.000496
Epoch: 170 	Ltrain: 0.000459 	Lval: 0.000484
Epoch: 175 	Ltrain: 0.000449 	Lval: 0.000474
Epoch: 180 	Ltrain: 0.000439 	Lval: 0.000464
Epoch: 185 	Ltrain: 0.000431 	Lval: 0.000453
Epoch: 190 	Ltrain: 0.000421 	Lval: 0.000441
Epoch: 195 	Ltrain: 0.000412 	Lval: 0.000430
Epoch: 200 	Ltrain: 0.000404 	Lval: 0.000421
Epoch: 205 	Ltrain: 0.000394 	Lval: 0.000410
Epoch: 210 	Ltrain: 0.000384 	Lval: 0.000402
Epoch: 215 	Ltrain: 0.000373 	Lval: 0.000388
Epoch: 220 	Ltrain: 0.000365 	Lval: 0.000376
Epoch: 225 	Ltrain: 0.000356 	Lval: 0.000368
Epoch: 230 	Ltrain: 0.000346 	Lval: 0.000358
Epoch: 235 	Ltrain: 0.000335 	Lval: 0.000346
Epoch: 240 	Ltrain: 0.000327 	Lval: 0.000340
Epoch: 245 	Ltrain: 0.000318 	Lval: 0.000329
Epoch: 250 	Ltrain: 0.000308 	Lval: 0.000318
Epoch: 255 	Ltrain: 0.000300 	Lval: 0.000306
Epoch: 260 	Ltrain: 0.000291 	Lval: 0.000300
Epoch: 265 	Ltrain: 0.000283 	Lval: 0.000288
Epoch: 270 	Ltrain: 0.000276 	Lval: 0.000282
Epoch: 275 	Ltrain: 0.000269 	Lval: 0.000275
Epoch: 280 	Ltrain: 0.000262 	Lval: 0.000268
Epoch: 285 	Ltrain: 0.000253 	Lval: 0.000256
Epoch: 290 	Ltrain: 0.000246 	Lval: 0.000247
Epoch: 295 	Ltrain: 0.000238 	Lval: 0.000241
Epoch: 300 	Ltrain: 0.000234 	Lval: 0.000239
Epoch: 305 	Ltrain: 0.000229 	Lval: 0.000230
Epoch: 310 	Ltrain: 0.000219 	Lval: 0.000221
Epoch: 315 	Ltrain: 0.000214 	Lval: 0.000215
Epoch: 320 	Ltrain: 0.000212 	Lval: 0.000213
Epoch: 325 	Ltrain: 0.000206 	Lval: 0.000207
Epoch: 330 	Ltrain: 0.000203 	Lval: 0.000205
Epoch 00332: reducing learning rate of group 0 to 7.5993e-06.
Epoch: 335 	Ltrain: 0.000188 	Lval: 0.000192
Epoch: 340 	Ltrain: 0.000186 	Lval: 0.000190
Epoch: 345 	Ltrain: 0.000185 	Lval: 0.000189
Epoch: 350 	Ltrain: 0.000185 	Lval: 0.000188
EarlyStopper: stopping at epoch 353 with best_val_loss = 0.000194

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.00011789199787394103
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.6565414905014397e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 20
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.070263 	Lval: 0.052498
Epoch: 5 	Ltrain: 0.011474 	Lval: 0.011330
Epoch: 10 	Ltrain: 0.011092 	Lval: 0.010327
Epoch: 15 	Ltrain: 0.010267 	Lval: 0.009943
Epoch: 20 	Ltrain: 0.009450 	Lval: 0.008996
Epoch: 25 	Ltrain: 0.007947 	Lval: 0.007623
Epoch: 30 	Ltrain: 0.007380 	Lval: 0.007265
Epoch: 35 	Ltrain: 0.006836 	Lval: 0.006800
Epoch: 40 	Ltrain: 0.006879 	Lval: 0.006648
Epoch: 45 	Ltrain: 0.006475 	Lval: 0.006387
Epoch: 50 	Ltrain: 0.006349 	Lval: 0.006550
Epoch: 55 	Ltrain: 0.006685 	Lval: 0.006097
Epoch: 60 	Ltrain: 0.006311 	Lval: 0.005933
Epoch: 65 	Ltrain: 0.005781 	Lval: 0.005810
Epoch: 70 	Ltrain: 0.006174 	Lval: 0.005731
Epoch: 75 	Ltrain: 0.005817 	Lval: 0.005560
Epoch: 80 	Ltrain: 0.005526 	Lval: 0.005452
Epoch: 85 	Ltrain: 0.005414 	Lval: 0.005522
Epoch: 90 	Ltrain: 0.005236 	Lval: 0.005244
Epoch 00095: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 95 	Ltrain: 0.005481 	Lval: 0.005609
Epoch: 100 	Ltrain: 0.005034 	Lval: 0.005164
Epoch: 105 	Ltrain: 0.005379 	Lval: 0.005179
Epoch 00108: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 110 	Ltrain: 0.004871 	Lval: 0.005167
Epoch: 115 	Ltrain: 0.005006 	Lval: 0.005155
Epoch 00120: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 120 	Ltrain: 0.005068 	Lval: 0.005154
EarlyStopper: stopping at epoch 123 with best_val_loss = 0.005153


	Fold 2/5
Epoch: 1 	Ltrain: 0.017323 	Lval: 0.010926
Epoch: 5 	Ltrain: 0.010690 	Lval: 0.010232
Epoch: 10 	Ltrain: 0.009187 	Lval: 0.008658
Epoch: 15 	Ltrain: 0.007301 	Lval: 0.007369
Epoch: 20 	Ltrain: 0.006625 	Lval: 0.006957
Epoch: 25 	Ltrain: 0.006207 	Lval: 0.006467
Epoch: 30 	Ltrain: 0.006007 	Lval: 0.006556
Epoch: 35 	Ltrain: 0.005599 	Lval: 0.005759
Epoch: 40 	Ltrain: 0.005434 	Lval: 0.005501
Epoch 00044: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 45 	Ltrain: 0.005271 	Lval: 0.005392
Epoch: 50 	Ltrain: 0.005020 	Lval: 0.005325
Epoch: 55 	Ltrain: 0.004943 	Lval: 0.005294
Epoch: 60 	Ltrain: 0.004906 	Lval: 0.005288
Epoch: 65 	Ltrain: 0.005008 	Lval: 0.005269
Epoch: 70 	Ltrain: 0.004945 	Lval: 0.005239
Epoch: 75 	Ltrain: 0.004912 	Lval: 0.005218
Epoch: 80 	Ltrain: 0.004950 	Lval: 0.005204
Epoch: 85 	Ltrain: 0.004823 	Lval: 0.005195
Epoch: 90 	Ltrain: 0.004785 	Lval: 0.005162
Epoch: 95 	Ltrain: 0.004800 	Lval: 0.005144
Epoch: 100 	Ltrain: 0.004840 	Lval: 0.005131
Epoch: 105 	Ltrain: 0.004789 	Lval: 0.005096
Epoch: 110 	Ltrain: 0.004736 	Lval: 0.005110
Epoch: 115 	Ltrain: 0.004693 	Lval: 0.005075
Epoch 00118: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 120 	Ltrain: 0.004768 	Lval: 0.005071
Epoch: 125 	Ltrain: 0.004732 	Lval: 0.005064
Epoch 00130: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 130 	Ltrain: 0.004786 	Lval: 0.005062
EarlyStopper: stopping at epoch 133 with best_val_loss = 0.005059


	Fold 3/5
Epoch: 1 	Ltrain: 0.029087 	Lval: 0.010461
Epoch: 5 	Ltrain: 0.010990 	Lval: 0.009942
Epoch: 10 	Ltrain: 0.008088 	Lval: 0.007780
Epoch: 15 	Ltrain: 0.006850 	Lval: 0.006599
Epoch: 20 	Ltrain: 0.006190 	Lval: 0.006213
Epoch: 25 	Ltrain: 0.005694 	Lval: 0.005803
Epoch: 30 	Ltrain: 0.005152 	Lval: 0.005493
Epoch: 35 	Ltrain: 0.004789 	Lval: 0.005510
Epoch 00037: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 40 	Ltrain: 0.004584 	Lval: 0.005010
Epoch: 45 	Ltrain: 0.004585 	Lval: 0.004977
Epoch: 50 	Ltrain: 0.004571 	Lval: 0.004970
Epoch: 55 	Ltrain: 0.004517 	Lval: 0.004963
Epoch 00060: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 60 	Ltrain: 0.004576 	Lval: 0.004918
Epoch: 65 	Ltrain: 0.004449 	Lval: 0.004926
Epoch: 70 	Ltrain: 0.004548 	Lval: 0.004925
Epoch 00072: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 75 	Ltrain: 0.004500 	Lval: 0.004920
EarlyStopper: stopping at epoch 75 with best_val_loss = 0.004917


	Fold 4/5
Epoch: 1 	Ltrain: 0.016382 	Lval: 0.010568
Epoch: 5 	Ltrain: 0.009132 	Lval: 0.008802
Epoch: 10 	Ltrain: 0.006997 	Lval: 0.007257
Epoch: 15 	Ltrain: 0.006132 	Lval: 0.006869
Epoch: 20 	Ltrain: 0.005381 	Lval: 0.005883
Epoch: 25 	Ltrain: 0.004934 	Lval: 0.005694
Epoch: 30 	Ltrain: 0.004546 	Lval: 0.005465
Epoch: 35 	Ltrain: 0.004492 	Lval: 0.005856
Epoch 00037: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 40 	Ltrain: 0.004170 	Lval: 0.004890
Epoch: 45 	Ltrain: 0.004152 	Lval: 0.004895
Epoch 00050: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 50 	Ltrain: 0.004156 	Lval: 0.004904
Epoch: 55 	Ltrain: 0.004132 	Lval: 0.004890
Epoch: 60 	Ltrain: 0.004141 	Lval: 0.004888
Epoch 00062: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 65 	Ltrain: 0.004123 	Lval: 0.004884
EarlyStopper: stopping at epoch 65 with best_val_loss = 0.004878


	Fold 5/5
Epoch: 1 	Ltrain: 0.016471 	Lval: 0.011313
Epoch: 5 	Ltrain: 0.009949 	Lval: 0.009510
Epoch: 10 	Ltrain: 0.006822 	Lval: 0.007574
Epoch: 15 	Ltrain: 0.005955 	Lval: 0.006950
Epoch: 20 	Ltrain: 0.005193 	Lval: 0.005930
Epoch: 25 	Ltrain: 0.004606 	Lval: 0.005543
Epoch: 30 	Ltrain: 0.004384 	Lval: 0.005340
Epoch: 35 	Ltrain: 0.004189 	Lval: 0.005105
Epoch: 40 	Ltrain: 0.004042 	Lval: 0.005046
Epoch: 45 	Ltrain: 0.004003 	Lval: 0.004754
Epoch: 50 	Ltrain: 0.004006 	Lval: 0.005086
Epoch 00053: reducing learning rate of group 0 to 1.1789e-05.
Epoch: 55 	Ltrain: 0.003791 	Lval: 0.004738
Epoch: 60 	Ltrain: 0.003805 	Lval: 0.004727
Epoch 00065: reducing learning rate of group 0 to 1.1789e-06.
Epoch: 65 	Ltrain: 0.003789 	Lval: 0.004741
Epoch: 70 	Ltrain: 0.003786 	Lval: 0.004727
Epoch: 75 	Ltrain: 0.003774 	Lval: 0.004720
Epoch 00077: reducing learning rate of group 0 to 1.1789e-07.
Epoch: 80 	Ltrain: 0.003789 	Lval: 0.004717
EarlyStopper: stopping at epoch 80 with best_val_loss = 0.004684

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.006896645339210913
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 7.866572452557487e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 22
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.122890 	Lval: 0.786923
Epoch: 5 	Ltrain: 0.016850 	Lval: 0.015498
Epoch: 10 	Ltrain: 0.008447 	Lval: 0.009735
Epoch: 15 	Ltrain: 0.005210 	Lval: 0.005219
Epoch: 20 	Ltrain: 0.006936 	Lval: 0.005067
Epoch 00025: reducing learning rate of group 0 to 6.8966e-04.
Epoch: 25 	Ltrain: 0.004597 	Lval: 0.005673
Epoch: 30 	Ltrain: 0.004944 	Lval: 0.004870
Epoch: 35 	Ltrain: 0.004202 	Lval: 0.004674
Epoch 00040: reducing learning rate of group 0 to 6.8966e-05.
Epoch: 40 	Ltrain: 0.004282 	Lval: 0.004662
Epoch: 45 	Ltrain: 0.005020 	Lval: 0.004613
Epoch: 50 	Ltrain: 0.004385 	Lval: 0.004609
Epoch 00052: reducing learning rate of group 0 to 6.8966e-06.
Epoch: 55 	Ltrain: 0.005098 	Lval: 0.004617
EarlyStopper: stopping at epoch 57 with best_val_loss = 0.004606


	Fold 2/5
Epoch: 1 	Ltrain: 0.270552 	Lval: 0.022941
Epoch: 5 	Ltrain: 0.009511 	Lval: 0.008331
Epoch: 10 	Ltrain: 0.005839 	Lval: 0.005614
Epoch: 15 	Ltrain: 0.004468 	Lval: 0.004715
Epoch: 20 	Ltrain: 0.004132 	Lval: 0.004666
Epoch: 25 	Ltrain: 0.003966 	Lval: 0.004447
Epoch: 30 	Ltrain: 0.003898 	Lval: 0.004763
Epoch: 35 	Ltrain: 0.003734 	Lval: 0.004372
Epoch 00036: reducing learning rate of group 0 to 6.8966e-04.
Epoch: 40 	Ltrain: 0.003381 	Lval: 0.003964
Epoch: 45 	Ltrain: 0.003216 	Lval: 0.003877
Epoch: 50 	Ltrain: 0.003068 	Lval: 0.003889
Epoch: 55 	Ltrain: 0.003075 	Lval: 0.003811
Epoch: 60 	Ltrain: 0.003173 	Lval: 0.003710
Epoch: 65 	Ltrain: 0.002946 	Lval: 0.003665
Epoch 00067: reducing learning rate of group 0 to 6.8966e-05.
Epoch: 70 	Ltrain: 0.003019 	Lval: 0.003678
Epoch: 75 	Ltrain: 0.002911 	Lval: 0.003693
Epoch 00079: reducing learning rate of group 0 to 6.8966e-06.
Epoch: 80 	Ltrain: 0.002902 	Lval: 0.003660
Epoch: 85 	Ltrain: 0.002879 	Lval: 0.003662
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.003658


	Fold 3/5
Epoch: 1 	Ltrain: 0.117565 	Lval: 0.012328
Epoch: 5 	Ltrain: 0.008094 	Lval: 0.006559
Epoch: 10 	Ltrain: 0.004607 	Lval: 0.004773
Epoch: 15 	Ltrain: 0.003992 	Lval: 0.004871
Epoch: 20 	Ltrain: 0.003630 	Lval: 0.004240
Epoch 00022: reducing learning rate of group 0 to 6.8966e-04.
Epoch: 25 	Ltrain: 0.002968 	Lval: 0.003690
Epoch: 30 	Ltrain: 0.002863 	Lval: 0.003681
Epoch: 35 	Ltrain: 0.002793 	Lval: 0.003537
Epoch: 40 	Ltrain: 0.002789 	Lval: 0.003485
Epoch 00043: reducing learning rate of group 0 to 6.8966e-05.
Epoch: 45 	Ltrain: 0.002725 	Lval: 0.003446
Epoch: 50 	Ltrain: 0.002770 	Lval: 0.003437
Epoch: 55 	Ltrain: 0.002718 	Lval: 0.003432
Epoch: 60 	Ltrain: 0.002804 	Lval: 0.003418
Epoch: 65 	Ltrain: 0.002712 	Lval: 0.003397
Epoch: 70 	Ltrain: 0.002656 	Lval: 0.003398
Epoch: 75 	Ltrain: 0.002647 	Lval: 0.003372
Epoch: 80 	Ltrain: 0.002661 	Lval: 0.003371
Epoch: 85 	Ltrain: 0.002639 	Lval: 0.003356
Epoch: 90 	Ltrain: 0.002616 	Lval: 0.003351
Epoch: 95 	Ltrain: 0.002662 	Lval: 0.003337
Epoch 00098: reducing learning rate of group 0 to 6.8966e-06.
Epoch: 100 	Ltrain: 0.002629 	Lval: 0.003337
Epoch: 105 	Ltrain: 0.002618 	Lval: 0.003337
Epoch: 110 	Ltrain: 0.002611 	Lval: 0.003335
Epoch: 115 	Ltrain: 0.002605 	Lval: 0.003330
Epoch: 120 	Ltrain: 0.002593 	Lval: 0.003327
Epoch 00123: reducing learning rate of group 0 to 6.8966e-07.
Epoch: 125 	Ltrain: 0.002625 	Lval: 0.003326
Epoch: 130 	Ltrain: 0.002728 	Lval: 0.003326
Epoch 00135: reducing learning rate of group 0 to 6.8966e-08.
Epoch: 135 	Ltrain: 0.002576 	Lval: 0.003326
EarlyStopper: stopping at epoch 135 with best_val_loss = 0.003330


	Fold 4/5
Epoch: 1 	Ltrain: 0.071232 	Lval: 0.014930
Epoch: 5 	Ltrain: 0.005997 	Lval: 0.006001
Epoch: 10 	Ltrain: 0.004287 	Lval: 0.004893
Epoch: 15 	Ltrain: 0.003575 	Lval: 0.004537
Epoch: 20 	Ltrain: 0.002973 	Lval: 0.003820
Epoch 00024: reducing learning rate of group 0 to 6.8966e-04.
Epoch: 25 	Ltrain: 0.003063 	Lval: 0.003663
Epoch: 30 	Ltrain: 0.002577 	Lval: 0.003521
Epoch: 35 	Ltrain: 0.002532 	Lval: 0.003483
Epoch 00036: reducing learning rate of group 0 to 6.8966e-05.
Epoch: 40 	Ltrain: 0.002587 	Lval: 0.003426
Epoch: 45 	Ltrain: 0.002496 	Lval: 0.003420
Epoch: 50 	Ltrain: 0.002508 	Lval: 0.003417
Epoch: 55 	Ltrain: 0.002497 	Lval: 0.003416
Epoch 00056: reducing learning rate of group 0 to 6.8966e-06.
Epoch: 60 	Ltrain: 0.002478 	Lval: 0.003414
Epoch: 65 	Ltrain: 0.002564 	Lval: 0.003412
Epoch 00068: reducing learning rate of group 0 to 6.8966e-07.
Epoch: 70 	Ltrain: 0.002487 	Lval: 0.003411
EarlyStopper: stopping at epoch 69 with best_val_loss = 0.003415


	Fold 5/5
Epoch: 1 	Ltrain: 0.083709 	Lval: 0.012069
Epoch: 5 	Ltrain: 0.005280 	Lval: 0.005720
Epoch: 10 	Ltrain: 0.004016 	Lval: 0.005216
Epoch 00015: reducing learning rate of group 0 to 6.8966e-04.
Epoch: 15 	Ltrain: 0.004172 	Lval: 0.005062
Epoch: 20 	Ltrain: 0.003424 	Lval: 0.004708
Epoch: 25 	Ltrain: 0.003390 	Lval: 0.004601
Epoch: 30 	Ltrain: 0.003358 	Lval: 0.004644
Epoch: 35 	Ltrain: 0.003247 	Lval: 0.004410
Epoch: 40 	Ltrain: 0.003288 	Lval: 0.004371
Epoch: 45 	Ltrain: 0.003249 	Lval: 0.004349
Epoch: 50 	Ltrain: 0.003125 	Lval: 0.004267
Epoch: 55 	Ltrain: 0.003122 	Lval: 0.004178
Epoch 00060: reducing learning rate of group 0 to 6.8966e-05.
Epoch: 60 	Ltrain: 0.003070 	Lval: 0.004215
Epoch: 65 	Ltrain: 0.002930 	Lval: 0.004035
Epoch: 70 	Ltrain: 0.002979 	Lval: 0.004013
Epoch: 75 	Ltrain: 0.002912 	Lval: 0.004028
Epoch: 80 	Ltrain: 0.002919 	Lval: 0.004005
Epoch: 85 	Ltrain: 0.002913 	Lval: 0.003998
Epoch 00090: reducing learning rate of group 0 to 6.8966e-06.
Epoch: 90 	Ltrain: 0.002939 	Lval: 0.003996
Epoch: 95 	Ltrain: 0.002919 	Lval: 0.003987
Epoch: 100 	Ltrain: 0.002986 	Lval: 0.003989
Epoch 00102: reducing learning rate of group 0 to 6.8966e-07.
Epoch: 105 	Ltrain: 0.002933 	Lval: 0.003988
Epoch: 110 	Ltrain: 0.002861 	Lval: 0.003988
Epoch 00114: reducing learning rate of group 0 to 6.8966e-08.
Epoch: 115 	Ltrain: 0.002878 	Lval: 0.003988
EarlyStopper: stopping at epoch 116 with best_val_loss = 0.003987

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.007687361858438284
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 1.0382521562225828e-06
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 25
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.019697 	Lval: 0.011986
Epoch: 5 	Ltrain: 0.006708 	Lval: 0.006615
Epoch: 10 	Ltrain: 0.006079 	Lval: 0.005138
Epoch: 15 	Ltrain: 0.005757 	Lval: 0.004859
Epoch: 20 	Ltrain: 0.004549 	Lval: 0.004402
Epoch: 25 	Ltrain: 0.004293 	Lval: 0.004263
Epoch 00029: reducing learning rate of group 0 to 7.6874e-04.
Epoch: 30 	Ltrain: 0.004694 	Lval: 0.004276
Epoch: 35 	Ltrain: 0.004098 	Lval: 0.003997
Epoch: 40 	Ltrain: 0.003755 	Lval: 0.003912
Epoch: 45 	Ltrain: 0.003625 	Lval: 0.003897
Epoch: 50 	Ltrain: 0.004287 	Lval: 0.003841
Epoch 00051: reducing learning rate of group 0 to 7.6874e-05.
Epoch: 55 	Ltrain: 0.003764 	Lval: 0.003827
Epoch: 60 	Ltrain: 0.003504 	Lval: 0.003787
Epoch: 65 	Ltrain: 0.003521 	Lval: 0.003780
Epoch 00067: reducing learning rate of group 0 to 7.6874e-06.
Epoch: 70 	Ltrain: 0.003618 	Lval: 0.003773
Epoch: 75 	Ltrain: 0.003476 	Lval: 0.003773
Epoch 00079: reducing learning rate of group 0 to 7.6874e-07.
Epoch: 80 	Ltrain: 0.003654 	Lval: 0.003773
Epoch: 85 	Ltrain: 0.003490 	Lval: 0.003773
EarlyStopper: stopping at epoch 87 with best_val_loss = 0.003771


	Fold 2/5
Epoch: 1 	Ltrain: 0.016610 	Lval: 0.010743
Epoch: 5 	Ltrain: 0.005859 	Lval: 0.005532
Epoch: 10 	Ltrain: 0.004317 	Lval: 0.004880
Epoch: 15 	Ltrain: 0.004238 	Lval: 0.005877
Epoch: 20 	Ltrain: 0.003741 	Lval: 0.004525
Epoch 00025: reducing learning rate of group 0 to 7.6874e-04.
Epoch: 25 	Ltrain: 0.003216 	Lval: 0.004177
Epoch: 30 	Ltrain: 0.002760 	Lval: 0.003353
Epoch: 35 	Ltrain: 0.002717 	Lval: 0.003307
Epoch: 40 	Ltrain: 0.002693 	Lval: 0.003228
Epoch 00045: reducing learning rate of group 0 to 7.6874e-05.
Epoch: 45 	Ltrain: 0.002698 	Lval: 0.003244
Epoch: 50 	Ltrain: 0.002651 	Lval: 0.003189
Epoch: 55 	Ltrain: 0.002638 	Lval: 0.003186
Epoch: 60 	Ltrain: 0.002632 	Lval: 0.003180
Epoch: 65 	Ltrain: 0.002621 	Lval: 0.003166
Epoch: 70 	Ltrain: 0.002613 	Lval: 0.003169
Epoch: 75 	Ltrain: 0.002667 	Lval: 0.003155
Epoch 00079: reducing learning rate of group 0 to 7.6874e-06.
Epoch: 80 	Ltrain: 0.002586 	Lval: 0.003157
Epoch: 85 	Ltrain: 0.002608 	Lval: 0.003156
Epoch: 90 	Ltrain: 0.002601 	Lval: 0.003155
Epoch 00091: reducing learning rate of group 0 to 7.6874e-07.
Epoch: 95 	Ltrain: 0.002700 	Lval: 0.003155
Epoch: 100 	Ltrain: 0.002597 	Lval: 0.003154
EarlyStopper: stopping at epoch 99 with best_val_loss = 0.003155


	Fold 3/5
Epoch: 1 	Ltrain: 0.017131 	Lval: 0.010535
Epoch: 5 	Ltrain: 0.004775 	Lval: 0.004720
Epoch: 10 	Ltrain: 0.004109 	Lval: 0.004539
Epoch: 15 	Ltrain: 0.003706 	Lval: 0.003921
Epoch: 20 	Ltrain: 0.002921 	Lval: 0.003625
Epoch: 25 	Ltrain: 0.002951 	Lval: 0.003477
Epoch: 30 	Ltrain: 0.002778 	Lval: 0.003311
Epoch 00034: reducing learning rate of group 0 to 7.6874e-04.
Epoch: 35 	Ltrain: 0.002558 	Lval: 0.003086
Epoch: 40 	Ltrain: 0.002395 	Lval: 0.003019
Epoch: 45 	Ltrain: 0.002388 	Lval: 0.003002
Epoch: 50 	Ltrain: 0.002377 	Lval: 0.002982
Epoch 00053: reducing learning rate of group 0 to 7.6874e-05.
Epoch: 55 	Ltrain: 0.002308 	Lval: 0.002970
Epoch: 60 	Ltrain: 0.002309 	Lval: 0.002962
Epoch: 65 	Ltrain: 0.002333 	Lval: 0.002957
Epoch: 70 	Ltrain: 0.002302 	Lval: 0.002958
Epoch: 75 	Ltrain: 0.002318 	Lval: 0.002953
Epoch 00076: reducing learning rate of group 0 to 7.6874e-06.
Epoch: 80 	Ltrain: 0.002307 	Lval: 0.002951
Epoch: 85 	Ltrain: 0.002299 	Lval: 0.002952
Epoch 00088: reducing learning rate of group 0 to 7.6874e-07.
Epoch: 90 	Ltrain: 0.002301 	Lval: 0.002951
EarlyStopper: stopping at epoch 89 with best_val_loss = 0.002957


	Fold 4/5
Epoch: 1 	Ltrain: 0.011618 	Lval: 0.007019
Epoch: 5 	Ltrain: 0.004503 	Lval: 0.004915
Epoch: 10 	Ltrain: 0.003229 	Lval: 0.003942
Epoch: 15 	Ltrain: 0.003076 	Lval: 0.003530
Epoch 00019: reducing learning rate of group 0 to 7.6874e-04.
Epoch: 20 	Ltrain: 0.002558 	Lval: 0.003221
Epoch: 25 	Ltrain: 0.002452 	Lval: 0.003181
Epoch: 30 	Ltrain: 0.002416 	Lval: 0.003197
Epoch: 35 	Ltrain: 0.002388 	Lval: 0.003147
Epoch: 40 	Ltrain: 0.002374 	Lval: 0.003048
Epoch: 45 	Ltrain: 0.002354 	Lval: 0.003063
Epoch: 50 	Ltrain: 0.002323 	Lval: 0.003087
Epoch 00052: reducing learning rate of group 0 to 7.6874e-05.
Epoch: 55 	Ltrain: 0.002274 	Lval: 0.003009
Epoch: 60 	Ltrain: 0.002258 	Lval: 0.003011
Epoch 00064: reducing learning rate of group 0 to 7.6874e-06.
Epoch: 65 	Ltrain: 0.002256 	Lval: 0.003004
Epoch: 70 	Ltrain: 0.002251 	Lval: 0.003000
Epoch: 75 	Ltrain: 0.002248 	Lval: 0.002997
Epoch 00076: reducing learning rate of group 0 to 7.6874e-07.
EarlyStopper: stopping at epoch 77 with best_val_loss = 0.002999


	Fold 5/5
Epoch: 1 	Ltrain: 0.012382 	Lval: 0.008577
Epoch: 5 	Ltrain: 0.004326 	Lval: 0.004665
Epoch: 10 	Ltrain: 0.003337 	Lval: 0.003676
Epoch 00015: reducing learning rate of group 0 to 7.6874e-04.
Epoch: 15 	Ltrain: 0.003065 	Lval: 0.004370
Epoch: 20 	Ltrain: 0.002535 	Lval: 0.003239
Epoch: 25 	Ltrain: 0.002474 	Lval: 0.003379
Epoch: 30 	Ltrain: 0.002437 	Lval: 0.003222
Epoch: 35 	Ltrain: 0.002435 	Lval: 0.003172
Epoch: 40 	Ltrain: 0.002436 	Lval: 0.003094
Epoch 00044: reducing learning rate of group 0 to 7.6874e-05.
Epoch: 45 	Ltrain: 0.002369 	Lval: 0.003105
Epoch: 50 	Ltrain: 0.002349 	Lval: 0.003100
Epoch: 55 	Ltrain: 0.002333 	Lval: 0.003103
Epoch 00056: reducing learning rate of group 0 to 7.6874e-06.
Epoch: 60 	Ltrain: 0.002323 	Lval: 0.003099
Epoch: 65 	Ltrain: 0.002326 	Lval: 0.003098
EarlyStopper: stopping at epoch 64 with best_val_loss = 0.003094

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 6
hidden_units    : 128
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.005416426106251328
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.063130982695353e-08
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 21
batch_sz        : 64
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.110678 	Lval: 0.011989
Epoch 00005: reducing learning rate of group 0 to 5.4164e-04.
Epoch: 5 	Ltrain: 0.018994 	Lval: 0.012330
Epoch: 10 	Ltrain: 0.011943 	Lval: 0.007864
Epoch: 15 	Ltrain: 0.009402 	Lval: 0.007810
Epoch: 20 	Ltrain: 0.007652 	Lval: 0.006824
Epoch: 25 	Ltrain: 0.006194 	Lval: 0.006150
Epoch: 30 	Ltrain: 0.006276 	Lval: 0.005792
Epoch: 35 	Ltrain: 0.008083 	Lval: 0.006168
Epoch 00037: reducing learning rate of group 0 to 5.4164e-05.
Epoch: 40 	Ltrain: 0.005962 	Lval: 0.006000
Epoch: 45 	Ltrain: 0.005703 	Lval: 0.005854
Epoch 00049: reducing learning rate of group 0 to 5.4164e-06.
Epoch: 50 	Ltrain: 0.005960 	Lval: 0.005677
EarlyStopper: stopping at epoch 53 with best_val_loss = 0.005619


	Fold 2/5
Epoch: 1 	Ltrain: 0.060995 	Lval: 0.013564
Epoch: 5 	Ltrain: 0.011559 	Lval: 0.008103
Epoch: 10 	Ltrain: 0.007023 	Lval: 0.009845
Epoch: 15 	Ltrain: 0.005258 	Lval: 0.005420
Epoch: 20 	Ltrain: 0.005469 	Lval: 0.004735
Epoch: 25 	Ltrain: 0.004638 	Lval: 0.005518
Epoch: 30 	Ltrain: 0.004535 	Lval: 0.004514
Epoch: 35 	Ltrain: 0.004293 	Lval: 0.004731
Epoch: 40 	Ltrain: 0.003742 	Lval: 0.004224
Epoch: 45 	Ltrain: 0.003738 	Lval: 0.004374
Epoch 00050: reducing learning rate of group 0 to 5.4164e-04.
Epoch: 50 	Ltrain: 0.004128 	Lval: 0.004065
Epoch: 55 	Ltrain: 0.003000 	Lval: 0.004101
Epoch: 60 	Ltrain: 0.003185 	Lval: 0.003917
Epoch: 65 	Ltrain: 0.003983 	Lval: 0.003782
Epoch: 70 	Ltrain: 0.003133 	Lval: 0.003774
Epoch: 75 	Ltrain: 0.002846 	Lval: 0.003685
Epoch 00080: reducing learning rate of group 0 to 5.4164e-05.
Epoch: 80 	Ltrain: 0.002918 	Lval: 0.003652
Epoch: 85 	Ltrain: 0.003016 	Lval: 0.003698
Epoch: 90 	Ltrain: 0.002750 	Lval: 0.003671
Epoch 00092: reducing learning rate of group 0 to 5.4164e-06.
Epoch: 95 	Ltrain: 0.003118 	Lval: 0.003673
EarlyStopper: stopping at epoch 96 with best_val_loss = 0.003644


	Fold 3/5
Epoch: 1 	Ltrain: 0.063000 	Lval: 0.110231
Epoch: 5 	Ltrain: 0.007566 	Lval: 0.009703
Epoch: 10 	Ltrain: 0.005459 	Lval: 0.005265
Epoch: 15 	Ltrain: 0.004562 	Lval: 0.004565
Epoch 00019: reducing learning rate of group 0 to 5.4164e-04.
Epoch: 20 	Ltrain: 0.004222 	Lval: 0.004339
Epoch: 25 	Ltrain: 0.003907 	Lval: 0.004391
Epoch: 30 	Ltrain: 0.003632 	Lval: 0.004532
Epoch 00031: reducing learning rate of group 0 to 5.4164e-05.
Epoch: 35 	Ltrain: 0.003690 	Lval: 0.004403
Epoch: 40 	Ltrain: 0.003669 	Lval: 0.004402
Epoch 00043: reducing learning rate of group 0 to 5.4164e-06.
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.004323


	Fold 4/5
Epoch: 1 	Ltrain: 0.048444 	Lval: 0.010523
Epoch: 5 	Ltrain: 0.006806 	Lval: 0.005891
Epoch: 10 	Ltrain: 0.004961 	Lval: 0.005249
Epoch 00012: reducing learning rate of group 0 to 5.4164e-04.
Epoch: 15 	Ltrain: 0.004040 	Lval: 0.004716
Epoch: 20 	Ltrain: 0.003784 	Lval: 0.004720
Epoch 00024: reducing learning rate of group 0 to 5.4164e-05.
Epoch: 25 	Ltrain: 0.003689 	Lval: 0.004777
Epoch: 30 	Ltrain: 0.003688 	Lval: 0.004738
EarlyStopper: stopping at epoch 33 with best_val_loss = 0.004694


	Fold 5/5
Epoch: 1 	Ltrain: 0.061191 	Lval: 0.018586
Epoch: 5 	Ltrain: 0.006022 	Lval: 0.006506
Epoch: 10 	Ltrain: 0.004120 	Lval: 0.005434
Epoch: 15 	Ltrain: 0.003851 	Lval: 0.005166
Epoch 00020: reducing learning rate of group 0 to 5.4164e-04.
Epoch: 20 	Ltrain: 0.003795 	Lval: 0.005391
Epoch: 25 	Ltrain: 0.003428 	Lval: 0.004797
Epoch: 30 	Ltrain: 0.003451 	Lval: 0.004768
Epoch: 35 	Ltrain: 0.003314 	Lval: 0.004679
Epoch: 40 	Ltrain: 0.003122 	Lval: 0.004653
Epoch: 45 	Ltrain: 0.003150 	Lval: 0.004570
Epoch: 50 	Ltrain: 0.003010 	Lval: 0.004513
Epoch: 55 	Ltrain: 0.003008 	Lval: 0.004409
Epoch: 60 	Ltrain: 0.003008 	Lval: 0.004351
Epoch: 65 	Ltrain: 0.002900 	Lval: 0.004222
Epoch: 70 	Ltrain: 0.002991 	Lval: 0.004161
Epoch: 75 	Ltrain: 0.002917 	Lval: 0.004177
Epoch 00078: reducing learning rate of group 0 to 5.4164e-05.
Epoch: 80 	Ltrain: 0.002853 	Lval: 0.004082
Epoch: 85 	Ltrain: 0.002758 	Lval: 0.004061
Epoch: 90 	Ltrain: 0.002846 	Lval: 0.004050
Epoch: 95 	Ltrain: 0.002915 	Lval: 0.004035
Epoch 00099: reducing learning rate of group 0 to 5.4164e-06.
Epoch: 100 	Ltrain: 0.002720 	Lval: 0.004035
Epoch: 105 	Ltrain: 0.002818 	Lval: 0.004033
Epoch: 110 	Ltrain: 0.002770 	Lval: 0.004031
Epoch 00112: reducing learning rate of group 0 to 5.4164e-07.
Epoch: 115 	Ltrain: 0.002799 	Lval: 0.004031
EarlyStopper: stopping at epoch 115 with best_val_loss = 0.004035

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 5
hidden_units    : 32
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.004442333753730127
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 2.910583951956108e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 24
batch_sz        : 16
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.013590 	Lval: 0.010674
Epoch: 5 	Ltrain: 0.007481 	Lval: 0.007657
Epoch: 10 	Ltrain: 0.005774 	Lval: 0.005367
Epoch: 15 	Ltrain: 0.006011 	Lval: 0.005449
Epoch: 20 	Ltrain: 0.004753 	Lval: 0.004581
Epoch: 25 	Ltrain: 0.004596 	Lval: 0.005110
Epoch: 30 	Ltrain: 0.005059 	Lval: 0.004818
Epoch 00031: reducing learning rate of group 0 to 4.4423e-04.
Epoch: 35 	Ltrain: 0.003801 	Lval: 0.003941
Epoch: 40 	Ltrain: 0.004027 	Lval: 0.003825
Epoch: 45 	Ltrain: 0.003765 	Lval: 0.003830
Epoch: 50 	Ltrain: 0.003767 	Lval: 0.003772
Epoch 00054: reducing learning rate of group 0 to 4.4423e-05.
Epoch: 55 	Ltrain: 0.003693 	Lval: 0.003772
Epoch: 60 	Ltrain: 0.003631 	Lval: 0.003752
Epoch: 65 	Ltrain: 0.003565 	Lval: 0.003750
Epoch 00066: reducing learning rate of group 0 to 4.4423e-06.
Epoch: 70 	Ltrain: 0.003633 	Lval: 0.003750
Epoch: 75 	Ltrain: 0.003710 	Lval: 0.003748
Epoch 00078: reducing learning rate of group 0 to 4.4423e-07.
Epoch: 80 	Ltrain: 0.003549 	Lval: 0.003749
Epoch: 85 	Ltrain: 0.003639 	Lval: 0.003748
EarlyStopper: stopping at epoch 84 with best_val_loss = 0.003750


	Fold 2/5
Epoch: 1 	Ltrain: 0.012538 	Lval: 0.010316
Epoch: 5 	Ltrain: 0.005774 	Lval: 0.005539
Epoch: 10 	Ltrain: 0.004903 	Lval: 0.004616
Epoch: 15 	Ltrain: 0.003743 	Lval: 0.004238
Epoch: 20 	Ltrain: 0.003286 	Lval: 0.004246
Epoch: 25 	Ltrain: 0.003185 	Lval: 0.003450
Epoch 00029: reducing learning rate of group 0 to 4.4423e-04.
Epoch: 30 	Ltrain: 0.002927 	Lval: 0.003368
Epoch: 35 	Ltrain: 0.002659 	Lval: 0.003156
Epoch: 40 	Ltrain: 0.002580 	Lval: 0.003110
Epoch: 45 	Ltrain: 0.002632 	Lval: 0.003087
Epoch: 50 	Ltrain: 0.002702 	Lval: 0.003033
Epoch: 55 	Ltrain: 0.002525 	Lval: 0.002989
Epoch: 60 	Ltrain: 0.002494 	Lval: 0.002946
Epoch 00063: reducing learning rate of group 0 to 4.4423e-05.
Epoch: 65 	Ltrain: 0.002463 	Lval: 0.002928
Epoch: 70 	Ltrain: 0.002456 	Lval: 0.002917
Epoch: 75 	Ltrain: 0.002552 	Lval: 0.002914
Epoch 00078: reducing learning rate of group 0 to 4.4423e-06.
Epoch: 80 	Ltrain: 0.002481 	Lval: 0.002912
Epoch: 85 	Ltrain: 0.002467 	Lval: 0.002911
Epoch 00090: reducing learning rate of group 0 to 4.4423e-07.
Epoch: 90 	Ltrain: 0.002448 	Lval: 0.002911
EarlyStopper: stopping at epoch 92 with best_val_loss = 0.002917


	Fold 3/5
Epoch: 1 	Ltrain: 0.017518 	Lval: 0.009382
Epoch: 5 	Ltrain: 0.004902 	Lval: 0.005705
Epoch: 10 	Ltrain: 0.004049 	Lval: 0.004547
Epoch: 15 	Ltrain: 0.003657 	Lval: 0.004208
Epoch: 20 	Ltrain: 0.003372 	Lval: 0.004263
Epoch: 25 	Ltrain: 0.002905 	Lval: 0.003526
Epoch: 30 	Ltrain: 0.002846 	Lval: 0.003326
Epoch 00032: reducing learning rate of group 0 to 4.4423e-04.
Epoch: 35 	Ltrain: 0.002432 	Lval: 0.003097
Epoch: 40 	Ltrain: 0.002403 	Lval: 0.002976
Epoch: 45 	Ltrain: 0.002386 	Lval: 0.002968
Epoch: 50 	Ltrain: 0.002377 	Lval: 0.002997
Epoch 00053: reducing learning rate of group 0 to 4.4423e-05.
Epoch: 55 	Ltrain: 0.002325 	Lval: 0.002931
Epoch: 60 	Ltrain: 0.002331 	Lval: 0.002930
Epoch: 65 	Ltrain: 0.002314 	Lval: 0.002926
Epoch 00067: reducing learning rate of group 0 to 4.4423e-06.
Epoch: 70 	Ltrain: 0.002322 	Lval: 0.002923
EarlyStopper: stopping at epoch 72 with best_val_loss = 0.002931


	Fold 4/5
Epoch: 1 	Ltrain: 0.012976 	Lval: 0.008349
Epoch: 5 	Ltrain: 0.004600 	Lval: 0.004920
Epoch: 10 	Ltrain: 0.004056 	Lval: 0.004496
Epoch: 15 	Ltrain: 0.003098 	Lval: 0.003933
Epoch: 20 	Ltrain: 0.002990 	Lval: 0.003753
Epoch 00022: reducing learning rate of group 0 to 4.4423e-04.
Epoch: 25 	Ltrain: 0.002493 	Lval: 0.003298
Epoch: 30 	Ltrain: 0.002467 	Lval: 0.003282
Epoch: 35 	Ltrain: 0.002455 	Lval: 0.003278
Epoch: 40 	Ltrain: 0.002422 	Lval: 0.003240
Epoch: 45 	Ltrain: 0.002444 	Lval: 0.003233
Epoch: 50 	Ltrain: 0.002415 	Lval: 0.003153
Epoch 00051: reducing learning rate of group 0 to 4.4423e-05.
Epoch: 55 	Ltrain: 0.002354 	Lval: 0.003146
Epoch: 60 	Ltrain: 0.002354 	Lval: 0.003136
Epoch: 65 	Ltrain: 0.002352 	Lval: 0.003129
Epoch 00069: reducing learning rate of group 0 to 4.4423e-06.
Epoch: 70 	Ltrain: 0.002350 	Lval: 0.003140
EarlyStopper: stopping at epoch 70 with best_val_loss = 0.003138


	Fold 5/5
Epoch: 1 	Ltrain: 0.014534 	Lval: 0.010935
Epoch: 5 	Ltrain: 0.004355 	Lval: 0.005156
Epoch: 10 	Ltrain: 0.003745 	Lval: 0.004851
Epoch: 15 	Ltrain: 0.003054 	Lval: 0.003735
Epoch: 20 	Ltrain: 0.002896 	Lval: 0.003594
Epoch: 25 	Ltrain: 0.002719 	Lval: 0.003341
Epoch 00030: reducing learning rate of group 0 to 4.4423e-04.
Epoch: 30 	Ltrain: 0.002563 	Lval: 0.003296
Epoch: 35 	Ltrain: 0.002313 	Lval: 0.003058
Epoch: 40 	Ltrain: 0.002289 	Lval: 0.003063
Epoch 00042: reducing learning rate of group 0 to 4.4423e-05.
Epoch: 45 	Ltrain: 0.002270 	Lval: 0.003035
Epoch: 50 	Ltrain: 0.002259 	Lval: 0.003034
Epoch: 55 	Ltrain: 0.002258 	Lval: 0.003027
Epoch: 60 	Ltrain: 0.002261 	Lval: 0.003027
Epoch 00063: reducing learning rate of group 0 to 4.4423e-06.
Epoch: 65 	Ltrain: 0.002258 	Lval: 0.003023
Epoch: 70 	Ltrain: 0.002260 	Lval: 0.003021
EarlyStopper: stopping at epoch 71 with best_val_loss = 0.003028

CURRENT CONFIGURATION:
n_hours_u       : 72
n_hours_y       : 24
model_class     : <class 'src.modelling.GRU.GRU'>
input_units     : 8
hidden_layers   : 4
hidden_units    : 64
output_units    : 2
Optimizer       : <class 'torch.optim.adam.Adam'>
lr_shared       : 0.0029643701506353666
scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>
scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}
w_decay         : 8.543481621735474e-07
loss_fn         : MSELoss()
epochs          : 5000
early_stopper   : <class 'src.modelling.EarlyStopper.EarlyStopper'>
patience        : 10
batch_sz        : 32
k_folds         : 5


	Fold 1/5
Epoch: 1 	Ltrain: 0.015784 	Lval: 0.010997
Epoch: 5 	Ltrain: 0.010911 	Lval: 0.007856
Epoch: 10 	Ltrain: 0.006886 	Lval: 0.006129
Epoch: 15 	Ltrain: 0.005287 	Lval: 0.005274
Epoch: 20 	Ltrain: 0.004517 	Lval: 0.005073
Epoch 00023: reducing learning rate of group 0 to 2.9644e-04.
Epoch: 25 	Ltrain: 0.005517 	Lval: 0.004792
Epoch: 30 	Ltrain: 0.004802 	Lval: 0.004778
Epoch: 35 	Ltrain: 0.004422 	Lval: 0.004687
Epoch 00039: reducing learning rate of group 0 to 2.9644e-05.
Epoch: 40 	Ltrain: 0.004470 	Lval: 0.004780
EarlyStopper: stopping at epoch 43 with best_val_loss = 0.004695


	Fold 2/5
Epoch: 1 	Ltrain: 0.013732 	Lval: 0.008948
Epoch: 5 	Ltrain: 0.006913 	Lval: 0.006750
Epoch: 10 	Ltrain: 0.004688 	Lval: 0.005032
Epoch: 15 	Ltrain: 0.004104 	Lval: 0.004433
Epoch 00019: reducing learning rate of group 0 to 2.9644e-04.
Epoch: 20 	Ltrain: 0.003952 	Lval: 0.004387
Epoch: 25 	Ltrain: 0.003902 	Lval: 0.004336
Epoch: 30 	Ltrain: 0.003804 	Lval: 0.004282
Epoch: 35 	Ltrain: 0.003970 	Lval: 0.004273
Epoch: 40 	Ltrain: 0.003703 	Lval: 0.004226
Epoch: 45 	Ltrain: 0.003702 	Lval: 0.004197
Epoch 00048: reducing learning rate of group 0 to 2.9644e-05.
Epoch: 50 	Ltrain: 0.003505 	Lval: 0.004156
Epoch: 55 	Ltrain: 0.003403 	Lval: 0.004160
Epoch: 60 	Ltrain: 0.003441 	Lval: 0.004143
EarlyStopper: stopping at epoch 60 with best_val_loss = 0.004149


	Fold 3/5
Epoch: 1 	Ltrain: 0.022032 	Lval: 0.010807
Epoch: 5 	Ltrain: 0.006284 	Lval: 0.005910
Epoch: 10 	Ltrain: 0.004433 	Lval: 0.004915
Epoch: 15 	Ltrain: 0.003995 	Lval: 0.004588
Epoch: 20 	Ltrain: 0.003649 	Lval: 0.004094
Epoch: 25 	Ltrain: 0.003292 	Lval: 0.004017
Epoch 00028: reducing learning rate of group 0 to 2.9644e-04.
Epoch: 30 	Ltrain: 0.002970 	Lval: 0.003555
Epoch: 35 	Ltrain: 0.002978 	Lval: 0.003514
Epoch: 40 	Ltrain: 0.002918 	Lval: 0.003530
Epoch 00042: reducing learning rate of group 0 to 2.9644e-05.
Epoch: 45 	Ltrain: 0.002854 	Lval: 0.003496
EarlyStopper: stopping at epoch 47 with best_val_loss = 0.003480


	Fold 4/5
Epoch: 1 	Ltrain: 0.013850 	Lval: 0.009364
Epoch: 5 	Ltrain: 0.004843 	Lval: 0.005354
Epoch: 10 	Ltrain: 0.004034 	Lval: 0.004725
Epoch 00011: reducing learning rate of group 0 to 2.9644e-04.
Epoch: 15 	Ltrain: 0.003644 	Lval: 0.004393
Epoch: 20 	Ltrain: 0.003490 	Lval: 0.004332
Epoch: 25 	Ltrain: 0.003432 	Lval: 0.004193
Epoch: 30 	Ltrain: 0.003174 	Lval: 0.004025
Epoch: 35 	Ltrain: 0.003030 	Lval: 0.003905
Epoch: 40 	Ltrain: 0.003008 	Lval: 0.003857
Epoch: 45 	Ltrain: 0.002973 	Lval: 0.003803
Epoch: 50 	Ltrain: 0.002955 	Lval: 0.003716
Epoch: 55 	Ltrain: 0.002931 	Lval: 0.003790
Epoch 00058: reducing learning rate of group 0 to 2.9644e-05.
Epoch: 60 	Ltrain: 0.002845 	Lval: 0.003709
EarlyStopper: stopping at epoch 63 with best_val_loss = 0.003670


	Fold 5/5
Epoch: 1 	Ltrain: 0.020928 	Lval: 0.008646
Epoch: 5 	Ltrain: 0.005109 	Lval: 0.005712
Epoch: 10 	Ltrain: 0.004073 	Lval: 0.004971
Epoch 00014: reducing learning rate of group 0 to 2.9644e-04.
Epoch: 15 	Ltrain: 0.003738 	Lval: 0.004513
Epoch: 20 	Ltrain: 0.003559 	Lval: 0.004467
Epoch: 25 	Ltrain: 0.003348 	Lval: 0.004428
Epoch: 30 	Ltrain: 0.003305 	Lval: 0.004373
Epoch: 35 	Ltrain: 0.003197 	Lval: 0.004163
Epoch: 40 	Ltrain: 0.003058 	Lval: 0.004027
Epoch 00043: reducing learning rate of group 0 to 2.9644e-05.
Epoch: 45 	Ltrain: 0.003015 	Lval: 0.003998
Epoch: 50 	Ltrain: 0.003000 	Lval: 0.003993
Epoch 00055: reducing learning rate of group 0 to 2.9644e-06.
Epoch: 55 	Ltrain: 0.003076 	Lval: 0.004008
EarlyStopper: stopping at epoch 54 with best_val_loss = 0.003998

Best hyperparameters: {'n_hours_u': 72, 'n_hours_y': 24, 'model_class': <class 'src.modelling.GRU.GRU'>, 'input_units': 8, 'hidden_layers': 6, 'hidden_units': 128, 'output_units': 2, 'Optimizer': <class 'torch.optim.adam.Adam'>, 'lr_shared': 0.003694478109386515, 'scheduler': <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>, 'scheduler_kwargs': {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}, 'w_decay': 3.223999424722098e-08, 'loss_fn': MSELoss(), 'epochs': 5000, 'early_stopper': <class 'src.modelling.EarlyStopper.EarlyStopper'>, 'patience': 13, 'batch_sz': 16, 'k_folds': 5}
Best validation loss: 0.0003852030805622538
