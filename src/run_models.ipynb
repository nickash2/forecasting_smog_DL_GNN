{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "For future models, a suggestion is to embed the training/testing functions in a Model class, instead of keeping them loose from each other. (With, optimally, inheritance from a base class, etc etc, such that there is minimal code duplication.) This way, the training procedure can be easily tailored per model. In the current set-up, different functions have to be called for fully-connected networks and hierarchical networks because they handle the data differently. Another way this would be a worth investment, is for implementation of physics-informed models, which require a whole physics injection into the training procedure. In that case, tight coupling is much recommended over the current state of this file. Therefore, I'd first change the code such that it works per model and such that only functionalities independent of model type are actually independent/loosely coupled from the models, therewith facilitating scalable experimentation.\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src\n",
      "MODEL_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src/results/models\n",
      "MINMAX_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/data/data_combined/contaminant_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_PATH = BASE_DIR / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR.parent / \"data\" / \"data_combined\" / \"contaminant_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 72                    # number of hours to use for input\n",
    "N_HOURS_Y = 24                    # number of hours to predict\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u')\n",
    "train_output_frames = get_dataframes('train', 'y')\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u')\n",
    "val_output_frames = get_dataframes('val', 'y')\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u')\n",
    "test_output_frames = get_dataframes('test', 'y')\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : HGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 4,\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "hp_space = []                       # grid search space, put in the hyperparameters\n",
    "                                    # to search over here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter search/training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n"
     ]
    }
   ],
   "source": [
    "print(\"starting training...\")\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_HGRU_at_{current_time}.txt'\n",
    "# train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of HGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     torch.save(model.state_dict(), f\"{MODEL_PATH}/results/model_HGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "HGRU(\n",
      "  (input_layer): GRU(10, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-3): 4 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 16, batch_first=True)\n",
      "        (1): GRU(16, 16, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model = HGRU(hp['n_hours_u'],\n",
    "                 hp['n_hours_y'],\n",
    "                 hp['input_units'],\n",
    "                 hp['hidden_layers'],\n",
    "                 hp['hidden_units'], \n",
    "                 hp['branches'],\n",
    "                 hp['output_units'])\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on full training set...\n",
      "Epoch: 1 \tLtrain: 0.011305 \tLval: 0.010877\n",
      "Epoch: 2 \tLtrain: 0.011735 \tLval: 0.012734\n",
      "Epoch: 3 \tLtrain: 0.011082 \tLval: 0.011529\n",
      "Epoch: 4 \tLtrain: 0.011007 \tLval: 0.010642\n",
      "Epoch: 5 \tLtrain: 0.009112 \tLval: 0.007835\n",
      "Epoch: 6 \tLtrain: 0.008229 \tLval: 0.007742\n",
      "Epoch: 7 \tLtrain: 0.006938 \tLval: 0.007301\n",
      "Epoch: 8 \tLtrain: 0.006323 \tLval: 0.005732\n",
      "Epoch: 9 \tLtrain: 0.005777 \tLval: 0.006547\n",
      "Epoch: 10 \tLtrain: 0.005304 \tLval: 0.005400\n",
      "Epoch: 11 \tLtrain: 0.005326 \tLval: 0.004559\n",
      "Epoch: 12 \tLtrain: 0.004814 \tLval: 0.004823\n",
      "Epoch: 13 \tLtrain: 0.004650 \tLval: 0.005044\n",
      "Epoch: 14 \tLtrain: 0.004256 \tLval: 0.004272\n",
      "Epoch: 15 \tLtrain: 0.004208 \tLval: 0.004502\n",
      "Epoch: 16 \tLtrain: 0.004401 \tLval: 0.004709\n",
      "Epoch: 17 \tLtrain: 0.004201 \tLval: 0.003773\n",
      "Epoch: 18 \tLtrain: 0.003911 \tLval: 0.003780\n",
      "Epoch: 19 \tLtrain: 0.003865 \tLval: 0.003776\n",
      "Epoch: 20 \tLtrain: 0.003925 \tLval: 0.004207\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch 00021: reducing learning rate of group 0 to 4.0000e-04.\n",
      "Epoch: 21 \tLtrain: 0.004008 \tLval: 0.004159\n",
      "Epoch: 22 \tLtrain: 0.003676 \tLval: 0.003772\n",
      "Epoch: 23 \tLtrain: 0.003664 \tLval: 0.003723\n",
      "Epoch: 24 \tLtrain: 0.003671 \tLval: 0.003602\n",
      "Epoch: 25 \tLtrain: 0.003655 \tLval: 0.003721\n",
      "Epoch: 26 \tLtrain: 0.003649 \tLval: 0.003588\n",
      "Epoch: 27 \tLtrain: 0.003636 \tLval: 0.003618\n",
      "Epoch: 28 \tLtrain: 0.003637 \tLval: 0.003622\n",
      "Epoch: 29 \tLtrain: 0.003624 \tLval: 0.003616\n",
      "Epoch: 30 \tLtrain: 0.003616 \tLval: 0.003541\n",
      "Epoch: 31 \tLtrain: 0.003624 \tLval: 0.003790\n",
      "Epoch: 32 \tLtrain: 0.003594 \tLval: 0.003617\n",
      "Epoch: 33 \tLtrain: 0.003613 \tLval: 0.003479\n",
      "Epoch: 34 \tLtrain: 0.003586 \tLval: 0.003555\n",
      "Epoch: 35 \tLtrain: 0.003573 \tLval: 0.003663\n",
      "Epoch: 36 \tLtrain: 0.003624 \tLval: 0.003819\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00037: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 00037: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 00037: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch 00037: reducing learning rate of group 0 to 4.0000e-05.\n",
      "Epoch: 37 \tLtrain: 0.003558 \tLval: 0.003642\n",
      "Epoch: 38 \tLtrain: 0.003545 \tLval: 0.003571\n",
      "Epoch: 39 \tLtrain: 0.003544 \tLval: 0.003572\n",
      "Epoch: 40 \tLtrain: 0.003542 \tLval: 0.003581\n",
      "Epoch: 41 \tLtrain: 0.003542 \tLval: 0.003591\n",
      "Epoch: 42 \tLtrain: 0.003540 \tLval: 0.003568\n",
      "Epoch: 43 \tLtrain: 0.003540 \tLval: 0.003556\n",
      "Epoch: 44 \tLtrain: 0.003538 \tLval: 0.003565\n",
      "Epoch: 45 \tLtrain: 0.003537 \tLval: 0.003553\n",
      "Epoch: 46 \tLtrain: 0.003537 \tLval: 0.003557\n",
      "Epoch: 47 \tLtrain: 0.003535 \tLval: 0.003568\n",
      "Epoch: 48 \tLtrain: 0.003534 \tLval: 0.003553\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 00049: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch: 49 \tLtrain: 0.003533 \tLval: 0.003554\n",
      "Epoch: 50 \tLtrain: 0.003533 \tLval: 0.003552\n",
      "Epoch: 51 \tLtrain: 0.003532 \tLval: 0.003557\n",
      "Epoch: 52 \tLtrain: 0.003532 \tLval: 0.003558\n",
      "Epoch: 53 \tLtrain: 0.003532 \tLval: 0.003550\n",
      "EarlyStopper: stopping at epoch 52 with best_val_loss = 0.003479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "                                        # Train the final model on the full training set,\n",
    "                                        # save the final model, and save the losses for plotting\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "        train_hierarchical(hp, train_loader, val_loader, True)\n",
    "    torch.save(model_final.state_dict(), f'{MODEL_PATH}/model_HGRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_HGRU_at_{current_time}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGRU(\n",
      "  (input_layer): GRU(10, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-3): 4 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 16, batch_first=True)\n",
      "        (1): GRU(16, 16, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_final = HGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "                     hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(torch.load(f\"{MODEL_PATH}/model_HGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MSE: 0.002871926175430417\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_hierarchical(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003586217119335765\n",
      "0.0035552000238870582\n",
      "0.002871926175430417\n",
      "\n",
      "MSE Training set:\n",
      "NO2 : 79.17088792382216\n",
      "O3  : 92.93203316665277\n",
      "PM10: 116.6365341558689\n",
      "PM25: 28.721079989177426\n",
      "\n",
      "MSE Validation set:\n",
      "NO2 : 57.7382615407308\n",
      "O3  : 107.65130297342937\n",
      "PM10: 132.43575795491537\n",
      "PM25: 37.195351918538414\n",
      "\n",
      "MSE Test set:\n",
      "NO2 : 49.507030169169106\n",
      "O3  : 78.67367490132649\n",
      "PM10: 97.43037923177083\n",
      "PM25: 30.48535426457723\n"
     ]
    }
   ],
   "source": [
    "print(test_hierarchical(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE Training set:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 : 8.897802441950919\n",
      "O3  : 9.640126220066746\n",
      "PM10: 10.799839547368762\n",
      "PM25: 5.359205163937785\n",
      "\n",
      "RMSE Validation set:\n",
      "NO2 : 7.598569703617306\n",
      "O3  : 10.375514588367624\n",
      "PM10: 11.508073598779049\n",
      "PM25: 6.098799219398718\n",
      "\n",
      "RMSE Test set:\n",
      "NO2 : 7.036123234364866\n",
      "O3  : 8.869818200015516\n",
      "PM10: 9.870682814870044\n",
      "PM25: 5.521354386794713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.00150654435589"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_hierarchical(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'clone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_pred_vs_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNO2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HOURS_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plot_pred_vs_gt(model_final, test_dataset, pair, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO3\u001b[39m\u001b[38;5;124m'\u001b[39m, N_HOURS_Y)\n\u001b[1;32m      4\u001b[0m plot_pred_vs_gt(model_final, test_dataset, pair, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPM10\u001b[39m\u001b[38;5;124m'\u001b[39m, N_HOURS_Y)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:365\u001b[0m, in \u001b[0;36mplot_pred_vs_gt\u001b[0;34m(model, dataset, row, comp, n_hours_y)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_pred_vs_gt\u001b[39m(\n\u001b[1;32m    354\u001b[0m         model: Any, dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, row: \u001b[38;5;28mint\u001b[39m, comp: \u001b[38;5;28mstr\u001b[39m, n_hours_y: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    355\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    Plots predictions (dotted) vs ground truth (solid) with basic lay-out\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    :param n_hours_y: number of hours in the output sequence (N_HOURS_Y)\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_plot_component_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     set_style()\n\u001b[1;32m    369\u001b[0m     sns\u001b[38;5;241m.\u001b[39mlineplot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(n_hours_y), y \u001b[38;5;241m=\u001b[39m pred, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, linestyle \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:160\u001b[0m, in \u001b[0;36mchoose_plot_component_values\u001b[0;34m(model, dataset, idx, comp, denorm, hier)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_plot_component_values\u001b[39m(\n\u001b[1;32m    142\u001b[0m         model: Any, dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m    143\u001b[0m         idx: \u001b[38;5;28mint\u001b[39m, comp: \u001b[38;5;28mstr\u001b[39m, denorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, hier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Chooses the correct component values for plotting by:\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    - getting the predictions and ground truth for the given index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    :return: tuple of predictions and ground truth for the component\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mget_pred_and_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     comp_idx \u001b[38;5;241m=\u001b[39m get_index(CONTAMINANTS, comp)\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(pred[:, :, comp_idx]), np\u001b[38;5;241m.\u001b[39msqueeze(gt[:, :, comp_idx])\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:80\u001b[0m, in \u001b[0;36mget_pred_and_gt\u001b[0;34m(model, dataset, idx, denorm, hier)\u001b[0m\n\u001b[1;32m     78\u001b[0m         pred \u001b[38;5;241m=\u001b[39m denormalise(torch\u001b[38;5;241m.\u001b[39mcat(model(input_data), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m), MINMAX_PATH)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m---> 80\u001b[0m         pred \u001b[38;5;241m=\u001b[39m \u001b[43mdenormalise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMINMAX_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     81\u001b[0m     gt \u001b[38;5;241m=\u001b[39m denormalise(ground_truth, MINMAX_PATH)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/denormalise.py:64\u001b[0m, in \u001b[0;36mdenormalise\u001b[0;34m(tensor_3D, path, contaminants)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenormalise\u001b[39m(\n\u001b[1;32m     47\u001b[0m         tensor_3D: torch\u001b[38;5;241m.\u001b[39mtensor,\n\u001b[1;32m     48\u001b[0m         path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     49\u001b[0m         contaminants: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNO2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPM10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPM25\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     50\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    Helper function for denormalising the predictions:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    - clones the tensor (because it's passed by reference)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m    :return: denormalised 3D tensor\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     tensor_3D_copy \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_3D\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     65\u001b[0m     dict_minmax \u001b[38;5;241m=\u001b[39m retrieve_min_max(path)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, cont \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(contaminants):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'clone'"
     ]
    }
   ],
   "source": [
    "pair = 5\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
