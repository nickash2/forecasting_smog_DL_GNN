{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "For future models, a suggestion is to embed the training/testing functions in a Model class, instead of keeping them loose from each other. (With, optimally, inheritance from a base class, etc etc, such that there is minimal code duplication.) This way, the training procedure can be easily tailored per model. In the current set-up, different functions have to be called for fully-connected networks and hierarchical networks because they handle the data differently. Another way this would be a worth investment, is for implementation of physics-informed models, which require a whole physics injection into the training procedure. In that case, tight coupling is much recommended over the current state of this file. Therefore, I'd first change the code such that it works per model and such that only functionalities independent of model type are actually independent/loosely coupled from the models, therewith facilitating scalable experimentation.\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src\n",
      "MODEL_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src/results/models\n",
      "MINMAX_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/data/data_combined/utrecht/contaminant_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "CITY_NAME = \"Utrecht\"\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_PATH = BASE_DIR / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR.parent / \"data\" / \"data_combined\" / CITY_NAME.lower() / \"contaminant_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 72                    # number of hours to use for input\n",
    "N_HOURS_Y = 24                    # number of hours to predict\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "CONTAMINANTS = ['NO2', 'O3', 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', CITY_NAME)\n",
    "train_output_frames = get_dataframes('train', 'y', CITY_NAME)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', CITY_NAME)\n",
    "val_output_frames = get_dataframes('val', 'y', CITY_NAME)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', CITY_NAME)\n",
    "test_output_frames = get_dataframes('test', 'y', CITY_NAME)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : HGRU,\n",
    "    'input_units' : train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    'branches' : 2,  # predicting only no2 and o3\n",
    "    'output_units' : train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "# Define hyperparameter space with explicit integer types\n",
    "hp_space = {\n",
    "    'hidden_units': hp['hidden_units'],\n",
    "    'hidden_layers': hp['hidden_layers'],\n",
    "    'branches': hp['branches'],\n",
    "    'epochs': hp['epochs'],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter search/training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"starting training...\")\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_HGRU_at_{current_time}.txt'\n",
    "train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     # If HABROK, print to external file, else print to stdout\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(f\"Grid search execution of HGRU at {current_time}\\n\")\n",
    "#                                     # Train on the full training set\n",
    "#     best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, True)\n",
    "#                                     # Externally save the best model\n",
    "#     # torch.save(model.state_dict(), f\"{MODEL_PATH}/results/model_HGRU.pth\")\n",
    "\n",
    "#     hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "#     print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "HGRU(\n",
      "  (input_layer): GRU(8, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-1): 2 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 32, batch_first=True)\n",
      "        (1): GRU(32, 32, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model_final = HGRU(hp['n_hours_u'],\n",
    "                 hp['n_hours_y'],\n",
    "                 hp['input_units'],\n",
    "                 hp['hidden_layers'],\n",
    "                 hp['hidden_units'], \n",
    "                 hp['branches'],\n",
    "                 hp['output_units'])\n",
    "    print(model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "#                                         # Train the final model on the full training set,\n",
    "#                                         # save the final model, and save the losses for plotting\n",
    "# with PrintManager(stdout_location, 'a', HABROK):\n",
    "#     print(\"\\nTraining on full training set...\")\n",
    "#     model_final, train_losses, test_losses, shared_losses, branch_losses = \\\n",
    "#         train_hierarchical(hp, train_loader, val_loader, True)\n",
    "#     torch.save(model_final.state_dict(), f'{MODEL_PATH}/model_HGRU.pth')\n",
    "\n",
    "# df_losses = pd.DataFrame({'L_train': train_losses, 'L_test': test_losses})\n",
    "# df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_HGRU_at_{current_time}.csv', \n",
    "#                  sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGRU(\n",
      "  (input_layer): GRU(8, 64, batch_first=True)\n",
      "  (shared_layer): GRU(64, 64, batch_first=True)\n",
      "  (branches): ModuleList(\n",
      "    (0-1): 2 x Branch(\n",
      "      (layers): ModuleList(\n",
      "        (0): GRU(64, 32, batch_first=True)\n",
      "        (1): GRU(32, 32, num_layers=3, batch_first=True)\n",
      "        (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model_final = HGRU(hp['input_units'], hp['hidden_layers'], hp['hidden_units'],\n",
    "#                      hp['branches'], hp['output_units'])\n",
    "model_final.load_state_dict(torch.load(f\"{MODEL_PATH}/model_HGRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MSE: 0.0031833447671184936\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "test_error = test_hierarchical(model_final, nn.MSELoss(), test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004611376337934195\n",
      "0.0037963902965808907\n",
      "0.0031833447671184936\n",
      "\n",
      "MSE Training set:\n",
      "NO2: 78.2882450382884\n",
      "O3 : 82.7113973105826\n",
      "\n",
      "MSE Validation set:\n",
      "NO2: 55.78401120503744\n",
      "O3 : 93.03183555603027\n",
      "\n",
      "MSE Test set:\n",
      "NO2: 48.342901865641274\n",
      "O3 : 73.50076611836751\n"
     ]
    }
   ],
   "source": [
    "print(test_hierarchical(model_final, nn.MSELoss(), train_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), val_loader))\n",
    "print(test_hierarchical(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE Training set:\n",
      "NO2: 8.848064626993153\n",
      "O3 : 9.094580727728033\n",
      "\n",
      "RMSE Validation set:\n",
      "NO2: 7.468869473021833\n",
      "O3 : 9.645301216448882\n",
      "\n",
      "RMSE Test set:\n",
      "NO2: 6.952906001496157\n",
      "O3 : 8.57325878055524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.805243863375719"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH)\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_hierarchical_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH)\n",
    ")\n",
    "np.sqrt(test_hierarchical(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplot_pred_vs_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNO2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HOURS_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plot_pred_vs_gt(model_final, test_dataset, pair, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO3\u001b[39m\u001b[38;5;124m'\u001b[39m, N_HOURS_Y)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)\u001b[39;00m\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:415\u001b[0m, in \u001b[0;36mplot_pred_vs_gt\u001b[0;34m(model, dataset, row, comp, n_hours_y)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_pred_vs_gt\u001b[39m(\n\u001b[1;32m    404\u001b[0m     model: Any, dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, row: \u001b[38;5;28mint\u001b[39m, comp: \u001b[38;5;28mstr\u001b[39m, n_hours_y: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    405\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    Plots predictions (dotted) vs ground truth (solid) with basic lay-out\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;124;03m    :param n_hours_y: number of hours in the output sequence (N_HOURS_Y)\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_plot_component_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     set_style()\n\u001b[1;32m    419\u001b[0m     sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(n_hours_y), y\u001b[38;5;241m=\u001b[39mpred, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:184\u001b[0m, in \u001b[0;36mchoose_plot_component_values\u001b[0;34m(model, dataset, idx, comp, denorm, hier)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_plot_component_values\u001b[39m(\n\u001b[1;32m    162\u001b[0m     model: Any,\n\u001b[1;32m    163\u001b[0m     dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m     hier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    168\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Chooses the correct component values for plotting by:\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    - getting the predictions and ground truth for the given index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    :return: tuple of predictions and ground truth for the component\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mget_pred_and_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     comp_idx \u001b[38;5;241m=\u001b[39m get_index(CONTAMINANTS, comp)\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(pred[:, :, comp_idx]), np\u001b[38;5;241m.\u001b[39msqueeze(gt[:, :, comp_idx])\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:92\u001b[0m, in \u001b[0;36mget_pred_and_gt\u001b[0;34m(model, dataset, idx, denorm, hier)\u001b[0m\n\u001b[1;32m     80\u001b[0m         pred \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     81\u001b[0m             denormalise(\n\u001b[1;32m     82\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mcat(torch\u001b[38;5;241m.\u001b[39mtensor(model(input_data)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m         pred \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m             denormalise(\n\u001b[0;32m---> 92\u001b[0m                 \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     93\u001b[0m                 MINMAX_PATH,\n\u001b[1;32m     94\u001b[0m                 contaminants\u001b[38;5;241m=\u001b[39mCONTAMINANTS,\n\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m     gt \u001b[38;5;241m=\u001b[39m denormalise(ground_truth, MINMAX_PATH)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "pair = 2\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)\n",
    "# plot_pred_vs_gt(model_final, test_dataset, pair, 'PM10', N_HOURS_Y)\n",
    "# plot_pred_vs_gt(model_final, test_dataset, pair, 'PM25', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
