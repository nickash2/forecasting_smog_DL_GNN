{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src\n",
      "MODEL_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src/results/models\n",
      "MINMAX_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/data/data_combined/utrecht/contaminant_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "CITY_NAME = \"Utrecht\"\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_PATH = BASE_DIR / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR.parent / \"data\" / \"data_combined\" / CITY_NAME.lower() / \"contaminant_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 72                    # number of hours to use for input\n",
    "N_HOURS_Y = 24                    # number of hours to predict\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "CONTAMINANTS = ['NO2', 'O3'] # 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_gru = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : GRU, # changed to GRU\n",
    "    'input_units' : 8, #train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 6,\n",
    "    'hidden_units' : 128,\n",
    "    # 'branches' : 2,  # predicting only no2 and o3\n",
    "    'output_units' : 2, #train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-5,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 15,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader, denorm=False, path=None) -> float:\n",
    "    model.eval()\n",
    "    test_loss = np.float64(0)\n",
    "    \n",
    "    # Ensure the model is on the correct device\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            batch_test_u = batch_test_u.to(device)\n",
    "            batch_test_y = batch_test_y.to(device)\n",
    "            \n",
    "            pred = model(batch_test_u)\n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y, path)\n",
    "            \n",
    "            test_loss += loss_fn(pred, batch_test_y).item()\n",
    "\n",
    "    return test_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_separately(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    test_loader,\n",
    "    denorm: bool = False,\n",
    "    path: str = None,\n",
    "    components=[\"NO2\", \"O3\", \"PM10\", \"PM25\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates on test set and returns test loss\n",
    "\n",
    "    :param model: model to evaluate, must be some PyTorch type model\n",
    "    :param loss_fn: loss function to use, PyTorch defined, or PyTorch inherited\n",
    "    :param test_loader: DataLoader to get batches from\n",
    "    :param denorm: whether to denormalise the data before calculating loss\n",
    "    :param path: path to the file containing the minmax values for the data\n",
    "    :return: dictionary with contaminant names as keys and losses as values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    test_losses = [np.float64(0) for _ in components]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            pred = model(batch_test_u.to(device))\n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y.to(device), path)\n",
    "\n",
    "            for comp in range(len(components)):\n",
    "                test_losses[comp] += loss_fn(\n",
    "                    pred[:, :, comp], batch_test_y[:, :, comp]\n",
    "                ).item()\n",
    "\n",
    "    for comp in range(len(components)):\n",
    "        test_losses[comp] /= len(test_loader)\n",
    "    return {comp: loss for comp, loss in zip(components, test_losses)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 different GRUs on each city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utrecht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "train_input_frames = get_dataframes('train', 'u', CITY_NAME)\n",
    "train_output_frames = get_dataframes('train', 'y', CITY_NAME)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', CITY_NAME)\n",
    "val_output_frames = get_dataframes('val', 'y', CITY_NAME)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', CITY_NAME)\n",
    "test_output_frames = get_dataframes('test', 'y', CITY_NAME)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "GRU(\n",
      "  (gru): GRU(8, 128, num_layers=6, batch_first=True)\n",
      "  (dense): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with PrintManager('.', 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    utrecht_model = GRU(hp_gru['n_hours_u'],\n",
    "                 hp_gru['n_hours_y'],\n",
    "                 hp_gru['input_units'],\n",
    "                 hp_gru['hidden_layers'],\n",
    "                 hp_gru['hidden_units'], \n",
    "                #  hp['branches'],\n",
    "                 hp_gru['output_units'])\n",
    "    print(utrecht_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRU(\n",
       "  (gru): GRU(8, 128, num_layers=6, batch_first=True)\n",
       "  (dense): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utrecht_model.load_state_dict(torch.load(MODEL_PATH / \"model_GRU_utrecht.pth\"))\n",
    "utrecht_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_GRU_at_{current_time}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on full training set...\n",
      "Epoch: 1 \tLtrain: 0.019557 \tLval: 0.010785\n",
      "Epoch: 5 \tLtrain: 0.006436 \tLval: 0.005834\n",
      "Epoch: 10 \tLtrain: 0.005904 \tLval: 0.004261\n",
      "Epoch: 15 \tLtrain: 0.005371 \tLval: 0.004182\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005230 \tLval: 0.004690\n",
      "Epoch: 25 \tLtrain: 0.004979 \tLval: 0.003930\n",
      "Epoch: 30 \tLtrain: 0.004962 \tLval: 0.003826\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 35 \tLtrain: 0.004928 \tLval: 0.004024\n",
      "EarlyStopper: stopping at epoch 36 with best_val_loss = 0.003820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp_gru['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp_gru['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "#                                         # Train the final model on the full training set,\n",
    "#                                         # save the final model, and save the losses for plotting\n",
    "with PrintManager('.', 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, val_losses = \\\n",
    "        train(hp_gru, train_loader, val_loader, True)\n",
    "    torch.save(model_final.state_dict(), f'{MODEL_PATH}/model_GRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_val': val_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_GRU_at_{current_time}_{CITY_NAME}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MSE: 0.0031511762257044515\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp_gru['batch_sz'], shuffle = False) \n",
    "loss_fn = nn.MSELoss()  # Instantiate the loss function\n",
    "test_error = test(utrecht_model, loss_fn, test_loader)\n",
    "\n",
    "with PrintManager('.', 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2: 6.949684371916662\n",
      "O3 : 8.454666984221609\n"
     ]
    }
   ],
   "source": [
    "print_dict_vertically_root(\n",
    "    test_separately(utrecht_model, nn.MSELoss(), test_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
