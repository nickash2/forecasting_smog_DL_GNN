{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Running the models using the 'modelling' package**\n",
    "\n",
    "A notebook through which different modelling configurations can be ran, using the ``modelling`` package. It follows the steps of:\n",
    "- preparing packages;\n",
    "- setting \"global\" variables;\n",
    "- getting the data;\n",
    "- defining hyperparameters;\n",
    "- running a grid search and/or training a model; and\n",
    "- evaluation.\n",
    "In the modelling package, variations can be made to the models and training functions to experiment. Don't forget to restart the notebook after making changes there.\n",
    "\n",
    "For future models, a suggestion is to embed the training/testing functions in a Model class, instead of keeping them loose from each other. (With, optimally, inheritance from a base class, etc etc, such that there is minimal code duplication.) This way, the training procedure can be easily tailored per model. In the current set-up, different functions have to be called for fully-connected networks and hierarchical networks because they handle the data differently. Another way this would be a worth investment, is for implementation of physics-informed models, which require a whole physics injection into the training procedure. In that case, tight coupling is much recommended over the current state of this file. Therefore, I'd first change the code such that it works per model and such that only functionalities independent of model type are actually independent/loosely coupled from the models, therewith facilitating scalable experimentation.\n",
    "\n",
    "Throughout the notebook, there are printing statements to clarify potential errors happening on Habrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script...\n",
      "\n",
      "Running __init__.py for data pipeline...\n",
      "Modelling package initialized\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting script...\")\n",
    "\n",
    "from modelling import *\n",
    "from modelling import GRU\n",
    "from modelling import HGRU\n",
    "from modelling.plots import set_minmax_path, set_contaminants\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GPU when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set \"global\" variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src\n",
      "MODEL_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/src/results/models\n",
      "MINMAX_PATH:  /home/nick/bachelor-project/forecasting_smog_DL_GNN/data/data_combined/utrecht/contaminant_minmax.csv\n"
     ]
    }
   ],
   "source": [
    "HABROK = bool(0)                  # set to True if using HABROK; it will print\n",
    "                                  # all stdout to a .txt file to log progress\n",
    "CITY_NAME = \"Utrecht\"\n",
    "BASE_DIR = Path.cwd()\n",
    "MODEL_PATH = BASE_DIR / \"results\" / \"models\"\n",
    "MINMAX_PATH = BASE_DIR.parent / \"data\" / \"data_combined\" / CITY_NAME.lower() / \"contaminant_minmax.csv\"\n",
    "\n",
    "print(\"BASE_DIR: \", BASE_DIR)\n",
    "print(\"MODEL_PATH: \", MODEL_PATH)\n",
    "print(\"MINMAX_PATH: \", MINMAX_PATH)\n",
    "\n",
    "torch.manual_seed(34)             # set seed for reproducibility\n",
    "\n",
    "N_HOURS_U = 72                    # number of hours to use for input\n",
    "N_HOURS_Y = 24                    # number of hours to predict\n",
    "N_HOURS_STEP = 24                 # \"sampling rate\" in hours of the data; e.g. 24 \n",
    "                                  # means sample an I/O-pair every 24 hours\n",
    "                                  # the contaminants and meteorological vars\n",
    "CONTAMINANTS = ['NO2', 'O3'] # 'PM10', 'PM25']\n",
    "COMPONENTS = ['NO2', 'O3', 'PM10', 'PM25', 'SQ', 'WD', 'Wvh', 'dewP', 'p', 'temp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load in data and create PyTorch *Datasets***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data\n"
     ]
    }
   ],
   "source": [
    "# Load in data and create PyTorch Datasets. To tune\n",
    "# which exact .csv files get extracted, change the\n",
    "# lists in the get_dataframes() definition\n",
    "\n",
    "train_input_frames = get_dataframes('train', 'u', CITY_NAME)\n",
    "train_output_frames = get_dataframes('train', 'y', CITY_NAME)\n",
    "\n",
    "val_input_frames = get_dataframes('val', 'u', CITY_NAME)\n",
    "val_output_frames = get_dataframes('val', 'y', CITY_NAME)\n",
    "\n",
    "test_input_frames = get_dataframes('test', 'u', CITY_NAME)\n",
    "test_output_frames = get_dataframes('test', 'y', CITY_NAME)\n",
    "\n",
    "print(\"Successfully loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(\n",
    "    train_input_frames,  # list of input training dataframes\n",
    "    train_output_frames, # list of output training dataframes\n",
    "    5,                   # number of dataframes put in for both\n",
    "                         # (basically len(train_input_frames) and\n",
    "                         # len(train_output_frames) must be equal)\n",
    "    N_HOURS_U,           # number of hours of input data\n",
    "    N_HOURS_Y,           # number of hours of output data\n",
    "    N_HOURS_STEP,        # number of hours between each input/output pair\n",
    ")\n",
    "val_dataset = TimeSeriesDataset(\n",
    "    val_input_frames,    # etc.\n",
    "    val_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "test_dataset = TimeSeriesDataset(\n",
    "    test_input_frames,\n",
    "    test_output_frames,\n",
    "    3,\n",
    "    N_HOURS_U,\n",
    "    N_HOURS_Y,\n",
    "    N_HOURS_STEP,\n",
    ")\n",
    "\n",
    "del train_input_frames, train_output_frames\n",
    "del val_input_frames, val_output_frames\n",
    "del test_input_frames, test_output_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, all (hyper)parameters are defined. The hyperparameters are defined in\n",
    "# a dictionary, which is then passed to the model and the training functions.\n",
    "# The grid search is performed by generating all possible combinations of the\n",
    "# hyperparameters defined in the hp_space dictionary, and then performing k-fold cross\n",
    "# validation on each of these configurations. The best configuration is then returned.\n",
    "# When the search is finished, comment out the hp_space dictionary and save the best found\n",
    "# hyperparameters in the hp dictionary, and train the final model with these.\n",
    "\n",
    "hp = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : GRU, # changed to GRU\n",
    "    'input_units' : 8, #train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 64,\n",
    "    # 'branches' : 2,  # predicting only no2 and o3\n",
    "    'output_units' : 2, #train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-7,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 20,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}                                   # The lr for the branched layer(s) is calculated\n",
    "                                    # based on the \"power ratio\" between the branched\n",
    "                                    # part of the network and the shared layer, which\n",
    "                                    # is *assumed* to be proportional to n_hidden_layers\n",
    "# hp['lr_branch'] = hp['lr_shared'] * hp['hidden_layers']\n",
    "\n",
    "hp_space = {\n",
    "    'n_hours_u': N_HOURS_U,  # Try different input time windows\n",
    "    'n_hours_y': N_HOURS_Y,  # Different forecast horizons\n",
    "    \n",
    "    'model_class': GRU,  # If you want to test multiple models, add them here\n",
    "    'input_units': [8],  # Number of input features (could match your dataset size)\n",
    "    'hidden_layers': [2, 4, 6],  # Number of hidden layers to test\n",
    "    'hidden_units': [32, 64, 128],  # Size of hidden units per layer\n",
    "    'output_units': [2],  # Fixed at 2 since you’re predicting NO2 and O3\n",
    "    \n",
    "    'Optimizer': [torch.optim.Adam],  # You could add others like SGD, RMSprop, etc.\n",
    "    'lr_shared': [1e-3, 1e-4, 5e-4],  # Learning rates to test\n",
    "    'scheduler': [torch.optim.lr_scheduler.ReduceLROnPlateau],\n",
    "    'scheduler_kwargs': [\n",
    "        {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True},\n",
    "        {'mode': 'min', 'factor': 0.5, 'patience': 5, 'cooldown': 10, 'verbose': True}\n",
    "    ],\n",
    "    'w_decay': [1e-7, 1e-6, 1e-5],  # Weight decay values\n",
    "    \n",
    "    'loss_fn': [nn.MSELoss()],\n",
    "    \n",
    "    'epochs': [1000, 3000, 5000],  # Different max epochs for longer/shorter training\n",
    "    'early_stopper': [EarlyStopper],\n",
    "    'patience': [10, 20, 30],  # Patience for early stopping\n",
    "    'batch_sz': [16, 32, 64],  # Mini-batch sizes\n",
    "    'k_folds': [5],  # Number of folds for cross-validation\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Start hyperparameter search/training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search execution of GRU at 20250223-123853\n",
      "\n",
      "CURRENT CONFIGURATION:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "\n",
      "\tFold 1/5\n",
      "Epoch: 1 \tLtrain: 0.040494 \tLval: 0.021509\n",
      "Epoch: 5 \tLtrain: 0.016046 \tLval: 0.015338\n",
      "Epoch: 10 \tLtrain: 0.011361 \tLval: 0.010846\n",
      "Epoch: 15 \tLtrain: 0.010357 \tLval: 0.009620\n",
      "Epoch: 20 \tLtrain: 0.009066 \tLval: 0.009063\n",
      "Epoch: 25 \tLtrain: 0.008425 \tLval: 0.008408\n",
      "Epoch: 30 \tLtrain: 0.007674 \tLval: 0.008015\n",
      "Epoch: 35 \tLtrain: 0.008271 \tLval: 0.008301\n",
      "Epoch: 40 \tLtrain: 0.007982 \tLval: 0.007638\n",
      "Epoch: 45 \tLtrain: 0.007368 \tLval: 0.007701\n",
      "Epoch: 50 \tLtrain: 0.007321 \tLval: 0.007454\n",
      "Epoch: 55 \tLtrain: 0.007363 \tLval: 0.007376\n",
      "Epoch: 60 \tLtrain: 0.007348 \tLval: 0.007316\n",
      "Epoch: 65 \tLtrain: 0.007300 \tLval: 0.007238\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 70 \tLtrain: 0.007488 \tLval: 0.007233\n",
      "Epoch: 75 \tLtrain: 0.007239 \tLval: 0.007199\n",
      "Epoch: 80 \tLtrain: 0.007465 \tLval: 0.007197\n",
      "Epoch: 85 \tLtrain: 0.007127 \tLval: 0.007187\n",
      "Epoch: 90 \tLtrain: 0.007007 \tLval: 0.007185\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 95 \tLtrain: 0.007387 \tLval: 0.007181\n",
      "EarlyStopper: stopping at epoch 95 with best_val_loss = 0.007183\n",
      "\n",
      "\n",
      "\tFold 2/5\n",
      "Epoch: 1 \tLtrain: 0.044034 \tLval: 0.025026\n",
      "Epoch: 5 \tLtrain: 0.012282 \tLval: 0.012952\n",
      "Epoch: 10 \tLtrain: 0.008943 \tLval: 0.009378\n",
      "Epoch: 15 \tLtrain: 0.007720 \tLval: 0.008384\n",
      "Epoch: 20 \tLtrain: 0.007117 \tLval: 0.007783\n",
      "Epoch: 25 \tLtrain: 0.006798 \tLval: 0.007817\n",
      "Epoch: 30 \tLtrain: 0.006686 \tLval: 0.007359\n",
      "Epoch: 35 \tLtrain: 0.006609 \tLval: 0.007301\n",
      "Epoch: 40 \tLtrain: 0.006494 \tLval: 0.007369\n",
      "Epoch: 45 \tLtrain: 0.006442 \tLval: 0.007175\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 50 \tLtrain: 0.006421 \tLval: 0.007528\n",
      "Epoch: 55 \tLtrain: 0.006283 \tLval: 0.007093\n",
      "Epoch: 60 \tLtrain: 0.006231 \tLval: 0.007080\n",
      "Epoch 00064: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 65 \tLtrain: 0.006250 \tLval: 0.007087\n",
      "Epoch: 70 \tLtrain: 0.006287 \tLval: 0.007087\n",
      "EarlyStopper: stopping at epoch 69 with best_val_loss = 0.007080\n",
      "\n",
      "\n",
      "\tFold 3/5\n",
      "Epoch: 1 \tLtrain: 0.030648 \tLval: 0.016533\n",
      "Epoch: 5 \tLtrain: 0.008597 \tLval: 0.010280\n",
      "Epoch: 10 \tLtrain: 0.007038 \tLval: 0.008824\n",
      "Epoch: 15 \tLtrain: 0.006215 \tLval: 0.007785\n",
      "Epoch: 20 \tLtrain: 0.005914 \tLval: 0.007491\n",
      "Epoch: 25 \tLtrain: 0.006001 \tLval: 0.007418\n",
      "Epoch: 30 \tLtrain: 0.005779 \tLval: 0.008034\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 35 \tLtrain: 0.005641 \tLval: 0.007364\n",
      "Epoch: 40 \tLtrain: 0.005625 \tLval: 0.007344\n",
      "EarlyStopper: stopping at epoch 41 with best_val_loss = 0.007252\n",
      "\n",
      "\n",
      "\tFold 4/5\n",
      "Epoch: 1 \tLtrain: 0.039128 \tLval: 0.024435\n",
      "Epoch: 5 \tLtrain: 0.008051 \tLval: 0.010480\n",
      "Epoch: 10 \tLtrain: 0.006241 \tLval: 0.008371\n",
      "Epoch: 15 \tLtrain: 0.005701 \tLval: 0.007590\n",
      "Epoch: 20 \tLtrain: 0.005366 \tLval: 0.008198\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 25 \tLtrain: 0.005343 \tLval: 0.007617\n",
      "Epoch: 30 \tLtrain: 0.005485 \tLval: 0.007381\n",
      "Epoch: 35 \tLtrain: 0.005241 \tLval: 0.007388\n",
      "EarlyStopper: stopping at epoch 38 with best_val_loss = 0.007291\n",
      "\n",
      "\n",
      "\tFold 5/5\n",
      "Epoch: 1 \tLtrain: 0.022805 \tLval: 0.020387\n",
      "Epoch: 5 \tLtrain: 0.007275 \tLval: 0.010105\n",
      "Epoch: 10 \tLtrain: 0.005857 \tLval: 0.008047\n",
      "Epoch: 15 \tLtrain: 0.005551 \tLval: 0.007555\n",
      "Epoch: 20 \tLtrain: 0.005364 \tLval: 0.007986\n",
      "Epoch: 25 \tLtrain: 0.005412 \tLval: 0.007279\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 30 \tLtrain: 0.005170 \tLval: 0.007436\n",
      "EarlyStopper: stopping at epoch 33 with best_val_loss = 0.007216\n",
      "\n",
      "Average final validation loss: 0.007264\n",
      "##### Best average validation loss so far: 0.007264 #####\n",
      "With configuration:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "CURRENT CONFIGURATION:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 4\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "\n",
      "\tFold 1/5\n",
      "Epoch: 1 \tLtrain: 0.020992 \tLval: 0.018943\n",
      "Epoch: 5 \tLtrain: 0.014547 \tLval: 0.013511\n",
      "Epoch: 10 \tLtrain: 0.011015 \tLval: 0.011071\n",
      "Epoch: 15 \tLtrain: 0.010025 \tLval: 0.009305\n",
      "Epoch: 20 \tLtrain: 0.009086 \tLval: 0.008741\n",
      "Epoch 00022: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 25 \tLtrain: 0.008257 \tLval: 0.008231\n",
      "Epoch: 30 \tLtrain: 0.008300 \tLval: 0.008183\n",
      "Epoch: 35 \tLtrain: 0.008190 \tLval: 0.008174\n",
      "Epoch: 40 \tLtrain: 0.008109 \tLval: 0.008089\n",
      "Epoch: 45 \tLtrain: 0.007819 \tLval: 0.008041\n",
      "Epoch: 50 \tLtrain: 0.007988 \tLval: 0.008005\n",
      "Epoch: 55 \tLtrain: 0.007865 \tLval: 0.007983\n",
      "Epoch: 60 \tLtrain: 0.007773 \tLval: 0.007939\n",
      "Epoch: 65 \tLtrain: 0.007734 \tLval: 0.007894\n",
      "Epoch: 70 \tLtrain: 0.007832 \tLval: 0.007861\n",
      "Epoch: 75 \tLtrain: 0.007867 \tLval: 0.007842\n",
      "Epoch: 80 \tLtrain: 0.008033 \tLval: 0.007816\n",
      "Epoch: 85 \tLtrain: 0.007732 \tLval: 0.007777\n",
      "Epoch: 90 \tLtrain: 0.007823 \tLval: 0.007760\n",
      "Epoch: 95 \tLtrain: 0.008259 \tLval: 0.007779\n",
      "Epoch: 100 \tLtrain: 0.007752 \tLval: 0.007732\n",
      "Epoch: 105 \tLtrain: 0.007662 \tLval: 0.007691\n",
      "Epoch: 110 \tLtrain: 0.007642 \tLval: 0.007666\n",
      "Epoch: 115 \tLtrain: 0.007478 \tLval: 0.007642\n",
      "Epoch: 120 \tLtrain: 0.007470 \tLval: 0.007660\n",
      "Epoch: 125 \tLtrain: 0.007497 \tLval: 0.007598\n",
      "Epoch: 130 \tLtrain: 0.007403 \tLval: 0.007589\n",
      "Epoch 00132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 135 \tLtrain: 0.007678 \tLval: 0.007585\n",
      "Epoch: 140 \tLtrain: 0.007334 \tLval: 0.007574\n",
      "Epoch: 145 \tLtrain: 0.007469 \tLval: 0.007570\n",
      "EarlyStopper: stopping at epoch 147 with best_val_loss = 0.007575\n",
      "\n",
      "\n",
      "\tFold 2/5\n",
      "Epoch: 1 \tLtrain: 0.055579 \tLval: 0.026069\n",
      "Epoch: 5 \tLtrain: 0.012903 \tLval: 0.013004\n",
      "Epoch: 10 \tLtrain: 0.008747 \tLval: 0.009646\n",
      "Epoch: 15 \tLtrain: 0.007854 \tLval: 0.008353\n",
      "Epoch: 20 \tLtrain: 0.007311 \tLval: 0.008157\n",
      "Epoch: 25 \tLtrain: 0.006899 \tLval: 0.007639\n",
      "Epoch: 30 \tLtrain: 0.007044 \tLval: 0.007622\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 35 \tLtrain: 0.006515 \tLval: 0.007373\n",
      "Epoch: 40 \tLtrain: 0.006501 \tLval: 0.007333\n",
      "Epoch: 45 \tLtrain: 0.006522 \tLval: 0.007314\n",
      "Epoch: 50 \tLtrain: 0.006469 \tLval: 0.007303\n",
      "Epoch 00054: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 55 \tLtrain: 0.006506 \tLval: 0.007306\n",
      "Epoch: 60 \tLtrain: 0.006451 \tLval: 0.007307\n",
      "EarlyStopper: stopping at epoch 59 with best_val_loss = 0.007303\n",
      "\n",
      "\n",
      "\tFold 3/5\n",
      "Epoch: 1 \tLtrain: 0.022517 \tLval: 0.018587\n",
      "Epoch: 5 \tLtrain: 0.008504 \tLval: 0.010105\n",
      "Epoch: 10 \tLtrain: 0.006445 \tLval: 0.008257\n",
      "Epoch: 15 \tLtrain: 0.006135 \tLval: 0.008342\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005830 \tLval: 0.007448\n",
      "Epoch: 25 \tLtrain: 0.005815 \tLval: 0.007462\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 30 \tLtrain: 0.005779 \tLval: 0.007501\n",
      "EarlyStopper: stopping at epoch 32 with best_val_loss = 0.007411\n",
      "\n",
      "\n",
      "\tFold 4/5\n",
      "Epoch: 1 \tLtrain: 0.022160 \tLval: 0.019506\n",
      "Epoch: 5 \tLtrain: 0.007388 \tLval: 0.009594\n",
      "Epoch: 10 \tLtrain: 0.005615 \tLval: 0.008937\n",
      "Epoch: 15 \tLtrain: 0.005555 \tLval: 0.007409\n",
      "Epoch: 20 \tLtrain: 0.005378 \tLval: 0.007334\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 25 \tLtrain: 0.005211 \tLval: 0.007281\n",
      "Epoch: 30 \tLtrain: 0.005105 \tLval: 0.007228\n",
      "EarlyStopper: stopping at epoch 30 with best_val_loss = 0.007193\n",
      "\n",
      "\n",
      "\tFold 5/5\n",
      "Epoch: 1 \tLtrain: 0.022052 \tLval: 0.015843\n",
      "Epoch: 5 \tLtrain: 0.008205 \tLval: 0.010322\n",
      "Epoch: 10 \tLtrain: 0.005880 \tLval: 0.007876\n",
      "Epoch: 15 \tLtrain: 0.005457 \tLval: 0.007774\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005297 \tLval: 0.007532\n",
      "Epoch: 25 \tLtrain: 0.005438 \tLval: 0.007464\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EarlyStopper: stopping at epoch 28 with best_val_loss = 0.007472\n",
      "\n",
      "Average final validation loss: 0.007461\n",
      "##### Best average validation loss so far: 0.007264 #####\n",
      "With configuration:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "CURRENT CONFIGURATION:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 6\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "\n",
      "\tFold 1/5\n",
      "Epoch: 1 \tLtrain: 0.047640 \tLval: 0.019791\n",
      "Epoch: 5 \tLtrain: 0.018651 \tLval: 0.018347\n",
      "Epoch: 10 \tLtrain: 0.013895 \tLval: 0.013212\n",
      "Epoch: 15 \tLtrain: 0.011336 \tLval: 0.010959\n",
      "Epoch: 20 \tLtrain: 0.009661 \tLval: 0.009644\n",
      "Epoch: 25 \tLtrain: 0.008796 \tLval: 0.009048\n",
      "Epoch: 30 \tLtrain: 0.008261 \tLval: 0.009573\n",
      "Epoch 00031: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 35 \tLtrain: 0.007877 \tLval: 0.007891\n",
      "Epoch: 40 \tLtrain: 0.007564 \tLval: 0.007853\n",
      "Epoch: 45 \tLtrain: 0.007735 \tLval: 0.007853\n",
      "Epoch: 50 \tLtrain: 0.007880 \tLval: 0.007823\n",
      "Epoch: 55 \tLtrain: 0.007871 \tLval: 0.007831\n",
      "Epoch: 60 \tLtrain: 0.007802 \tLval: 0.007797\n",
      "Epoch: 65 \tLtrain: 0.007910 \tLval: 0.007779\n",
      "Epoch: 70 \tLtrain: 0.008001 \tLval: 0.007778\n",
      "Epoch: 75 \tLtrain: 0.007676 \tLval: 0.007761\n",
      "Epoch: 80 \tLtrain: 0.007899 \tLval: 0.007744\n",
      "Epoch: 85 \tLtrain: 0.007570 \tLval: 0.007716\n",
      "Epoch: 90 \tLtrain: 0.007608 \tLval: 0.007708\n",
      "Epoch: 95 \tLtrain: 0.007669 \tLval: 0.007708\n",
      "Epoch: 100 \tLtrain: 0.007881 \tLval: 0.007681\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 105 \tLtrain: 0.007517 \tLval: 0.007672\n",
      "Epoch: 110 \tLtrain: 0.007657 \tLval: 0.007670\n",
      "Epoch: 115 \tLtrain: 0.007431 \tLval: 0.007667\n",
      "EarlyStopper: stopping at epoch 114 with best_val_loss = 0.007672\n",
      "\n",
      "\n",
      "\tFold 2/5\n",
      "Epoch: 1 \tLtrain: 0.031325 \tLval: 0.022156\n",
      "Epoch: 5 \tLtrain: 0.016867 \tLval: 0.017214\n",
      "Epoch: 10 \tLtrain: 0.008960 \tLval: 0.009536\n",
      "Epoch: 15 \tLtrain: 0.007631 \tLval: 0.008550\n",
      "Epoch: 20 \tLtrain: 0.007261 \tLval: 0.007839\n",
      "Epoch: 25 \tLtrain: 0.006900 \tLval: 0.007690\n",
      "Epoch: 30 \tLtrain: 0.007097 \tLval: 0.008255\n",
      "Epoch: 35 \tLtrain: 0.006728 \tLval: 0.007428\n",
      "Epoch: 40 \tLtrain: 0.006611 \tLval: 0.007465\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 45 \tLtrain: 0.006284 \tLval: 0.007119\n",
      "Epoch: 50 \tLtrain: 0.006258 \tLval: 0.007096\n",
      "Epoch: 55 \tLtrain: 0.006196 \tLval: 0.007091\n",
      "Epoch: 60 \tLtrain: 0.006207 \tLval: 0.007084\n",
      "Epoch 00061: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 65 \tLtrain: 0.006234 \tLval: 0.007063\n",
      "EarlyStopper: stopping at epoch 66 with best_val_loss = 0.007070\n",
      "\n",
      "\n",
      "\tFold 3/5\n",
      "Epoch: 1 \tLtrain: 0.028343 \tLval: 0.020914\n",
      "Epoch: 5 \tLtrain: 0.010510 \tLval: 0.011921\n",
      "Epoch: 10 \tLtrain: 0.007092 \tLval: 0.008306\n",
      "Epoch: 15 \tLtrain: 0.006371 \tLval: 0.007587\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005784 \tLval: 0.007489\n",
      "Epoch: 25 \tLtrain: 0.005770 \tLval: 0.007464\n",
      "Epoch: 30 \tLtrain: 0.005728 \tLval: 0.007456\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 35 \tLtrain: 0.005707 \tLval: 0.007413\n",
      "EarlyStopper: stopping at epoch 38 with best_val_loss = 0.007341\n",
      "\n",
      "\n",
      "\tFold 4/5\n",
      "Epoch: 1 \tLtrain: 0.031744 \tLval: 0.019078\n",
      "Epoch: 5 \tLtrain: 0.008744 \tLval: 0.010672\n",
      "Epoch: 10 \tLtrain: 0.006548 \tLval: 0.008738\n",
      "Epoch: 15 \tLtrain: 0.005771 \tLval: 0.008671\n",
      "Epoch: 20 \tLtrain: 0.005594 \tLval: 0.007509\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 25 \tLtrain: 0.005332 \tLval: 0.007745\n",
      "Epoch: 30 \tLtrain: 0.005266 \tLval: 0.007446\n",
      "EarlyStopper: stopping at epoch 30 with best_val_loss = 0.007300\n",
      "\n",
      "\n",
      "\tFold 5/5\n",
      "Epoch: 1 \tLtrain: 0.017931 \tLval: 0.016600\n",
      "Epoch: 5 \tLtrain: 0.007556 \tLval: 0.009323\n",
      "Epoch: 10 \tLtrain: 0.005668 \tLval: 0.007955\n",
      "Epoch: 15 \tLtrain: 0.005449 \tLval: 0.007512\n",
      "Epoch: 20 \tLtrain: 0.005192 \tLval: 0.007849\n",
      "Epoch: 25 \tLtrain: 0.005283 \tLval: 0.007729\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 30 \tLtrain: 0.004955 \tLval: 0.007166\n",
      "Epoch: 35 \tLtrain: 0.005020 \tLval: 0.007243\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 40 \tLtrain: 0.004974 \tLval: 0.007059\n",
      "EarlyStopper: stopping at epoch 41 with best_val_loss = 0.006988\n",
      "\n",
      "Average final validation loss: 0.007305\n",
      "##### Best average validation loss so far: 0.007264 #####\n",
      "With configuration:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 32\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "CURRENT CONFIGURATION:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 64\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "\n",
      "\tFold 1/5\n",
      "Epoch: 1 \tLtrain: 0.030548 \tLval: 0.021283\n",
      "Epoch: 5 \tLtrain: 0.011932 \tLval: 0.011499\n",
      "Epoch: 10 \tLtrain: 0.009428 \tLval: 0.009346\n",
      "Epoch: 15 \tLtrain: 0.008718 \tLval: 0.008432\n",
      "Epoch: 20 \tLtrain: 0.008051 \tLval: 0.008099\n",
      "Epoch: 25 \tLtrain: 0.007794 \tLval: 0.007602\n",
      "Epoch: 30 \tLtrain: 0.008381 \tLval: 0.007523\n",
      "Epoch: 35 \tLtrain: 0.008280 \tLval: 0.007680\n",
      "Epoch: 40 \tLtrain: 0.007899 \tLval: 0.007755\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 45 \tLtrain: 0.007494 \tLval: 0.007229\n",
      "Epoch: 50 \tLtrain: 0.007037 \tLval: 0.007201\n",
      "Epoch: 55 \tLtrain: 0.007140 \tLval: 0.007186\n",
      "Epoch: 60 \tLtrain: 0.007102 \tLval: 0.007175\n",
      "Epoch: 65 \tLtrain: 0.007583 \tLval: 0.007171\n",
      "Epoch: 70 \tLtrain: 0.007352 \tLval: 0.007158\n",
      "Epoch 00073: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 75 \tLtrain: 0.007014 \tLval: 0.007159\n",
      "EarlyStopper: stopping at epoch 77 with best_val_loss = 0.007160\n",
      "\n",
      "\n",
      "\tFold 2/5\n",
      "Epoch: 1 \tLtrain: 0.036449 \tLval: 0.020921\n",
      "Epoch: 5 \tLtrain: 0.009749 \tLval: 0.010366\n",
      "Epoch: 10 \tLtrain: 0.007866 \tLval: 0.008426\n",
      "Epoch: 15 \tLtrain: 0.007140 \tLval: 0.008052\n",
      "Epoch: 20 \tLtrain: 0.006760 \tLval: 0.007485\n",
      "Epoch: 25 \tLtrain: 0.006573 \tLval: 0.007361\n",
      "Epoch: 30 \tLtrain: 0.006529 \tLval: 0.007328\n",
      "Epoch: 35 \tLtrain: 0.006482 \tLval: 0.007240\n",
      "Epoch 00037: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 40 \tLtrain: 0.006459 \tLval: 0.007195\n",
      "Epoch: 45 \tLtrain: 0.006338 \tLval: 0.007193\n",
      "Epoch 00049: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 50 \tLtrain: 0.006306 \tLval: 0.007168\n",
      "EarlyStopper: stopping at epoch 51 with best_val_loss = 0.007157\n",
      "\n",
      "\n",
      "\tFold 3/5\n",
      "Epoch: 1 \tLtrain: 0.022139 \tLval: 0.013675\n",
      "Epoch: 5 \tLtrain: 0.007451 \tLval: 0.008705\n",
      "Epoch: 10 \tLtrain: 0.006030 \tLval: 0.007904\n",
      "Epoch: 15 \tLtrain: 0.005811 \tLval: 0.007453\n",
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005848 \tLval: 0.007489\n",
      "Epoch: 25 \tLtrain: 0.005595 \tLval: 0.007229\n",
      "Epoch: 30 \tLtrain: 0.005582 \tLval: 0.007293\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EarlyStopper: stopping at epoch 33 with best_val_loss = 0.007193\n",
      "\n",
      "\n",
      "\tFold 4/5\n",
      "Epoch: 1 \tLtrain: 0.017707 \tLval: 0.012604\n",
      "Epoch: 5 \tLtrain: 0.006543 \tLval: 0.008549\n",
      "Epoch: 10 \tLtrain: 0.005537 \tLval: 0.008250\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 15 \tLtrain: 0.005382 \tLval: 0.007638\n",
      "Epoch: 20 \tLtrain: 0.005208 \tLval: 0.007331\n",
      "EarlyStopper: stopping at epoch 20 with best_val_loss = 0.007326\n",
      "\n",
      "\n",
      "\tFold 5/5\n",
      "Epoch: 1 \tLtrain: 0.016082 \tLval: 0.015132\n",
      "Epoch: 5 \tLtrain: 0.006436 \tLval: 0.008260\n",
      "Epoch: 10 \tLtrain: 0.005575 \tLval: 0.008088\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 15 \tLtrain: 0.005290 \tLval: 0.007489\n",
      "Epoch: 20 \tLtrain: 0.005252 \tLval: 0.007560\n",
      "Epoch: 25 \tLtrain: 0.005214 \tLval: 0.007675\n",
      "Epoch 00028: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 30 \tLtrain: 0.005212 \tLval: 0.007426\n",
      "EarlyStopper: stopping at epoch 33 with best_val_loss = 0.007303\n",
      "\n",
      "Average final validation loss: 0.007261\n",
      "##### Best average validation loss so far: 0.007261 #####\n",
      "With configuration:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 2\n",
      "hidden_units    : 64\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "CURRENT CONFIGURATION:\n",
      "n_hours_u       : 72\n",
      "n_hours_y       : 24\n",
      "model_class     : <class 'modelling.GRU.GRU'>\n",
      "input_units     : 8\n",
      "hidden_layers   : 4\n",
      "hidden_units    : 64\n",
      "output_units    : 2\n",
      "Optimizer       : <class 'torch.optim.adam.Adam'>\n",
      "lr_shared       : 0.001\n",
      "scheduler       : <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\n",
      "scheduler_kwargs: {'mode': 'min', 'factor': 0.1, 'patience': 3, 'cooldown': 8, 'verbose': True}\n",
      "w_decay         : 1e-07\n",
      "loss_fn         : MSELoss()\n",
      "epochs          : 1000\n",
      "early_stopper   : <class 'modelling.EarlyStopper.EarlyStopper'>\n",
      "patience        : 10\n",
      "batch_sz        : 16\n",
      "k_folds         : 5\n",
      "\n",
      "\n",
      "\tFold 1/5\n",
      "Epoch: 1 \tLtrain: 0.050800 \tLval: 0.023623\n",
      "Epoch: 5 \tLtrain: 0.014879 \tLval: 0.014543\n",
      "Epoch: 10 \tLtrain: 0.011003 \tLval: 0.010816\n",
      "Epoch: 15 \tLtrain: 0.009479 \tLval: 0.009629\n",
      "Epoch: 20 \tLtrain: 0.008845 \tLval: 0.008390\n",
      "Epoch: 25 \tLtrain: 0.008211 \tLval: 0.007817\n",
      "Epoch: 30 \tLtrain: 0.007842 \tLval: 0.007696\n",
      "Epoch: 35 \tLtrain: 0.007831 \tLval: 0.007435\n",
      "Epoch: 40 \tLtrain: 0.007433 \tLval: 0.007724\n",
      "Epoch: 45 \tLtrain: 0.007477 \tLval: 0.007912\n",
      "Epoch 00046: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 50 \tLtrain: 0.007126 \tLval: 0.007179\n",
      "Epoch: 55 \tLtrain: 0.007300 \tLval: 0.007170\n",
      "Epoch: 60 \tLtrain: 0.007431 \tLval: 0.007152\n",
      "Epoch: 65 \tLtrain: 0.007326 \tLval: 0.007128\n",
      "Epoch: 70 \tLtrain: 0.007481 \tLval: 0.007120\n",
      "Epoch: 75 \tLtrain: 0.007095 \tLval: 0.007107\n",
      "Epoch: 80 \tLtrain: 0.007066 \tLval: 0.007106\n",
      "Epoch: 85 \tLtrain: 0.007047 \tLval: 0.007091\n",
      "Epoch: 90 \tLtrain: 0.007029 \tLval: 0.007078\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 95 \tLtrain: 0.007337 \tLval: 0.007074\n",
      "Epoch: 100 \tLtrain: 0.006953 \tLval: 0.007070\n",
      "Epoch: 105 \tLtrain: 0.006927 \tLval: 0.007071\n",
      "EarlyStopper: stopping at epoch 104 with best_val_loss = 0.007074\n",
      "\n",
      "\n",
      "\tFold 2/5\n",
      "Epoch: 1 \tLtrain: 0.037514 \tLval: 0.023224\n",
      "Epoch: 5 \tLtrain: 0.011317 \tLval: 0.011225\n",
      "Epoch: 10 \tLtrain: 0.007873 \tLval: 0.008495\n",
      "Epoch: 15 \tLtrain: 0.007054 \tLval: 0.007727\n",
      "Epoch: 20 \tLtrain: 0.006993 \tLval: 0.007400\n",
      "Epoch 00024: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 25 \tLtrain: 0.006783 \tLval: 0.007406\n",
      "Epoch: 30 \tLtrain: 0.006425 \tLval: 0.007324\n",
      "Epoch: 35 \tLtrain: 0.006418 \tLval: 0.007310\n",
      "Epoch: 40 \tLtrain: 0.006413 \tLval: 0.007249\n",
      "Epoch: 45 \tLtrain: 0.006431 \tLval: 0.007295\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 50 \tLtrain: 0.006391 \tLval: 0.007238\n",
      "Epoch: 55 \tLtrain: 0.006479 \tLval: 0.007244\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 60 \tLtrain: 0.006315 \tLval: 0.007237\n",
      "EarlyStopper: stopping at epoch 59 with best_val_loss = 0.007238\n",
      "\n",
      "\n",
      "\tFold 3/5\n",
      "Epoch: 1 \tLtrain: 0.020903 \tLval: 0.017491\n",
      "Epoch: 5 \tLtrain: 0.008461 \tLval: 0.010558\n",
      "Epoch: 10 \tLtrain: 0.006391 \tLval: 0.007459\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 15 \tLtrain: 0.005934 \tLval: 0.007972\n",
      "Epoch: 20 \tLtrain: 0.005687 \tLval: 0.007341\n",
      "Epoch: 25 \tLtrain: 0.005682 \tLval: 0.007416\n",
      "Epoch 00027: reducing learning rate of group 0 to 1.0000e-05.\n",
      "EarlyStopper: stopping at epoch 27 with best_val_loss = 0.007345\n",
      "\n",
      "\n",
      "\tFold 4/5\n",
      "Epoch: 1 \tLtrain: 0.017966 \tLval: 0.015130\n",
      "Epoch: 5 \tLtrain: 0.006875 \tLval: 0.008849\n",
      "Epoch: 10 \tLtrain: 0.005755 \tLval: 0.007840\n",
      "Epoch: 15 \tLtrain: 0.005300 \tLval: 0.007931\n",
      "Epoch: 20 \tLtrain: 0.005470 \tLval: 0.007244\n",
      "Epoch: 25 \tLtrain: 0.005144 \tLval: 0.006980\n",
      "Epoch: 30 \tLtrain: 0.005062 \tLval: 0.006862\n",
      "Epoch: 35 \tLtrain: 0.004941 \tLval: 0.006779\n",
      "Epoch 00040: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 40 \tLtrain: 0.005023 \tLval: 0.006834\n",
      "Epoch: 45 \tLtrain: 0.004616 \tLval: 0.006452\n",
      "Epoch: 50 \tLtrain: 0.004609 \tLval: 0.006397\n",
      "Epoch: 55 \tLtrain: 0.004622 \tLval: 0.006338\n",
      "Epoch: 60 \tLtrain: 0.004571 \tLval: 0.006327\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 65 \tLtrain: 0.004545 \tLval: 0.006386\n",
      "EarlyStopper: stopping at epoch 66 with best_val_loss = 0.006279\n",
      "\n",
      "\n",
      "\tFold 5/5\n",
      "Epoch: 1 \tLtrain: 0.024179 \tLval: 0.019429\n",
      "Epoch: 5 \tLtrain: 0.007089 \tLval: 0.009809\n",
      "Epoch: 10 \tLtrain: 0.005634 \tLval: 0.008268\n",
      "An exception of type <class 'KeyboardInterrupt'> occurred with value \n",
      "Traceback: <traceback object at 0x7faca1239300>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid search execution of GRU at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m                                 \u001b[38;5;66;03m# Train on the full training set\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model, best_hp, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m                                 \u001b[38;5;66;03m# Externally save the best model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/model_GRU.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/grid_search.py:152\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(hp, hp_space, train_dataset, verbose, hier)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    149\u001b[0m     print_current_config(config_dict)\n\u001b[1;32m    151\u001b[0m current_config_loss, _best_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 152\u001b[0m     \u001b[43mk_fold_cross_validation_expanding_hierarchical\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhier\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_config_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n\u001b[1;32m    157\u001b[0m     best_hp \u001b[38;5;241m=\u001b[39m config_dict\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/cross_validation.py:139\u001b[0m, in \u001b[0;36mk_fold_cross_validation_expanding_hierarchical\u001b[0;34m(hp, train_dataset, verbose, hier)\u001b[0m\n\u001b[1;32m    135\u001b[0m     _, _, val_losses, _, _ \u001b[38;5;241m=\u001b[39m train_hierarchical(\n\u001b[1;32m    136\u001b[0m         hp, train_loader, val_loader, verbose\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     _best_model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m val_losses_kfold\u001b[38;5;241m.\u001b[39mappend(val_losses)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# return average of final validation losses\u001b[39;00m\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/train.py:377\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(hp, train_loader, val_loader, verbose, device)\u001b[0m\n\u001b[1;32m    374\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss_fn(model(batch_train_u), batch_train_y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    375\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 377\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# backward pass\u001b[39;00m\n\u001b[1;32m    378\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    379\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# update the weights\u001b[39;00m\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[1;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/autograd/profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(\"starting training...\")\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "stdout_location = f'results/grid_search_exe_s/exe_of_GRU_at_{current_time}.txt'\n",
    "train_dataset_full = ConcatDataset([train_dataset, val_dataset])\n",
    "#                                     # If HABROK, print to external file, else print to stdout\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(f\"Grid search execution of GRU at {current_time}\\n\")\n",
    "                                    # Train on the full training set\n",
    "    model, best_hp, val_loss = grid_search(hp, hp_space, train_dataset_full, hier=False, verbose=True)\n",
    "                                    # Externally save the best model\n",
    "    torch.save(model.state_dict(), f\"{MODEL_PATH}/model_GRU.pth\")\n",
    "\n",
    "    hp = update_dict(hp, best_hp)   # Update the hp dictionary with the best hyperparameters\n",
    "    print_dict_vertically(best_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay out model architecture with optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_gru = {\n",
    "    'n_hours_u' : N_HOURS_U,\n",
    "    'n_hours_y' : N_HOURS_Y,\n",
    "\n",
    "    'model_class' : GRU, # changed to GRU\n",
    "    'input_units' : 8, #train_dataset.__n_features_in__(),\n",
    "    'hidden_layers' : 4,\n",
    "    'hidden_units' : 128,\n",
    "    # 'branches' : 2,  # predicting only no2 and o3\n",
    "    'output_units' : 2, #train_dataset.__n_features_out__(),\n",
    "\n",
    "    'Optimizer' : torch.optim.Adam,\n",
    "    'lr_shared' : 1e-3,\n",
    "    'scheduler' : torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'scheduler_kwargs' : {'mode' : 'min',\n",
    "                          'factor' : 0.1,\n",
    "                          'patience' : 3,\n",
    "                          'cooldown' : 8,\n",
    "                          'verbose' : True},\n",
    "    'w_decay' : 1e-5,\n",
    "    'loss_fn' : torch.nn.MSELoss(),\n",
    "\n",
    "    'epochs' : 5000,\n",
    "    'early_stopper' : EarlyStopper,\n",
    "    'patience' : 15,\n",
    "    'batch_sz' : 16,\n",
    "    'k_folds' : 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Printing model:\n",
      "GRU(\n",
      "  (gru): GRU(8, 128, num_layers=4, batch_first=True)\n",
      "  (dense): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nPrinting model:\")\n",
    "    model_final = GRU(hp['n_hours_u'],\n",
    "                 hp_gru['n_hours_y'],\n",
    "                 hp_gru['input_units'],\n",
    "                 hp_gru['hidden_layers'],\n",
    "                 hp_gru['hidden_units'], \n",
    "                #  hp['branches'],\n",
    "                 hp_gru['output_units'])\n",
    "    print(model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on complete training dataset (= train + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on full training set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLtrain: 0.018103 \tLval: 0.010405\n",
      "Epoch: 5 \tLtrain: 0.006252 \tLval: 0.005842\n",
      "Epoch: 10 \tLtrain: 0.005726 \tLval: 0.005462\n",
      "Epoch: 15 \tLtrain: 0.005309 \tLval: 0.004166\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 20 \tLtrain: 0.005038 \tLval: 0.004245\n",
      "Epoch: 25 \tLtrain: 0.005032 \tLval: 0.003877\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 30 \tLtrain: 0.005005 \tLval: 0.004012\n",
      "Epoch: 35 \tLtrain: 0.004999 \tLval: 0.004051\n",
      "Epoch: 40 \tLtrain: 0.005005 \tLval: 0.004037\n",
      "EarlyStopper: stopping at epoch 39 with best_val_loss = 0.003877\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = hp_gru['batch_sz'], shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = hp_gru['batch_sz'], shuffle = False) \n",
    "                                            \n",
    "#                                         # Train the final model on the full training set,\n",
    "#                                         # save the final model, and save the losses for plotting\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print(\"\\nTraining on full training set...\")\n",
    "    model_final, train_losses, val_losses = \\\n",
    "        train(hp_gru, train_loader, val_loader, True)\n",
    "    torch.save(model_final.state_dict(), f'{MODEL_PATH}/model_GRU.pth')\n",
    "\n",
    "df_losses = pd.DataFrame({'L_train': train_losses, 'L_val': val_losses})\n",
    "df_losses.to_csv(f'{os.path.join(os.getcwd(), \"results/final_losses\")}/losses_HGRU_at_{current_time}.csv', \n",
    "                 sep = ';', decimal = '.', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (gru): GRU(8, 128, num_layers=4, batch_first=True)\n",
      "  (dense): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_final.load_state_dict(torch.load(f\"{MODEL_PATH}/model_GRU.pth\"))\n",
    "print(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader, denorm=False, path=None) -> float:\n",
    "    model.eval()\n",
    "    test_loss = np.float64(0)\n",
    "    \n",
    "    # Ensure the model is on the correct device\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            batch_test_u = batch_test_u.to(device)\n",
    "            batch_test_y = batch_test_y.to(device)\n",
    "            \n",
    "            pred = model(batch_test_u)\n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y, path)\n",
    "            \n",
    "            test_loss += loss_fn(pred, batch_test_y).item()\n",
    "\n",
    "    return test_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing MSE: 0.003313543042168021\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = hp['batch_sz'], shuffle = False) \n",
    "loss_fn = nn.MSELoss()  # Instantiate the loss function\n",
    "test_error = test(model_final, loss_fn, test_loader)\n",
    "\n",
    "with PrintManager(stdout_location, 'a', HABROK):\n",
    "    print()\n",
    "    print(\"Testing MSE:\", test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_separately(\n",
    "    model,\n",
    "    loss_fn,\n",
    "    test_loader,\n",
    "    denorm: bool = False,\n",
    "    path: str = None,\n",
    "    components=[\"NO2\", \"O3\", \"PM10\", \"PM25\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates on test set and returns test loss\n",
    "\n",
    "    :param model: model to evaluate, must be some PyTorch type model\n",
    "    :param loss_fn: loss function to use, PyTorch defined, or PyTorch inherited\n",
    "    :param test_loader: DataLoader to get batches from\n",
    "    :param denorm: whether to denormalise the data before calculating loss\n",
    "    :param path: path to the file containing the minmax values for the data\n",
    "    :return: dictionary with contaminant names as keys and losses as values\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    test_losses = [np.float64(0) for _ in components]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_test_u, batch_test_y in test_loader:\n",
    "            pred = model(batch_test_u.to(device))\n",
    "            if denorm:\n",
    "                pred = denormalise(pred, path)\n",
    "                batch_test_y = denormalise(batch_test_y.to(device), path)\n",
    "\n",
    "            for comp in range(len(components)):\n",
    "                test_losses[comp] += loss_fn(\n",
    "                    pred[:, :, comp], batch_test_y[:, :, comp]\n",
    "                ).item()\n",
    "\n",
    "    for comp in range(len(components)):\n",
    "        test_losses[comp] /= len(test_loader)\n",
    "    return {comp: loss for comp, loss in zip(components, test_losses)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005013745170222913\n",
      "0.004090381398176153\n",
      "0.003313543042168021\n",
      "\n",
      "MSE Training set:\n",
      "NO2: 86.59752915545208\n",
      "O3 : 85.67562345179115\n",
      "\n",
      "MSE Validation set:\n",
      "NO2: 63.03963724772135\n",
      "O3 : 91.78997039794922\n",
      "\n",
      "MSE Test set:\n",
      "NO2: 52.24487431844076\n",
      "O3 : 70.96932665506999\n"
     ]
    }
   ],
   "source": [
    "print(test(model_final, nn.MSELoss(), train_loader))\n",
    "print(test(model_final, nn.MSELoss(), val_loader))\n",
    "print(test(model_final, nn.MSELoss(), test_loader))\n",
    "\n",
    "print(\"\\nMSE Training set:\")\n",
    "print_dict_vertically(\n",
    "    test_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nMSE Validation set:\")\n",
    "print_dict_vertically(\n",
    "    test_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nMSE Test set:\")\n",
    "print_dict_vertically(\n",
    "    test_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE Training set:\n",
      "NO2: 9.305779356645862\n",
      "O3 : 9.256112731246459\n",
      "\n",
      "RMSE Validation set:\n",
      "NO2: 7.939750452484093\n",
      "O3 : 9.580708240936534\n",
      "\n",
      "RMSE Test set:\n",
      "NO2: 7.2280615878975985\n",
      "O3 : 8.424329448393504\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRMSE Training set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_separately(model_final, nn.MSELoss(), train_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nRMSE Validation set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_separately(model_final, nn.MSELoss(), val_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")\n",
    "print(\"\\nRMSE Test set:\")\n",
    "print_dict_vertically_root(\n",
    "    test_separately(model_final, nn.MSELoss(), test_loader, True, MINMAX_PATH, components=[\"NO2\", \"O3\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m set_minmax_path(MINMAX_PATH)\n\u001b[1;32m      4\u001b[0m set_contaminants(CONTAMINANTS)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mplot_pred_vs_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNO2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HOURS_Y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m plot_pred_vs_gt(model_final, test_dataset, pair, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO3\u001b[39m\u001b[38;5;124m'\u001b[39m, N_HOURS_Y)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:431\u001b[0m, in \u001b[0;36mplot_pred_vs_gt\u001b[0;34m(model, dataset, row, comp, n_hours_y)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_pred_vs_gt\u001b[39m(\n\u001b[1;32m    420\u001b[0m     model: Any, dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset, row: \u001b[38;5;28mint\u001b[39m, comp: \u001b[38;5;28mstr\u001b[39m, n_hours_y: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m    421\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    Plots predictions (dotted) vs ground truth (solid) with basic lay-out\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    :param n_hours_y: number of hours in the output sequence (N_HOURS_Y)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_plot_component_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     set_style()\n\u001b[1;32m    435\u001b[0m     sns\u001b[38;5;241m.\u001b[39mlineplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(n_hours_y), y\u001b[38;5;241m=\u001b[39mpred, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:200\u001b[0m, in \u001b[0;36mchoose_plot_component_values\u001b[0;34m(model, dataset, idx, comp, denorm, hier)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_plot_component_values\u001b[39m(\n\u001b[1;32m    178\u001b[0m     model: Any,\n\u001b[1;32m    179\u001b[0m     dataset: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m     hier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Chooses the correct component values for plotting by:\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    - getting the predictions and ground truth for the given index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    :return: tuple of predictions and ground truth for the component\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mget_pred_and_gt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     comp_idx \u001b[38;5;241m=\u001b[39m get_index(CONTAMINANTS, comp)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(pred[:, :, comp_idx]), np\u001b[38;5;241m.\u001b[39msqueeze(gt[:, :, comp_idx])\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/plots.py:108\u001b[0m, in \u001b[0;36mget_pred_and_gt\u001b[0;34m(model, dataset, idx, denorm, hier)\u001b[0m\n\u001b[1;32m     96\u001b[0m         pred \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     97\u001b[0m             denormalise(\n\u001b[1;32m     98\u001b[0m                 torch\u001b[38;5;241m.\u001b[39mcat(model(input_data), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m         pred \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    107\u001b[0m             denormalise(\n\u001b[0;32m--> 108\u001b[0m                 \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    109\u001b[0m                 MINMAX_PATH,\n\u001b[1;32m    110\u001b[0m                 contaminants\u001b[38;5;241m=\u001b[39mCONTAMINANTS,\n\u001b[1;32m    111\u001b[0m             )\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m     gt \u001b[38;5;241m=\u001b[39m denormalise(ground_truth, MINMAX_PATH)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/src/modelling/GRU.py:64\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mForward pass of the GRU:\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m- pass input tensor through big pile of GRU neurons, and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m  the results, and transpose the stack to the desired shape\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru\u001b[38;5;241m.\u001b[39mflatten_parameters()\n\u001b[0;32m---> 64\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# take last time step(s) of each sequence in GRU output\u001b[39;00m\n\u001b[1;32m     65\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_n_hours_y :, :]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# apply dense layer to each time step, then stack,\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# which yields (N_HOURS_Y, N_BATCHES, N_OUTPUT_UNITS),\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# then transpose to (N_BATCHES, N_HOURS_Y, N_OUTPUT_UNITS)\u001b[39;00m\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/bachelor-project/forecasting_smog_DL_GNN/.venv/lib/python3.10/site-packages/torch/nn/modules/rnn.py:998\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1001\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1002\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "pair = 2\n",
    "\n",
    "set_minmax_path(MINMAX_PATH)\n",
    "set_contaminants(CONTAMINANTS)\n",
    "\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'NO2', N_HOURS_Y)\n",
    "plot_pred_vs_gt(model_final, test_dataset, pair, 'O3', N_HOURS_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
